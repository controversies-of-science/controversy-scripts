[{"name":"The Comet of Justinian","summary":"Around the Same Time as the Devastating Outbreak of Plague in the 6th Century AD, a Comet Appeared / This Comet is Recorded to have Induced \"a Remarkable Paleness of the Sun\" for 3 Full Years / The Mosaics of Ravenna from this Time Period Appear to Record the Conditions which Contributed to Europe Losing 40% of its Population","image":"https://lh3.googleusercontent.com/-O_jrQ5TAdlc/WGaOP7f8wSI/AAAAAAAAKBs/b5ICJiqI1E4Yarqk_GP6ycuAVDrc4o1lgCJoC/w1800-h2520/the-comet-of-justinian-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/CRr5PNTpahz","publishDate":"2016-12-30T16:42:12.116Z","updateDate":"2017-01-03T01:27:13.939Z","text":"<b>The Comet of Justinian: Around the Same Time as the Devastating Outbreak of Plague in the 6th Century AD, a Comet Appeared / This Comet is Recorded to have Induced &quot;a Remarkable Paleness of the Sun&quot; for 3 Full Years / The Mosaics of Ravenna from this Time Period Appear to Record the Conditions which Contributed to Europe Losing 40% of its Population: Not Just Plague, but also Terrible Explosions, Earthquakes, a Dry Fog in the Air, and Fire and Stones Raining from the Sky</b><br /><br /><i>The History of the Decline and Fall of the Roman Empire</i> (Vol 1)<br />by Edward Gibbon<br /><br />(The first volume of this classic was published in 1776.  It is one of the most read historical works of modern times.)<br /><br /><a href=\"https://books.google.com/books?id=PnZEAQAAMAAJ&amp;pg=PA749&amp;lpg=PA749&amp;dq=comet+of+justinian&amp;source=bl&amp;ots=DPB27ZETvw&amp;sig=YnQMKlaKyJMXSDQbfxB7Q8gkVq4&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiin9j05JrRAhUUVWMKHXgBC_QQ6AEIOzAE#v=onepage&amp;q=comet%20of%20justinian&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=PnZEAQAAMAAJ&amp;pg=PA749&amp;lpg=PA749&amp;dq=comet+of+justinian&amp;source=bl&amp;ots=DPB27ZETvw&amp;sig=YnQMKlaKyJMXSDQbfxB7Q8gkVq4&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiin9j05JrRAhUUVWMKHXgBC_QQ6AEIOzAE#v=onepage&amp;q=comet%20of%20justinian&amp;f=false</a><br /><br /><i>&quot;I shall conclude this chapter with the comets, the earthquakes, and the plague, which astonished or afflicted the age of Justinian.</i><br /><br /><i>Comets A.D. 531-539</i><br /><br /><i>I. </i><b><i>In the fifth year of his reign, and in the month of September, a comet was seen during twenty days in the western quarter of the heavens, and which shot its rays into the north.  Eight years afterwards, while the sun was in Capricorn, another comet appeared to follow in the Sagittary: the size was gradually increasing: the head was in the east, the tail in the west, and it remained visible about forty days.  The nations, who gazed with astonishment, expected wars and calamities from their baleful influence; and these expectations were abundantly fulfilled.</i></b><i>  The astronomers dissembled their ignorance of the nature of these blazing stars, which they affected to represent as the floating meteors of the air; and few among them embraced the simple notion of Seneca and the Chaldeans, that they are only planets of a longer period, and more eccentric motion.  Time and science have justified the conjectures and predictions of the Roman sage: the telescope has opened new worlds to the eyes of astronomers; and, in the narrow space of history and fable, one and the same comet is already found to have revisited the earth in seven equal revolutions of five hundred and seventy-five years.  The first, which ascends beyond the christian era one thousand seven hundred and sixty-seven years, is coeval with Ogyges the father of Grecian antiquity.  And this appearance explains the tradition which Varro has preserved, that under his reign the planet Venus changed her colour, size, figure, and course: a prodigy without example either in past or succeeding ages.  The second visit, in the year eleven hundred and ninety-three, is darkly implied in the fable of Electra the seventh of the Pleiads, who have been reduced to six since the time of the Trojan war.  That nymph, the wife of Dardanus, was unable to support the ruin of her country: she abandoned the dances of her sister orbs, fled from the zodiac to the north pole, and obtained, from her dishevelled locks, the name of the comet.  The third period expires in the year six hundred and eighteen, a date that exactly agrees with the tremendous comet of the Sibyl, and perhaps of Pliny, which arose in the west two generations before the reign of Cyrus.  The fourth apparaition, forty-four years before the birth of Christ, is of all others the most splendid and important.  After the death of Caesar, a long-haired star was conspicuous to Rome and to the nations, during the games which were exhibited by young Octavian, in honour of Venus and his uncle.  The vulgar opinion, that it conveyed to heaven the divine soul of the dictator, was cherished and consecrated by the piety of a statesman: while his secret superstition referred the comet to the glory of his own times.  </i><b><i>The fifth visit has been already ascribed to the fifth year of Justinian, which coincides with the five hundred and thirty-first of the christian era.  And it may deserve notice, that in this, as in the preceding instance, the comet was followed, though at a longer interval, by a remarkable paleness of the sun.</i></b><i>  The sixth return, in the year eleven hundred and six, is recorded by the chronicles of Europe and China; and in the first fervour of the crusades, the christians and the Mahometans might surmise, with equal reason, that it portended the destruction of the infidels.  The seventh phenomenon, of one thousand six hundred and eighty, was presented to the eyes of an enlightened age.  The philosophy of Bayle dispelled a prejudice which Milton&#39;s muse had so recently adorned, that the comet from its horrid air shakes pestilence and war.  Its road in the heavens was observed with exquisite skill by Flamstead and Cassini; and the mathematical science of Bernoulli, Newton and Halley, investigated the laws of its revolution.  At the eighth period, in the year two thousand two hundred and fifty-five, their calculations may perhaps be verified by the astronomers of some future capital in the Siberian or American wilderness.</i><br /><br /><b>The Ravenna Mosaics</b><br /><br /><a href=\"https://utoronto.academia.edu/RuthDwyer\" class=\"ot-anchor\">https://utoronto.academia.edu/RuthDwyer</a><br /><br />Eclipses, Comets and Craters, Raining Fire, Justinian’s Plague, the Hagia Sophia and the Ravenna Mosaics. The Comet of 536<br /><br /><i>&quot;As the Emperor Justinian was building the Hagia Sophia in Constantinople (532-537), a series of spectacularly dramatic climate and celestial events occurred. </i><b><i>These were not just ordinary pestilence and earthquakes, these were biblical in their enormity. In the midst of construction, in the year 536, the earth was rocked with terrible &#39;explosions and earthquakes.&#39; Fire and stones &#39;rained&#39; from heaven, and a dry fog, so dense and so thick that a &#39;weak sun&#39; was visible for only 2 hours per day, hung in the air for more than three years. The air temperature dropped precipitously. Snow was reported in August. Eclipses occurred, which had been predicted. Famines and plague took the lives of 40% of Europe’s population. Scholars have been debating the source for the earthquakes and the thick dry fog. The mosaics of Ravenna tell us the truth: the earth had a very close encounter with a comet.</i></b><i> And once we know how to read the marble decorations of the Hagia Sophia we will be astonished to learn that the eclipses which were predicted are pictured on the walls.&quot;</i><br /><br />These detailed mosaics have been interpreted by Dr. Ruth Dwyer in a series of online videos.  The second one of the series is the most important for our purposes here ...<br /><br /><a href=\"http://www.ruthdwyer.com/academia.htm\" class=\"ot-anchor\">http://www.ruthdwyer.com/academia.htm</a><br /><br />The imagery of the mosaics are visual indications of the celestial catastrophe and subsequent plague that struck Constantinople and the east.  Regardless of the recent debates over the reconstruction of this time period&#39;s chronology (like by Heinsohn), the fall of Rome plainly created a power vacuum during this era.  In that vacuum, Christianity and Islam rose to power simultaneously.  Clark Whelton remarks that, <i>&quot;both religions offer competing ritual formulae for pacifying the heavens and avoiding future celestial disasters.&quot;</i><br /><br /><b>A Firsthand Account</b><br /><br /><a href=\"https://archive.org/details/letterscassiodo00hodggoog\" class=\"ot-anchor\">https://archive.org/details/letterscassiodo00hodggoog</a><br /><br /><i>The letters of Cassiodorus</i><br />Translated by Thomas Hodgkin – 1886<br />Page 518<br /><br />25. Senator, Praetorian Praefect, to his Deputy Ambrosius, an Illustris.<br /><br />[This letter appears to have been written in the early autumn of 538, about a year after the three last letters, and also after Letters 27 and 28, which precede it in order of date, though they follow it in this collection. For an account of the terrible famine in Italy, the beginning of which is here described, see Procopius, De Bello Gotthico ii. 20.]<br /><br />Famine in Italy<br /><br /><i>&quot;Since the world is not governed by chance, but by a Divine Ruler who does not change His purposes at random, men are alarmed, and naturally alarmed, at the extraordinary signs in the heavens, and ask with anxious hearts what events these may portend.</i><br /><br /><b><i>The Sun, first of stars, seems to have lost his wonted light, and appears of a bluish colour.</i></b><br /><br /><b><i>We marvel to see no shadows of our bodies at noon</i></b><i>, to feel the mighty vigour of his heat wasted into feebleness, and the phenomena which accompany a transitory eclipse prolonged through a whole year.</i><br /><br /><i>The Moon too, even when her orb is full, is empty of her natural splendour.</i><br /><br /><i>Strange has been the course of the year thus far.</i><br /><br /><b><i>We have had a winter without storms, a spring without mildness, and a summer without heat.</i></b><br /><br /><b><i>Whence can we look for harvest</i></b><i>, since the months which should have been maturing the corn have been chilled by Boreas?</i><br /><br /><b><i>How can the blade open if rain, the mother of all fertility, is denied to it?</i></b><br /><br /><i>These two influences, </i><b><i>prolonged frost and unseasonable drought</i></b><i>, must be adverse to all things that grow.</i><br /><br /><b><i>The seasons seem to be all jumbled up together</i></b><i>, and the fruits, which were wont to be formed by gentle showers, cannot be looked for from the parched earth.</i><br /><br /><i>But as last year was one that boasted of an exceptionally abundant harvest, you are to collect all of its fruits that you can, and store them up for the coming months of scarcity, for which it is well able to provide.</i><br /><br /><i>And that you may not be too much distressed by the signs in the heavens of which I have spoken, return to the consideration of Nature, and apprehend the reason of that which makes the vulgar gape with wonder.</i><br /><br /><i>The middle air is thickened by the rigour of snow and rarefied by the beams of the Sun.</i><br /><br /><b><i>This is the great Inane, roaming between the heavens and the earth.</i></b><br /><br /><b><i>When it happens to be pure and lighted up by the rays of the sun it opens out its true aspect; but when alien elements are blended with it, it is stretched like a hide across the sky, and suffers neither the true colours of the heavenly bodies to appear nor their proper warmth to penetrate.</i></b><br /><br /><i>This often happens in cloudy weather for a time; it is only its extraordinary prolongation which has produced these disastrous effects, causing the reaper to fear a new frost in harvest, making the apples to harden when they should grow ripe, souring the old age of the grape-cluster.</i><br /><br /><i>All this, however, though it would be wrong to construe it as an omen of Divine wrath, cannot but have an injurious effect on the fruits of the earth.</i><br /><br /><i>Let it be your care to see that the scarcity of this one year does not bring ruin on us all.</i><br /><br /><i>Even thus was it ordained by the first occupant of our present dignity, that the preceding plenty should avail to mitigate the present penury.&quot;</i><br /><br /><b>Mainstream Suspicions</b><br /><br /><a href=\"http://www.livescience.com/42048-halleys-comet-linked-to-ancient-famine.html\" class=\"ot-anchor\">http://www.livescience.com/42048-halleys-comet-linked-to-ancient-famine.html</a><br /><br />Why Halley&#39;s Comet May Be Linked to Famine 1,500 Years Ago<br />By Mike Wall, Senior Writer | December 18, 2013 07:53am ET<br /><br /><i>SAN FRANCISCO - The ancients had ample reason to view comets as harbingers of doom, it would appear.</i><br /><br /><b><i>A piece of the famous Halley&#39;s comet likely slammed into Earth in A.D. 536, blasting so much dust into the atmosphere that the planet cooled considerably, a new study suggests. This dramatic climate shift is linked to drought and famine around the world, which may have made humanity more susceptible to &#39;Justinian&#39;s plague&#39; in A.D. 541-542 -- the first recorded emergence of the Black Death in Europe.</i></b><br /><br /><b><i>The new results come from an analysis of Greenland ice that was laid down between A.D. 533 and 540. The ice cores record large amounts of atmospheric dust during this seven-year period, not all of it originating on Earth.</i></b><br /><br /><b><i>&#39;I have all this extraterrestrial stuff in my ice core,&#39;</i></b><i> study leader Dallas Abbott, of Columbia University&#39;s Lamont-Doherty Earth Observatory, told LiveScience here last week at the annual meeting of the American Geophysical Union.</i><br /><br /><b><i>Certain characteristics, such as high levels of tin, identify a comet as the origin of the alien dust, Abbott said. And the stuff was deposited during the Northern Hemisphere spring, suggesting that it came from the Eta Aquarid meteor shower -- material shed by Halley&#39;s comet that Earth plows through every April-May.</i></b><br /><br /><i>The Eta Aquarid dust may be responsible for a period of mild cooling in 533, Abbott said, but it alone cannot explain the global dimming event of 536-537, during which the planet may have cooled by as much as 5.4 degrees Fahrenheit (3 degrees Celsius). For that, something more dramatic is required.</i><br /><br /><i>Ice core data record evidence of a volcanic eruption in 536, but it almost certainly wasn&#39;t big enough to change the climate so dramatically, Abbott said.</i><br /><br /><b><i>&#39;There was, I think, a small volcanic effect,&#39; she said. &#39;But I think the major thing is that something hit the ocean.&#39;</i></b><br /><br /><i>She and her colleagues have found circumstantial evidence of such an impact. </i><b><i>The Greenland ice cores contain fossils of tiny tropical marine organisms -- specifically, certain species of diatoms and silicoflagellates.</i></b><br /><br /><b><i>An extraterrestrial impact in the tropical ocean likely blasted these little low-latitude organisms all the way to chilly Greenland</i></b><i>, researchers said. And Abbott believes the object responsible was once a piece of Halley&#39;s comet.</i><br /><br /><i>Halley zooms by Earth once every 76 years or so. It appeared in Earth&#39;s skies in A.D. 530 and was astonishingly bright at the time, Abbott said. (In fact, observations of Halley&#39;s comet go way back, with research suggesting the ancient Greeks saw the comet streaking across their skies in 466 B.C.)</i><br /><br /><i>&#39;Of the two brightest apparitions of Comet Halley, one of them is in 530,&#39; Abbott said. &#39;Comets are normally these dirty snowballs, but when they&#39;re breaking up or they&#39;re shedding lots of debris, then that outer layer of dark stuff goes away, and so the comet looks brighter.&#39;</i><br /><br /><i>It&#39;s unclear where exactly the putative comet chunk hit Earth or how big it was, she added. However, a 2004 study estimated that a comet fragment just 2,000 feet (600 meters) wide could have caused the 536-537 cooling event if it exploded in the atmosphere and its constituent dust were spread evenly around the globe.&quot;</i><br /><br /><b>A Hypothesis on the Subsequent Rise of Islam and Christianity</b><br /><br />Louis Hissink links the emergence of the world&#39;s religions to this same event ... (again, I&#39;m intentionally leaving the debate over chronology out of this particular discussion, so that we may discuss these two issues separately) ...<br /><br /><a href=\"https://lhcrazyworld.wordpress.com/2016/07/12/islam-and-the-comet-of-justinian/\" class=\"ot-anchor\">https://lhcrazyworld.wordpress.com/2016/07/12/islam-and-the-comet-of-justinian/</a><br /><br /><i>&quot;... </i><b><i>If the Comet of Justinian catastrophe was global and resulted in the death and disruption of human civilisations, then the actions of the survivors would be quite specific and aimed at one goal -- survival</i></b><i> ...</i><br /><br /><i>One of the core beliefs Mohammed had was that wealth should always be devoted to the common good and this, more or less, makes Islam another variant of socialism and wealth redistribution. But was Islam an ideological imposition on the nomadic Arabs by Mohammed </i><b><i>or was it what they needed to do in order to survive the Justinian Catastrophe? The point is that Islam appeared out of the middle of the 1st millennium CE at about the same time as the appearance of the Comet of Justinian.</i></b><br /><br /><b><i>Imagine civilisations operating on the division of labour and elsewhere nomadic peoples more or less existing at the subsistence level. Add a climate catastrophe and the various metropolitan cities and towns would simply collapse, mainly from mass deaths and the collapse of trade and markets that such civilisations depend on when organised on the division of labour. Such a catastrophe would have less effect on nomadic peoples who would be more or less self-sufficient and would constitute the bulk of the survivors.  This is not to say that nomadic peoples would be unaffected by a climate catastrophe, but that being self-sufficient would ensure that these peoples would have a better chance of surviving than town-based peoples utterly reliant on the daily exchange of goods and services that the division of labour needs; town folk simply are not self-sufficient as a result, and are the first to suffer from a natural catastrophe.</i></b><br /><br /><b><i>This leads to the hypothesis that the ascendancy of Islam during this period was the result of the nomadic Arabs, being more or less self-sufficient and thus the majority survivors of the Justinian Catastrophe, and because their trading systems would also have been destroyed, becoming reliant on the plundering of the surviving remnants of the Levant and Byzantine civilisations, even to the extent of taking over those ruined cities and towns, and then forcibly sharing their plunder with the rest of the survivors in their region as described, more or less, as the policies of Mohammed at the time.</i></b><br /><br /><i>It seems plausible to think Mohammed might have been the right man at the right time to organise the survival of his tribe by implementing the necessary, and compulsory, sharing of all the wealth among the survivors of the Roman and Arabic worlds in his region.</i><br /><br /><i>Communism or socialism is an appropriate economic system for a previously civilised people, mostly destroyed by a massive global catastrophe, to survive. Freedom becomes irrelevant when survival is paramount and all trading systems are destroyed. In this sense Islam is most certainly a socialist or communist system of social organisation, with Allah representing the cometary apparition in the heavens sent to punish humankind. It is of course to be expected that a traumatised illiterate and nomadic peoples would fabricate fantastical explanations to explain their immediate past, and subsequently codified in their oral and later, literary traditions. Consequently, </i><b><i>once the cometary or heavenly apparition disappeared from the heavens, surviving peoples would then tend to memorialise it as an abstract idea or ‘authority’ that would return if the people started to sin again, the belief being that the just passed catastrophe caused by an heavenly apparition was to punish people in the first place.</i></b><i> (That it was more likely an accident or unpredictable natural phenomenon would not have entered their analysis, especially in the case of illiterate nomads that most of the Arabs were probably at the time).</i><br /><br /><i>And having set up, by absolute necessity, a socialist or communist system in order to survive, the followers of Mohammed would then have, over time, become habituated with this state of affairs and established an institutionalised governing system that simply gelled into existence from habit and developed into the theocratic regimes that exist now.</i><br /><br /><b><i>But let us not forget the survivors of the recently destroyed Roman Civilisation, for they too would have slowly started rebuilding their world, but finding their ancient homes now invaded by the nomadic Arabs under the ideology/theology of Islam, apart from the invasion of other people elsewhere, would strive to regain their previous living spaces and cities, hence the Crusades and the continuing conflict between the Christian and Islamic worlds.</i></b><br /><br /><i>It thus seems plausible that the rump of the Roman and Byzantine worlds then adopted socialist or communist policies in order to survive, and instituted Christianity as an explanation for that change. Which version of Christianity is entirely another matter and definitely not gone into here. As the climate crisis dissipated and peoples returned to the humdrum of daily existence, the need for autocratic control in order to simply survive became unnecessary and people started to demand more freedoms.&quot;</i><br /><br />Ruth Dwyer&#39;s video on the comet is a worthwhile watch ...<br /><br /><b>The Comet of 536 and the Ravenna Mosaics</b><br /><a href=\"https://www.youtube.com/watch?v=t4WBALFPmw4\" class=\"ot-anchor\">https://www.youtube.com/watch?v=t4WBALFPmw4</a><br /><br />There is some additional analysis and conjecture on the origin of the debris here:<br /><br /><a href=\"https://malagabay.wordpress.com/2016/05/01/justinians-raging-bulls/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2016/05/01/justinians-raging-bulls/</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Vitrified Stonework","summary":"Ancient Stone Structures Across the World Exhibit in Places a Thin Glassy Melted Layer Recognizable by the Way it Reflects, Refracts and Diffracts Light / Attempts to Replicate the Effect by Burning Wood Next to the Rock have Failed / The Required Temperature Appears to be Around 1,100°C / Some Authors would have Us Believe that this is Man-Made and Evidence for an Ancient Technology / It's a Mistake to Consider these Observations in Isolation from Other Types of Evidence for a Global Catastrophe","image":"https://lh3.googleusercontent.com/-LJySLjZzdfI/WGViILAyVLI/AAAAAAAAKAY/NNgrAnA6RXEJAoq-57yzMYNpph5ebgCgACJoC/w1800-h2520/vitrified-stonework-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/5JbAfustZpE","publishDate":"2016-12-29T19:21:48.462Z","updateDate":"2017-01-03T01:28:52.135Z","text":"<b>Vitrified Stonework: Ancient Stone Structures Across the World Exhibit in Places a Thin Glassy Melted Layer Recognizable by the Way it Reflects, Refracts and Diffracts Light / Attempts to Replicate the Effect by Burning Wood Next to the Rock have Failed / The Required Temperature Appears to be Around 1,100°C / Some Authors would have Us Believe that this is Man-Made and Evidence for an Ancient Technology / It&#39;s a Mistake to Consider these Observations in Isolation from Other Types of Evidence for a Global Catastrophe</b><br /><br />See link below for many fascinating examples!<br /><br /><a href=\"https://grahamhancock.com/jongjp1/\" class=\"ot-anchor\">https://grahamhancock.com/jongjp1/</a><br /><br /><b><i>&quot;Vitrified stones are simply stones that have been melted to a point where they form a glass or glaze. There is much debate in archaeological circles over the ancient examples under study for two reasons. Firstly, few cases are known to have been tested and even if they have, there are many questions over how they were made.</i></b><br /><br /><b><i>Glassy rocks form naturally under conditions of high temperature and pressures found in and around volcanoes.</i></b><i> Glass or glazes are traditionally created using a furnace. Furnace or kiln examples are found on everyday objects such as glassware and ceramics. The ceramics glazes are created by pasting certain finely crushed stones, sometimes with tinctures, onto fired pots and plates. </i><b><i>The whole is then fired to temperatures usually in excess of 1000 degrees centigrade.</i></b><br /><br /><b><i>The difficulty with many of the curious ancient vitrified examples is that they are found on objects so large that they cannot be placed in a furnace.</i></b><i> The vitrification process itself is quite a mystery. A team of chemists on Arthur C. Clarke’s Mysterious World subjected rock samples from 11 forts to rigorous chemical analysis. </i><b><i>They concluded that the temperatures needed to produce the vitrification were up to 1,100°C. Simply burning the walls with wood interlaced with stone could not achieve such temperatures. Recent experiments along these lines have had virtually no success at all.</i></b><br /><br /><b><i>There are several confirmed cases of unusual vitrified remnants from across the globe.</i></b><i> In Europe, there are several forts and buildings with vitrified ramparts. The crude stone enclosure walls seem to have been subjected to the action of heat. No mortar has been found in any of these structures. Despite this, </i><b><i>the rocks seem to be fused together.</i></b><br /><br /><i>This fusion is uneven throughout the various forts and even in a single wall. Some stones are only partially melted and calcined. Whilst in others their adjoining edges are fused firmly together. </i><b><i>In many instances, pieces of rock are enveloped in a glassy enamel-like coating, which binds them into a whole. At times, the entire length of the wall presents one solid mass of vitreous substance.</i></b><br /><br /><i>It is not clear why or how the walls were vitrified. </i><b><i>Some have argued that it was done to strengthen the wall, but the heating weakens the structure. Battle damage, as some have proposed, is unlikely to be the cause. The walls would need carefully maintained fires to ensure vitrification.</i></b><br /><br /><b><i>There are about fifty examples that have been discovered in Scotland. It was thought that these forts were peculiar to Scotland. However, they are also found in County Londonderry and County Cavan, in Ireland. On mainland Europe, they have been identified in Upper Lusatia, Bohemia, Silesia, Saxony and Thuringia. A further example can be found in the Ucker Lake, in Brandenburg, where the walls are formed of burnt and smelted bricks. There are also displays in several places in France, such as Châteauvieux, Péran, La Courbe, Sainte Suzanne, Puy de Gaudy and Thauron.</i></b><br /><br /><i>There are some forts that have been placed on practically infusible rock. The quartz conglomerates of the Old Red Sandstone at Craig Phadraic and on the limestone of Dun Mac Uisneachain are good cases. Here pieces of fusible rocks were selected and carried to the top from a considerable distance. This demonstrates that the act of vitrification was deliberate.&quot;</i><br /><br />Does it really?<br /><br /><b><i>&quot;There are many more examples from Malta, Egypt, Iraq, Sudan, South East Asia and others that are speculated to fall into the grouping. However, these have not all been subjected to scientific testing like the European cases. They simply appear to be glazed finishes on equally large objects or on walls that are impossible to fire conventionally.</i></b><i> In many cases, it looks as if there has been the deliberate action of a great heat*</i><br /><br /><i>There has been much discussion about the Inca vestiges in the Peruvian Andes. It mostly revolves around whether the stones are vitrified or not. In these cases, vitrification appears to be present on different kind of stones, and seems to have been caused by deliberate action. This article will now concentrate on these Peruvian cases where there are indications of heat treatment.</i><br /><br /><i>THE PERUVIAN CASE STUDY</i><br /><br /><i>The vitrified examples under study for this paper come from famous Peruvian sites, considered to belong to the Incas, in South America. To the author’s knowledge, there have been no scientific tests made on these stones. This has left the debate open to claims of unusual polishing techniques, natural degradation, lava flows and many other odd explanations. The analysis below eliminates some of these ideas.</i><br /><br /><b><i>The vitrified stones of Peru were first brought to popular attention by Erich von Daniken in the 1970s. He saw the vitrification at Sacsayhuaman and noted it in his book Chariots of the Gods. Peruvian Alfredo Gamarra had identified this vitrification earlier. The identification and cataloging of these intriguing stones has been carried on by Alfredo’s son Jesus Gamara, and Jan Peter de Jong.</i></b><br /><br /><i>In Sacsayhuaman, there are many indications of the use of heat. Strange marks on the stones ... can be found; </i><b><i>shiny, completely smooth and with another color to the rest of the rock.</i></b><br /><br /><i>Vitrification appears on different kinds of stones and structures. It is found on the perfectly fitted walls with irregular blocks. It is also observed on walls made with regular oblong blocks. </i><b><i>It has been spotted on mountainsides, caves and rocks in situ</i></b><i>. The location arrangements vary as well. Some sites are surrounded or overbuilt by walls whilst others have single exposed isolated stones. There seems to have been some very adaptable ancient technology at work. </i><b><i>A list of vestiges where stonework seems to have been treated with this technology include; Inside the city of Cusco: the walls of Koricancha and Loreto Street, Sacsayhuaman, Kenko, Tetecaca, Templo de la Luna (or Amaru Machay), Zona X, Tambo Machay, Puca Pucara, Pisac, Ollantaytambo, Chinchero, Machu Picchu, Raqchi and in Bolivia in Tiahuanaco.</i></b><br /><br /><i>Archaeologists assume that the perfect fitting stones are the most developed style of the Incas. Regardless, there is no explanation of the shiny surfaces that can be observed. These often appear on the borders where the stones join perfectly. There has been nothing other than simple geological analysis of these stones to determine what the phenomenon is. No chemical analysis is known to have been executed. </i><b><i>It is normally assumed that these parts were simply polished by the Incas.</i></b><br /><br /><i>During many visits to the vestiges mentioned, Jesus Gamarra and Jan Peter de Jong have examined these stones with highly reflective surfaces. They have captured many of them on video. </i><b><i>Through personal observations and analysis of the video material, they have concluded that something other than polishing must have occurred.</i></b><br /><br /><i>The material convinces in several ways. Many cases display some or all of the following qualities mentioned below. The vitrified spots show discoloration and smoothness around the particular areas. They clearly look like the stone has been melted just in those spots.</i><br /><br /><i>A simple flashlight test was developed, which helps to identify the layers of glaze or glass. Filming was carried out at night with a flashlight beam passing through the glaze. This shows the reflection and diffraction of the light as it passes through the surface. Sacsayhuaman, Kenko and Loreto Street were all filmed at night using a flashlight or the nocturnal illumination to capture the effect.</i><br /><br /><i>Identifying Vitrified Stones.</i><br /><br /><i>The following traits help to identify vitrified stones:</i><br /><br />- <i>The melted effect is obvious</i><br />- <b><i>Reflection is high</i></b><br />- <b><i>The layer refracts, diffracts and diffuses light</i></b><br />- <i>A separate vitrified layer is present on the surface</i><br />- <i>Damaged layers show a &#39;film&#39; on the stone</i><br />- <i>The glazed layer is independent of rock type</i><br />- <i>The surface is smooth to the touch even if the surface is irregular</i><br />- <b><i>There is often associated heat discoloration surrounding the glaze</i></b><br /><br /><i>The diffraction effect can be seen in the video of &#39;the Inca Throne&#39; at Sacsayhuaman. </i><b><i>The rainbow effect is clearly captured by the camera. This is directly linked to the light passing through the glass layer and splitting into its constituent parts.</i></b><i> After noticing this effect, it was also detected on videos of other vitrified stones. This can be viewed on this short video: </i><i><a href=\"http://www.youtube.com/watch?v=ae_8ri2fiwI\" class=\"ot-anchor\">http://www.youtube.com/watch?v=ae_8ri2fiwI</a></i><i>, and on the DVD that will be available shortly.</i><br /><br /><i>The DVD &#39;The Cosmogony of the 3 Worlds&#39; shows an overview of this phenomenon in the chapter on Vitrified Stones.</i><br /><br />_This is available on youtube: <a href=\"http://www.youtube.com/watch?v=x81-5SWVtUQ\" class=\"ot-anchor\">http://www.youtube.com/watch?v=x81-5SWVtUQ</a>&quot;_<br /><br />[...]<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Dark Earth","summary":"aka Black Earth / Across the World, and Especially at the Top of Former Roman Settlements, Archaeologists have Since the 80s Known of a Black-Colored Soil High in Organic Matter, Usually .6 - 2 Meters Deep / Its Dark Color Comes from Charcoal / It May also Contain Fragments of Pottery, Tile, Animal Bone and Other Artefacts / When it was Also Discovered in the Amazon, it was Noted that 'The Textbooks Say it Shouldn't be there' / Is it Not Possible that a Global Fire Carbonized the World's Biomass, and then Smashed it All to Bits in a Global Flood?","image":"https://lh3.googleusercontent.com/--G_-CYiXrO4/WGSmrHorbWI/AAAAAAAAJ_Q/37WT0TvU8pYFO7nyMq6xCxAx7T5W7tulQCJoC/w7142-h9999/dark-earth-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/8T94Nm9N3ZL","publishDate":"2016-12-29T06:01:45.209Z","updateDate":"2017-01-03T01:29:33.185Z","text":"<b>Dark Earth: aka Black Earth / Across the World, and Especially at the Top of Former Roman Settlements, Archaeologists have Since the 80s Known of a Black-Colored Soil High in Organic Matter, Usually .6 - 2 Meters Deep / Its Dark Color Comes from Charcoal / It May also Contain Fragments of Pottery, Tile, Animal Bone and Other Artefacts / When it was Also Discovered in the Amazon, it was Noted that &#39;The Textbooks Say it Shouldn&#39;t be there&#39; / Is it Not Possible that a Global Fire Carbonized the World&#39;s Biomass, and then Smashed it All to Bits in a Global Flood?</b><br /><br />GRAPHIC REFERENCES:<br /><br />[1] <a href=\"https://upload.wikimedia.org/wikipedia/commons/7/71/Giovanni_Paolo_Panini_-_View_of_the_Roman_Forum_-_Walters_372366.jpg\" class=\"ot-anchor\">https://upload.wikimedia.org/wikipedia/commons/7/71/Giovanni_Paolo_Panini_-_View_of_the_Roman_Forum_-_Walters_372366.jpg</a><br /><br />[2] <a href=\"http://en.academic.ru/pictures/enwiki/86/VasiLiberatrice.jpg\" class=\"ot-anchor\">http://en.academic.ru/pictures/enwiki/86/VasiLiberatrice.jpg</a><br /><br />[3] <a href=\"https://commons.wikimedia.org/wiki/File:Santa_Maria_Liberatrice_al_Foro_Romano.jpg\" class=\"ot-anchor\">https://commons.wikimedia.org/wiki/File:Santa_Maria_Liberatrice_al_Foro_Romano.jpg</a><br /><br />[4] <a href=\"https://commons.wikimedia.org/wiki/File:Santa_Maria_Liberatrice_al_Foro_Romano_distruzione.jpg\" class=\"ot-anchor\">https://commons.wikimedia.org/wiki/File:Santa_Maria_Liberatrice_al_Foro_Romano_distruzione.jpg</a><br /><br />[5] Clark Whelton, personal correspondence<br /><br />[6][7] <a href=\"https://malagabay.wordpress.com/2014/08/16/friends-romans-countrymen/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/08/16/friends-romans-countrymen/</a><br /><br />[8] - [10] <a href=\"https://malagabay.wordpress.com/2016/05/16/black-earth-dark-earth/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2016/05/16/black-earth-dark-earth/</a><br /><br />[11] See comments at <a href=\"https://malagabay.wordpress.com/2016/05/16/black-earth-dark-earth/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2016/05/16/black-earth-dark-earth/</a><br /><br />[12] - [14] <a href=\"https://en.wikipedia.org/wiki/Dark_earth\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Dark_earth</a><br /><br />[15] <a href=\"https://en.wikipedia.org/wiki/Vivianite\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Vivianite</a><br /><br />[16] <a href=\"https://en.wikipedia.org/wiki/Glauconite\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Glauconite</a><br /><br />[17] <a href=\"http://multimedia.inrap.fr/atlas/tours/sites/2446/Square-Prosper-Merimee#.WGR5aWQrK3A\" class=\"ot-anchor\">http://multimedia.inrap.fr/atlas/tours/sites/2446/Square-Prosper-Merimee#.WGR5aWQrK3A</a> (processed imagery)<br /><br />Scientists seem to make a lot of assumptions about what it is they are seeing, in many cases looking through the popular science lenses of sustainable agriculture and climate change.  They also openly discuss the enigmas created by these very assumptions ...<br /><br /><a href=\"http://terrapreta.bioenergylists.org/forestsorg\" class=\"ot-anchor\">http://terrapreta.bioenergylists.org/forestsorg</a><br /><br />Amazon&#39;s mysterious black earth earth: Soil found along region riverbanks; Rich in nutrients, stores more carbon<br /><br />Post date:<br />Saturday, January 20, 2007 - 8:48pm<br /><br />Tom Miles<br /><br />(Formerly published at: Amazon&#39;s mysterious black earth: Soil found along region riverbanks; Rich in nutrients, stores more carbon, Forests.org, February 25, 2006)<br /><br /><i>&quot;Above ground, rainforests like the Amazon basin flourish as biological hot spots with exuberant growth and a riot of plant and animal species.</i><br /><br /><b><i>But the red and yellow soils below are notoriously poor in nutrients and organic matter.</i></b><i> Once the lush vegetation is cleared, the heavy rains and tropical sun quickly decompose even that small reservoir.</i><br /><br /><b><i>Except not in thousands of patches dotted along the Amazon River and its tributaries, where dark, friable soil extends metres deep, fertile in nutrients and organic material. In total, an area the size of France may be covered by this Indian black earth, called terra preta do Indio in Portuguese.</i></b><br /><br /><b><i>&#39;The textbooks say it shouldn&#39;t be there.</i></b><i> That&#39;s justification enough for me to explore why it is there,&#39; says Johannes Lehmann, a Cornell University professor specializing in the chemistry and geology of soils.</i><br /><br /><i>Lehmann is one of a small band of researchers in the U.S., Europe and Brazil who are deciphering the mysteries of terra preta </i><b><i>after the phenomenon was &#39;discovered&#39; for the third time a decade ago.</i></b><i> At a major scientific meeting here last week, they explained that the black earth of the Amazon is exciting for widely disparate scientific reasons:</i><br /><br /><i>1) Archaeology and anthropology. </i><b><i>Terra preta typically is stuffed full of shards of sophisticated pottery discarded as much as 2,000 years ago by an advanced civilization whose existence wasn&#39;t even suspected.</i></b><br /><br /><i>2) Sustainable agriculture. Unlike the typical rainforest soils, </i><b><i>the black earth can be worked for years, with minimal fertilization. Yet, it is essentially man-made, created by community activities that could be reproduced today by industrial means.</i></b><br /><br /><i>3) Climate change. The key step in producing terra preta traps large amounts of carbon in the soil, avoiding the release of carbon dioxide, the leading greenhouse gas from human activities. In addition, lab tests demonstrate that black earth also gives off less than regular soils of two other atmospheric bad actors: methane and nitrous oxide.</i><br /><br /><i>The crucial step is a method of weed control and land clearing called &#39;cool burning&#39; or slash-and-char, to distinguish it from slash-and-burn, widely used in tropical regions and widely condemned.</i><br /><br /><i>In slash-and-burn, dry brush and grass are burned in open fires, spewing vast quantities of carbon dioxide into the atmosphere and leaving only small amounts of nutrients in the ash that&#39;s then dug into the ground.</i><br /><br /><b><i>By contrast, slash-and-char involves burning wet vegetation, so it smoulders underneath a layer of dirt and straw. Robbed of oxygen, the fire only partly burns any wood or stalks, leaving most as tiny chunks of charcoal. This bio-char is turned into the soil.</i></b><br /><br /><i>Only 7 per cent of the carbon content of vegetation gets transferred into the soil by the slash-and-burn approach, Lehmann told a news conference at the annual meeting of the American Association for the Advancement of Science. The slash-and-char method transfers almost half the carbon and most of the nutrients like phosphorus and nitrogen.</i><br /><br /><b><i>Why people living on hills overlooking many rivers in Brazil two millennia ago devised this approach is still a subject of debate, as is how they added all the extra organic content and ensured the soil was teeming with beneficial bugs and other micro-organisms.</i></b><br /><br /><b><i>Because many patches are about half a hectare in size and loaded with pottery artifacts, University of São Paulo archaeologist Eduardo Neves favours the idea of very big communal backyard compost piles. Successive generations could have swept food refuse -- especially fish and animal bones -- from their dwellings and then added human and animal excrement.</i></b><br /><br /><i>&#39;I don&#39;t think the people ever used this soil for agriculture. They may have grown some medicinal plants in a house garden,&#39; Neves says.</i><br /><br /><i>But geographer William Woods, of the University of Kansas, disagrees.</i><br /><br /><i>In addition to smallish tracts of black earth tracts located right in settlements, Woods has also found a second kind of terra preta. These can cover hundreds of hectares and surround settlements, just where you&#39;d expect to find fields for crops. This soil contains no artefacts, has a slightly lower nutrient level and isn&#39;t as dark in colour, more a blending of black-and-white.</i><br /><br /><i>&#39;I&#39;m calling it terra mulata, deliberately,&#39; Woods says, using the Portuguese word for mulatto.</i><br /><br /><i>A modern technology called low-temperature pyrolysis can produce bio-char on an industrial scale. Using wood or agricultural wastes for fuel, pyrolysis would generate heat for electricity while also actually reducing net emissions of carbon dioxide since the organic matter would otherwise decompose.</i><br /><br /><i>&#39;This might be the beginning of a bio-char revolution,&#39; Lehmann says.</i><br /><br /><i>Yet, this particular revolution could have come much earlier. In 1903, a German geologist published an account of terra preta, noting that farming this dark earth would feed many people.</i><br /><br /><i>&#39;Apparently no one read his book,&#39; Woods says.</i><br /><br /><i>Even further back, Cornell professor Charles Hartt led an expedition of biologists to the Amazon River basin in 1874. He discovered supporters of the Confederate side in the U.S. Civil War who had selected the super-fertile terra preta locales for their sugar plantations. </i><b><i>No one paid any attention to his account, either.&quot;</i></b><br /><br />Scientists across the world are playing a dangerous game imposing their &quot;happy thoughts&quot; narratives upon the data, because one reasonable way to create &quot;dark earth&quot; would be to incinerate the Earth&#39;s biomass, and then smash it to bits with an enormous flood.<br /><br />The Wikipedia entry for dark earth scandalously leads the audience to infer that humans intentionally created this soil:<br /><br /><a href=\"https://en.wikipedia.org/wiki/Dark_earth\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Dark_earth</a><br /><br /><i>&quot;Dark earth in archaeology is an archaeological horizon, as much as 1 m (2 – 3 ft) thick, </i><b><i>indicating settlement over long periods of time</i></b><i>. The material is high in organic matter, including charcoal, which gives it its characteristic dark colour; it may also contain fragments of pottery, tile, animal bone and other artefacts. </i><b><i>It is interpreted as soil enriched with the sooty remains of thatched roofs from houses without chimneys, with other waste materials</i></b><i>. In some areas it appears to give the soil added fertility.&quot;</i><br /><br />When the observations of the Amazon seem to mirror those of the Roman Empire, we&#39;d be wise to ask critical questions.<br /><br /><b>GRAPHIC EXAMPLE 1:</b><br /><br />A review of the excavation of the Coliseum in Rome should bolster the skepticism.  A 17th century church, the Santa Maria Liberatrice, was constructed near the remains of the Coliseum.  The modern streets which lead to this church were constructed on top of some 40 ft of dark earth that in turn rests on the pavement of the Forum.  The Coliseum’s lower precincts were not excavated until 1935.  The Roman remains which you are familiar with today were excavated from an astonishing 40 feet of dark earth.  The three standing columns can be used to visualize the amount of earth which had to be removed.<br /><br /><b>GRAPHIC EXAMPLE 2:</b><br /><br />A second interesting example from the Roman Empire is presented of an unexpected discovery of a Roman aqueduct buried beneath 7 meters of sand and gravel.  The area is being mined for its coal.<br /><br /><a href=\"http://www.q-mag.org/_media/ewald-ernst-on-trevor-obelisks-aqueaducts-01-08-2014.pdf\" class=\"ot-anchor\">http://www.q-mag.org/_media/ewald-ernst-on-trevor-obelisks-aqueaducts-01-08-2014.pdf</a><br /><br />Toppling of Rome’s Obelisks and Aqueducts - Ewald Ernst - August 2014<br /><br /><i>&quot;Rome’s first aqueduct supplied a water-fountain sited at the city’s cattle market. By the third century AD, the city had eleven aqueducts, sustaining a population of over a million in a water-extravagant economy; most of the water supplied the city’s many public baths.&quot;</i><br /><br /><a href=\"https://en.wikipedia.org/wiki/Roman_aqueduct\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Roman_aqueduct</a><br /><br /><i>&quot;None of these marvelous constructions -- erected and working for over half a millennium -- was still functioning in the 4th c. CE ...</i><br /><br /><i>It is believed that all these aqueducts were demolished by barbarians.</i><br /><br /><b><i>Yet, it is not understood what could have driven conquerors -- not only of Rome but of hundreds of other cities within the empire -- to cut themselves off from the supply of water, the most important ingredient for survival.</i></b><br /><br /><b><i>Moreover, after every former attack on Rome the aqueducts, if damaged at all, were immediately repaired.</i></b><br /><br /><i>Therefore, this wonderwork of civil engineering was kept intact over more than five centuries.</i><br /><br /><i>In reality, the destruction of the aqueducts happened swiftly, and with a power no humans had at their disposal.</i><br /><br /><i>This happened, in 234 CE, only eight years after the last system had been completed under Alexander Severus in 226 CE ...</i><br /><br /><i>At the same time, </i><b><i>Rome’s population was reduced from nearly one million to no more than 50,000</i></b><i> ...</i><br /><br /><i>The cataclysm had struck with such force that more than half a millennium passed before Europeans could begin to slowly regain the technological competence of imperial Rome ...</i><br /><br /><b><i>Near Cologne (Rhineland), to give an example, in the lignite area of the Elsbachtal, the gigantig mechanical diggers used to clear away the debris covering the precious coal, a small Roman aqueduct, dated to 224 CE, was brought to light after 7 m of sand and gravel had been removed.</i></b><br /><br /><b><i>So far, one does not understand the geological mechanism that could have laid down such an immense volume of material strangling a once fertile Roman region.</i></b><br /><br /><i>At least, nobody dares to point to barbarians as the culprits.</i><br /><br /><i>The catastrophe that befell Rome soon after the completion of the Aqua Alexandriana in 226 CE, also devastated the Elsbachtal 1,100 km further north.&quot;</i><br /><br />The following research comes from Malaga Bay at <a href=\"https://malagabay.wordpress.com/2016/05/16/black-earth-dark-earth/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2016/05/16/black-earth-dark-earth/</a><br /><br /><a href=\"http://research.historicengland.org.uk/redirect.aspx?id=2830%7CSOIL%20REPORT%20ON%20RANGOON%20ST,%20THE%20CITY\" class=\"ot-anchor\">http://research.historicengland.org.uk/redirect.aspx?id=2830%7CSOIL%20REPORT%20ON%20RANGOON%20ST,%20THE%20CITY</a><br /><br />Soil Report on Rangoon Street, City of London<br />R. I. Macphail – 1984<br />Ancient Monuments Laboratory Report 4443<br /><br />&#39;Dark Earth&#39;<br /><br /><i>&quot;Structure: compacted medium sub-angular blocky with fine (120um) to large (1mm) microaggregates; crack structure:</i><br /><br /><i>Porosity (30%) common very coarse cracks; few compound packing voids and fine (15um), short (50-250um) channels intrapedally:</i><br /><br /><i>Mineral: Coarse/Fine 65/ 35: very dominant very fine (angular to sub-angular) medium and coarse (sub-rounded to sub-angular) sand size quartz: unsorted: few flints: very few opaques and sharp-edged nodules: glauconite, limestone fragments present: few artefacts; very few pottery, mortar and burned daub; &#39;brickearth&#39;, soil fragments, </i><b><i>bone and shell present</i></b><i>.</i><br /><br /><i>Fine: a) very dominant dark brown, black (PPL), greyish; </i><b><i>includes ash</i></b><i>, dark brown (RL):</i><br /><br /><b><i>Organic: Coarse: few charcoal: root fragments present.</i></b><br /><br /><i>Fine: a) </i><b><i>common charcoal charred plant material; dominant amorphous organic matter</i></b><i>, well preserved: amorphous &#39;organo-phosphate&#39; present:</i><br /><br /><i>Groundmass: a) low birefringence, weak crystallitic: enaulic:</i><br /><br /><i>Pedofeatures: common extremely thin elongate excrements, coalescing to moderately thin: frequent moderately broad rugose:</i><br /><br /><i>Textural: fine (30um) dusty coatings present:</i><br /><br /><i>Crystalline: very few, calcitic coatings: </i><b><i>vivianite present</i></b><i>:</i><br /><br /><b><i>Amorphous: frequent impregnative ferro-manganiferous nodules: pale yellow-brown possible &#39;organo-phosphate&#39; (includes vivianite crystals)&quot;</i></b><br /><br /><a href=\"https://en.wikipedia.org/wiki/Vivianite\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Vivianite</a><br /><br /><i>&quot;Vivianite (Fe2+Fe2+2(PO4)2·8H2O) is a </i><b><i>hydrated iron phosphate</i></b><i> mineral found in a number of geological environments ...</i><br /><br /><i>Vivianite crystals are often found </i><b><i>inside fossil shells</i></b><i>, such as those of bivalves and gastropods, or </i><b><i>attached to fossil bone</i></b><i> ...</i><br /><br /><i>Vivianite is a secondary mineral found in a number of geologic environments: The oxidation zone of metal ore deposits, in granite pegmatites containing phosphate minerals, in clays and glauconitic sediments, and in recent alluvial deposits </i><b><i>replacing organic material such as peat, lignite, bog iron ores and forest soils</i></b><i> (All).</i><br /><br /><b><i>Bones and teeth buried in peat bogs are sometimes replaced by vivianite.&quot;</i></b><br /><br /><a href=\"https://en.wikipedia.org/wiki/Glauconite\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Glauconite</a><br /><br /><i>&quot;Glauconite, also known as ‘green sand’ is an iron potassium phyllosilicate (mica group) mineral of characteristic green color with very low weathering resistance and very friable ...</i><br /><br /><b><i>Normally, glauconite is considered a diagnostic mineral indicative of continental shelf marine depositional environments with slow rates of accumulation.</i></b><br /><br /><i>For instance, it appears in Jurassic/lower Cretaceous deposits of greensand, so-called after the coloration caused by glauconite.</i><br /><br /><i>It can also be found in sand or clay formations, or in impure limestones and in chalk.&quot;</i><br /><br /><a href=\"https://en.wikipedia.org/wiki/Dark_earth\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Dark_earth</a><br /><br />(Note that it seems that changes have been made to this wiki since it was quoted by Malaga Bay ...)<br /><br /><i>&quot;In archaeology the term black earth, in use since the 1980s, refers to a layer between 0.6 m to 2 m thick, covering archaeological sites ...</i><br /><br /><i>In England black earth covers Roman remains, especially in urban areas, including London</i><br /><br /><i>Excavations in Belgium, the Brussels-Capital Region, frequently uncover layers of black earth.</i><br /><br /><i>In Sweden, an area of 40 hectares of black earth was discovered in Uppåkra (southern Sweden, formerly Denmark), where human settlement and a city existed during the first millennium before the city was moved to Lund.</i><br /><br /><i>Seven hectares of black earth were uncovered in the Viking town of Björkö (now called Birka), in central Sweden, near Stockholm.</i><br /><br /><i>Black earth were also encountered in Köpingsvik, in the island of Öland off the coast of southern Sweden.</i><br /><br /><i>In France, it has been discovered in Bavay, Corseul, Noyon, Tours at multiple sites and in Paris at two sites, to name a few examples amongst many.&quot;</i><br /><br /><a href=\"https://archaeologydataservice.ac.uk/archiveDS/archiveDownload?t=arch-281-1/dissemination/pdf/cba_rr_070.pdf\" class=\"ot-anchor\">https://archaeologydataservice.ac.uk/archiveDS/archiveDownload?t=arch-281-1/dissemination/pdf/cba_rr_070.pdf</a><br /><br />Early Development of Roman London west of the Walbrook<br />Dominic Perring &amp; Steve Roskams with Patrick Allen<br />The Archaeology of Roman London – Volume 2 – CBA Research Report 70 – 1991<br /><br /><i>&quot;Therefore the changed nature of the latest buildings, the different character of their decay and disuse, and the alterations to the road system can be put beside the dark earth itself to suggest that, in the 3rd century, </i><b><i>all the sites discussed here underwent a similar radical transformation from their previous use</i></b><i>, in a way already anticipated in the later 2nd century </i><b><i>but which remains otherwise unexplained.&quot;</i></b><br /><br /><a href=\"http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=9433499\" class=\"ot-anchor\">http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=9433499</a><br /><br />The ‘dark earth’ and late Roman London – Brian Yule<br />Antiquity – Volume 64 – Issue 244 – September 1990<br /><br />The ‘dark earth’ and the deposits it overlies<br /><br /><b><i>&quot;It is almost invariably the case that, where the strata survive, the latest Roman levels are sealed by deposits of dark-coloured loam, commonly called ‘dark earth’ (formerly ‘black earth’).</i></b><br /><br /><i>In the London area the ‘dark earth’ generally appears as a dark grey, rather silty loam with various inclusions, especially building material.</i><br /><br /><i>The deposit is usually without stratification and homogeneous in appearance. It can be 1m or more in thickness.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Debate Over Phantom Centuries Using Eclipse Data","summary":"A Number of Theorists have Attempted to Prove the Existence of Phantom Centuries in AD Chronology / One Approach to Settling these Claims has been to Point to the Accuracy of Eclipse Retro-Calculations / But, this Claimed Accuracy Seems to Depend on Which Eclipses are Analyzed and Who is Analyzing them","image":"https://lh4.googleusercontent.com/-fAPKmTxYDoM/WGQsF_sSt4I/AAAAAAAAJ94/9U6ug8bj500b63UmadNhKVzpmREdmbVsQCL0B/the-debate-over-phantom-centuries-using-eclipse-data-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/2MHU8mSM4hK","publishDate":"2016-12-28T21:19:06.419Z","updateDate":"2017-01-03T01:30:53.768Z","text":"<b>The Debate Over Phantom Centuries Using Eclipse Data: A Number of Theorists have Attempted to Prove the Existence of Phantom Centuries in AD Chronology / One Approach to Settling these Claims has been to Point to the Accuracy of Eclipse Retro-Calculations / But, this Claimed Accuracy Seems to Depend on Which Eclipses are Analyzed and Who is Analyzing them</b><br /><br />GRAPHIC REFERENCES:<br /><br />[1] <a href=\"http://www.jahr1000wen.de/jtw/Eklipsen-HistE.html\" class=\"ot-anchor\">http://www.jahr1000wen.de/jtw/Eklipsen-HistE.html</a>, for more from this author see <a href=\"http://www.jahr1000wen.de/year1000.html\" class=\"ot-anchor\">http://www.jahr1000wen.de/year1000.html</a><br /><br />[2] <a href=\"http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?2004JHA....35..327M&amp;defaultprint=YES&amp;filetype=.pdf\" class=\"ot-anchor\">http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?2004JHA....35..327M&amp;defaultprint=YES&amp;filetype=.pdf</a><br /><br />[3] The impressive image of the eclipse attributes to Colleen Pinski (Note that it&#39;s been processed)<br /><br />The graphic presents one side of the debate.  The following text will illuminate the other ...<br /><br /><a href=\"http://maverickscience.com/eclipses.htm\" class=\"ot-anchor\">http://maverickscience.com/eclipses.htm</a><br /><br />by Ev Cochrane<br /><br /><i>&quot;Astronomical Retrocalculations and Chronology</i><br /><br /><i>In the past several decades, various researchers have sought to &#39;correct&#39; the conventional chronology of ancient and medieval history by deleting one or more centuries.  A significant number of these researchers -- Rose, Heinsohn, Marx, Whelton, James, Rohl, and various others -- were influenced by the writings of Immanuel Velikovsky, who argued that some five centuries needed to be removed from Greek history, thereby eliminating the so-called Dark Ages.  Velikovsky also envisioned shifting Ramses II and Hattusilis III forward in time by some seven centuries to the time of Nebuchadnezzar II (c. 600 BCE).  For those of us who are not trained historians or intimately familiar with the relevant historical data, it is difficult to know what to make of these seemingly erudite and occasionally arcane arguments.  </i><b><i>Is it really possible, as Marx and Fomenko would have us believe, that conventional chronology is off by more than a thousand years and that Alexander the Great died in 900 AD?  Are we to believe, as Illig and Whelton would have us do, that Middle Age chronology is inflated by two or three centuries?  Other revisions, such as that offered by Heinsohn and Illig, are even more radical, calling for a deletion of several millennia at various points since Neolithic times (these two researchers hold that the Great Pyramid at Giza was built some time after 500 BCE!).</i></b><br /><br /><i>A detailed analysis of the specific claims of Heinsohn, Illig, Fomenko and the rest is impossible here and, in any case, would require a legion of scholars trained in various specialized fields of study (archaeology, ancient art and literature, linguistics, radiocarbon dating, ancient architecture, etc).  Instead we would propose to focus solely on the issue of whether conventional chronology contains a number of phantom centuries and is capable of being shortened to the extent favored by Heinsohn, Marx, Illig and others.  </i><b><i>In order to assess such claims, the science of astronomical retrocalculation proves to be an indispensable tool.</i></b><br /><br /><b><i>As it happens, there exist a wealth of astronomical observations from ancient and medieval times describing solar eclipses, Halley&#39;s comet, and planetary movements against the celestial backdrop.  In principle, it should be possible to compare the dates and astronomical observations recorded in the extant historical records with retrocalculated celestial data.  If a close match is found between the ancient observations and modern retrocalculations at various points throughout recorded history, it follows that there is no basis for the claim of phantom centuries since it is quite impossible that such a result could be achieved by accident or that the celestial order would somehow &#39;reset&#39; itself in accordance with some preordained plan.  Indeed, it is known that precise arrangements of the celestial bodies against the celestial backdrop will not repeat themselves during historical times due to precession of the equinoxes and rotation of the Earth&#39;s axis.</i></b><br /><br /><i>In what follows I offer a representative sample of ancient and medieval astronomical observations in order to provide an overview of the information to be found in such records, much of which is of vital importance for the proper understanding of ancient chronology.  Our survey will necessarily be selective and cursory, since there are many thousands of such records available to the historian of astronomy.  </i><b><i>The records selected will then subjected to analysis by employing either the mathematical calculations of various authorities (Stephenson, Newton, Meeus, Mucke, etc.) or the handy astronomy software Skychart III, which allows for the precise retrocalculation of the sky at any given point in time and space.</i></b><br /><br /><i>In order to test the accuracy of our software, let&#39;s examine an observational report of the renowned astronomer Johannes Kepler from 1591:</i><br /><br /><i>&#39;Michael Maestlin of Tubingen and I saw Jupiter totally eclipsed by Mars in the year 1591 on 9 January.  The fiery red colour of Mars showed that Mars was inferior (i.e., nearer the Earth).&#39;</i><br /><br /><b><i>Upon directing Skychart III to reconstruct the skies for January 9th, 1591 at Prague -- the site of Kepler&#39;s observations -- we find that Jupiter and Mars were indeed in conjunction, being so close together as to be virtually indistinguishable (less than 0.01 degrees separated the two planets).</i></b><i>  [See chart below for the results of the computer simulation]  Systematic tests of various other observations of Kepler produced equally dramatic results.  One can thus be absolutely certain that Kepler&#39;s observation of a conjunction of Jupiter and Mars accurately described the medieval skies and that the fundamental order of the solar system has not changed since 1591.  It also follows from this test that conventional chronology from Kepler&#39;s time until the present is entirely secure in its general outline; e.g. entire years or centuries are not missing.</i><br /><br /><b><i>The all-important question here is the following: How far back into antiquity is it possible to document similar correspondences between astronomical observations and modern retrocalculations?</i></b><br /><br /><i>Having already discussed various planetary observations preserved in ancient Babylonian astronomical diaries, we will here consider ancient eclipse reports.  Observational reports of solar eclipses provide a good deal of valuable information for reconstructing ancient chronology.  How well, then, do the historical records pertaining to solar eclipses stand up in the light of modern calculations?</i><br /><br /><i>The definitive source in this field is the recently published book by F.R. Stephenson, Historical Eclipses and the Earth&#39;s Rotation (Cambridge, 1997).  Of the numerous observations of solar and lunar eclipses preserved from the Middle Ages, Stephenson writes that: &#39;In most cases, the date is accurately reported and the precise place of observation is known.&#39; (p. 376)  Most important, however, is the fact that the majority of these reports agree with modern retrocalculations: &#39;Most recorded dates when converted to the Julian calendar (or the Gregorian calendar from AD 1582) are in exact accord with the tabular dates of eclipses which according to calculation were large in Europe.&#39; (p. 379)</i><br /><br /><i>A few specific examples will illustrate the level of agreement.   We have a number of detailed records of the eclipse of August 2nd, 1133 from different parts of Europe.  The following report comes from a monastery in Germany:</i><br /><br /><i>&#39;In the year of the Incarnation of our Lord 1133 ... on the 4th day before the Nones of August (Aug 2), the 4th day of the week (Wednesday) when the day was declining towards the ninth hour, the Sun in a single moment became black as pitch, day was turned into night, very many stars were seen ...&#39; (p. 392, quoting from Notae Halesbrunnenses)</i><br /><br /><i>A complimentary report comes from Austria:</i><br /><br /><i>&#39;1133.  That great eclipse of the Sun occurred on the 4th day before the Nones of August, the 27th day of the Moon, the 13th year of the Indiction.  After midday, between the 7th and 8th hours, an eclipse of the Sun was seen in Leo ... Very many stars were seen near the Sun; the hearts of many were transfixed, despairing of the light.  The Sun, as if it did not exist, was entirely concealed; for about half an hour it was like night.  The face of the world was sad, terrible, black, wonderful ...&#39; (p. 393)</i><br /><br /><i>Setting Skychart III to August 2, 1133 one obtains the following view of the skies (See chart below).  As will be noted, the Sun is near Leo when eclipsed and both Venus and Mercury appear in the immediate proximity to the Sun, in close agreement with the report that during the eclipse various &#39;stars&#39; were seen.</i><br /><br /><i>There are a number of eclipse reports tracing to the period of the Middle Ages (700 to 900) which Illig and his defenders claim never existed.  One of the earliest detailed accounts of a total solar eclipse dates to 812 AD:</i><br /><br /><i>&#39;In the year 1123 (Seleucid), on the 14th of Ayyar (=May) there was a total eclipse of the Sun from the ninth to the 11th hours.  The darkness was as profound as night; the stars were seen and people lit torches.  The Sun eventually reappeared over about an hour.&#39; (p. 423, quoting the Chronicle of Michael the Syrian, Book XII, chapter 7)</i><br /><br /><i>An invaluable resource for studying solar eclipses is the monumental Canon of Solar Eclipses ­2003 to +2526 by Hermann Mucke and Jean Meeus (Vienna, 1983).  This book lists the vital statistics for every single solar eclipse for the past four thousand years, while providing maps of the region of the Earth affected.  Turning to the year 812 AD, we find that there was indeed a total eclipse of the Sun on May 14th affecting Syria, the presumed region of Michael&#39;s report. (p. 792).</i><br /><br /><i>During the Middle Ages, while much of Europe was shrouded in an intellectual Dark Age, the Arab world was experiencing something of a renaissance of astronomical knowledge otherwise lost since the days of Ptolemy.  Although many of the Arab astronomical reports and chronicles remain generally unknown and hidden away in obscure texts, enough have been recovered to indicate the great skill of Arabic astronomers during this period.  Indeed, Stephenson remarks that &#39;the eclipse observations made by medieval Arab astronomers are among the most accurate and reliable data from the whole of the pre-telescopic period.&#39; (p. 456)</i><br /><br /><i>Consider the following report from 1061:</i><br /><br /><i>&#39;(453 AH.)  On Wednesday, when two nights remained to the completion of (the month of) Jumada al-Ula, two hours after daybreak, the Sun was eclipsed totally.  There was darkness and the birds fell whilst flying.&#39; (p. 439, quoting Ibn al-Jawzi)</i><br /><br /><i>Here Stephenson notes that the date provided by Ibn al-Jawzi &#39;is exactly correct.&#39;</i><br /><br /><i>Further removed in time from the present is the following report of a solar eclipse from Theon (c. 370 AD), included in his commentary on Ptolemy&#39;s Almagest:</i><br /><br /><i>&#39;The exact ecliptic conjunction which we have discussed, and which took place according to the Egyptian calendar in the 1112th year from the reign of Nabonassar, 2 5/6 equal or equinoctial hours after midday on the 24th of Thoth, and according to the Alexandrian calendar reckoned by simple civil days in the 1112th year of the same reign, 2 5/6 equal or equinoctial hours after midday on the 22nd of Payni ... And moreover we observed with the greatest certainty the time of the beginning of contact, reckoned by civil and apparent time, as 2 5/6 equinoctial hours after midday, and the time of the middle of the eclipse as 3 4/5 hours, and the time of complete restoration as 4 1/2 hours approximately after the said midday on the 22nd of Payni.&#39; (Theon of Alexandria, 332, quoted in Stephenson, op. cit., p. 364)</i><br /><br /><i>The report of Theon, when converted to the Julian calendar, dates to June 16th, 364 AD.  As Stephenson points out, this date agrees exactly with modern computations. (p. 364)  Turning to the maps provided by Mucke and Meeus, we find that the area of Earth affected by this particular eclipse cuts across a narrow band of Africa, including Alexandria.  (p. 762)</i><br /><br /><i>The most famous astronomer of all antiquity was Ptolemy.  His Almagest, written around 150 AD, includes a handful of eclipse observations.  Various writers, including Robert Newton, have suggested that Ptolemy doctored his data or reported as observations phenomena that were, in reality, calculations.  How well, then, do the solar eclipses mentioned by Ptolemy in the Almagest measure up against modern retrocalculations?  According to Stephenson, of the six observations included by Ptolemy, &#39;in every case, the reduced date is in exact accord with that of a tabular eclipse.&#39; (p. 369).</i><br /><br /><i>As is well-known, Ptolemy used ancient Babylonian observations in formulating his astronomical theories.  The earliest eclipse record cited by him dates to 721 BCE.  Dozens of ancient Babylonian eclipse observations are still extant.  Here, too, Stephenson found that the vast majority of dated astronomical reports agreed closely with modern retrocalculations.  Consider, for example, the sole total eclipse recorded in Late Babylonian texts-that of April 15th, 136 BCE.  The relevant portion of this text (BM 34034=LBAT 1285), as quoted by Stephenson, is as follows:</i><br /><br /><i>&#39;SE 175, month XII2.  The 29th, solar eclipse.  When it began on the south-west side, in 18 deg daytime in the morning it became entirely total ... (It began) at 24 deg after sunrise.&#39; (129)</i><br /><br /><i>A goal-year text from Babylon includes additional information of interest with regards to this particular eclipse:</i><br /><br /><i>&#39;SE 175, [king] Arsaces, [month XII2].  The 29th, at 24 deg after sunrise, solar eclipse; when it began on the south and west side, [...] [Ven]us, Mercury and the Normal stars were visible; Jupiter and Mars, which were in their period of invisibility, were visible in its eclipse [...].&#39; (&#39;Stephenson, pp. 129-130)</i><br /><br /><i>Turning to the maps provided by Mucke and Meeus, we find that a total eclipse occurred on April 15th, 136 BCE (p. 728).  The affected area included the greater part of Asia and portions of the Persian gulf.  If we now turn to Skychart III and set the skies to April 15th, 136 BCE, it will be found that Jupiter and Mars were indeed very close to the Sun on this date, making their sudden appearance during the eclipse of the Sun a notable sight (See chart below for a map of the Babylonian skies).  Venus and Mercury were also present, as reported by the Babylonian astronomer.  This test-together with the calculations provided by Mucke and Meeus-constitutes a dramatic confirmation of the accuracy of the Babylonian observations.</i><br /><br /><i>Yet this record is hardly unique for its accuracy.  With regards to the solar eclipse of 190 BC, Stephenson offered the following summary of Late Babylonian eclipse reports:</i><br /><br /><i>&#39;When converted to the Julian calendar, the above date (BC 190 Mar 14) proves to be in exact accord with that of a tabular solar eclipse (e.g. as listed by von Oppolzer, 1887).  Such accuracy is characteristic of all Late Babylonian observations of both solar and lunar eclipses for which a date is well preserved.&#39; (F. R. Stephenson, Historical Eclipses and Earth&#39;s Rotation, 1997, p. 121)</i><br /><br /><i>Although Babylonian astronomical observations can be traced to roughly 700 BCE, reliable reports of solar eclipses are only preserved from the fourth century on (Stephenson, op. cit., p. 128).  This gap in our knowledge can be filled, to some extent, by considering the astronomical observations from ancient China, where records of solar eclipses go nearly four centuries further back in time.  Of the oldest Chinese chronicle still extant, Stephenson wrote as follows:</i><br /><br /><i>&#39;The only state chronicle of ancient China which is still intact-possibly as a result of the &#39;Burning of the Books&#39; at the command of Emperor Ch&#39;in Shih Huang-ti in 213 BC-the Spring and Autumn Annals records as many as 36 eclipses of the Sun.  This series of observations, which commences with the event of Feb 22 in 720 BC, is the earliest from any part of the world ... Nearly all dates, when so reduced, prove to be in exact accord with those of eclipses listed in modern tables.  This is a remarkable result for such an ancient work and it gives strong support to the authenticity of the purely historical data recorded in the Classic.&#39; (pp. 221-223).</i><br /><br /><i>A Chinese account from 709 BCE is the earliest report of a complete obscuration of the Sun in any civilization:</i><br /><br /><i>&#39;Duke Huan, 3rd year, 7th month, day jen-ch&#39;en [cyclical day number=29], the first day (of the month).  The Sun was eclipsed and it was total.&#39; (p. 226)</i><br /><br /><i>Of this particular report, Stephenson offered the following commentary: &#39;The recorded date, when reduced to the Julian calendar, agrees exactly with that of a computed solar eclipse.&#39; (226)</i><br /><br /><i>According to the Canon of Mucke and Meeus, a total solar eclipse occurred on July 17th, 709 BC (p. 171).  Turning to the maps provided by F.R. Stephenson and M.A. Houlden in their Atlas of Historical Eclipse Maps: East Asia 1500 BC-AD 1900 (Cambidge, 1986), we find that a total eclipse affected a portion of China on July 17, 709 BCE (p. 99).</i><br /><br /><i>With regards to the records dated after the Han period, Stephenson wrote as follows: &#39;It should be emphasised that throughout Chinese history from the Han onwards, recorded dates of solar eclipses, when converted to the Julian calendar, usually agree precisely with the calculated dates of these phenomena.&#39; (p. 230)</i><br /><br /><i>Having traced solar eclipse observations back well over two thousand years, we have found a striking accord between the ancient observations and modern retrocalculations.  It follows from this finding that the historical reconstructions offered by Heinsohn, Marx, Illig, Whelton and others -- in which several or more centuries are to be deleted during the past 2500 years -- are untenable.&quot;</i><br /><br /><b>The Rebuttal</b><br /><br />There are two major ways to respond to such arguments:<br /><br />(1) <b>The first is to suggest that eclipses are simply selected such that they produce matches along the &quot;standard&quot; eclipse calibration curve.</b><br /><br />See graphic for an example of such a calibration curve.  At 1000 B.C., the divergence becomes 24,500 seconds (or 7 hours) -- which would seem to basically confirm that there can be no phantom centuries.<br /><br />However, the argument would, in theory, propose that those eclipses which do not appear to match the established calibration curve would basically be tossed out as mistaken or fictitious.  This is of course basically the same approach which we can see already at play with the creation of chronologies using radiocarbon dating and dendrochronology.<br /><br />And, sure enough, if we look, we can find against-the-mainstream retro-calculations which purport to show a variety of oddball eclipse claims, like this inventory here (also shown in graphic) ...<br /><br /><a href=\"http://www.jahr1000wen.de/jtw/Eklipsen-HistE.html\" class=\"ot-anchor\">http://www.jahr1000wen.de/jtw/Eklipsen-HistE.html</a><br /><br /><i>&quot;70 Eclipse Reports from Antiquity</i><br /><br /><i>reported year, conventional identification, alternative retrocalculation, deviation of dates</i><br /><br /><i>Remark: If there were no logical interrelation between the historical eclipses and the retocalculated ones, one should expect just a few matches and no cumulation for particular spans of deviation.</i><br /><br /><b><i>In fact, from the traditionally reported eclipses, 17 deviate by 300 and 2 by 299 years from retocalculation. 12 of these show the exactly identical deviation with 300 years minus 46 days (i.e. 109529 days = 15647 weeks = 3709 synodical months). Another 10 eclipses deviate by 354 years minus 13 days!&quot;</i></b><br /><br />References:<br /><br />Historical reports:<br /><br />Demandt, Alexander: Verformungstendenzen in der Überlieferung antiker Sonnen- und Mondfinsternisse [Deformation-tendencies (!) within the tradition of antique solar and lunar eclipses], Mainz 1970, (Akademie der Wissenschaften und der Literatur).<br /><br />R. Gautschy, Eclipsecitations:  <a href=\"http://www.gautschy.ch/~rita/archast/solec/eclipsecitations.pdf\" class=\"ot-anchor\">www.gautschy.ch/~rita/archast/solec/eclipsecitations.pdf</a><br /><br />Ginzel, F. K.: Handbuch der mathematischen und technischen Chronologie Bd II + III, 1914 <a href=\"http://www.archive.org/stream/handbuchdermathe02ginzuoft#page/522\" class=\"ot-anchor\">http://www.archive.org/stream/handbuchdermathe02ginzuoft#page/522</a> <a href=\"http://www.archive.org/stream/handbuchdermathe03ginzuoft#page/n5\" class=\"ot-anchor\">http://www.archive.org/stream/handbuchdermathe03ginzuoft#page/n5</a><br /><br />Modern retrocalculation:<br /><br />Espenak, Fred: Solar Eclipse Atlas: <a href=\"http://sunearth.gsfc.nasa.gov/eclipse/SEatlas/SEatlas.html\" class=\"ot-anchor\">http://sunearth.gsfc.nasa.gov/eclipse/SEatlas/SEatlas.html</a><br /><br />(2) The second possibility -- which can be true independent of (1) -- is that <b>document forgers might have simply adjusted these eclipse observations to match their revisions.</b>  The problem of document forgery has become a hot topic in recent years, and is apparently acknowledged even amongst mainstream chronologists.  We&#39;d be wise to not simply dismiss the idea as some conspiracy, as there are many various reasons why forgeries might have been useful in past eras -- while also devilishly difficult to expose in those former times (arguably making them highly effective).<br /><br /><b>The Implications for a Controversy-First Science Education</b><br /><br />What I want to propose with this and other similar chronology controversies is that we should not overly concern ourselves with the truth of the matter.  Our personal inclination is commonly a desire to immediately resolve the controversy, in service to our need to feel that we know.  But, from the perspective of science educators, we should be wary of teaching conclusions over process.  <b>The truth will come on its own terms through a focus upon educational process.</b><br /><br />What I am trying to get people to see is that we can create a new form of science education here.  When we run into any controversy for which the answer boils down to rote computation, statistics or simulation, <b>we should recognize that as an opportunity to teach the math from the context of the controversy</b>.<br /><br />This is maybe my most important invention: <b>The creation of a new science curriculum which inverts the standard approach to controversies.</b>  The established curriculum has us think about controversies only after people have specialized -- by which time the students have already become immersed in the anti-patterns of scientific culture.  We instruct them on the tools of science, but absent of the context which would motivate our interest and transfer to the student a &quot;common sense&quot; for scientific debate.  We&#39;d be wise to expect that many students will predictably grow bored of science without that larger context to motivate their interest.  The controversy of science is an important part of what makes science interesting.<br /><br />What I am suggesting is that there is a second way to teach science which we should explore: By mapping out all of the controversies of science, and then mapping each of those controversies to various lessons about science (as we do every day in the Controversies of Science collection), we can motivate students to want to learn science as a tool for reasoning through everyday problems.  <b>Some of those lessons will of course be methods in problem-solving.</b><br /><br />In the case of Illig&#39;s Phantom Time Hypothesis, we have a fantastic example which could be taught as a lesson to 6th- or 7th-grade algebra class.  There are numerous lessons which can be taught about statistics through the claims made about Halton Arp&#39;s work with quasars.  The problem-solving lessons of this debate over eclipses would seem to offer us an excellent opportunity to teach trigonometry.  We should identify these areas for further elaboration of the curriculum, and quickly move -- for the time being -- to the next controversy.<br /><br />This is the epitome of the constructivist physics education approach.  For various reasons, the universities have completely missed this methodology.  We&#39;re inventing it here.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Many Assumptions of Radiocarbon Dating","summary":"Proponents of Radiocarbon Dating would have us Believe that when Used Properly, the Tool Generates Accurate Results / \"Radiocarbon Dating has been Repeatedly Tested, Demonstrating its Accuracy ... Multiple Samples from a Single Object have been Dated Independently, Yielding Consistent Results\" / These Arguments Seem to Completely Avoid the Issue of the Numerous Assumptions which Must be True for this Tool to be Accurate","image":"https://lh3.googleusercontent.com/-iBF6T6i7kLA/WGGY0VfwJ8I/AAAAAAAAJ9M/8iDVzYH4RRUg4Z3u8FAPuofUHRWUWUGWACJoC/w7142-h9999/the-many-assumptions-of-radiocarbon-dating-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/cHVvLhDhJNr","publishDate":"2016-12-26T22:25:26.094Z","updateDate":"2017-01-03T01:31:27.728Z","text":"<b>The Many Assumptions of Radiocarbon Dating: Proponents of Radiocarbon Dating would have us Believe that when Used Properly, the Tool Generates Accurate Results / &quot;Radiocarbon Dating has been Repeatedly Tested, Demonstrating its Accuracy ... Multiple Samples from a Single Object have been Dated Independently, Yielding Consistent Results&quot; / These Arguments Seem to Completely Avoid the Issue of the Numerous Assumptions which Must be True for this Tool to be Accurate</b><br /><br />GRAPHIC REFERENCES:<br /><br />[1] <a href=\"http://cdiac.ornl.gov/epubs/ndp/ndp057a/ndp057a.htm\" class=\"ot-anchor\">http://cdiac.ornl.gov/epubs/ndp/ndp057a/ndp057a.htm</a><br /><br />[2] <a href=\"https://malagabay.wordpress.com/2014/05/25/carbon-14-lifting-the-veil/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/25/carbon-14-lifting-the-veil/</a><br /><br />[3] <a href=\"https://malagabay.wordpress.com/2014/05/22/carbon-14-willards-world/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/22/carbon-14-willards-world/</a><br /><br />[4] <a href=\"https://malagabay.wordpress.com/2014/05/25/carbon-14-lifting-the-veil/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/25/carbon-14-lifting-the-veil/</a><br /><br />[5] <a href=\"http://physicsworld.com/cws/article/news/2008/oct/02/the-mystery-of-the-varying-nuclear-decay\" class=\"ot-anchor\">http://physicsworld.com/cws/article/news/2008/oct/02/the-mystery-of-the-varying-nuclear-decay</a><br /><br />[6] <a href=\"http://www.21stcenturysciencetech.com/articles/time.html\" class=\"ot-anchor\">http://www.21stcenturysciencetech.com/articles/time.html</a><br /><br />[7] <a href=\"http://www.kayelaby.npl.co.uk/general_physics/2_7/2_7_7.html\" class=\"ot-anchor\">http://www.kayelaby.npl.co.uk/general_physics/2_7/2_7_7.html</a><br /><br />[8] <a href=\"https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/</a><br /><br />[9] <a href=\"http://www.sciencedirect.com/science/article/pii/S0277379100001712\" class=\"ot-anchor\">http://www.sciencedirect.com/science/article/pii/S0277379100001712</a><br /><br />[10] <a href=\"https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/</a><br /><br />[11] George F. Carter &quot;Physical Geography,&quot; Geogr. Rev., Vol. 43, (1953), pp. 121-124. and R. B. Morrison, &quot;The Pleistocene Boundary: An analysis of various criteria for determining it on a provincial basis and establishing it worldwide,&quot; Geol. Mijnb., Vol. 48, (1969), p. 351-371.<br /><br />[12] <a href=\"http://hbar.phys.msu.su/gorm/dating/dating.pdf\" class=\"ot-anchor\">http://hbar.phys.msu.su/gorm/dating/dating.pdf</a><br /><br />[13] <a href=\"https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/</a><br /><br />[14] <a href=\"https://journals.uair.arizona.edu/index.php/radiocarbon/article/download/3478/2994\" class=\"ot-anchor\">https://journals.uair.arizona.edu/index.php/radiocarbon/article/download/3478/2994</a><br /><br />[15] <a href=\"https://malagabay.wordpress.com/2014/05/25/carbon-14-lifting-the-veil/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/25/carbon-14-lifting-the-veil/</a><br /><br />[16] <i>The Cycle of Cosmic Catastrophes - Flood, Fire and Famine in the History of Civilization</i>, by Richard &amp; Allen West &amp; Simon Warwick-Smith Firestone<br /><br />[17] <a href=\"http://www.varchive.org/ce/c14.htm\" class=\"ot-anchor\">http://www.varchive.org/ce/c14.htm</a><br /><br />[18] <a href=\"https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/</a>, and the source for the table is <a href=\"http://quebec.hwr.arizona.edu/classes/hwr696t/gosse01-cosmogenic-nuclide-review.pdf\" class=\"ot-anchor\">http://quebec.hwr.arizona.edu/classes/hwr696t/gosse01-cosmogenic-nuclide-review.pdf</a>, attributed to JC Gosse, FM Phillips, Quaternary Science Reviews 20 (2001)<br /><br />[19] <a href=\"http://en.wikipedia.org/wiki/Carbon-14\" class=\"ot-anchor\">http://en.wikipedia.org/wiki/Carbon-14</a><br /><br />[20] Tim Flannery, <i>The Future Eaters</i>, (New York, 1995), p. 151.<br /><br />COMMENTARY:<br /><br />Let&#39;s look at an example of an argument made in defense of the accuracy of radiocarbon dating.  This is a perfect example of the complete lack of rigor that has become commonplace on Internet sites dedicated to controversial science.  This webpage is badly out of date, and at this point is misinforming people on the assumptions inherent to this tool ...<br /><br /><a href=\"http://www.talkorigins.org/indexcc/CD/CD011.html\" class=\"ot-anchor\">http://www.talkorigins.org/indexcc/CD/CD011.html</a><br /><br />The TalkOrigins Archive<br />Exploring the Creation/Evolution Controversy<br /><br /><i>&quot;Claim CD011:</i><br /><br /><i>Carbon-14 dating gives unreliable results.</i><br /><br /><i>Source:</i><br /><br /><i>Lee, Robert E., 1981. Radiocarbon: Ages in error. Anthropological Journal of Canada 19(3): 9-29. Reprinted in Creation Research Society Quarterly 19(2): 117-127 (1982).</i><br /><br /><i>Response:</i><br /><br /><b><i>Any tool will give bad results when misused.</i></b><i> Radiocarbon dating has some known limitations. Any measurement that exceeds these limitations will probably be invalid. In particular, radiocarbon dating works to find ages as old as 50,000 years but not much older. Using it to date older items will give bad results. Samples can be contaminated with younger or older carbon, again invalidating the results. Because of excess 12C released into the atmosphere from the Industrial Revolution and excess 14C produced by atmospheric nuclear testing during the 1950s, materials less than 150 years old cannot be dated with radiocarbon (Faure 1998, 294).</i><br /><br /><b><i>In their claims of errors, creationists do not consider misuse of the technique.</i></b><i> It is not uncommon for them to misuse radiocarbon dating by attempting to date samples that are millions of years old (for example, Triassic &#39;wood&#39;) or that have been treated with organic substances. In such cases, the errors belong to the creationists, not the carbon-14 dating method.</i><br /><br /><b><i>Radiocarbon dating has been repeatedly tested, demonstrating its accuracy. It is calibrated by tree-ring data, which gives a nearly exact calendar for more than 11,000 years back. It has also been tested on items for which the age is known through historical records, such as parts of the Dead Sea scrolls and some wood from an Egyptian tomb (MNSU n.d.; Watson 2001). Multiple samples from a single object have been dated independently, yielding consistent results. Radiocarbon dating is also concordant with other dating techniques (e.g., Bard et al. 1990).&quot;</i></b><br /><br />References:<br /><br />Bard, Edouard, Bruno Hamelin, Richard G. Fairbanks and Alan Zindler, 1990. Calibration of the 14C timescale over the past 30,000 years using mass spectrometric U-Th ages from Barbados corals. Nature 345: 405-410.<br /><br />Faure, Gunter, 1998. Principles and Applications of Geochemistry, 2nd ed. Upper Saddle River, NJ: Prentice Hall.<br /><br />MNSU, n.d. Radio-carbon dating. <a href=\"http://emuseum.mnsu.edu/archaeology/dating/radio_carbon.html\" class=\"ot-anchor\">http://emuseum.mnsu.edu/archaeology/dating/radio_carbon.html</a><br /><br />Watson, Kathie, 2001. Radiometric time scale. <a href=\"http://pubs.usgs.gov/gip/geotime/radiometric.html\" class=\"ot-anchor\">http://pubs.usgs.gov/gip/geotime/radiometric.html</a><br /><br />Further Reading:<br /><br />Higham, Tom, 1999. Radiocarbon WEB-Info. <a href=\"http://www.c14dating.com/\" class=\"ot-anchor\">http://www.c14dating.com/</a><br /><br />Thompson, Tim, 2003. A radiometric dating resource list. <a href=\"http://www.tim-thompson.com/radiometric.html#reliability\" class=\"ot-anchor\">http://www.tim-thompson.com/radiometric.html#reliability</a><br /><br />It&#39;s remarkable that they&#39;ve presented the response to the claim, but not the original claim itself.  Let&#39;s take a look at a portion of that original claim ...<br /><br />R. E. Lee, 1981, Radiocarbon: Ages in error, <i>Anthropological Journal of Canada</i>, 19 (3): 9-29, 1981. (Reprinted in the <i>Creation Research Society Quarterly</i>, 19 (2): 117-127; quotes are from pages 123 and 125).<br /><br /><b><i>&quot;The early authorities began the charade by stressing that they were &#39;not aware of a single significant disagreement&#39; on any sample that had been dated at different labs.</i></b><i> Such enthusiasts continue to claim, incredible though it may seem, that &#39;no worse discrepancies are apparent.&#39; Surely 15,000 years of difference on a single block of soil is indeed a gross discrepancy! </i><b><i>And how could the excessive disagreement between the labs be called insignificant, when it has been the basis for the reappraisal of the standard error associated with each and every date in existence?</i></b><br /><br /><i>Why did geologists and archaeologists still spend their scarce money on costly radiocarbon determinations? </i><b><i>They do so because occasional dates appear to be useful. While the method cannot be counted on to give good, unequivocal results, the numbers do impress people, and save them the trouble of thinking excessively. Expressed in what look like precise calendar years, figures seem somehow better both to layman and professional not versed in statistics than complex stratigraphic or cultural correlations, and are more easily retained in one&#39;s memory. &#39;Absolute&#39; dates determined by a laboratory carry a lot of weight, and are extremely helpful in bolstering weak arguments</i></b><i> ....</i><br /><br /><b><i>No matter how &#39;useful&#39; it is, though, the radiocarbon method is still not capable of yielding accurate and reliable results. There are gross discrepancies, the chronology is uneven and relative, and the accepted dates are actually selected dates. &#39;This whole blessed thing is nothing but 13th-century alchemy, and it all depends upon which funny paper you read ...&#39;</i></b><br /><br /><i>In the light of what is known about the radiocarbon method and the way it is used, it is truly astonishing that many authors will cite agreeable determinations as &#39;proof&#39; for their beliefs ...</i><br /><br /><b><i>Radiocarbon dating has somehow avoided collapse onto its own battered foundation, and now lurches onward with a feigned consistency. The implications of pervasive contamination and ancient variations in carbon-14 levels are steadfastly ignored by those who base their arguments upon the dates.&quot;</i></b><br /><br /><a href=\"http://www.varchive.org/ce/c14.htm\" class=\"ot-anchor\">http://www.varchive.org/ce/c14.htm</a><br /><br /><i>&quot;The Pitfalls of Radiocarbon Dating</i><br /><br /><i>Offering in 1952 his new radiocarbon method for calculating the age of organic material (the time interval since the plant or the animal died), W. F. Libby clearly saw the limitations of the method and the conditions under which his theoretical figures would be valid:</i><br /><br /><i>A. Of the three reservoirs of radiocarbon on earth -- the atmosphere, the biosphere, and the hydrosphere, the richest is the last -- the oceans with the seas. </i><b><i>The correctness of the method depends greatly on the condition that in the last 40 or 50 thousand years the quantity of water in the hydrosphere (and carbon diluted in it) has not substantially changed</i></b><br /><br /><i>B. </i><b><i>The method depends also on the condition that during the same period of time the influx of cosmic rays or energy particles coming from the stars and the sun has not suffered substantial variations.</i></b><br /><br /><i>To check on the method before applying it on various historical and paleontological material, Libby chose material of Egyptian archaeology, under the assumption that no other historical material from over 2,000 years ago is so secure as to its absolute dating. When objects of the Old Kingdom and Middle Kingdom of Egypt yielded carbon dates that appeared roughly comparable with the historical dates, Libby made his method known.</i><br /><br /><b><i>With initial large margin of error and anything that did not square with expectation, judged as contaminated, the method appeared to work and was hailed as completely reliable -- just as the atomic clock is reliable -- and this nobody doubted.</i></b><br /><br /><b><i>But as the method was refined, it started to show rather regular anomalies. First, it was noticed that, when radiocarbon dated, wood grown in the 20th century appears more ancient than wood grown in the 19th century. Suess explained the phenomenon by the fact that the increased industrial use of fossil carbon in coal and in oil changed the ratio between the dead carbon C12 and the C14 (radiocarbon) in the atmosphere and therefore also in the biosphere.</i></b><i> In centuries to come a body of a man or animal who lived and died in the 20th century would appear paradoxically of greater age since death than the body of a man or animal of the 19th century, and if the process of industrial use of fossil, therefore dead, carbon continues to increase, as it is expected will be the case, the paradox will continue into the forthcoming centuries.</i><br /><br /><b><i>As years passed and more tests were made (soon by laboratories counted in scores), a rather consistent deviation between radiocarbon age and historical age started to receive the attention of researchers. The radiocarbon dates diverge from the historical dates by several hundred years (often 500 to 700)</i></b><i>, and, interestingly, in the Egyptian samples more so than in samples from most other ancient civilizations. This led Libby to write in 1963: &#39;The data [in the Table] are separated into two groups -- Egyptian and non-Egyptian. This separation was made because the whole Egyptian chronology is interlocking and subject to possible systematic errors ...&#39; Also, &#39;Egyptian historical dates beyond 4000 years ago may be somewhat too old, perhaps 5 centuries too old at 5000 years ago ...&#39; (Science, 140, 278).</i><br /><br /><i>The combined efforts of several researchers led them to believe that one of the conditions stipulated by Libby for a flawless functioning of his method was not historically sustained; it is claimed that the influx of cosmic rays varied with time ...</i><br /><br /><b><i>To determine the extent of correction necessary to render the radiocarbon method reliable, dendrochronologists devised a plan to control the radiocarbon dates by building a chronology of tree rings of the white bristlecone pine, the longest living tree.</i></b><i> The method caught the fancy of the radiocarbon researchers. However, three or four rings formed in one year is not uncommon, especially if the tree grows on a slope, with the ground several times in a year turning wet and dry because of rapid outflow of water (Glueck et al., Botanical Review, 7, 649-713; and 21, 245-365). And certainly the building of tree &#39;ladders,&#39; or carrying on the count from one tree to another may cause erroneous conclusions. One and the same year may be dry in South California and wet in the northern part of the state.</i><br /><br /><i>Now let us review in the light of research in cosmic catastrophism the correctives that, in our view, need to be introduced into the method ...</i><br /><br /><b><i>Bursts of cosmic rays and of electrical discharges on an interplanetary scale would make organic-life surviving the catastrophes much richer in radiocarbon and therefore, when carbon dated, that organic matter would appear much closer to our time than actually true. But if the invasion of the terrestrial atmosphere by &#39;dead&#39; (non-radioactive) carbon from volcanic eruptions, from meteoric dust, from burning oil and coal and centuries-old forests, predominated the picture, then the changed balance of radioactive and of radio-inert carbon would make everything in the decades following the event appear much older. Thus, it is the competition of these factors that would decide the issue in each separate case.&quot;</i></b><br /><br />[...]<br /><br /><b>Nine Key Assumptions of Radiocarbon Dating</b><br /><br /><b>Libby&#39;s Square-Centimeter Model</b><br /><br /><a href=\"https://malagabay.wordpress.com/2014/05/22/carbon-14-willards-world/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/22/carbon-14-willards-world/</a><br /><br /><i>&quot;Willard Libby believed he could model the entire world of Carbon 14 by using a grid system.</i><br /><br /><i>This grid system is based upon a cell size of one square centimetre.</i><br /><br /><b><i>In this grid system every cell is identical and every cell behaves in exactly the same way</i></b><i> ...</i><br /><br /><b><i>Willard Libby chose to draw a veil over fossil fuels because introducing these carbon deposits would also introduce the associated carbon flows between his Radiocarbon Dating model and the real world.</i></b><br /><br /><b><i>In the real world there are numerous processes that deposit carbon and release carbon.</i></b><br /><br /><i>Regardless of their net effect, these processes are not evely distributed around the globe.</i><br /><br /><i>Therefore, Willard Libby&#39;s Radiocarbon Dating model is clearly invalid because every square centimetre of the Earth&#39;s surface does not encapsulate the same amount of carbon and these square centimetres do not all behave in exactly the same way.&quot;</i><br /><br /><b>Assumes a Constant Radioactive Decay Rate</b><br /><br />It&#39;s to this day remarkably easy to find sweeping statements like this online ...<br /><br /><a href=\"http://www.whoi.edu/nosams/what-is-carbon-dating\" class=\"ot-anchor\">http://www.whoi.edu/nosams/what-is-carbon-dating</a><br /><br /><b><i>&quot;The rate at which C14 decays is absolutely constant&quot;</i></b><br /><br />But, in recent times, we&#39;ve come to understand that this is not exactly correct.  Not only do we today understand that there exist a variety of small astronomical components to decay rate, but we also see a small correlation between decay rate and violent solar activity.<br /><br /><a href=\"https://www.thunderbolts.info/tpod/2005/arch05/050404antarctic-fossil.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2005/arch05/050404antarctic-fossil.htm</a><br /><br /><i>&quot;Russian researchers S. E. Schnoll, et al, have been studying the effects of celestial cycles on the decay constant for over 30 years. They document changes in the decay constant and in chemical reaction rates that correlate with moonrise/moonset, eclipses, the sidereal and synodic day, the year, and the sunspot cycle.</i><br /><br /><b><i>Each of these cycles shows that the decay rate is connected to something</i></b><i>. From an Electric Universe point of view, that something is probably electric currents in space. </i><b><i>The synodic day variations (sunrise to sunrise) would correlate to the Sun&#39;s electric currents, while the sidereal day variations (from star-rise to star-rise, just under four minutes shorter than the synodic day) would correlate to something from beyond the solar system, such as the galactic electric currents.&quot;</i></b><br /><br />These are clues which have the potential to point us towards a very important realization:<br /><br />The point is that all matter is fundamentally connected.  If quantum theory is telling us anything at all, it is telling us that there exist resonant effects in matter -- so that the atoms that are disturbed on the Sun leading up to the solar flare, that disturbance -- that change in the resonance of those atoms -- is available to radioactive atoms on Earth.<br /><br />And when it comes to quantum resonant effects, it also suggests -- and this has been born out by some experiments which have been forgotten about -- and that is that low-frequency electromagnetic resonances would induce changes in radioactive decay.  So, what we are looking at there is being able to influence the dance of the nucleus by sending in a signal which beats with that nucleus -- and in the process, can either increase the instability -- that is increase the radioactive decay rate -- or slow it down, depending on the interaction.<br /><br />It is luring to say that since these effects are small, we can safely ignore them.  But, there&#39;s a hidden assumption embedded into the approach ...<br /><br />The big takeaway lesson here is that the adoption of this assumption is itself a form of circular reasoning which presupposes an uneventful history for the Earth and our solar system: The technique is only as valid as the assumption that the decay rate never seriously deviates from what we see today; to apply the technique is to adopt that conclusion (which of course should remain open, subject to observations) as a necessary condition.<br /><br /><b>Assumes a Constant Cosmic Ray Flux Over Time</b><br /><br />Since Libby&#39;s process for the production of C14 depends upon cosmic rays, he must assume that cosmic rays are constant.  Today we understand that this is certainly <b>not</b> true ...<br /><br /><a href=\"http://www.kayelaby.npl.co.uk/general_physics/2_7/2_7_7.html\" class=\"ot-anchor\">http://www.kayelaby.npl.co.uk/general_physics/2_7/2_7_7.html</a><br /><br /><b><i>&quot;During the course of the 11-year sunspot cycle, the flux of neutron-generating particles near sea level varies by about 20% ... the variation is far from smooth&quot;</i></b><br /><br /><a href=\"http://www.holoscience.com/wp/opportunity-favors-the-heretic/\" class=\"ot-anchor\">http://www.holoscience.com/wp/opportunity-favors-the-heretic/</a><br /><br /><i>&quot;[A]stronomers tell geologists that the planets were formed about 4.5 billion years ago.  Geologists tell astronomers that craters were formed primarily by impacts of comets, asteroids and meteors.  Astronomers tell geologists that there is an invisible reservoir of objects that caused the impacts.  </i><b><i>Physicists tell geologists that the process of radioactive decay can be trusted as a reliable clock to date rocks.  The geologists assure the particle physicists that nothing could have happened in the past to upset these radioactive clocks.</i></b><i>  Physicists tell astronomers that most of the stable elements which make up the planets and stars were formed primordially in a series of supernova events.</i><br /><br /><b><i>These are all simply stories.  Countless facts don&#39;t fit the stories but they are not allowed to spoil the telling.&quot;</i></b><br /><br />Popular Science, November 1979, from <a href=\"https://books.google.com/books?id=QQEAAAAAMBAJ&amp;pg=PA79&amp;lpg=PA79&amp;dq=redwood+trees+radiocarbon+dating&amp;source=bl&amp;ots=Mbt8dPHNMo&amp;sig=h6p8xg2KloILcvYhQk48rxitEI4&amp;hl=en&amp;sa=X&amp;ei=wFq9VO_CL4ieyAT49YDwAQ&amp;ved=0CFsQ6AEwCzgK#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=QQEAAAAAMBAJ&amp;pg=PA79&amp;lpg=PA79&amp;dq=redwood+trees+radiocarbon+dating&amp;source=bl&amp;ots=Mbt8dPHNMo&amp;sig=h6p8xg2KloILcvYhQk48rxitEI4&amp;hl=en&amp;sa=X&amp;ei=wFq9VO_CL4ieyAT49YDwAQ&amp;ved=0CFsQ6AEwCzgK#v=onepage&amp;q&amp;f=false</a><br /><br /><i>&quot;Was the rate of decay not constant, after all?  No, realized the chronometrists.  The problem was C-14 production.  And the villain?  The sun.  It is not as stable and predictable as most solar scientists had believed.  Every so often (the periodicity hasn&#39;t yet been precisely determined), the sun -- for reasons still poorly understood -- enters a time of quiet, a time of virtually no sunspots.  And during these times, more cosmic rays are able to penetrate the atmosphere to significantly increase levels of C-14.</i><br /><br /><b><i>Such a period occurred, for example, from 1645 to 1715 -- about the time of Stradivarius.  Trees growing during that period absorbed so much C-14 that, according to radiocarbon dating, violins made from the wood were fitted together only last week.&quot;</i></b><br /><br />And in recent times, there has been an acknowledgement that the Earth is sometimes subjected to a strong bombardment of radiation ...<br /><br /><a href=\"http://www.nature.com/news/mysterious-radiation-burst-recorded-in-tree-rings-1.10768\" class=\"ot-anchor\">http://www.nature.com/news/mysterious-radiation-burst-recorded-in-tree-rings-1.10768</a><br /><br /><i>&quot;Just over 1,200 years ago, the planet was hit by an extremely intense burst of high-energy radiation of unknown cause, scientists studying tree-ring data have found.</i><br /><br /><i>The radiation burst, which seems to have hit between AD 774 and AD 775, was detected by looking at the amounts of the radioactive isotope carbon-14 in tree rings that formed during the AD 775 growing season in the Northern Hemisphere. The increase in C14 levels is so clear that the scientists, led by Fusa Miyake, a cosmic-ray physicist from Nagoya University in Japan, conclude that the atmospheric level of C14 must have jumped by 1.2% over the course of no longer than a year, about 20 times more than the normal rate of variation. Their study is published online in Nature today.&quot;</i><br /><br />Further, there remains some debate over the sources of cosmic rays ...<br /><br /><a href=\"https://www.thunderbolts.info/wp/2012/04/24/cosmic-ions/\" class=\"ot-anchor\">https://www.thunderbolts.info/wp/2012/04/24/cosmic-ions/</a><br /><br /><b>New studies suggest that the origin of the strongest cosmic rays is still mysterious.</b><br /><br /><i>&quot;Cosmic rays are energetic ions from space that arrive in the Sun’s local neighborhood traveling at extremely high velocities. About 90% of all cosmic rays are single protons, or hydrogen nuclei, followed by 5% helium, with the remainder being all other elements in the periodic table ...</i><br /><br /><b><i>Recently, scientists working with the IceCube neutrino detector at the South Pole announced that &#39;cosmic ray hotspots&#39; have been seen coming from specific locations in space. Since cosmic rays are &#39;only known&#39; to be generated by supernovae or the mysterious Gamma Ray Bursters (GRB), the hotspots are creating confusion: no such sources exist close enough to create such high velocity ions. Cosmic rays are electrically charged, so magnetic fields beyond a certain distance ought to randomize the direction from which they arise and prevent such highly localized conditions</i></b><i> ...&quot;</i><br /><br />(see source for further explanation)<br /><br /><b>Assumes Cosmic Rays Do Not Interact with Dead Creatures</b><br /><br /><a href=\"https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/</a><br /><br /><i>&quot;Ironically, by using the count of neutrons produced by cosmic rays at the Earth’s surface [Mount Evans] Libby implicitly acknowledged that his assertion that the &#39;continuous labelling&#39; of the biosphere by Carbon 14 is &#39;terminated at death&#39; is not the whole truth.</i><br /><br /><i>&#39;The real beginning of the history of radiocarbon dating as such was in the realization that the cosmic-ray production of radiocarbon in the high atmosphere leads to a continuous labelling of the biosphere and living matter, which is terminated at death.&#39;</i><br /><br /><b><i>This is because cosmic rays that reach the Earth’s surface collide with atoms [to produce neutrons which can produce Carbon 14] above and below ground level.</i></b><br /><br /><i>Therefore, when a collision occurs within a biosphere host any resulting neutrons may react with nitrogen to produce Carbon 14 </i><b><i>regardless of whether the biosphere host is dead or alive</i></b><i> ...</i><br /><br /><b><i>Furthermore, the more powerful cosmic rays act as bunker busters that can penetrate rock [and probably pyramids] before they produce neutrons which can trigger the formation of Carbon 14 below ground [or in a buried biosphere body].&quot;</i></b><br /><br /><b>Assumes C14 Consistency Across Ecosystems, Organisms and Body Parts</b><br /><br />George F. Carter &quot;Physical Geography,&quot; Geogr. Rev., Vol. 43, (1953), pp. 121-124.<br /><br />... and ...<br /><br />R. B. Morrison, &quot;The Pleistocene Boundary: An analysis of various criteria for determining it on a provincial basis and establishing it worldwide,&quot; Geol. Mijnb., Vol. 48, (1969), p. 351-371.<br /><br /><b><i>&quot;Dr. George Carter has concluded that different plants in different regions absorb different amounts of carbon that can unbalance the relation of old and new carbon in them.&quot;</i></b><br /><br /><b>Assumes Latitudinal and Altitudinal C14 Consistency</b><br /><br /><a href=\"https://malagabay.wordpress.com/2014/05/25/carbon-14-lifting-the-veil/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/25/carbon-14-lifting-the-veil/</a><br /><br /><i>&quot;Clearly, each square centimetre of the Earth’s surface is not identical.</i><br /><br /><i>Clearly, there are variations in the distribution of clouds, precipitation, temperature, fresh water, sea water, ice, soils, rocks, carbon deposits and life forms.</i><br /><br /><i>The real world is not uniformly average.</i><br /><br /><b><i>Much of this natural variability is controlled by insolation which varies in intensity and duration by latitude.&quot;</i></b><br /><br />(Insolation is the total amount of solar radiation energy received on a given surface area during a given time.)<br /><br /><i>&quot;The Earth’s sea surface temperatures clearly illustrate this latitudinal dependency upon insolation.</i><br /><br /><b><i>These variations in sea surface temperatures directly influence the carbon levels in the oceans because &#39;more than twice as much CO2 can dissolve into cold polar waters than in the warm equatorial waters&#39; so that, typically, &#39;tropical waters release CO2 to the atmosphere, whereas high-latitude oceans take up CO2 from the atmosphere&#39;.&quot;</i></b><br /><br />Libby sought to minimize this problem, in advance, by suggesting that ocean mixing would smooth out any potential differences ...<br /><br /><a href=\"http://www.nobelprize.org/nobel_prizes/chemistry/laureates/1960/libby-lecture.pdf\" class=\"ot-anchor\">http://www.nobelprize.org/nobel_prizes/chemistry/laureates/1960/libby-lecture.pdf</a><br /><br /><b><i>&quot;we know that there is world-wide mixing which occurs&quot;</i></b><br /><br />Libby admits that the rate of this mixing is on the order of 750, minimum.<br /><br /><a href=\"https://malagabay.wordpress.com/2014/05/25/carbon-14-lifting-the-veil/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/25/carbon-14-lifting-the-veil/</a><br /><br /><b><i>&quot;[W]hat Libby fails to mention is that the plants and animals living within [and around] the oceans will only benefit from this mixing if they live for [somewhere between] 750 and 8,000 years.&quot;</i></b><br /><br />Further ...<br /><br /><a href=\"https://malagabay.wordpress.com/2014/05/12/carbon-14-cultures/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/12/carbon-14-cultures/</a><br /><br /><b><i>&quot;The northern and southern hemispheres have atmospheric circulation systems that are sufficiently independent of each other that there is a noticeable time lag in mixing between the two.&quot;</i></b><br /><br />Studies of the vertical profile for C14 across the Pacific, Southern and Indian Oceans -- not all that surprisingly -- show significant variation in the upper 1200 meters of the ocean.<br /><br />There are of course similar differences by atmospheric altitude.  Neither of these two observations should be at all surprising in light of the idea that C14 is created by cosmic rays.<br /><br />Nevertheless, Libby sought to minimize all of these effects ...<br /><br /><i>&quot;The plan was to measure living materials from various places on earth and to see whether they had the same radiocarbon content per gram of carbon ... </i><b><i>They show no appreciable differences even though they come from places varying in latitude from near the South Pole to near the North Pole.&quot;</i></b><br /><br /><a href=\"https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/</a><br /><br /><i>&quot;The National Physical Laboratory is more forthcoming when it comes to the &#39;poorly known&#39; Carbon 14 because they acknowledge there are latitudinal differences and quote a Carbon 14 production rate of 4 atoms per second per square centimetre at a latitude of 45° when solar activity is high ...&quot;</i><br /><br /><a href=\"http://www.kayelaby.npl.co.uk/general_physics/2_7/2_7_7.html\" class=\"ot-anchor\">http://www.kayelaby.npl.co.uk/general_physics/2_7/2_7_7.html</a><br /><br /><i>&quot;Nuclear interactions of cosmic rays in the atmosphere generate about 6 x 10^4 neutrons s^- 1 per m^2 of the Earth at 45 deg latitude, of which 4 x 10^4 m^- 2s^- 1 are absorbed by nitrogen to generate C14 when solar activity is low: </i><b><i>the long-term all-Earth average would be about 60% of this.&quot;</i></b><br /><br /><a href=\"https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/</a><br /><br /><b><i>&quot;The &#39;long-term all-Earth average&#39; of 2.4 atoms of Carbon 14 per second per square centimetre provided by the National Physical Laboratory indicates that Willard Libby underestimated Carbon 14 production rates by about 16%.&quot;</i></b><br /><br /><b>Assumes that the Earth is a Closed Carbon System, i.e. No Catastrophes</b><br /><br /><a href=\"https://books.google.com/books?id=zr_-7VVWyR4C&amp;pg=PA10&amp;lpg=PA10&amp;dq=%22At+one+time,+scientists+thought+the+radiocarbon+in+the+atmosphere+remained+constant,+but+to+their+surprise,+they+discovered+that+radiocarbon+levels+varied+considerably+over+thousands+of+years%22&amp;source=bl&amp;ots=KQd6VnWwjO&amp;sig=VsWxG5Ka8YEKpF91FuPTzUod72I&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwio7KuexZLRAhVO3GMKHdEEDFIQ6AEIGzAA#v=onepage&amp;q=%22At%20one%20time%2C%20scientists%20thought%20the%20radiocarbon%20in%20the%20atmosphere%20remained%20constant%2C%20but%20to%20their%20surprise%2C%20they%20discovered%20that%20radiocarbon%20levels%20varied%20considerably%20over%20thousands%20of%20years%22&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=zr_-7VVWyR4C&amp;pg=PA10&amp;lpg=PA10&amp;dq=%22At+one+time,+scientists+thought+the+radiocarbon+in+the+atmosphere+remained+constant,+but+to+their+surprise,+they+discovered+that+radiocarbon+levels+varied+considerably+over+thousands+of+years%22&amp;source=bl&amp;ots=KQd6VnWwjO&amp;sig=VsWxG5Ka8YEKpF91FuPTzUod72I&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwio7KuexZLRAhVO3GMKHdEEDFIQ6AEIGzAA#v=onepage&amp;q=%22At%20one%20time%2C%20scientists%20thought%20the%20radiocarbon%20in%20the%20atmosphere%20remained%20constant%2C%20but%20to%20their%20surprise%2C%20they%20discovered%20that%20radiocarbon%20levels%20varied%20considerably%20over%20thousands%20of%20years%22&amp;f=false</a><br /><br /><i>The Cycle of Cosmic Catastrophes - Flood, Fire and Famine in the History of Civilization,</i> by Richard &amp; Allen West &amp; Simon Warwick-Smith Firestone<br /><br /><i>&quot;Although this technique is reliable in most cases, radiocarbon researchers have discovered a major flaw in the theory.  </i><b><i>At one time, scientists thought the radiocarbon in the atmosphere remained constant, but to their surprise, they discovered that radiocarbon levels varied considerably over thousands of years -- a fact that makes radiocarbon dating substantially less reliable for those times when atmospheric radiocarbon changed dramatically.</i></b><br /><br /><b><i>At its worst, the effect is so dramatic that scientists refer to it as a &#39;radiocarbon plateau,&#39; or a reversal, meaning that the C14 dates are in reverse order, with younger dates seeming to predate older ones. During such a reversal, radiocarbon dates can be off by many thousands of years, as you can see from the graph of IntCal04 dates near 18,000 and near 13,000 years ago (fig 1.4), just the time when the Clovis people and the giant animals were disappearing.</i></b><i> Bill suspected that this was no coincidence.&quot;</i><br /><br />The fact that around half of all dates are thrown away challenges the claim above that the &quot;technique is reliable in most cases&quot;.  We truthfully have no surefire way to actually know it&#39;s reliability, because we do not know what the correct chronology is.<br /><br />There appear to be a number of these apparent external disruptions to the atmosphere&#39;s C14 content.  As previously mentioned, there was seemingly another &quot;event&quot; around 774 AD ...<br /><br /><a href=\"http://sciences.blogs.liberation.fr/files/c14-774-apr%C3%A8s-jc.pdf\" class=\"ot-anchor\">http://sciences.blogs.liberation.fr/files/c14-774-apr%C3%A8s-jc.pdf</a><br /><br /><b><i>&quot;We find a rapid increase of about 12% in the C14 content from AD 774 to 775, which is about 20 times larger than the change attributed to ordinary solar modulation.&quot;</i></b><br /><br />It&#39;s not clear how much we can trust such numbers, as it&#39;s a bit similar to saying that &quot;we measured that the length of a meter has changed&quot;.  There are some deep epistemology issues at play when your measuring stick is shown to not be entirely consistent over time.<br /><br />To emphasize the confusion of dates created by catastrophe, the Firestone group noted the following ...<br /><br /><a href=\"https://books.google.com/books?id=zr_-7VVWyR4C&amp;pg=PA12&amp;lpg=PA12&amp;dq=%22seemed+to+show+that+the+long+dead+Indians+miraculously+came+back+to+life%22&amp;source=bl&amp;ots=KQd6VnXujJ&amp;sig=anQhhg6mAmc57_1vT2-MPGYv_7k&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjkvcKZyJLRAhUC8WMKHVxpD-QQ6AEIGzAA#v=onepage&amp;q=%22seemed%20to%20show%20that%20the%20long%20dead%20Indians%20miraculously%20came%20back%20to%20life%22&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=zr_-7VVWyR4C&amp;pg=PA12&amp;lpg=PA12&amp;dq=%22seemed+to+show+that+the+long+dead+Indians+miraculously+came+back+to+life%22&amp;source=bl&amp;ots=KQd6VnXujJ&amp;sig=anQhhg6mAmc57_1vT2-MPGYv_7k&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjkvcKZyJLRAhUC8WMKHVxpD-QQ6AEIGzAA#v=onepage&amp;q=%22seemed%20to%20show%20that%20the%20long%20dead%20Indians%20miraculously%20came%20back%20to%20life%22&amp;f=false</a><br /><br /><i>The Cycle of Cosmic Catastrophes - Flood, Fire and Famine in the History of Civilization,</i> by Richard &amp; Allen West &amp; Simon Warwick-Smith Firestone<br /><br /><i>&quot;RADIOCARBON SAYS CLOVIS INDIANS STILL ALIVE!</i><br /><br /><b><i>Exploring the radiocarbon issue further, Bill discovered that some Paleo-Indian radiocarbon dates were laughably wrong.</i></b><i>  For example, dates from the Paleo-Indian sites at Leavitt and Gainey, in Michigan, came from layers that scientists knew were 13,000 years old; and yet the radiocarbon date came back suggesting that, inexplicably, the long-vanished Ice Age Indians were still hunting extinct camels when the Egyptian pharaohs were building the Temple of Karnak 2,800 years ago.</i><br /><br /><i>Another 13,000-year-old site, at Thedford, Ontario, Canada, seemed to show that the long dead Indians miraculously came back to life and lived up until about the time of Jesus. In addition, the most astounding Clovis-era site of all was at Grant Lake in Nunavut Province in northern Canada, where the long gone Ice Age Paleo-Indians had apparently been hunting mammoths during the time of the Battle of Gettysburg in the U.S. Civil War!</i><br /><br /><b><i>Clearly, these dates and others are impossibly wrong, although many others are correct. This indicated to Bill one of two things: either something had dumped extra radiocarbon onto the planet at that time, selectively resetting the radiocarbon clocks in some areas and not others; or some terrestrial process had produced the erroneous radiocarbon dates.&quot;</i></b><br /><br /><a href=\"https://malagabay.wordpress.com/2014/09/02/catastrophic-dendrochronology/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/09/02/catastrophic-dendrochronology/</a><br /><br /><i>&quot;Catastrophic Dendrochronology assumes that any perturbation of the atmospheric 12C/14C balance [following a &#39;cosmic catastrophe&#39;] will naturally dissipate within 40 or 50 years.</i><br /><br /><i>This assumption is based upon the observed decline in carbon-14 levels in the northern hemisphere during the second half of the 20th century.&quot;</i><br /><br /><b>Assumes One Dominant Process for C14 Production</b><br /><br /><a href=\"https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/</a><br /><br /><b><i>&quot;Adding an extra twist to the Radiocarbon Dating saga is the observation that Carbon 14 production is not limited to the (n-p) reaction that is triggered when a neutron collides with Nitrogen 14</i></b><i> ...</i><br /><br /><i>Unsurprisingly, Wikipedia prefers to draw a veil over the Oxygen 16 reactions because the natural abundance of Oxygen 16 is 99.76% and the lower atmosphere contains 20.946% Oxygen ...</i><br /><br /><i>Therefore, even the National Physical Laboratory value of 4 atoms of Carbon 14 per second per square centimetre at a latitude of 45 degrees is misleading because their carefully worded information fails to mention Oxygen 16 reactions.&quot;</i><br /><br /><b>Assumes No Contamination Per Chosen Sample</b><br /><br />Of all of the various assumptions which must be true in order for a radiocarbon date to be valid, it is contamination which gets all of the blame.  In light of everything that is known today about these assumptions, we should maybe ask why that is.  Is it because casting blame there does not invite any skepticism of the technique itself?<br /><br />The Extinction of the Mammoth, Charles Ginenthal, The Velikovskian: A Journal of Myth, History and Science, Vol III, No&#39;s 2 and 3 and J. Gordon Ogden, III, &quot;Radiocarbon and Pollen Evidence for a Sudden Change in Climate in the Great Lakes Region approximately 10,000 years go,&quot; Quaternary Paleoecology, E. J. Cushing, H. E. Wright, Jr., eds. (New Haven, CT, 1967) p. 119.<br /><br /><i>&quot;If ground water is saturated with old carbon, it can drastically change the age of the plant.  And this has been known from almost the inception of the radiocarbon dating process in the 1950&#39;s.  J. Gordon Ogden, III describes how radiocarbon in lakes from run-off and ground water becomes contaminated with old carbon.</i><br /><br /><i>&#39;The dilution of atmospheric carbon 14 by waters rich in Paleozoic limestone constitutes a major problem in dating sedimentary sequences from lake basins ... Broecker and Walter (1959) have shown that dissolved carbonate minerals can result [add old carbon] in surface material with as little as 50% of modern carbon 14 activity.  Living aguatic plants from Montezuma Well in Arizona show a carbon 14 activity corresponding to an average of 17,300 and 24,750 [years] respectively.&#39;&quot;</i><br /><br />Tim Flannery, <i>The Future Eaters</i>, (New York, 1995), p. 151.<br /><br /><i>&quot;Tim Flannery further points out with respect to ground water and radiocarbon dating, &#39;... </i><b><i>what is not often realized is that organic matter carried in ground water can contaminate samples guite easily. This kind of contamination is ... difficult to detect.&#39;&quot;</i></b><br /><br />Scientific Dating Methods in Ruins, by Charles Ginenthal, originally from 26: Science News (August 7, 1993): 87.<br /><br /><i>&quot;In essence, water in the ground will contaminate all organic materials and can significantly alter carbon-14 dates.  </i><b><i>Organic materials lying in ground where there is significant water, such as deltas, areas with moderate to heavy rainfall, and flooded regions such as river valleys or Arctic muck, will give significantly older dates.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Heinson's Chronology","summary":"Gunnar Heinsohn is a German Professor who Claims that the Accepted Western Chronology of Human History has been Padded by a Full 700 Years / His Claims have Evoked a Passionate - Largely Internal - Debate Amongst Catastrophists / The Problem he Points to is that Not a Single Archaeological Site - of 2,500 Roman Digs - has Revealed Enough Strata for the Stretch of 1,000 Years / He Blames a Devastating Event Around 230 AD","image":"https://lh3.googleusercontent.com/-rdn-OZy99sI/WGC1uYbOOLI/AAAAAAAAJ8M/b73oBKomR2M2ggLoPSiE4csVJ8tkFqGHwCJoC/w7142-h9999/heinsons-chronology-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/RPkUrMbPFxH","publishDate":"2016-12-26T06:16:49.124Z","updateDate":"2017-01-03T01:31:45.034Z","text":"<b>Heinson&#39;s Chronology: Gunnar Heinsohn is a German Professor who Claims that the Accepted Western Chronology of Human History has been Padded by a Full 700 Years / His Claims have Evoked a Passionate - Largely Internal - Debate Amongst Catastrophists / The Problem he Points to is that Not a Single Archaeological Site - of 2,500 Roman Digs - has Revealed Enough Strata for the Stretch of 1,000 Years / He Blames a Devastating Event Around 230 AD</b><br /><br />(Many thanks to Clark Whelton for his assistance with assembling the contents of this article.)<br /><br /><a href=\"http://www.q-mag.org/_iserv/dlfiles/dl.php?ddl=gunnar-creation-of-the-1st-millennium-new16-11-2013.pdf\" class=\"ot-anchor\">http://www.q-mag.org/_iserv/dlfiles/dl.php?ddl=gunnar-creation-of-the-1st-millennium-new16-11-2013.pdf</a><br /><br />Gunnar Heinsohn (November 2013), Creation of the First Millennium CE<br /><br /><b><i>&quot;No one can play semantic games with archaeological strata.</i></b><i> You cannot employ one and the same stratum two or three times to fill a time span you automatically take for granted even if you have never thought about its creation. Neither can you present three distinct strata as just one stratum. Thus, it is not easy to play stratigraphic games. </i><b><i>What is possible, of course, is that simultaneous strata from two different sites can be used to represent two different time spans, even if the two sites are quite close to one another.</i></b><br /><br /><i>[...]</i><br /><br /><i>What happened to 1st millennium CE chronology? Why was it inflated with imaginary centuries? Was there a conspiracy at work? The author is aware of theories pointing in that direction. But instead he favours the idea that </i><b><i>there was a profound sense of helplessness after the widespread devastation and death toll of the 230s, devastation that not only cost the lives of so many but also caused a loss of continuity in calendrical computation. For the survivors, it was time for a new start. There might have been a powerful charm in the idea of a &#39;year 1000 CE,&#39; which was described in Scripture as a fateful date.</i></b><i> Someone selected that year -- maybe Michael Psellos (1017-1078 -- a choice favoured by Jan Beaufort [University of Würzburg), or Frutolf von Michelsberg (+1103). </i><b><i>Very soon after the decision to settle on a year called 1000 AD -- i.e. sometime in the beginning of the 2nd millennium CE -- the need arose to furnish the centuries of the 1st millennium with credible history, even though the true length of that millenium was unknown at the time. To accomplish such a difficult task -- in a world without the science of archaeology nobody could verify assumed centuries against proven stratigraphies -- chronologically parallel blocks of history in different geographical regions were put into a chronological sequence. This arduous work appears to have followed a simple principle. The more east and north the region of available historical narratives was located, the later it was dated, and -- an initially unintended, proto-racist, consequence -- the more backward it was thought to be.</i></b><br /><br /><i>[...]</i><br /><br /><b><i>Although such a technique can temporarily hide the contemporaneity of all the periods involved it cannot hide the striking similarity.</i></b><br /><br /><i>[...]</i><br /><br /><b><i>So far not a single site touched by the spade (out of roughly 2.500 Roman cities, and a multitude of that in villae rusticae) has revealed enough strata for the stretch of 1,000 years expected for the 1st millennium CE.</i></b><i> Where building layers are found for 1-235, they are missing for 285-520. Where they are found for 6th/7th to the 10th century (Slavic North-East), they are missing -- roughly speaking -- from 1-700. Therefore, ad hoc theories had to be devised to explain away early south-western Roman items from the 2nd/3rd c. that were found in south-eastern &#39;late&#39; antiquity strata assigned to the 5th/6th c. or even -- in more eastern Slavic territories -- to the 9th/10th century. Thus was born the now ubiquitous theory of heirlooms or private museums of supposed 1st millennium art collectors.&quot;</i><br /><br /><a href=\"http://www.q-mag.org/_media/ewald-ernst-on-trevor-obelisks-aqueaducts-01-08-2014.pdf\" class=\"ot-anchor\">http://www.q-mag.org/_media/ewald-ernst-on-trevor-obelisks-aqueaducts-01-08-2014.pdf</a><br /><br />Toppling of Rome’s Obelisks and Aqueducts - Ewald Ernst - August 2014<br /><br /><i>&quot;Rome’s first aqueduct supplied a water-fountain sited at the city’s cattle market. By the third century AD, the city had eleven aqueducts, sustaining a population of over a million in a water-extravagant economy; most of the water supplied the city’s many public baths.&quot;</i><br /><br /><a href=\"https://en.wikipedia.org/wiki/Roman_aqueduct\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Roman_aqueduct</a><br /><br /><i>&quot;None of these marvelous constructions -- erected and working for over half a millennium -- was still functioning in the 4th c. CE ...</i><br /><br /><i>It is believed that all these aqueducts were demolished by barbarians.</i><br /><br /><b><i>Yet, it is not understood what could have driven conquerors -- not only of Rome but of hundreds of other cities within the empire -- to cut themselves off from the supply of water, the most important ingredient for survival.</i></b><br /><br /><b><i>Moreover, after every former attack on Rome the aqueducts, if damaged at all, were immediately repaired.</i></b><br /><br /><i>Therefore, this wonderwork of civil engineering was kept intact over more than five centuries.</i><br /><br /><i>In reality, the destruction of the aqueducts happened swiftly, and with a power no humans had at their disposal.</i><br /><br /><i>This happened, in 234 CE, only eight years after the last system had been completed under Alexander Severus in 226 CE ...</i><br /><br /><i>At the same time, </i><b><i>Rome’s population was reduced from nearly one million to no more than 50,000</i></b><i> ...</i><br /><br /><i>The cataclysm had struck with such force that more than half a millennium passed before Europeans could begin to slowly regain the technological competence of imperial Rome ...</i><br /><br /><b><i>Near Cologne (Rhineland), to give an example, in the lignite area of the Elsbachtal, the gigantig mechanical diggers used to clear away the debris covering the precious coal, a small Roman aqueduct, dated to 224 CE, was brought to light after 7 m of sand and gravel had been removed.</i></b><br /><br /><b><i>So far, one does not understand the geological mechanism that could have laid down such an immense volume of material strangling a once fertile Roman region.</i></b><br /><br /><i>At least, nobody dares to point to barbarians as the culprits.</i><br /><br /><i>The catastrophe that befell Rome soon after the completion of the Aqua Alexandriana in 226 CE, also devastated the Elsbachtal 1,100 km further north.&quot;</i><br /><br />Wikipedia informs us that <i>&quot;Gunnar Heinsohn is a German author, sociologist and economist and professor emeritus at the University of Bremen ... Heinsohn has published on a wide array of topics, starting from economics, demography and its relationship with security policy and genocide, and revisionist chronology theories in the tradition of Immanuel Velikovsky.&quot;</i><br /><br />Even if the claims can at times seem somewhat academic, there is some value to briefly reviewing Heinsohn&#39;s thesis insofar as we can get a better feel for the nature of modern chronology controversy.  There are some interesting observable patterns and challenges in this area.<br /><br /><b>Gunnar Heinsohn’s chronology research claims that Antiquity, Late Antiquity and the Early Middle Ages happened in parallel, not in sequence.  And he suggests that placing these contemporary periods into sequence added hundreds of imaginary years to AD chronology.</b><br /><br />For example, if the histories of France, the UK and Germany between 1815 and 1914 were placed in sequence, instead of in parallel, imaginary centuries would be added to European history.  In one of those centuries, France would be prominent while evidence for the UK and Germany would be sketchy and puzzling, etc. Each nation would begin this 99-year period at the end of one war, and end it at the beginning of another.  There would appear to be six wars in all, while in fact there were only two.<br /><br /><b>According to mainstream history, the western Roman Empire of Antiquity suffered a devastating &quot;crisis&quot; around 235 AD.  About 300 years later, the Late Antique Roman Empire was devastated by a catastrophe associated with &quot;the comet of Justinian.&quot;  About 400 years after that, as Heinsohn shows, many northern and eastern European regions -- stretching from Norway via the Baltic to the Black See -- reveal evidence of a vast catastrophe in the 930s AD (give or take a decade).  Physical evidence shows the depopulations were as severe as in areas tied to the 3rd or the 6th century plagues and catastrophes. Yet, written sources pointing to the causes or consequences of the 10th century mass deaths appear to be missing entirely.</b><br /><br />By placing three contemporary periods in sequence, three catastrophes appear in the history books, whereas only one such period actually happened.<br /><br />In short, architecture dated to Late Antique Rome was in fact created during Antiquity.  Material evidence for Charlemagne, which is dated by the mainstream to the 8th and 9th centuries AD, also belongs in Antiquity, in the late 2nd century AD.  In Heinsohn’s revision, Charlemagne was a leader of Roman foederati, around the time of Marcus Aurelius.  He was not a reviver of Roman culture centuries after the empire fell.  He was a Roman ally.<br /><br /><b>For years there has been an ongoing and very bitter conflict between Heinsohn and his supporters, and another group of historical revisionists who we&#39;ll refer to here as Saturnists.  Each side wants to revise the human history chronology, but in quite different ways.</b><br /><br />The Saturnists are against reductions in chronology -- especially the radical cuts that Heinsohn makes.  They need a longer history for their Saturnian scenario, and therefore support mainstream chronology.  In a sense, they might be called &quot;neo-uniformitarians,&quot; because <b>they support catastrophism, but only in the distant past.  When more recent time spans are called into question, they dig in their heels and use mainstream textbooks to attack Heinsohn and others.</b><br /><br />If Heinsohn is right, we are living today in the year ca. 1316 AD.  But if we leave the year 2016 AD where it is now and count backward, then Rome and its empire were struck down by a massive global catastrophe ca. 930 AD.<br /><br />Velikovsky wanted to remove 600 years from ancient chronology.  Heinsohn wants to bring down the origin of high civilization to below 1500 BCE, perhaps down to ca. 1200 BCE.  This would bring western chronology into line with the rest of the world.<br /><br />As he does in his work on AD chronology, Heinsohn concludes that <b>there is simply too little physical evidence for a longer chronology, which was constructed by adding layers from one site to layers from another.  In any single archaeological site in the Ancient Near East, there are only four settlement periods in the stratigraphic record, not eight or ten as the textbooks say should exist.</b><br /><br /><b>For years the Chinese have been working hard to find evidence of a longer history that would give China the same chronological age as western history.  Gunnar has been trying to warn the Chinese that their shorter chronology is correct, and that it’s a mistake to imitate the false chronology of the west, which is inflated by dark ages.</b><br /><br />see <a href=\"http://www.nytimes.com/2000/11/10/world/in-china-ancient-history-kindles-modern-doubts.html\" class=\"ot-anchor\">http://www.nytimes.com/2000/11/10/world/in-china-ancient-history-kindles-modern-doubts.html</a><br /><br />In China, Ancient History Kindles Modern Doubts<br />By ERIK ECKHOLM<br />NOV. 10, 2000<br /><br /><i>&quot;Everyone here knows that Chinese civilization has 5,000 years of uninterrupted history, a truism proudly repeated by schoolchildren and President Jiang Zemin alike. </i><b><i>But as serious scholars have long conceded, hard proof of the first 2,000 years is missing.</i></b><br /><br /><i>Today, scholars announced the results of an urgent government-sponsored research program that -- using &#39;the superiority of socialism to develop a multidisciplinary approach&#39; -- has filled in key gaps in the ancient record of China&#39;s first kings and dynasties. The project, which mobilized more than 200 scholars for five years, has been hailed for shedding light on the murky origins of Chinese civilization. But it has also raised questions about the role of nationalism in scholarship.</i><br /><br /><i>Ample evidence does exist of early cultures in the Yellow River Valley, where </i><b><i>legend holds that the Chinese language and imperial system took form under a mythical Yellow Emperor 5,000 years ago.</i></b><i> But no firmly documented chronology of rulers, reigns and conquests -- of the sort that exists for ancient Egypt and Mesopotamia -- actually goes back beyond 841 B.C.</i><br /><br /><i>&#39;This has been a major regret for Chinese history and world history,&#39; said Li Xueqin, a prominent historian, today at a news conference to disclose the results of the project.</i><br /><br /><i>Mr. Li, the project&#39;s director, was certainly understating the despair that many scholars and officials have felt about the history problem. China is a country obsessed with its past, as a source of national worth and an explanation for every foible.</i><br /><br /><i>Mr. Li announced that the Xia-Shang-Zhou Chronology Project, named for the three early dynasties under study, &#39;has been able to solve a series of longstanding questions about early Chinese civilization.&#39; He said the project had yielded the most reliable time line yet for these dynasties, the earliest of which is said to date back more than 4,000 years.</i><br /><br /><i>He also said scholars in disciplines including archaeology, astronomical history, early manuscripts and the parsing of inscriptions on bronze vessels and divination bones had made many new discoveries and synthesized the sketchy evidence. Project leaders hope their newly detailed dating of early emperors will soon enter the world&#39;s textbooks and museum exhibits.</i><br /><br /><i>Tonight the report was featured in television news and newspapers, which ran headlines like &#39;Chinese History Pushed Back 1,229 Years.&#39;</i><br /><br /><i>But the project has been questioned by other scholars, here and abroad, who say its authors, driven by a political urge to document Chinese culture&#39;s primacy and uniqueness, have tried to leapfrog the slow, disorderly march of science. Project researchers resolutely deny anyone told them what to find, but </i><b><i>critics say they have forced an illusion of consensus in some cases.</i></b><br /><br /><b><i>&#39;There&#39;s a chauvinistic desire to push the historical record back into the third millennium B.C., putting China on a par with Egypt,&#39; said Edward L. Shaughnessy, a historian at the University of Chicago. &#39;It&#39;s much more a political and a nationalistic urge than a scholarly one.&#39;</i></b><br /><br /><i>Several Chinese historians and archaeologists have argued with the project leaders or refused to take part, said one scholar, who spoke on condition of anonymity. But today Mr. Li and others were adamant about the conclusions&#39; having been drawn fairly and cautiously, through a form of &#39;academic democracy.&#39;</i><br /><br /><i>The research enterprise was begun in 1995 by Song Jian, a senior official overseeing China&#39;s science policies.</i><br /><br /><b><i>&#39;A history without chronology is no history at all,&#39; Mr. Song wrote in a newspaper article this fall. &#39;It can only be called rumor or myth.&#39;</i></b><br /><br /><i>Li Tieying, a member of the Communist Party Politburo as well as president of the Chinese Academy of Social Sciences, has said: &#39;The project is an important scientific study and also has major political and cultural significance. Explicating Chinese civilization will necessarily strengthen our national cohesiveness and raise our national self-confidence and pride.&#39;</i><br /><br /><i>From the outset, the scholars acknowledged that they were unlikely to stretch history all the way back to the fabled Yellow Emperor. Instead they set out to enrich knowledge of three dynasties that they say reigned from about 2070 B.C., the beginning of the reputed Xia Dynasty, to 771 B.C., when the Zhou Dynasty fell. All are believed to have been agricultural societies with elaborate rituals of divination and sacrifice.</i><br /><br /><b><i>Evidence for the very existence of a Xia Dynasty remains slender. Though it is mentioned in history books centuries later, the name does not appear on archaeological finds from the period, or even in inscriptions from the centuries that followed its supposed demise. Some Western scholars feel it remains more legend than fact.</i></b><br /><br /><i>Still, </i><b><i>the project takes as already proven the existence of the Xia as a precursor empire</i></b><i> to the better-established Shang, pointing to a site uncovered at Erlitou, Henan Province, in 1959 as the probable capital.</i><br /><br /><i>Today the presumed palace walls at Erlitou lie under farmland. After the site was excavated and documented, it was covered for protection and local farmers moved in. A few hundred yards away, trained villagers dig in a related excavation, and a few yards beyond them, new buildings are being built.</i><br /><br /><i>The report today concludes that the Xia ruled from around 2070 B.C. to 1600 B.C., though it does not try to date the reign of each Xia ruler.</i><br /><br /><i>The scholars were able to muster new findings about the Shang Dynasty, which is believed to have reigned in the Yellow River area centered on present-day Henan and Shaanxi Provinces for the following 550 years, but fixed rulers&#39; exact dates only for the later Shang era, after 1300 B.C.</i><br /><br /><i>Early in the 20th century, many scholars doubted traditional claims of a Shang Dynasty, too. But then discoveries of &#39;oracle bones&#39; -- animal bones bearing inscriptions used to decipher the future -- proved its existence. There was also evidence that the Shang engaged in human sacrifice and may have held slaves.</i><br /><br /><i>By all accounts, the project has also helped clarify the formerly confused chronology of the early Zhou Dynasty, which produced beautiful bronzes seen in world museums.</i><br /><br /><i>But one of the most contentious and important questions involves the timing of the Zhou conquest of the Shang. </i><b><i>The event&#39;s date is vital to chronology before and after, but scholars must sort out contradictory signs from early inscriptions, reports of Jupiter&#39;s position in the sky and accounts in ancient documents of disputed authenticity.</i></b><br /><br /><i>The project&#39;s decision to settle on 1046 B.C. as the probable date for the fall of the Shang -- the date became widely known when draft reports were circulated -- angered some scholars.</i><br /><br /><i>One historian, Jiang Xiaoyuan of Jiaotong University in Shanghai, complained in an interview that his published conclusions about the conquest year, developed under project auspices, had been set aside by project leaders to support the date they preferred. But Mr. Jiang also acknowledged that the project had brought fresh funding and modern equipment to starved disciplines and had &#39;vastly advanced the study of China&#39;s ancient history.&#39;</i><br /><br /><i>David S. Nivison, emeritus professor of Chinese studies at Stanford University, said he was outraged by the selection of 1046, which he said contradicted research he had submitted to the project. His favored date is 1040, but </i><b><i>whoever is right, he says, insisting on a single date now is intellectually dishonest.</i></b><br /><br /><b><i>&#39;It&#39;s going to be a mess,&#39; Mr. Nivison said, adding that international scholars were likely to tear the report &#39;to pieces.&#39;</i></b><br /><br /><b><i>&#39;These are going to be seen as the dates pronounced to be correct by the Chinese government,&#39; Mr. Nivison said. &#39;For the government of China to be in this position may poison scholarship for generations.&#39;</i></b><br /><br /><i>Mr. Li, the project director, replied today: &#39;Our findings are, I believe, the best that can be obtained at present. That doesn&#39;t mean there can&#39;t be further progress.&#39;&quot;</i><br /><br />China&#39;s experiences with chronology are in a general sense representative of a larger global confusion and disagreement that suggests a multi-faceted problem.<br /><br />For instance, the issue of whether or not the Xia Dynasty is real or mythical is in fact a common problem -- leaving no firm root to anchor a subsequent series of events in.  The West is not immune to this problem.  In making the case for David Talbott&#39;s own &quot;Saturnian&quot; reconstruction, he convincingly cites <b>the widespread problem that all of the kings were seeking to cast themselves as an incarnation of a mythic prototype of warriors and warrior-kings</b>.<br /><br />Thus, we are confronted with a fundamentally confusing situation for historians seeking to establish deep human chronology: <b>without a firm grasp of the subject of mythology, one expected pattern for these chronologies is that they will fail to properly discriminate between true historical and mythological figures.</b><br /><br />In the light of that problem, how can a scientist with any confidence then apply some sort of statistical analysis to the data?  The nature of the information appears to limit the set of tools we can apply -- suggesting the possibility that we may never have a confident answer to some of these questions.<br /><br />Clark Whelton raises yet another issue to be aware of in this domain: The Saturnists, he argues, use mainstream scholarship to rebut ideas they disagree with, while simultaneously defending their own revised histories of the solar system and related topics against mainstream criticism.  In other words, they think <i>&quot;mainstream history refutes your work, but not mine.&quot;</i>  Note, of course, that the mainstream is dismissive of both sides of the debate.<br /><br />We have a bit of a dilemma with this chronology business.<br /><br />&quot;Wie lange waehrte das Erste Jahrtausend?&quot;<br />(&quot;How Long Was the First Millennium?&quot;)<br />by Gunnar Heinsohn<br /><br />XIX. ENGLISH SUMMARY<br />ANTIQUITY == LATE ANTIQUITY == EARLY MIDDLE AGES:<br /><br />The Discovery of Carolingian Living Quarters in Aachen and Europe<br /><br /><b><i>&quot;From the 8th to 10th centuries CE, Aachen resembles a bombed-out wasteland, in the centre of which stand, in ghostly solitude, the remains of a Carolingian government district. Beyond these regnal buildings [Palatinate-Ensemble], Aachen has no archaeology at all for its Early Medieval period of the 700s-930s CE. Roads, residential areas, plazas, barracks, stables, workshops, monasteries, churches, aqueducts and thermal baths -- even latrines and sewers -- are missing.</i></b><br /><br /><i>Because of this missing archaeology, dissidents who support the ideas of Heribert Illig deny the existence of the Carolingians, along with the centuries of the Early Middle Ages in which they supposedly lived. </i><b><i>Mainstream scholars reply, inter alia, by pointing to small finds, and to the Europe-wide excavation of Carolingian coins.</i></b><br /><br /><b><i>Grudgingly, however, the experts concede that Aachen does indeed lack an Early Medieval urban infrastructure.</i></b><i> In turn, the Illig school does not put into question the existence of the Carolingian government buildings, but moves them up to the 12th century with Emperor Barbarossa, who supposedly did not only permit a Romanesque Four-Ship Church of St. Foillan for Aachen’s citizens but, at the same time, had the 16-sided Hexadecagon and the other Roman-style Palatinate buildings erected for himself right next to it.</i><br /><br /><b><i>Local experts balk at moving the Early Middle Ages forward into the Romanesque period of the High Middle Ages. They emphasize, time and again, the consistently antique appearance (1st-3rd c.) of not only the Palatine buildings, but also of their architectural sculpture, floor tiles, bronzes, and other decorations down to pigments and glass pastes. The 1st-3rd c. style of these materials, they insist, definitely rules out a High Medieval interpretation (12th c.).</i></b><br /><br /><i>Where the dissidents move the time of the Palatinate buildings from the 9th up to the 12th century, the experts do not move them down to the 2nd, the century in which they identify the ancient models from which, 700 years later, the Palatinate buildings were supposedly copied. With all due right, the experts keep them in the 9th century because this stratum is contingent with Aachen’s 10th/11th stratum of the High Middle Ages. Therefore, they insist, the 9th century constructions must be copies of Roman buildings of the 2nd century. </i><b><i>The two opponents, thus, are a millennium apart for the same Roman-looking style of the Palatinate-Emsemble.</i></b><br /><br /><i>The possibility that they are looking at original buildings from the 2nd century that are merely dated some 700 years too early is strongly rejected by both parties. Building historian Volker Hoffmann, who also rules out a Romanesque 12th c. date for Charlemagne’s Palatinate, moves the hexadecagon from Early Medieval times down to Late Antiquity (4th-6th c.). But he stops short of Antiquity.</i><br /><br /><i>Although the dissidents and the experts are irreconcilable, they still share a lot of common ground. Their strongest similarity is that they agree there are no genuine Early Medieval urban structures beyond Aachen’s Palatinate-Ensemble. To the Illigists, this is proof that history books must be changed. To mainstream scholars, it is a puzzle that causes them embarrassment, even despair.</i><br /><br /><i>Another area where the opposing parties agree is in their deep conviction that the missing Early Medieval living quarters of Aachen as well as other European cities must never be sought in the 1st-3rd centuries, the era in which the city has rich layers with private and public buildings of Antiquity to which nobody lays claim. Such an idea sounds too implausible because chronologists have inserted about 700 years between the two eras. It was quite all right, the experts concede, that the Palatine structures of the 9th century were built in the style of the 2nd century, and all right that even the decorations and pigments for these buildings were inspired by 700-year-old architecture. However, they claimed, the building parts with a 1st-3rd c. appearance were actually spolia, ancient stonework re-used 700 years later. </i><b><i>The discovery that the architectural sculpture by no means consists of spolia, but was newly manufactured in the 9th c., was countered with the theory of &#39;pseudo-spolia.&#39; Pseudo-spolia have the antique imperial look, it is said, but are made new to avoid the shabbiness of resorting to second hand dealers in ancient stonework. Therefore, it is believed, construction specialists all over Europe had been trained to recreate 700-year-old techniques, forms and paint, right down to its chemical fingerprint. Yet, in the written sources there is no hint whatsoever of such unprecedented endeavours.</i></b><br /><br /><i>Mainstream scholars have another serious problem. The sources, that are silent on Charlemagne’s alleged revival of Roman culture, clearly state that he chose Aachen as his capital because of its hot springs, which were already famous in the time of Emperor Augustus (31 BCE to 14 CE). Here again mainstream must concede complete agreement with their nemesis, the Illig school. For both sides, </i><b><i>there are no baths in Aachen built in the Early Middle Ages. A rich 2nd c. Roman bath (&#39;Buechel-Therme&#39;), however, was found in close walking distance to Charlemagne’s 9th century Palatinate.</i></b><i> Mainstream does not turn that structure into a fake ancient bath built from pseudo-spolia. Illig is careful not to re-date it to the 12th c. though it looks no less Roman than the Palatinate buildings he has already moved to the High Middle Ages. Again, mainstreamers and dissidents agree: this 2nd c. Roman bath must never be considered as a candidate for the vividly described thermal pleasures of the early medieval rulers and their entourage.</i><br /><br /><i>And yet, over and over again, the archaeologists excavating Aachen see the possibility that 700-year-old roads, sewers, latrines or even residential buildings from the city’s 2nd century Roman period were still usable in the 9th century. For the Carolingian cities of Zurich (Switzerland) or Spoleto (Italy) -- and even for all of Bavaria -- </i><b><i>it is claimed explicitly that from the 700s to the 930s people still lived in dwellings from the 1-230 period. Similar claims are made for Rome, Ephesos or Syracuse.</i></b><i> Transposed into the 2nd millennium, one would have to imagine Europeans from 1700-1930 living in unaltered houses from 1000-1230. Back in the 1st millennium, such durability -- after the West Roman Empire had been felled in the 3rd c. with the East Roman following suit in the 6th c. -- would be no less than a miracle. </i><b><i>On top of that, in the 230s a layer of dark mud covered the flattened Aachen of Roman provenance like a shroud. How, then, could the buildings of the 1-230s period still have been in perfect shape in the 700-930 period?</i></b><i> That would be possible only if &#39;1-230s&#39; and &#39;700-930s&#39; are simply different chronological labels for the same archaeological 230-year period that immediately precedes the High Middle Ages starting after the Tenth Century Collapse of the 930s.</i><br /><br /><b><i>The author -- based on his finding that CE sites everywhere have, at best, enough vertical stratigraphic layers for only some 300 of the required 1000 years -- proposes, since autumn 2013, a synthesis, not only for Aachen but for the entire dispute over the Early Middle Ages. Against the inability of mainstream history to defend the very existence of the Early Middle Ages with substantial living quarters, streets, squares and water systems, and against the elimination of early medieval history by dissidents, the author takes the position that the Early Medieval buildings of Aachen’s Palatinate look ancient because they really belong to Antiquity, which, the author holds, should not be placed in the 1st to 3rd centuries but -- in accordance with the evidence of stratigraphy -- in the 8th to 10th centuries.</i></b><br /><br /><i>Therefore, Aachen’s 1st-3rd c. Roman quarters, in combination with the one Early Medieval block of the Palatinate-Ensemble (8th-10th c.), form the desperately searched-for early medieval city in all its complete splendour known from the sources. The geographical deviation of the buildings in the Palatinate-block along the course of Roman roads is due to the program of the Franks to establish a Christian seat of government within the city that they managed to take over during the 2nd century of Antiquity (stratigraphically equaling the 9th c. of the Early Middle Ages). Therefore, Roman buildings are (in the 2nd=9th c.) demolished and built over, in east west orientation, only in the one block chosen by the Franks to become their administrative centre laid out to please their Christian deity.</i><br /><br /><i>The buildings in this peculiar block are indeed younger than the urban structures in the Roman quarters around it. Due to their construction there are more layers in the Palatinate block than found in the rest of Roman Aachen. Still, these new structures, too, belong to Antiquity (1st-3rd c.), i.e. are Frankish-Roman. Yet, they represent its final 2nd/3rd c. stage which, at the same time, belongs to the final stage of the Early Middle Ages of the 9th/10th century. Thus, Aachen’s Christian regnal quarter of the 9th/10th c. (Palatinate-Ensemble) belongs to Antiquity’s 2nd/3rd c., as well as to Late Antiquity’s 5th/6th century, with all of them together stratigraphically belonging to the 9th/10th century.</i><br /><br /><i>In countless places, sites dated to the 3rd (or 6th) century transition directly into the High Middle Ages during the 10th century (ca. 930s). Volker Hoffmann’s movement of Aachen’s octagon dome into Late Antiquity is accurate to the extent that it unknowingly reveals that Antiquity and Late Antiquity are not only culturally indistinguishable but stratigraphically, i.e., chronologically parallel. In turn, mainstream history has every right to leave the Palatinate buildings in the 8th-10th c. Early Middle Ages because -- however unimaginable it may be to them -- Antiquity, Late Antiquity and the Early Middle Ages are parallel in the time span of the latter. Stratigraphically, this parallel chronology of periods, set 700 years apart, has been seen by many an archaeologist, but it is not consciously perceived.</i><br /><br /><i>Since the 1940s, exceptional thinkers such as Richard Krautheimer (1897-1994) have tried to make sense of a 1st millennium with stalled evolution but a triple-Antiquity by defining the 4th century of Late Antiquity as a renaissance of Antiquity’s 1st century that undergoes an additional renaissance in Early Medieval 8th century. </i><b><i>The indistinguishability of ground floors, building materials, color pigments and sophisticated glass pastes etc., thus, had been brought about by larger than life rulers who were determined to enforce these renaissances to bring back the Roman Empire’s superiority.</i></b><i> They even managed to reinstall slave-markets -- seen as culprits of the Third Century Crisis as well as the 6th century demise of Late Antiquity -- for a third time. Yet, such mega-characters of the Early Middle Ages would have had to be unbelievably powerful in order to bring evolution to a standstill. Even lawmaking as well as major languages like Hebrew, Greek or Latin -- including poetry -- they supposedly could force into petrification. </i><b><i>In the development of arms, too, they insisted on absolute stagnation. The spatha - designed in the last century BCE -- becomes the standard sword of Antiquity. It returns in Late Antiquity, and is in full swing again during the Early Middle Ages as the so-called Viking or Carolingian sword. Could those leaders really have convinced all humans from Scandinavia to Bulgaria -- the territories analyzed in this study -- to adopt lifestyles, languages, and architectural structures that were 700 years out of date</i></b><i>, while, at the same time, hiding forever all traces of what must have been the largest secret operation of all time? And yet, mainstream scholars can only defend their view of 1st millennium history by assuming that such a gigantic conspiracy had been repeated first in Late Antiquity, and a second time in the Early Middle Ages. Since the High Middle Ages, however, they no longer see such godlike creatures at work, and, therefore, see no fourth coming of Antiquity. Only Illig’s dissidents opt for a fourth Roman Antiquity -- after Antiquity, Late Antiquity, and the Early Middle Ages -- by moving Aachen’s Palatinate block with all its definitely Roman-looking splendour to the High Middle Ages of the 12th century.</i><br /><br /><i>Yet, whatever their preferred conspiracy theories, our combattants fail to place Antiquity right before the High Middle Ages (beginning in the 10th c.) where it stratigraphically belongs. That places Aachen’s huge Roman bath (&#39;Büchel-Therme&#39;) in the Early Middle Ages, too. Therefore, the source references regarding the Franks’ choice of Aachen as their European capital because of its hot springs can now be defended against being discarded by the Illig school, or shamefully ignored by mainstream.</i><br /><br /><i>Before abandoning the sources as unreliable, thereby ending up in bed with the dissidents once again, the Aachen experts should look for the alleged non-discoverability of Early Medieval urban structures in the rich material evidence of the city’s Antiquity strata. If, in Aachen as well as Europe, only the layers of Antiquity (&#39;1st to 3rd&#39; centuries) can provide the long-desired urban evidence for the Early Middle Ages (8th to 10th centuries), then directly above Aachen’s early 3rd (equals early 10th) century Roman ruins are the massive, dark mud layers of the 930s (=230s), where primitive pit houses sunk into them form the depressive beginning of the High Middle Ages after the 930s.</i><br /><br /><i>Because of the parallel existence of Antiquity/Late Antiquity/Early Middle Ages, Aachen’s excavators will soon realize that the Roman military camp -- discovered under Aachen’s Market Square on August 25, 2015 -- cannot be dated, as was hastily done, to Late Antiquity in order to eventually have buildings for Aachen’s 4th-6th centuries. This would require 1st-3rd c. strata beneath that small Roman castrum. Since former excavations have revealed only 1st-3rd c. small finds in its location, the author bets that buildings for these 300 years will not be found under the camp. Its site will provide no more building layers than the rest of 1st millennium Aachen, i.e. a maximum of 300 years. The following schematic overview of the 1st millennium (excluding 600-700) shows two options for its historical order with the left side representing the textbook view whereas the right side stands -- cum grano salis -- for the stratigraphic approach employed by the author:&quot;</i><br /><br />[See Graphic]<br /><br /><i>&quot;Stratigraphic evidence for six of the most thoroughly discussed cities in this text -- Aachen, Kalisz, Rome, Athens, Byzantium, Jerusalem, and Samarra -- is hereby presented in schematic overviews to facilitate objections, and to underline the claim that each site experiences just one devastating destruction during the 1st millennium CE that, in each case, is the same that brings about, at the beginning of the 10th c. CE, the dramatic shift from the Early Middle Ages to the High Middle Ages.&quot;</i><br /><br />[See Graphic]<br /><br /><b><i>&quot;Of course, an analysis that claims to obsolesce nearly one thousand years of historiography - beginning with Michael Psellos (1018-1078) -- on the course of 1st millennium AD will face all possible means of refutation. Therefore, the central thesis of this study -- the contemporaneity of Antiquity (1st-3rd c. CE), Late Antiquity (4th-6th c. CE), and Early Middle Ages (8th-10th c. CE) during the time span of the latter -- is laid out in a way that makes it an easy target for falsification by experts and laymen alike. Of the roughly 20,000 sites of Roman culture (urban spaces and villae rusticae), as well as of the many thousand sites north of the Danube, east of the Rhine, and all over Scandinavia, this hypothesis does not need thousands or hundreds of examples to disprove it. A single site whose stratigraphy illustrates textbook history of the first millennium CE will lay this book’s hypothesis to rest.</i></b><i> For such a site to do this demolition job, the following sequence of building strata is required:</i><br /><br /><i>Stratigraphy confirming textbook periodization of the 1st millennium CE that is required to falsify this study’s reconstruction of the same history.</i><br /><br /><i>(7) Destruction traces for the 10th CENTURY COLLAPSE.</i><br /><br /><i>(6) Building strata with distinct evolutionary architecture and art for the EARLY MIDDLE AGES, with the expected internal developments from the 8th to 10th c. CE.</i><br /><br /><i>(5) Architectural evolution for the INTERMEDIATE time span of the 7th c. CE.</i><br /><br /><i>(4) Destruction traces for the FALL OF LATE ANTIQUITY in the 6th c. CE.</i><br /><br /><i>(3) Building strata with distinct evolutionary architecture and art for LATE ANTIQUITY, with the expected internal developments from the 4th to 6th c. CE.</i><br /><br /><i>(2) Destruction traces for the CRISIS OF THE THIRD CENTURY between the 230s and 280s CE.</i><br /><br /><i>(1) Building strata of art and architecture for ANTIQUITY (1-230s CE), with the expected internal evolution from Emperor Augustus to Emperor Alexander Severus.</i><br /><br /><i>In actual fact, falsification will already be achieved by pointing to a single site that exhibits an evolution of super-imposed building strata for the three periods, with some 600 years between (1) and (3). A stratigraphy with the evolution of super-imposed buildingstrata for the four periods (3) to (7), running from the late 3rd to the early 10th c. CE, would do the same job.&quot;</i><br /><br />There are places to go for more information on this debate.  In particular, see Ev Cochrane&#39;s rebuttal to Heinson&#39;s chronology here:<br /><br /><a href=\"http://www.maverickscience.com/history.htm\" class=\"ot-anchor\">http://www.maverickscience.com/history.htm</a><br /><br />There is some back-and-forth debate occasionally published here, viewable with a yearly subscription:<br /><br /><a href=\"http://www.sis-group.org.uk/\" class=\"ot-anchor\">http://www.sis-group.org.uk/</a><br /><br />And there is the mentioned coverage at Q-Mag as well:<br /><br /><a href=\"http://www.q-mag.org/gunnar-heinsohns-latest.html\" class=\"ot-anchor\">http://www.q-mag.org/gunnar-heinsohns-latest.html</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Velikovsky's Chronology","summary":"Of his Many Claims, Velikovsky Famously Challenged the \"Gold Standard\" Egyptian Chronology / While his Claims on Venus and Electromagnetism are Perceived by Some as Groundbreaking, there are Reasons to be Cautious about his Proposed Revisions to Chronology / Historian Clark Whelton Provides a Deeply Personal Account of his Own Misgivings that Velikovsky's Chronology Seems Rooted in Fundamentalism / There Might Not be a Better Example of the Risk of Rejecting Everything a Man has Said / In Science, we Run into Trouble when we Stop Asking 'Why?'","image":"https://lh3.googleusercontent.com/-9iWJS2u7DwQ/WF2L3T-sZ0I/AAAAAAAAJ6M/bOV7Cv-HG1QDg0o4M0nErF9espFLtq-GACJoC/w1800-h2520/velikovskys-chronology-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/5wuThq7vd98","publishDate":"2016-12-23T20:41:50.258Z","updateDate":"2017-01-03T01:33:01.552Z","text":"<b>Velikovsky&#39;s Chronology: Of his Many Claims, Velikovsky Famously Challenged the &quot;Gold Standard&quot; Egyptian Chronology / While his Claims on Venus and Electromagnetism are Perceived by Some as Groundbreaking, there are Reasons to be Cautious about his Proposed Revisions to Chronology / Historian Clark Whelton Provides a Deeply Personal Account of his Own Misgivings that Velikovsky&#39;s Chronology Seems Rooted in Fundamentalism / There Might Not be a Better Example of the Risk of Rejecting Everything a Man has Said / In Science, we Run into Trouble when we Stop Asking &#39;Why?&#39;</b><br /><br />Immanuel Velikovsky carefully researched volumes of original ancient texts, comparing the historical events and characters described by their authors. His realignment was the result of matching the features of story and plot among world texts.<br /><br />Current historical timelines of the period BC depend on the accepted &quot;gold standard&quot; of Egyptian Pharaoh dynasties. Velikovsky challenged the accuracy of the timeline, which is quite arbitrarily constructed in many instances.<br /><br />He used the Ipuwer papyrus to realign the catastrophic events in Biblical Exodus with reports in Egyptian history of similar chaos that in the standard version contains no reference to equivalent events of the period. Velikovsky researched Mayan, Hindu and Chinese history to draw similar cross-references.<br /><br />He equated Bathsheba with Cleopatra, thus removing the mysterious identity of the former by losing the 400 year duplicated period. Likewise, he chops out the more recent Dark Ages as being filler to make up for the same invented period based on a misinterpretation of Egyptian records.<br /><br />All that said, it&#39;s important to emphasize that what is being attempted here is a seriously complex endeavor.  The construction of reliable chronologies is at the very edge of our capabilities, because as with dating, we lack any firm signpost to which the &quot;true facts&quot; are moored.  At a certain point back, you&#39;ve got the Bible, a scatter of other documents, and you&#39;ve got archaeological artifacts not always firmly rooted in a particular time period.  We should perhaps not expect to get this right on the first, or even second, try -- and we should seek to map out all of the competing chronologies.<br /><br />Let&#39;s start with a sample of Velikovsky&#39;s writings on this subject ...<br /><br /><b>Immanuel Velikovsky on the Egyptian Chronology</b><br /><br /><a href=\"http://www.varchive.org/ce/c14.htm\" class=\"ot-anchor\">http://www.varchive.org/ce/c14.htm</a><br /><br />[...]<br /><br /><b><i>&quot;The sustained effort of radiocarbon researchers to find support in Egyptian chronology, and their reliance on that chronology, is fundamentally a mistake. As I tried to show in Ages in Chaos, the Egyptian chronology is basically wrong. I drew the attention of Libby to this fact in my letter of October 7, 1953, and I sent him a copy of Ages in Chaos; his answer was that he is not at all learned in ancient history; thus he continued to rely on what is unreliable.</i></b><i> He cannot be blamed for it because in historical circles the conventional chronology is still the accepted dating in absolute and in comparative sense ...</i><br /><br /><i>Now if the historical basis of radiocarbon studies fails so completely, many conclusions drawn and much data left unpublished require reconsideration. From some correspondence that originated at the Metropolitan Museum of Art, I have concluded that when Libby first asked for specimens, he received not only those dating from the Old and Middle Kingdoms, but also from the New Kingdom -- </i><b><i>but nothing ever was published of those early tries on New Kingdom specimens.</i></b><i> A similar situation concerns more recently tested short-living organic material from the tomb of Tutankhamen.</i><br /><br /><b><i>After many efforts (from 1952 to 1963) to have the New Kingdom of Egypt tested in a systematic way I succeeded in having three little pieces of wood from the tomb of Tutankhamen handed over</i></b><i> by the Laboratory Director of the Cairo Museum to Mrs. Ilse Fuhr of Munich, who was directed by me to send them to Dr. Elizabeth Ralph of the University of Pennsylvania Laboratory. Two of the pieces were from the comparatively short-lived thorn plant, Spina Christi, and one from the long-living Cedar of Lebanon. The three small pieces were processed together, since a test requires ca. 30 grams (I ounce) of material. </i><b><i>The result was - 1120 ± 52</i></b><i> (or following Libby’s half life of C14, - 1030 ± 50). </i><b><i>Now the accepted chronology has Tutankhamen dying in - 1350; my reconstruction has him entombed in ca. - 830</i></b><i>. According to Dr. Iskander Hanna of the Cairo Museum, the wood was from 30 to 50 years dried before being used for funerary equipment. The Lebanon Cedar would not have been cut as sapling -- the tree reaches thousands of years of age. The sample could have been from inner rings of a trunk. Dr. E. Ralph confirmed to me on March 5, 1964, that tree rings, when carbon dated, show the date of their formation, not of the year the tree was felled. </i><b><i>I wrote to her on March 2, 1964, suggesting that if short-living material (like seeds, papyrus, linen or cotton) should be subjected to tests from the tomb of Tutankhamen, most probably the result will show &#39;ca. -840.&#39;</i></b><i> [2]</i><br /><br /><b><i>In spring, 1971, or seven years later, the British Museum processed palm kernels and mat reed from the tomb of Tutankhamen. The result, according to Dr. Edwards, Curator of the Egyptian Department of the British Museum, was -899 and -846 respectively. [3] These results were never published.</i></b><br /><br /><b><i>These cases make me appeal that all tests, irrespective of how much the results disagree with the accepted chronological data, should be made public.</i></b><i> I believe also that the curiosity of the British Museum Laboratory officials should have induced them to ask for additional material from the Tutankhamen tomb instead of discontinuing the quest on the assumption that tested material was contaminated. The tomb of Tutankhamen had not been opened since soon after the entombment. It is dry -- water did not percolate through its roof or walls ...</i><br /><br /><b><i>Now we see that not only were the warning signals that Libby offered with his method disregarded, but also an unearned reliance on the accepted version of ancient history has caused much stumbling in the dark, more and more tests of diminished value, and a maze of findings, with many undisclosed results of tests, wrong deductions and much exasperation that mark the first 20 years of application of Libby’s most imaginative method.&quot;</i></b><br /><br />[...]<br /><br />For those not well versed in the complexities of the Egyptian Chronology, it can be difficult to argue with -- or even understand -- everything that is in play here.  Immanuel Velikovsky is asserting that a series of technical errors have been made here.  We follow along with his claims, realizing that he has always had detractors.<br /><br />But, along the way, we might miss something important: When we question the history of the planet Venus or the role of electromagnetism in the planetary sciences, we are asking questions which are fundamentally matters of physics.  But, can we treat debates over chronology in precisely the same manner?<br /><br />Egyptian Chronology is history.  With the use of radiocarbon dating, it also becomes a matter of science.  But, it&#39;s also culture and religion -- and we must never forget that Velikovsky, like all human beings, is a <b>product</b> of culture.  We exist <b>within</b> culture, and we see the world <b>through</b> its lens.  We cannot so simply extract ourselves from our own cultures -- just as we cannot, without assistance, look at our own retinas.<br /><br />Clark&#39;s personal reactions -- noted some years ago in Canada -- to Velikovsky&#39;s chronological tinkerings reminds us of the true complexity we are dealing with when asking the big questions in science.  We must question everything a man says, and always be on guard for motives that not even he may be savvy to.  Nobody gets a free pass.  We must critically analyze all of the reasons for our own beliefs.  If we can get that process right, then we stand a chance of dealing with the remaining complexities.<br /><br /><a href=\"http://saturniancosmology.org/files/velikovsky/whelton.txt\" class=\"ot-anchor\">http://saturniancosmology.org/files/velikovsky/whelton.txt</a><br /><br />Society for Historical Research<br /><br />Velikovsky, Fundamentalism, and the Revised Chronology<br />by Clark Whelton<br /><br /><i>&quot;In the summer of 1977 I spent a day at Immanuel Velikovsky&#39;s home in Princeton. We sat in the living room and talked about history. Peoples of the Sea had just appeared in print. Velikovsky was brimming with optimism. By following volume I of his &#39;Ages in Chaos&#39; series with volume V, and leaving the middle three installments till last, he intended to establish and secure the perimeter of his revised chronology. There were both advantages and risks to this method. Jumping to the end of the series allowed Velikovsky to champion a radically lowered date (4th century BCE) for Ramses III without having to demonstrate a logical sequence of events beginning where volume I left off (the end of the 18th dynasty, ca. 840 BCE). The principal risk was that critics would be loath to accept such a startling displacement of Ramses III unless the necessary sequence was provided.</i><br /><br /><i>Also, it seemed to me that Velikovsky was risking the same kind of trouble that plagues the chronology of ancient Greece. The history of Greece is written in opposite directions. We follow the trail of evidence from classical times backward until it fades out in the 7th and 8th centuries BCE. At the same time we track the development of the Mycenaean period (conventionally anchored in the 2nd millennium BCE by archaeological ties to the 18th dynasty) forward until it fades out in 12th and 11th centuries BCE. Because these converging histories fail to converge, the resulting gap is called a &#39;dark age.&#39; It occurred to me that Velikovsky, by sandwiching his revision between the unbending bookends of volumes I and V, might be inviting similar problems. As it turned out, a &#39;dark age&#39; does exist in the center of the Ages in Chaos series. Volume III, The Assyrian Conquest, still hasn&#39;t seen the light of day.</i><br /><br /><i>In the summer of 1977, however, I shared Velikovsky&#39;s optimism. </i><b><i>Nevertheless, I couldn&#39;t help wondering why, instead of jumping forward, he didn&#39;t simply continue down the trail that began so promisingly in volume I.</i></b><i> Using Biblical chronology as a guide to the reconstruction of ancient history had apparently served him well. Hadn&#39;t the true identities of Shishak and the Queen of Sheba emerged from the mists of history? I&#39;d come to accept Velikovsky&#39;s method completely, and believed what he said in Ages in Chaos. On page 99, after carefully showing that the histories of Israel and Egypt were out of sync by some 600 years, he wrote:</i><br /><br /><i>&#39;Whose history is to be moved by these centuries? Is it possible to place (King) David in the sixteenth century before this era? No student of ancient history will see the slightest possibility of altering the history of the kings of Jerusalem by a single century, much less by six, without disrupting all established data and concepts. The Biblical annals record the succession of the kings of Judah and of Israel, king after king, and give the years of their reigns. If there are, here and there, some discrepancies or difficulties in the double account of the kings of Judah and Israel, they are of an entirely different dimension, and may amount at most to one or two decades, but not hundreds of years.&#39;</i><br /><br /><b><i>In spite of this persuasive statement and the apparent success of volume I, it still wasn&#39;t clear why Velikovsky had jumped ahead 500 years in Peoples of the Sea. Had the Biblical guide gone astray? As we discussed this question, I mentioned something that had been said to me by an historian from New Jersey.</i></b><br /><br /><b><i>&#39;You&#39;ve got to be careful with Ages in Chaos,&#39; he cautioned. &#39;It&#39;s obvious that Velikovsky is personally involved with the history of Israel. He&#39;s very much aware of the implications of his work for modern Israel. I think he has a special interest in having things come out a certain way.&#39;</i></b><br /><br /><b><i>Velikovsky&#39;s eyes blazed with anger. &#39;In other words,&#39; he snapped, &#39;this man is calling me a liar.&#39; I tried to explain that I didn&#39;t think such an accusation was being made, but Velikovsky got up, walked across the room, and picked up a Bible. Barely in control of his emotions, he said: &#39;I will read you something.&#39; He turned to Jeremiah 52, the story of Israel&#39;s rebellion against Nebuchadnezzar. As King Zedekiah tried to escape from besieged Jerusalem, he was captured near Jericho and taken before Nebuchadnezzar at Riblah. &#39;The leaders of Judah were killed on the spot,&#39; Velikovsky said. &#39;But Zedekiah, the last Hebrew king to sit on the throne of David, was not so lucky. His sons were slaughtered in front of him, and then his eyes were put out. They took him alive to Babylon, so that for the rest of his days his last sight would be his children dying in agony.&#39; Velikovsky&#39;s eyes filled with tears. Overcome by empathy, he had to leave the room. When he returned he pointed to the Bible and said, &#39;This is the history of my people. About the Scriptures I do not tell lies!&#39;</i></b><br /><br /><i>Later we went to a diner on Route 1. While we ate lunch Velikovsky discussed the current political situation in Israel. A debate was raging over whether or not to return the Sinai to Egypt. I asked his opinion. &#39;They should return part, but not all,&#39; he said.</i><br /><br /><i>&#39;Why do you say that?&#39;</i><br /><br /><i>&#39;You have read Ages in Chaos. You know that the historical borders of the Israel of David and Solomon were the Euphrates in the north and El Arish in the south. We are in El Arish now. To give up historical borders would be a mistake.&#39;</i><br /><br /><b><i>The words of the historian from New Jersey came back to me at that moment. Clearly, Velikovsky was personally involved in both the ancient and modern history of Israel.</i></b><i> But if he had a hidden agenda I couldn&#39;t see it. His revised chronology was based on solid research and established principles of scholarship. His heavy reliance on Biblical history made sense because the Bible was just as he described it, a generation by generation account of ancient times, the only such text in existence. Abandoning the Bible as a guidepost would indeed &#39;disrupt all established data and concepts.&#39; At the time it didn&#39;t occur to me that Velikovsky was methodically disrupting all established data and concepts in Egyptology, the keystone and fulcrum of ancient history. </i><b><i>Under Velikovsky&#39;s revision, the Bible assumed this pivotal role.</i></b><br /><br /><i>Several weeks later I interviewed Cyrus Gordon, the great philologist and professor of Hebrew studies, in his office at New York University. I asked Dr. Gordon about Velikovsky&#39;s suggestion (A.in C., p. 69) that the unusual Hebrew phrase &#39;evil angels&#39; (Psalms 78:49) should be translated &#39;king shepherds,&#39; and used as evidence that the Biblical Amalekites were the Hyksos. Gordon called Velikovsky&#39;s idea &#39;a brilliant emendation,&#39; but expressed strong doubts about the overall accuracy of his revised chronology. First, he questioned Velikovsky&#39;s identification of the &#39;Prst,&#39; who fought against Ramses III, as the Persians. Gordon supported the accepted view that the &#39;Prst&#39; were Philistines.</i><br /><br /><i>&#39;The distinctive &#39;Prst&#39; helmet is pictured on the Phaistos disk from Crete,&#39; he said. The disk is conventionally dated to the 2nd millennium BCE. &#39;But there are larger issues. </i><b><i>You have to understand that Velikovsky has a particular vision of the Hebrew role in history.</i></b><i> Do you know where Velikovsky says the Amalekites-Hyksos came from?&#39;</i><br /><br /><i>&#39;From Arabia.&#39;</i><br /><br /><i>&#39;Yes. Barbarians from Arabia conquer and oppress Egypt, the center of ancient civilization. They rule Egypt with great cruelty until their capital city is conquered and sacked by ...?&#39;</i><br /><br /><i>&#39;By King Saul.&#39;</i><br /><br /><i>&#39;Yes. </i><b><i>Under Velikovsky&#39;s revision, Israelites are not only exonerated of the charge that they themselves were the Hyksos, but Israel also throws out the Arab barbarians. Israel becomes the liberator of Egypt and the savior of civilization.&#39; He paused. &#39;Do you see what I&#39;m getting at?&#39;</i></b><br /><br /><i>&#39;Yes, I think so.&#39; I did think so. I thought Gordon was saying exactly what he said. But the implications of his words had escaped me completely. It wasn&#39;t until February 1988, when I met Dr. Gordon at Roger Wescott&#39;s home and we talked about the points we&#39;d discussed 11 years before, that I suddenly understood what he meant. </i><b><i>Under Velikovsky&#39;s revision, ancient Israel -- like modern Israel -- battles against Arabs in defense of its borders and the values it upholds. The significance of the earlier struggle illuminates the present one.</i></b><br /><br /><i>This conversation with Cyrus Gordon came at a crucial moment in my ongoing effort to evaluate Velikovsky&#39;s revised chronology in light of later research. Although I&#39;d come to have strong doubts about the Velikovsky&#39;s Peoples of the Sea and Ramses II and His Time, the intriguing identifications and parallels in Ages in Chaos had continued to hold my attention. But I also found myself convinced by the relentless logic of Gunnar Heinsohn&#39;s sweeping and radical reconstruction of Mesopotamian history.</i><br /><br /><i>Heinsohn points out that until 1868, five major empires were known to have preceded the Hellenistic Greeks: Chaldeans, Assyrians, Late Chaldeans, Medes, and Persians. </i><b><i>Toward the end of the 19th century, excavators recognized that archaeological evidence from Mesopotamia was not sufficient to justify a history extending back to the third millennium BCE, when -- according to Biblical chronology -- high civilizations existed in both Mesopotamia and Egypt. Scholars realized that either Biblical dating had to be shortened by many centuries or additional evidence had to be found to support the Biblical time frame. According to Heinsohn, Biblical chronology eventually won the day because scholars interpreted the archaeological evidence in a way that made it conform with Bible stories, stories they presumed to be true. They did it by doubling the five known empires to 10</i></b><i>: Early Sumerians, Old Akkadians, Neo-Sumerians, Old Babylonians, Kassites, Mitanni, Assyrians, Late Chaldeans, Medes, and Persians.</i><br /><br /><i>However, curious things occurred when this new chronology was applied to the physical evidence. Archaeological remains for such well-known people as the Chaldeans dwindled to virtually nothing, while the Early Sumerians -- &#39;unknown to even the most brilliant scholars of antiquity,&#39; to quote Heinsohn -- were suddenly discovered to have built a flourishing empire. Similarly, physical evidence for the previously-unknown 2nd millennium BCE &#39;Mitanni&#39; assumed a prominent place in the textbooks while the 1st millennium Medes -- who are mentioned repeatedly by other ancient nations and authors and who were co-conquerors of the Assyrian Empire -- became archaeological shadows. The newly-found &#39;Old Babylonians&#39; of the 2nd millennium ascended to fame and glory while evidence for the mighty Persian Empire of the 1st millennium couldn&#39;t be found at all.</i><br /><br /><b><i>It seemed obvious to me that if Heinsohn was right Biblical history was in peril.</i></b><i> For a while I was hopeful that both sides could be accommodated by a synthesis of the competing chronologies. But when Heinsohn withdrew his support for Velikovsky&#39;s 9th century placement of the Amarna period, and moved this key epoch of Egyptian history down to the end of the 7th century BCE, it was obvious that hope of a compromise was gone. There was no way to shift Saul, David, Solomon and the Queen of Sheba to the 7th century and keep the accepted chronology of the Bible intact. Either Heinsohn or Velikovsky was wrong.</i><br /><br /><b><i>On the emotional level, I supported Velikovsky. I found (and still find) his revised chronology to be both satisfying and reassuring. On the levels of scholarship and reason, however, Heinsohn carried the day, even though I found myself resenting his work.</i></b><i> Nevertheless, I recognized that while he might well be wrong on specific points, his brilliant breakthrough in Mesopotamia had compelled a radical shortening of the histories of Egypt and Israel, whether I liked it or not. Synchronisms established by the Amarna letters convinced me that the end of the Late Bronze Age had to come down to the last years of the 7th century BCE. And yet I couldn&#39;t put my doubts aside, until testimony from another source finally tipped the balance in Heinsohn&#39;s favor.</i><br /><br /><i>Yehoshua Etzion is a violinist with the Jerusalem Symphony and an amateur historian with a solid understanding of archaeology. From what I know of his forthcoming book The Lost Bible, I can tell you that Etzion will be making a major contribution to our understanding of stratigraphy in the land of Israel. Of particular importance will be Etzion&#39;s revelations about the Iron Age, and where to look for Persian strata, which are missing not only in Mesopotamia but in major sites in Israel, as well. In early 1988, during an exchange of letters with Etzion, I asked him what he thought of Heinsohn&#39;s revised chronology. He expressed serious doubts about Heinsohn&#39;s revision as it applied to Israel. Etzion had found Biblical chronology to be a reasonably accurate guide for archaeological research. In one area, however, he showed support for Heinsohn&#39;s work. Based on evidence from Israeli stratigraphy, Etzion described Heinsohn&#39;s late placement of the Amarna period as &#39;very promising.&#39;</i><br /><br /><i>&#39;In fact,&#39; he told me during a visit to New York, &#39;I might even place it later than Heinsohn.&#39;</i><br /><br /><i>I asked how he could back Heinsohn in this important area and support Biblical chronology at the same time. &#39;My book is based on archaeology, not Bible studies,&#39; he replied. &#39;You must always keep in mind that there is a great difference between archaeology and Bible studies. </i><b><i>Also, when it comes to understanding the history of the Bible you must always beware of hidden fundamentalism.&#39;</i></b><br /><br /><b><i>Hidden fundamentalism. It was a term that Etzion used several times in our correspondence and again during our meetings in New York. The words struck home. I&#39;d been trying to get at the source of my lingering resentment toward Heinsohn&#39;s work, and gradually it became clear. I was angry with him because he was undermining Biblical history.</i></b><br /><br /><i>Now, I am not what is usually called a &#39;religious&#39; person. I&#39;m not an atheist, but I don&#39;t attend religious services except on special occasions, and only then to make someone else happy. I would resent it mightily if anyone called me a &#39;fundamentalist.&#39; </i><b><i>But the more I thought it, the more I had to admit that my interest in Velikovsky&#39;s revised chronology was tinged with a streak of &#39;hidden fundamentalism.&#39; Velikovsky offered me a way to get my religion in secular doses without having to swallow it whole.</i></b><br /><br /><b><i>Velikovsky was keenly aware that his work is closely connected with religious sentiment. Opponents of Worlds in Collision accused Velikovsky of pandering to fundamentalist faith by attempting to prove that the miraculous events of the Scriptures had a basis in historical fact. Velikovsky was amused by accusations of fundamentalism because he&#39;d been hailed by a society of British atheists for proving that the so-called miraculous events of the Scriptures were nothing more than natural disasters on a global scale.</i></b><br /><br /><b><i>I had long since come to the conclusion that Velikovsky was not a religious fundamentalist because he demonstrated a repeated willingness to question the literal truth of Bible stories.</i></b><i> For example, on page 32 of Ages in Chaos Velikovsky casts doubt on the statements in Exodus that &#39;all of the firstborn and only the firstborn&#39; were killed on the first night of the Exodus plagues &#39;because events can never attain that degree of coincidence. No credit should be given to such a record.&#39; </i><b><i>In Worlds in Collision Velikovsky also cautions his readers to seek the correct interpretation of Scriptural passages and not to take them literally.</i></b><br /><br /><i>So if my intense interest in the revised chronology can in fact be traced to feelings of hidden fundamentalism, it would appear that none of this has anything to with Velikovsky. </i><b><i>But in fact I think it does. It wouldn&#39;t be fair to call Velikovsky a religious fundamentalist, or a theological fundamentalist. But I believe the evidence shows that Velikovsky is a chronological fundamentalist. He accepts the later chronology of Biblical history as literal truth.</i></b><i> On page 76 of Ages in Chaos Velikovsky weighs the evidence for determining the length of the Hyksos period and concludes that Biblical chronology is correct. This is the pattern for his entire post-Exodus revision.</i><br /><br /><i>In retrospect, Velikovsky&#39;s chronological fundamentalism should not surprise anyone who rereads the paragraph from Ages in Chaos I quoted earlier.</i><br /><br /><b><i>&#39;Whose history (Israel or Egypt) is to be moved by (six) centuries? ... No student of ancient history will see the slightest possibility of altering the history of the kings of Jerusalem by a single century, much less by six ...&#39;</i></b><br /><br /><b><i>That last sentence is not a statement. It&#39;s an order. It&#39;s an order which many of Velikovsky&#39;s admirers still follow devotedly. But there&#39;s no real reason why they should. Who says that &#39;No student of ancient history will see the slightest possibility of altering the chronology of the kings of Jerusalem&#39;? Many scholars have proposed such alterations. Books have been written on the subject. Archaeologists have long noted a distressing lack of evidence that confirms Biblical chronology. Proof that David and Solomon were real people is missing altogether.</i></b><br /><br /><b><i>Velikovsky should have begun by questioning Biblical chronology, by stating that Biblical dating may or may not be accurate, or may be partially accurate. Bible stories are not archaeology. They are stories, which may or may not contain elements of historical fact. Velikovsky should have constantly questioned the various dating schemes while applying his method of drawing literary parallels between the histories of Egypt and Israel.</i></b><br /><br /><b><i>But Velikovsky didn&#39;t take this cautious approach. He saw no reason to, because altering Biblical chronology by more than &#39;one or two decades&#39; would &#39;disrupt all established data and concepts.&#39; A man who made a career out of disrupting the established history of the ancient world should have been more careful. He should have realized that no one&#39;s history is immune to disruption. He should have let that realization permeate his work. Instead, he began with a vision of the truth and followed that vision down a road that took him astray. He never succeeded in piecing together the complex puzzle of ancient chronology. And yet his courage and leadership made possible the breakthroughs that came later.</i></b><br /><br /><b><i>Did Velikovsky have a hidden agenda? Was he deliberately manipulating the evidence as he shaped his revised chronology? I don&#39;t think so. I found Velikovsky to be open and honest. It&#39;s true he was passionately involved in the history of Israel, past and present. In my opinion, however, Velikovsky was motivated not by a desire to deceive, but to believe. If he had a hidden agenda, it was hidden from himself, as well.</i></b><br /><br /><b><i>The final irony is that Velikovsky, who so eloquently showed us how catastrophist fears became uniformitarian certainties in the age of Newton, was part of the Newtonian age himself. When the comforts of geocentric theology collapsed, Newton re-established the shaken tenets of religious faith on the terra firma of modern science. Velikovsky tried to shore up the shaken chronology of the Bible, a chronology that had been challenged by Egyptologists and by a lack of archaeological evidence. Like Newton, Velikovsky sought validation of the Bible through modern scholarship and secular proofs.</i></b><br /><br /><b><i>Today, in the light of Gunnar Heinsohn&#39;s discoveries, we are coming to see that the accepted chronologies of both Egypt and the Bible are wrong. As we gain a more accurate understanding of ancient chronology, religious and social customs derived from the Scriptures are being subjected to increasing pressure, just as they were in Newton&#39;s time when we gained a more accurate understanding of the solar system. I doubt that Biblical chronology can stand against the strength of Heinsohn&#39;s reconstruction. But whatever the fate of the Bible as a historical document, the central message of the Scriptures will endure. It will endure because the human predicament endures. We live on a dangerous planet. The Bible -- on its most basic level -- tells us that there is survival value in believing we&#39;re not powerless in the face of disaster.</i></b><br /><br /><i>In Worlds in Collision (p. 189), Velikovsky mentions the Snohomish tribe on Puget Sound. According to tribal legend, there was a time when the sky was so low people couldn&#39;t stand erect. They attempted to lift the oppressive clouds by shouting &#39;Yahu!&#39; at the heavens. We can&#39;t be sure what kind of calamity the tribe had been struck by. It could have been anything from a foggy winter to a global upheaval. Whatever it was, we know the tribe survived because the legend survived. We may also speculate that if the disaster was severe enough, some tribes ceased to exist as social units.</i><br /><br /><b><i>Why do some groups survive a catastrophe while others perish? Luck plays a role, of course. There&#39;s nothing like being at the right place at the right time. But those who survive physically must also survive psychologically. Under conditions of chaos, hopelessness and despair are the greatest threats to psychological survival. The Snohomish tribe fought against despair by shouting at the sky. Other tribes and nations tried (and still try) their own kinds of communication with the heavens. Sometimes prayer or chanting was used, sometimes human sacrifice. Rules and rituals were established. The Bible is a complete source book on our efforts to control the sky by controlling ourselves.</i></b><br /><br /><i>In his Mesopotamian reconstruction, Heinsohn demonstrates that textbook chronology has been decisively influenced by Biblical dates for Abraham. </i><b><i>In effect, therefore, a defense of the accepted chronology is a defense of the Bible in all its historical and psychological complexity. Those of us whose interest in ancient history was inspired by Velikovsky may not be aware of the extent to which a seemingly secular subject has been affected by hidden fundamentalism.</i></b><i> Heinsohn has written that Velikovsky would have discovered the true chronology of the ancient world &#39;if only he had understood that the Greek &#39;dark age&#39; did not last 500 years but 700 years, with the Thucydides-derived date of - 776 a mere desk fabrication. Then (Velikovsky) would have brought Akhnaton down to something like - 600. His direction was right ...&#39;</i><br /><br /><b><i>Velikovsky&#39;s direction was right, but he was unable to continue in that direction for the required length of time because it would have forced him to abandon Biblical chronology. The Bible is the greatest catastrophist document of all time, a handbook for survival, an antidote to despair. That should be enough for anyone. It&#39;s a mistake to ask more of the Bible than the Bible can give.</i></b><br /><br /><i>Heinsohn, to a greater extent than his critics, has been able to free himself from preconceptions about the Bible and look at the archaeological evidence as it actually exists. </i><b><i>Velikovsky never gained that perspective because he believed in advance that the Bible was right. And so -- on one level or another -- do most of Heinsohn&#39;s critics. Hidden fundamentalism is our greatest single obstacle to an accurate understanding of the ancient world.&quot;</i></b><br /><br />Copyright 1988, 1990 Clark Whelton<br />Society for Historical Research [1]Hom [2]e<br /><br />REFERENCES:<br /><br />1. <a href=\"http://www.interport.net/~henryz/shr.html\" class=\"ot-anchor\">http://www.interport.net/~henryz/shr.html</a><br />2. <a href=\"http://www.interport.net/~henryz/shr.html\" class=\"ot-anchor\">http://www.interport.net/~henryz/shr.html</a><br /><br />There is an attempt at a rebuttal here:<br /><br /><a href=\"http://www.velikovsky.info/Fundamentalism\" class=\"ot-anchor\">http://www.velikovsky.info/Fundamentalism</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"The Tree Ring","summary":"There are Good Reasons to Believe that Tree Ring Data is Not Simply a Function of Temperature / There is a Surprisingly Strong Correlation between Annual Growth Anomaly and Galactic Cosmic Ray Flux / \"Of all the Variables Investigated, it is by Far the One Most Correlated with the Annual Growth Anomaly\" / Is that Why Dendrochronogists have Sometimes Reported Cyclic Phenomena in Long Time-Series of Tree Rings?","image":"https://lh3.googleusercontent.com/-012cxL-PV8c/WFz3S1GnV-I/AAAAAAAAJ4o/VoD2LIZfGSYWqpaFCPYQIhHJnAjn0lNlACJoC/w7142-h9999/the-tree-ring-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/2ih7nPJ3yCV","publishDate":"2016-12-23T10:08:43.927Z","updateDate":"2017-01-03T01:33:27.178Z","text":"<b>The Tree Ring: There are Good Reasons to Believe that Tree Ring Data is Not Simply a Function of Temperature / There is a Surprisingly Strong Correlation between Annual Growth Anomaly and Galactic Cosmic Ray Flux / &quot;Of all the Variables Investigated, it is by Far the One Most Correlated with the Annual Growth Anomaly&quot; / Is that Why Dendrochronogists have Sometimes Reported Cyclic Phenomena in Long Time-Series of Tree Rings?</b><br /><br /><a href=\"http://www.worldclimatereport.com/index.php/2008/02/11/a-2000-year-global-temperature-record/\" class=\"ot-anchor\">http://www.worldclimatereport.com/index.php/2008/02/11/a-2000-year-global-temperature-record/</a><br /><br />Originally: Loehle, C. 2007. A 2000-year global temperature reconstruction based on non-tree ring proxies. <i>Energy and Environment</i>, 18, 1049-1058.<br /><br /><i>&quot;There are reasons to believe that tree ring data may not capture long-term climate changes (100+ years) because tree size, root/shoot ratio, genetic adaptation to climate, and forest density </i><b><i>can all shift in response to prolonged climate changes, among other reasons.”</i></b><i> Furthermore, Loehle notes </i><b><i>&quot;Most seriously, typical reconstructions assume that tree ring width responds linearly to temperature, but trees can respond in an inverse parabolic manner to temperature, with ring width rising with temperature to some optimal level, and then decreasing with further temperature increases.&quot; Other problems include tree responses to precipitation changes, variations in atmospheric pollution levels, diseases, pest outbreaks, and the obvious problem of enrichment that comes along with ever higher levels of atmospheric carbon dioxide. Trees are not simple thermometers!&quot;</i></b><br /><br />Looks like they might have overlooked another ...<br /><br /><a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1469-8137.2009.03026.x/abstract\" class=\"ot-anchor\">http://onlinelibrary.wiley.com/doi/10.1111/j.1469-8137.2009.03026.x/abstract</a><br /><br />Rapid report<br /><br />A relationship between galactic cosmic radiation and tree rings<br /><br />Sigrid Dengel, Dominik Aeby and John Grace<br />Institute of Atmospheric and Environmental Science, School of GeoSciences, Crew Building, University of Edinburgh, EH9 3JN, UK<br /><br /><i>&quot;Summary</i><br /><br />- <b><i>Here, we investigated the interannual variation in the growth rings formed by Sitka spruce</i></b><i> (Picea sitchensis) trees in northern Britain (55 deg N, 3 deg W) over the period 1961–2005 in an attempt to disentangle the influence of atmospheric variables acting at different times of year.</i><br /><br />- <b><i>Annual growth rings</i></b><i>, measured along the north radius of freshly cut (frozen) tree discs </i><b><i>and climatological data</i></b><i> recorded at an adjacent site were used in the study. Correlations were based on Pearson product–moment correlation coefficients between the annual growth anomaly and these climatic and atmospheric factors.</i><br /><br />- <b><i>Rather weak correlations between these variables and growth were found. However, there was a consistent and statistically significant relationship between growth of the trees and the flux density of galactic cosmic radiation. Moreover, there was an underlying periodicity in growth, with four minima since 1961, resembling the period cycle of galactic cosmic radiation.</i></b><br /><br /><i>[...]</i><br /><br /><i>Galactic cosmic ray fluxes are known to be dependent on latitude (Svensmark &amp; Friis-Christensen, 1997; Palle´ &amp; Butler, 2000; Kirkby, 2007). Monthly corrected galactic cosmic ray flux data recorded at Kiel Neutron Monitor are held by the Christian-Albrechts-Universita¨t zu Kiel and provided by the National Geophysical Data Center, (Boulder, CO, USA; (</i><i><a href=\"http://www.ngdc.noaa.gov/stp/\" class=\"ot-anchor\">http://www.ngdc.noaa.gov/stp/</a></i><i> SOLAR/ftpcosmicrays.html). Information on the locality and time of volcanic eruptions, as well as on the volcanic explosivity index relevant for this study originate from the Smithsonian Institution, Global Volcanism Program (http:// </i><i><a href=\"http://www.volcano.si.edu\" class=\"ot-anchor\">www.volcano.si.edu</a></i><i>).</i><br /><br /><i>Following the approach of Grace &amp; Norton (1990) we tested the correlation between the annual growth anomaly and the climatological and atmospheric variables, month by month using the Pearson product–moment correlation coefficients. Although the ring width is a time-averaged property, the rings do not form throughout the entire year, and are dependent on stored assimilates. Often the early dendrochronologists found a ‘lag effect’ whereby the cond- itions of the previous year influence the growth of the present year (Fritts, 1962; Creber, 1977). We therefore tested the effect of the variables month by month, for the present year and the previous year.</i><br /><br /><i>[...]</i><br /><br /><i>Most variables were only weakly correlated with the annual growth anomaly: total solar radiation was never statistically significantly correlated with the growth anomaly but diffuse radiation was significantly correlated in some months (Fig. 2). When considering the diffuse radiation received over the spring and summer months, we observed a statistically significant correlation: the amount of diffuse radiation received over the spring and summer (March– August) period was statistically positively correlated with the annual growth anomaly, with a correlation coefficient of +0.29 (P = 0.05; n = 45).</i><br /><br /><b><i>Temperature was negatively correlated with growth in the months of June and September, and precipitation was negatively correlated with growth in February and October. There were correlations between the annual growth anomaly and both the water vapour pressure deficit (VPD) and the calculated height of the most frequently occurring cloud base.</i></b><br /><br /><b><i>The galactic cosmic ray (GCR) flux shows a well-known periodicity that is anticorrelated with the sunspot number, with four maxima in the period 1961–2005 (Fig. 3). Surprisingly, the tree ring data show a similar periodicity although it is possible to identify particular years where the relationship is broken by instances of extreme weather: for example 1995 was an especially warm and dry year in Scotland as in much of Europe, and under these conditions the tree rings were reduced in size. The probability of such a good relationship between the annual growth anomaly and galactic cosmic ray flux occurring by chance alone is 0.008 (n = 45, r = 0.39) (see the inset of Fig. 3). Of all the variables investigated, it is by far the one most correlated with the annual growth anomaly.</i></b><br /><br /><i>Diffuse radiation is associated with periods of high volcanism. When the fraction of the incoming solar radiation that is received as diffuse radiation is examined in relation to the occurrence of volcanoes with a volcanic explosivity index (VEI) of 3 and higher which have erupted upwind (to the west of the UK) it appears that the fraction of diffuse radiation is often less in periods where there are few volcanoes (Fig. 4).</i><br /><br /><i>Discussion</i><br /><br /><i>There were correlations between annual growth anomaly and both the water vapour pressure deficit (VPD) and the height of the most frequently occurring cloud base. The VPD has long been known to influence photosynthesis for the reason that the stomata close when the air is dry, especially in this species (Grace et al., 1975; Neilson &amp; Jarvis, 1975). Cloudiness per se may be expected to reduce plant growth through a reduction in solar insolation, although Williams et al. (2008) have found that the effect of cloud cover on tree growth, and thus ring width, may vary depending on the type of cloud, the time of day, and the time of year.</i><br /><br /><b><i>We were surprised to see that the GCR flux (Kiel Neutron Monitor, 54 deg 34 arcminute N; 10 deg 12 arcminute E, 54 m a.s.l.) was statistically significantly correlated with the annual growth anomaly in all months</i></b><i>, and the first presumption is that GCRs create aerosols and thus change the radiation field. However, processes other than GCR flux are involved in aerosol production and may also modify the radiation field, and thus mask any effect of galactic cosmic radiation.</i><br /><br /><i>First, volcanic eruptions affect the flux of diffuse radiation received at the Earth’s surface even when they are many thousands of miles away. While sulphur aerosols are capable of remaining in the atmosphere over 1–3 yr and ashes only a few months, aerosols resulting from GCRs have a much shorter lifespan. According to Yu &amp; Turco (2000); Kristjansson et al. (2002) these aerosols have a lifespan of only a few days, so cloud formation and any consequent impact on photosynthesis should take place within this short time. Second, local aerosol production by coniferous forests has been observed (Kulmala et al., 2001, 2007), and may be expected to modify the diffuse radiation flux, possibly at a regional scale in highly forested areas.</i><br /><br /><b><i>It has been observed that GCR cycles are correlated with cloud cover</i></b><i> (Svensmark &amp; Friis-Christensen, 1997; Marsh &amp; Svensmark, 2000; Palle´ &amp; Butler, 2000). Moreover, a description of this process is presented by Yu &amp; Turco (2000) and Harrison &amp; Carslaw (2003). Substantial ionizing radiation is also produced from radioactive decay of elements below the surface of the soil (Kotaka &amp; Krueger, 1978) and released to the atmosphere according to soil depth and moisture, but does not vary in cycles as we see in GCRs. The correlative studies linking GCRs to cloud formation have, however, been challenged (Kristjansson et al., 2002; Laut, 2003).</i><br /><br /><i>[...]</i><br /><br /><b><i>We cannot, however, rule out the possibility of a direct stimulatory effect of GCR on the growth of trees, as beneficial effects have sometimes been demonstrated in biological materials exposed to GCR in space (Hammond et al., 1996), despite the prevalence of chromosomal aberrations in such materials (Nevzgodina, 1999).</i></b><br /><br /><b><i>Dendrochronogists have sometimes reported cyclic phenomena in long time-series of tree-rings but they have rarely offered an explanation (Douglass, 1927; Siren &amp; Hari, 1971; Briffa, 1994; Rigozo et al., 2007). For example, in a study of 305 tree-ring chronologies from North America, periods of 18.6 yr and 10.5 yr were found in 286 and 244 instances (Currie, 1991), respectively. These observations have been largely ignored, perhaps because no underlying mechanism could be found to explain the intriguing results.</i></b><br /><br /><i>Acknowledgements</i><br /><br /><i>We thank Dr Barry A. Gardiner and Dr Mike Perks from Forest Research, Northern Station (Roslin, Scotland, UK) for providing us with freshly cut tree discs used in the current study, for information on Forest of Ae and the initial help with the analysis of tree ring data with WINDENDRO. Furthermore, the authors would like to thank the British Atmospheric Data Centre (BADC) for the provision of meteorological data (UK MetOffice) for Eskdalemuir. Galactic cosmic ray flux data recorded at Kiel Neutron Monitor are held by the department of Extraterrestrial Physics, Institute for Experimental and Applied Physics, Christian-Albrechts-Universita¨t zu Kiel and provided by the National Geophysical Data Center, Boulder, Colorado (USA) (</i><i><a href=\"http://www.ngdc.noaa.gov/stp/SOLAR/\" class=\"ot-anchor\">http://www.ngdc.noaa.gov/stp/SOLAR/</a></i><i> ftpcosmicrays.html). Information on the locality and time of volcanic eruptions, as well as on the Volcanic Explosivity Index relevant for this study originate from the Smithsonian Institution, Global Volcanism Program (</i><i><a href=\"http://www\" class=\"ot-anchor\">http://www</a></i><i>. </i><i><a href=\"http://volcano.si.edu\" class=\"ot-anchor\">volcano.si.edu</a></i><i>). Prof. Alan Blyth kindly commented on the manuscript. SD was funded by the Torrance Bequest.&quot;</i><br /><br />References<br /><br />Aeby D. 2007. Changes in solar irradiation at four stations in the British Isles. MSc Thesis, The University of Edinburgh, Edinburgh, UK.<br /><br />Briffa KR. 1994. Grasping at shadows? A selective review of the search for sunspot-related variability in tree rings. The Solar Engine and Its Influence on Terrestrial Atmosphere and Climate, E. Nesme-Ribes Ed., NATOASI Series, Vol. I 25, Springer-Verlag, 417–434.<br /><br />Buckland SM, Grime JP, Hodgson JG, Thompson K. 1997. A comparison of plant responses to the extreme drought of 1995 in northern England. Journal of Ecology 85: 875–882.<br /><br />Burt SD. 1985. Sleet and snow in June 1985. Weather 40: 222.<br /><br />Byrne AK, Green C. 2004. The role of forests in the global carbon cycle and in climate change policy. Irish Forestry 61: 7–15.<br /><br />Carslaw K. 2009. Atmospheric physics: Cosmic rays, clouds and climate. Nature 460: 332–333.<br /><br />Carslaw KS, Harrison RG, Kirkby J. 2002. Cosmic rays, clouds, and climate. Science 298: 1732–1737.<br /><br />Creber GT. 1977. Tree rings: a natural data-storage system. Biological Reviews 52: 349–381.<br /><br />Currie RG. 1991. Deterministic signals in tree-rings from North-America. International Journal of Climatology 11: 861–876.<br /><br />Curtius J, Lovejoy E, Froyd K. 2006. Atmospheric ion-induced aerosol nucleation. Space Science Reviews 125: 159–167.<br /><br />Dinwoodie JM. 1962. Some ring-width pattern in Sitka spruce timber from North America. Forestry 35: 22–26.<br /><br />Douglass AE. 1927. Solar records in tree growth. Science 65: 220–221.<br /><br />Federer CA, Tanner CB. 1966. Spectral distribution of light in forest. Ecology 47: 555–560.<br /><br />Fritts HC. 1962. An approach to dendroclimatology: screening by means of multiple regression techniques. Journal of Geophysical Research 67: 1413–1420.<br /><br />Fritts HC. 1966. Growth-rings of trees: their correlation with climate. Science 154: 973–979.<br /><br />Grace J. 2004. Understanding and managing the global carbon cycle. Journal of Ecology 92: 189–202.<br /><br />Grace J, Norton DA. 1990. Climate and growth of Pinus sylvestris at its upper altitudinal limit in Scotland: evidence from tree growth-rings. Journal of Ecology 78: 601–610.<br /><br />Grace J, Malcolm DC, Bradbury IK. 1975. The effect of wind and humidity on leaf diffusive resistance in Sitka spruce seedlings. The Journal of Applied Ecology 12: 931–940.<br /><br />Gu L, Baldocchi DD, Wofsy SC, Munger JW, Michalsky JJ, Urbanski SP, Boden TA. 2003. Response of a deciduous forest to the Mount Pinatubo eruption: enhanced photosynthesis. Science 299: 2035–2038.<br /><br />Hammond EC, Bridgers K, Berry FD. 1996. Germination, growth rates, and electron microscope analysis of tomato seeds flown on the LDEF. Radiation Measurements 26: 851–861.<br /><br />Harrison RG, Carslaw KS. 2003. Ion-aerosol-cloud processes in the lower atmosphere. Reviews of Geophysics 41: 1012.<br /><br />Harrison RG, Stephenson DB. 2006. Empirical evidence for a nonlinear effect of galactic cosmic rays on clouds. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Science 462: 1221–1233.<br /><br />Jarvis NJ, Mullins CE. 1987. Modeling the effects of drought on the growth of Sitka spruce in Scotland. Forestry 60: 13–30.<br /><br />Kay J. 2004. Dust to dust: the summer of 1976. Weather 59: 247–250.<br /><br />Kerr RA. 2009. Study Challenges Cosmic Ray-Climate Link. Science 324: 576–577.<br /><br />Kirkby J. 2007. Cosmic rays and climate. Surveys in Geophysics 28: 333–375.<br /><br />Kotaka S, Krueger AP. 1978. Effects of air ions on microorganisms and other biological materials. Critical Reviews in Microbiology 6: 109–150.<br /><br />Kristjansson JE, Staple A, Kristiansen J, Kaas E. 2002. A new look at possible connections between solar activity, clouds and climate. Geophysical Research Letters 29: 2107–2110.<br /><br />Kulmala M, Ha¨meri K, Aalto PP, Ma¨kela¨ JM, Pirjola L, Nilsson ED, Buzorius G, RannikU ¨ , Maso MD, SeidlWet al. 2001. Overview of the international project on biogenic aerosol formation in the boreal forest (BIOFOR). Tellus B 53: 324–343.<br /><br />Kulmala M, Riipinen I, Sipila¨ M, Manninen HE, Peta¨ja¨ T, Junninen H, Dal Maso M, Mordas G, Mirme A, Vana M et al. 2007. Toward direct measurement of atmospheric nucleation. Science 318: 89–92.<br /><br />Laut P. 2003. Solar activity and terrestrial climate: an analysis of some purported correlations. Journal of Atmospheric and Solar–Terrestrial Physics 65: 801–812.<br /><br />Marsh N, Svensmark H. 2000. Cosmic rays, clouds, and climate. Space Science Reviews 94: 215–230.<br /><br />Mercado LM, Bellouin N, Sitch S, Boucher O, Huntingford C, Wild M, Cox PM. 2009. Impact of changes in diffuse radiation on the global land carbon sink. Nature 458: 1014–1017.<br /><br />Mochan S, Gardiner B. 2007. Timber properties of Sitka spruce from south Scotland: a study to identify causes of increased failure during stress grading. Forest Research, Northern Research Station, Roslin, UK. Internal Report.<br /><br />Morison JIL, Jarvis PG. 1983. Direct and indirect effects of light on stomata. I. In Scots pine and Sitka spruce. Plant, Cell &amp; Environment 6: 95–101.<br /><br />Morren G. 1980. The rural ecology of the British drought of 1975–1976. Human Ecology 8: 33–63.<br /><br />Neilson RE, Jarvis PG. 1975. Photosynthesis in Sitka Spruce (Picea sitchensis (Bong.) Carr.). VI. Response of stomata to temperature. Journal of Applied Ecology 12: 879–891.<br /><br />Nevzgodina LV. 1999. Chromosomal aberrations as a biomarker for cosmic radiation. Fundamentals for the Assessment of Risks from Environmental Radiation 55: 203–208.<br /><br />Palle´ EP, Butler CJ. 2000. The influence of cosmic rays on terrestrial clouds and global warming. Astronomy &amp; Geophysics 41: 18–22.<br /><br />Phipps RL. 1982. Comments on interpretation of climatic information from tree rings eastern North America. Tree-Ring Bulletin 42: 11–22.<br /><br />Pierce JR, Adams PJ. 2009. Can cosmic rays affect cloud condensation nuclei by altering new particle formation rates? Geophysical Research Letters 36: doi: 10.1029/2009GL037946.<br /><br />Rigozo NR, Nordemann DJR, Souza Echer MP, Echer E, da Silva HE, Prestes A, Guarnieri FL. 2007. Solar activity imprints in tree ring width from Chile (1610–1991). Journal of Atmospheric and Solar-Terrestrial Physics 69: 1049–1056.<br /><br />Roderick ML. 2006. The ever-flickering light. Trends in Ecology &amp; Evolution 21: 3–5.<br /><br />Roderick M, Farquhar G, Berry S, Noble I. 2001. On the direct effect of clouds and atmospheric particles on the productivity and structure of vegetation. Oecologia 129: 21–30.<br /><br />Schimel DS. 1995. Terrestrial ecosystems and the carbon cycle. Global Change Biology 1: 77–91.<br /><br />Siren G, Hari P. 1971. Coinciding periodicity in recent tree rings and glacial clay sediments. Annales Universitatis Turkuensis Series A II Biologica-Geographica-Geologica 47: 155–157.<br /><br />Spiecker H. 1999. Overview of recent growth trends in European forests. Water, Air, &amp; Soil Pollution 116: 33–46.<br /><br />Stanhill G, Cohen S. 2001. Global dimming: a review of the evidence for a widespread and significant reduction in global radiation with discussion of its probable causes and possible agricultural consequences. Agricultural and Forest Meteorology 107: 255–278.<br /><br />Suzaki T, Kume A, Ino Y. 2003. Evaluation of direct and diffuse radiation densities under forest canopies and validation of the light diffusion effect. Journal of Forest Research 8: 283–290.<br /><br />Svensmark H, Friis-Christensen E. 1997. Variation of cosmic ray flux and global cloud coverage – a missing link in solar-climate relationships. Journal of Atmospheric and Solar–Terrestrial Physics 59: 1225–1232.<br /><br />UK MetOffice (ed) 2003. News release; 2003 Summary. http:// <a href=\"http://www.metoffice.gov.uk/climate/uk/interesting/2003summary.html\" class=\"ot-anchor\">www.metoffice.gov.uk/climate/uk/interesting/2003summary.html</a> (11.08.2009).<br /><br />UK MetOffice (ed) 2008a. News release. <a href=\"http://www.metoffice.gov.uk/climate/uk/2008/March.html\" class=\"ot-anchor\">http://www.metoffice.gov.uk/climate/uk/2008/March.html</a> (11.08.2009).<br /><br />UK MetOffice (ed) 2008b. News release. UK climate. 2008 summary. <a href=\"http://www.metoffice.gov.uk/climate/uk/2008/summer.html\" class=\"ot-anchor\">http://www.metoffice.gov.uk/climate/uk/2008/summer.html</a> (11.08.2009).<br /><br />Urban O, Janousˇ D, Acosta M, Czerny´ R, Markova´ I, Navra´til M, Pavelka M, Pokorny´ R,S ˇ prtova´ M, Zhang R et al. 2007. Ecophysiological controls over the net ecosystem exchange of mountain spruce stand. Comparison of the response in direct vs. diffuse solar radiation. Global Change Biology 13: 157–168.<br /><br />Williams A, Still C, Fischer D, Leavitt S. 2008. The influence of summertime fog and overcast clouds on the growth of a coastal Californian pine: a tree-ring study. Oecologia 156: 601–611.<br /><br />Yu FQ, Turco RP. 2000. Ultrafine aerosol formation via ion-mediated nucleation. Geophysical Research Letters 27: 883–886.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Cosmology in Crisis","summary":"Cosmology in Crisis","image":"https://lh3.googleusercontent.com/-l0Aocmbzjns/WFy4AbvE9fI/AAAAAAAAJ3Y/T8VX_HoEqzkNYjVgc53o_Rr5t4DG-hMfACJoC/w7142-h9999/cosmology-in-crisis-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/NrCxUHZ7Sji","publishDate":"2016-12-23T05:37:18.025Z","updateDate":"2017-01-03T01:33:48.552Z","text":"<b>Cosmology in Crisis</b><br /><br />Website no longer works, but original page can be viewed here ...<br /><br /><a href=\"https://web.archive.org/web/20140401081546/http://cosmologystatement.org/\" class=\"ot-anchor\">https://web.archive.org/web/20140401081546/http://cosmologystatement.org/</a><br /><br /><b>An Open Letter to the Scientific Community</b><br /><b><a href=\"http://cosmologystatement.org\" class=\"ot-anchor\">cosmologystatement.org</a></b><br /><br /><b>(Published in New Scientist, May 22, 2004)</b><br /><br /><i>&quot;The big bang today relies on a growing number of hypothetical entities, things that we have never observed -- inflation, dark matter and dark energy are the most prominent examples. Without them, there would be a fatal contradiction between the observations made by astronomers and the predictions of the big bang theory. In no other field of physics would this continual recourse to new hypothetical objects be accepted as a way of bridging the gap between theory and observation. It would, at the least, raise serious questions about the validity of the underlying theory.</i><br /><br /><i>But the big bang theory can&#39;t survive without these fudge factors. Without the hypothetical inflation field, the big bang does not predict the smooth, isotropic cosmic background radiation that is observed, because there would be no way for parts of the universe that are now more than a few degrees away in the sky to come to the same temperature and thus emit the same amount of microwave radiation.</i><br /><br /><i>Without some kind of dark matter, unlike any that we have observed on Earth despite 20 years of experiments, big-bang theory makes contradictory predictions for the density of matter in the universe. Inflation requires a density 20 times larger than that implied by big bang nucleosynthesis, the theory&#39;s explanation of the origin of the light elements. And without dark energy, the theory predicts that the universe is only about 8 billion years old, which is billions of years younger than the age of many stars in our galaxy.</i><br /><br /><i>What is more, the big bang theory can boast of no quantitative predictions that have subsequently been validated by observation. The successes claimed by the theory&#39;s supporters consist of its ability to retrospectively fit observations with a steadily increasing array of adjustable parameters, just as the old Earth-centered cosmology of Ptolemy needed layer upon layer of epicycles.</i><br /><br /><i>Yet the big bang is not the only framework available for understanding the history of the universe. Plasma cosmology and the steady-state model both hypothesize an evolving universe without beginning or end. These and other alternative approaches can also explain the basic phenomena of the cosmos, including the abundances of light elements, the generation of large-scale structure, the cosmic background radiation, and how the redshift of far-away galaxies increases with distance. They have even predicted new phenomena that were subsequently observed, something the big bang has failed to do.</i><br /><br /><i>Supporters of the big bang theory may retort that these theories do not explain every cosmological observation. But that is scarcely surprising, as their development has been severely hampered by a complete lack of funding. Indeed, such questions and alternatives cannot even now be freely discussed and examined. An open exchange of ideas is lacking in most mainstream conferences. Whereas Richard Feynman could say that &#39;science is the culture of doubt&#39;, in cosmology today doubt and dissent are not tolerated, and young scientists learn to remain silent if they have something negative to say about the standard big bang model. Those who doubt the big bang fear that saying so will cost them their funding.</i><br /><br /><i>Even observations are now interpreted through this biased filter, judged right or wrong depending on whether or not they support the big bang. So discordant data on red shifts, lithium and helium abundances, and galaxy distribution, among other topics, are ignored or ridiculed. This reflects a growing dogmatic mindset that is alien to the spirit of free scientific inquiry.</i><br /><br /><i>Today, virtually all financial and experimental resources in cosmology are devoted to big bang studies. Funding comes from only a few sources, and all the peer-review committees that control them are dominated by supporters of the big bang. As a result, the dominance of the big bang within the field has become self-sustaining, irrespective of the scientific validity of the theory.</i><br /><br /><i>Giving support only to projects within the big bang framework undermines a fundamental element of the scientific method -- the constant testing of theory against observation. Such a restriction makes unbiased discussion and research impossible. To redress this, we urge those agencies that fund work in cosmology to set aside a significant fraction of their funding for investigations into alternative theories and observational contradictions of the big bang. To avoid bias, the peer review committee that allocates such funds could be composed of astronomers and physicists from outside the field of cosmology.</i><br /><br /><i>Allocating funding to investigations into the big bang&#39;s validity, and its alternatives, would allow the scientific process to determine our most accurate model of the history of the universe.&quot;</i><br /><br /><b>Signed:</b><br /><br /><b>(Institutions for identification only)</b><br />Highlighted names are linked to related web pages<br /><br />Halton Arp, Max-Planck-Institute Fur Astrophysik (Germany)<br />Andre Koch Torres Assis, State University of Campinas (Brazil)<br />Yuri Baryshev, Astronomical Institute, St. Petersburg State University (Russia)<br />Ari Brynjolfsson, Applied Radiation Industries (USA)<br />Hermann Bondi, Churchill College, University of Cambridge (UK)<br />Timothy Eastman, Plasmas International (USA)<br />Chuck Gallo, Superconix, Inc.(USA)<br />Thomas Gold, Cornell University (emeritus) (USA)<br />Amitabha Ghosh, Indian Institute of Technology, Kanpur (India)<br />Walter J. Heikkila, University of Texas at Dallas (USA)<br />Michael Ibison, Institute for Advanced Studies at Austin (USA)<br />Thomas Jarboe, University of Washington (USA)<br />Jerry W. Jensen, ATK Propulsion (USA)<br />Menas Kafatos, George Mason University (USA)<br />Eric J. Lerner, Lawrenceville Plasma Physics (USA)<br />Paul Marmet, Herzberg Institute of Astrophysics (retired) (Canada)<br />Paola Marziani, Istituto Nazionale di Astrofisica, Osservatorio Astronomico di Padova (Italy)<br />Gregory Meholic, The Aerospace Corporation (USA)<br />Jacques Moret-Bailly, Université Dijon (retired) (France)<br />Jayant Narlikar, IUCAA(emeritus) and College de France (India, France)<br />Marcos Cesar Danhoni Neves, State University of Maringá (Brazil)<br />Charles D. Orth, Lawrence Livermore National Laboratory (USA)<br />R. David Pace, Lyon College (USA)<br />Georges Paturel, Observatoire de Lyon (France)<br />Jean-Claude Pecker, College de France (France)<br />Anthony L. Peratt, Los Alamos National Laboratory (USA)<br />Bill Peter, BAE Systems Advanced Technologies (USA)<br />David Roscoe, Sheffield University (UK)<br />Malabika Roy, George Mason University (USA)<br />Sisir Roy, George Mason University (USA)<br />Konrad Rudnicki, Jagiellonian University (Poland)<br />Domingos S.L. Soares, Federal University of Minas Gerais (Brazil)<br />John L. West, Jet Propulsion Laboratory, California Institute of Technology (USA)<br />James F. Woodward, California State University, Fullerton (USA)\t<br />New signers of the Open letter since publication<br /><br /><b>Scientists and Engineers</b><br /><br />Jorge Marao Universidade Estadual de Londrina Brazi<br />Martin John Baker, Loretto School Musselburgh, UK<br />Peter J Carroll,  Psychonaut Institute, UK<br />Roger Y. Gouin, Ecole Superieure d&#39;Electricite, France<br />John Murray, Sunyata Composite Ltd, UK<br />Jonathan Chambers, University of Sheffield, UK<br />Michel  A.  Duguay, Laval University, Canada<br />Qi Pan, Fitzwilliam College, Cambridge, UK<br />Fred Rost, University of NSW (Emeritus), Australia<br />Louis Hissink, Consulting Geologist, Australia<br />Hetu Sheth, Earth Sciences, Indian Institute of Technology Bombay, India<br />Lassi Hyvärinen, IBM(Ret), France<br />Max Whisson, University of Melbourne, Australia<br />R.S.Griffiths, CADAS, UK<br />Adolf Muenker, Brane Industries, USA<br />Emre Isik    Akdeniz University    Turkey<br />Felipe de Oliveira Alves, Federal University of Minas Gerais, Brazil<br />Jean-Marc Bonnet-Bidaud, Service d&#39;Astrophysique, CEA, France<br />Kim George, Curtin University of Technology, Australia<br />Tom Van Flandern, Meta Research, USA<br />Doneley Watson, IBM (ret.), USA<br />Fred Alan Wolf, Have Brains / Will Travel, USA<br />Robert Wood, IEEE, Canada<br />D. W. Harris, L-3 Communications, USA<br />Eugene Sittampalam, Engineering consultant, Sri Lanka<br />Joseph.B. Krieger, Brooklyn College, CUNY, USA<br />Pablo Vasquez, New Jersey Institute of Technology, USA<br />Peter F. Richiuso, NASA, KSC, USA<br />Roger A. Rydin, University of Virginia (Emeritus), USA<br />Stefan Rydstrom, Royal Institute of Technology, Sweden<br />Sylvan J. Hotch, The MITRE Corporation (Retired), USA<br />Thomas R. Love, CSU Dominguez Hills, USA<br />Andrew Coles, Embedded Systems, USA<br />Eit Gaastra, infinite universe researcher,  The Netherlands<br />Franco Selleri, Università di Bari, Dipartimento di Fisica, Italy<br />Gerald Pease, The Aerospace Corporation, USA<br />S.N. Arteha, Space Research Institute, Russia<br />Miroslaw Kozlowski, Warsaw University (emeritus), Poland<br />John Hartnett, School of Physics, University of Western Australia, Australia<br />Robert Zubrin, Pioneer Astronautics, USA<br />Tibor Gasparik, SUNY at Stony Brook, USA<br />Alexandre Losev, Bulgarian Academy of Sciences, Bulgaria<br />Henry Hall, University of Manchester, UK<br />José da Silva, Universidade Federal de Minas Gerais, Brazil<br />Markus Rohner, Griesser AG, Switzerland<br />William C. Mitchell, Institute for Advanced Cosmological Studies, USA<br />Aurea Garcia-Rissmann, UFSC, Brazil<br />Cristian R. Ghezzi, Universidade Estadual de Campinas, Brazil<br />Daniel Nicolato Epitácio Pereira, Federal University of Rio de Janeiro, Brazil<br />Gregory M. Salyards, US Naval Sea Systems Command (ret.), USA<br />Luiz Carlos Barbosa, Unicamp, Brazil<br />Luiz Carlos Jafelice,  Federal University of the Rio Grande do Norte, Brazil<br />Michael Sosteric, Athabasca University, Canada<br />Steven Langley Guy, University of Elizabeth (Physics Department), Australia<br />Robert Fritzius, Shade Tree Physics, USA<br />Irineu Gomes Varella, Escola Municipal de Astrofísica, Brazil<br />Tom Walther, Southern Cross University Australia , Australia<br />Mauro Cosentino, University of São Paulo, Brazil<br />Moacir Lacerda, Univeersidade Federal de Mato Grosso do Sul, Brazil<br />Roberto Assumpcao, PUC Minas, Brazil<br />Roberto Lopes Parra, University of Sao Paulo, Brazil<br />Ronaldo Junio Camppos Batista, Universidade Federal de Minas Gerais, Brazil<br />Ermenegildo Caccese, University of Basilicata, Italy<br />Felipe Sofia Zanuzzo, Federal University of São Carlos, Brazil<br />Edival de Morais, Sociedade Brasileira de Física,  Brazil<br />Graham Coupe, KAZ Technology Services, Australia<br />Gordon Petrie, High Altitude Observatory, NCAR, USA,<br />Jose B. Almeida, University of Minho, Portugal,<br />Antonio Cleiton, Laboratório de Sistemas Complexos - UFPI, Brazil<br />Sergey Karpov, L.V.Kirensky Institute of Physics  Russian Academy of Sciences, Russia<br />Wagner Patrick Junqueira de Souza Coelho Nicácio, Universidade Federal de Minas Gerais, Brazil<br />Sokolov Vladimir, Special Astrophysical Observatory  of  RAS, Russia<br />Edwin G. Schasteen, TAP-TEN Research Foundation International, USA<br />Gerry Zeitlin, <a href=\"http://openseti.org\" class=\"ot-anchor\">openseti.org</a>, USA<br />Henry H. Bauer, Virginia Polytechnic Institute &amp; State University, USA<br />Yasha Fard,H.R. Cosmology Institute, Canada<br />Roberto Caimmi, Astronomy Department, Padua University, Italy<br />Tobias Keller, ETH (SFIT) Zurich, Earth Sciences,   Switzerland,<br />Deborah Foch, Center for the Study of Extraterrestrial Intelligence, USA,<br />Henry Reynolds, UC Santa Cruz, USA,<br />Paramahamsa Tewari, Nuclear Power Corporation (ret.),India<br />Jouko Seppänen, Helsinki University of Technology, Finland,<br />Cristiane Ribeiro Bernardes, Universidade Federal de Minas Gerais, Brazil<br />Eric Blievernicht. TRW,  USA<br />Dr. Robert Bennett, Kolbe Center, USA,<br />Hilton Ratcliffe, Astronomical Society of South Africa, South Africa<br />Wieslaw  Sztumski, Silesian University, Poland<br />Lars Wåhlin,Colutron Research Corporation,USA<br />Riccardo Scarpa, European Southern Observatory, Italy,<br />Olivier Marco, European Southern Observatory,   France<br />Joseph Garcia, International Radiation Protection, Germany,<br />Arkadiusz Jadczyk, International Institute of Mathematical Physics, Lithuania<br />Jean de Pontcharra, Commissariat à l&#39;Energie Atomique, France<br />Gerardus D. Bouw, Baldwin-Wallace College,  USA<br />Josef Lutz, Chemnitz University of Technology, Germany,<br />Harold E. Puthoff, Institute for Advanced Studies at Austin, USA.<br />Hermann Dürkop, Nabla Systemberatung,  Germany,<br />Klaus Fischer, Universität Trier, Germany,<br />Werner Holzmüller, University Leipzig, Germany<br />Sol Aisenberg, International Technology Group, USA<br />Richard Gancarczyk, University of Nottingham, UK<br />Steve Humphry, Murdoch University, Australia<br />Alberto Bolognesi, Università di Perugia, Italy<br />Daniele Carosati, Armenzano Observatory, Italy<br />Brendan Dean, H.R. Cosmology Institute, Canada<br />W. Jim Jastrzebski, Warsaw University, Poland<br />Gero Rupprecht, European Southern Observatory, Germany<br />Rainer Herrmann    TEWS-Elektronik    Germany       <br />Felix Pharand    University of Montreal Canada<br />Jerry Bergman    Northwest State University     USA      <br />Tibor Gasparik    SUNY at Stony Brook  USA       <br />Rei Gunn    University of Nantucket     USA<br />Sinan Alis    Eyuboglu Twin Observatories    Turkey<br />Esat Rennan Pekünlü    University of EGE Turkey       <br />Anne M. Hofmeister     Washington U.  USA<br />Quentin Foreman  IEEE  New Zeala0<br />Marc Berndl  University of Toronto  Canada<br />Y. P. Varshni  University of Ottawa   Canada<br />Robert Martinek  McMaster University  Canada<br />Bob Criss  Washington University  USA<br />Sol Aisenberg, International Technology Group, USA<br />Paul LaViolette, The Starburst Foundation, U.S.A.<br />Seetesh Pandé, Universite Claude Bernard, Lyon France<br />TAHIR MAQSOOD, PSA, PAKISTAN<br />Hartmut Traunmüller, University of Stockholm, Sweden<br />Nico F. Benschop, Amspade Research, Netherlands<br />Aaron Blake, USAF, USA   <br />M. Ross Fergus, University of Memphis, USA<br />Sonu Bhaskar, Council of Scientific and Industrial Research, India<br />Frederico V. F., Lima Universidade de Sao Paulo, Brazil<br />Andrei Kirilyuk,  Institute of Metal Physics of the National Academy of Sciences of Ukraine, Ukraine<br />Christian Jooss, Institut fuer Materialphysik, University of Goettingen, Germany<br />Sonu Bhaskar, BCISR, India<br />Robert O. Myers, ROM Technologies, USA<br />Ana Cristina Oliveira, Universidade Federal de Minas Gerais, Brazil<br />John Wey, Idaho National Laboratory, USA<br />Jorge Francisco Maldonado Serrano, UIS, Colombia<br />Pasquale Galianni, Dipartimento di Fisica Università di Lecce, Italy<br />Martín López-Corredoira, Instituto de Astrofísica de Canarias, Spain<br />Michael A. Ivanov, Belarus State University of Informatics and Radioelectronics, Belarus<br />Xiao Jianhua , Shanghai Jiaotong University, China<br />Pierre J. Beaujon, Hoornbeeck College, The Netherlands<br />J.Georg von Brzeski Helios Labs. USA<br />vidyardhi nanduri , Cosmology Research center , India<br />Mike Rotch NBSA USA<br />Paul Noel, Independent_Researcher, USA<br />Swee Eng, AW Royal College of Pathologists, SINGAPORE<br />Ricardo Rodríguez , La Laguna University , Spain<br />Linda Camp Harvard University USA<br />James B. Schwartz , The Noah&#39;s Ark Research Foundation , Philippines<br />Marshall Douglas Smith , TeddySpeaks Foundation ,USA<br />Abbé Grumel , Association Copernic , France<br />Ives van Leth Waterboard Utrecht The Netherlands<br />Francis Michael C. Perez, Department of Transportation, USA<br />AHMED A. EL-DASH UNICAMP BRAZIL<br />David C Ware, University of Auckland, New Zealand<br />Alek Atevik, Skopje Astronomy Society, Macedonia<br />Peter Rowlands, University of Liverpool, UK<br />Robert Day, Suntola Consulting, Ltd., USA<br />Luís Paulo Sousa Loureiro, Portugal<br />Maingot Fabrice, Université Louis Pasteur, France<br />Kris Krogh, University of California, USA<br />Pierre-Marie Robitaille, The Ohio State University, United States<br />Charles Creager Jr, Creation Research Society, United States<br />Stephan Gift, The University of the West Indies, St Augustine Campus, Trinidad and Tobago<br />Joseph J. Smulsky, Institute of Earth&#39;s Cryosphere Siberian Branch Russian Academy of Sciences, Russia<br />Jorge Marao Universidade Estadual de Londrina Brazil<br />Jim O&#39;Reilly Orion Consultants USA<br />Robert MacKay University of Warwick UK   <br />Chris Vermeulen Astronomical Society of Southern Africa South Africa<br />Emilson Pereira Leite Institute of Astronomy and Geophysics Brazil<br />Allen W Green ATK Space Systems USA<br />Ron Balsys Central Queensland University Australia<br />Paul ten Boom University of New South Wales Australia<br />Mosheh Thezion The Empirical Church, USA<br />Markus, Karsten,, Wilhelm-Foerster-Observatory Berlin e.V<br />Don. C. Wilson,: Process Technology and Development, USA<br />Marek Gajewski, Raytheon Co.,USA<br />Tuncay DOGAN, University of EGE, Turkey  <br />Andrew M Uhl, Pennsylvania State Univeristy, USA<br />Klaus Wieder, Independent_Researcher, Germany<br />John Caley, Omegafour Pty Ltd, Australia<br />Keith Scott-Mumby, Capital University for Integrative Medicine/California<br />Institute for Human Sciences, USA<br /><br /><b>Independent Researchers</b><br /><br />Garth A Barber, independent researcher, UK<br />Alberto Bolognesi, Independent Researcher, Italy<br />DEAN  L  MAMAS, Independent Researcher,  USA<br />David Blackford, Independent_Researcher, UK<br />Alan Rees, Independent Researcher, Sweden<br />Udayan Chakravarty, Independent Researcher,  India<br />Georg Gane, Independent Researcher, Germany<br />Robin Whittle, Independent Researcher,  Australi,<br />Joseph A. Rybczyk, Independent Researcher, USA<br />G.Srinivasan, Independent_Researcher, India,<br />Geoffrey E. Willcher independent researcher USA<br />Douglas S. Robbemond independent researcher the Netherlands<br />khosrow fariborzi independent researcher Iran<br />Etienne Bielen independent researcher Belgium<br />Steve Newman independent researcher USA<br />Ethan Skyler, Independent researcher, USA<br />Yvon Dufour, Independent Researcher, Canada<br />Jorge Ales Corona    Independent Researcher    Spain<br />Cristiano De Angelis, Independent Researcher, Italy<br />Roland Le Houillier, Independent Researcher, Canada<br />Richard Tobey  Independent researcher  USA<br />Steve McMahon  Independent researcher  USA   <br />Eugene Savov, Independent researcher, Bulgaria<br />Lars Woldseth, Independent researcher, Norway<br />Robert L. Brueck, Independent researcher, USA<br />Mark S Thornhill, Independent Researcher, United Kingdom<br />Nainan. K. Varghese, Independent Researcher,  India,<br />Andrew Kulikovsky, Independent Researcher, Australia<br />Charles Sven, Independent Researcher, USA<br />Gabriele Manzotti, Independent Researcher, Italy<br />Brian S. Clark independent researcher USA<br />Jim O&#39;Reilly Orion Consultants USA<br />Geoffrey E. Willcher independent researcher USA<br />Douglas S. Robbemond independent researcher the Netherlands<br />khosrow fariborzi independent researcher Iran<br />Etienne Bielen independent researcher Belgium<br />Steve Newman independent researcher USA<br />Thomas G. Franzel independent researcher USA<br />Bernhard Reddemann independent researcher Germany<br />Ives van Leth Waterboard Utrecht The Netherlands<br />Jeroen van der Rijst independent researcher The Netherlands<br />Harry Costas independent researcher Australia<br />Andrei Wasylyk independent researcher Canada<br />Jack Ruijs independent researcher The Netherlands<br />Leo Sarasúa independent researcher The Netherlands<br />Edward Smith independent researcher USA<br />Linda Camp Harvard University USA<br />Gary Meade independent researcher USA<br />Stan Kabacinski independent researcher Australia<br />Jack Dejong independent researcher USA<br />J.Georg von Brzeski Helios Labs. USA<br />Nigel Edwards, Independent Researcher,  Australia<br />Dieter Schumacher, Independent Researcher, Germany  <br />Rudolf Kiesslinger, Independent Researcher, Germany<br />Gerd Schulte, Independent Researcher, Germany<br />Stuart Eves, Independent Researcher, UK<br />James Marsen independent researcher USA<br />Edgar Paternina independent researcher Colombia<br />Donald E. Scott  Independent_Researcher: USA<br />José M?df; Cat Casanovas, Independent researcher, Spain<br />Aaron Hill, Independent Researcher, USA,<br />Hans-Dieter Radecke, Independent Researcher, Germany<br />Mawell P Davis    Independent Researcher    New Zealand <br />Gordon E. Mackay    Independent Researcher  USA<br />Dave Sagar    Independent Researcher    USA      <br />Benjamin I. Iglesias    Independent Researcher    Spain       <br />Alper Kozan    Independent Researcher     Turkey<br />Hartmut Warm, Independent Researcher, Germany<br />Jan Mugele    Independent Researcher   Germany<br />Andrew Rigg    Independent Researcher   Australia<br />Thomas Riedel  Independent researcher Denmark<br />Helen Workman  Independent researcher  Canada<br />Morris Anderson, Independent researcher, USA<br />Mario Cosentino, Independent researcher, France<br />Paul Richard Price, Independent researcher, United States<br />Philip Lilien, Independent Researcher, USA<br />Ott Köstner, Independent researcher, Estonia<br />Bozidar Kornic, Independent researcher, USA<br />William F. Hamilton, Independent researcher, U.S.A.<br />Joel Morrison, Independent researcher, USA<br />James R. Frass, Independent Researcher, Canada<br />Arnold Wittkamp, Independent Researcher, Netherlands<br />Dimi Chakalov, Independent Researcher, Bulgaria<br />Herb Doughty, Independent Researcher,  USA<br />Robert F. Beck, Independent Researcher, UK<br />Tuomo Suntola, Independent Researcher, Finland<br />Richard Hillgrove, Independent Researcher, New Zealand<br />Herbert J. Spencer,  Independent Researcher,  Canada<br />Thomas B. Andrews, Independent Researcher, USA<br />John Dean , Independent Researcher , South Africa<br />Peter Loster , Independent Researcher , Germany<br />Charles Francis, Independent Researcher , UK<br />Ahmed Mowaffaq AlANNI , Independent Researcher , Iraq<br />Mogens Wegener , Independent Researcher , DENMARK<br />Peter Jakubowski, Independent Researcher , Germany<br />John Brodix Merryman Jr. , Independent Researcher , USA<br />Christian Boland , Independent Researcher , Belgium<br />Warren S. Taylor, Independent Researcher, USA<br />Constantin Leshan, Independent Researcher , Moldova<br />Avid Samwaru, Independent Researcher, USA<br />Thomas Goodey, Independent Researcher, UK<br />Johan Masreliez, Independent Researcher, USA<br />Efrèn Cañedo, Independent Researcher , Mèxico<br />Michael Bliznetsov, Independent Researcher, Russia<br />Peter Michalicka, Independent Researcher, Austria<br />Ivan D. Alexander , Independent Researcher,<br />S. Ray DeRusse, Independent Researcher, USA<br />Chris Maharaj, Independent Researcher, Trinidad<br />Peter Warlow, Independent Researcher, United Kingdom<br />Gordon Wheeler, Independent Researcher, United States<br />Boxer Ma, Independent Researcher, Thailand<br />Robert Wido, Independent Researcher, United States<br />John Hunter independent researcher U.K<br />Marcelo de Almeida Bueno independent researcher Brazil<br />Jean-Pierre Ady Fenyo, Independent Researcher, United Status<br />Adam W.L. Chan , Independent Researcher , Hong Kong<br />Renato Giussani independent researcher Italy<br />Brian S. Clark independent researcher USA<br />Mustafa Kemal Oyman, Independent Researcher, Turkey<br />Richard Wayte, independent researcher, UK<br />Ron Ragusa independent researcher USA<br />N.Vivian Pope independent researcher UK<br />Roy Caswell independent researcher United Kingdom<br />Erin S. Myers independent researcher USA<br />Ugo Nwaozuzu independent researcher Singapore<br />Daniel Coman independent researcher USA<br />Birgid Mueller independent researcher Mexico<br />Mihail Gonta independent researcher Moldova<br />Vladimir Rogozhin independent researcher Russia<br />J. J. Weissmuller independent researcher USA<br />Muhammed Anwar independent researcher India<br />Geldtmeijer Djamidin independent researcher Netherlands<br />Scott G. Beach independent researcher Canada<br />Neil Hargreaves independent researcher UK<br />julian braggins independent researcher Australia<br />Kari Saarikoski, Independent_Researcher, Finland<br />Marcelo de Almeida Bueno independent researcher Brazil<br />Ron Ragusa independent researcher USA<br />Brian S. Clark independent researcher USA<br />Geoffrey E. Willcher independent researcher USA<br />Douglas S. Robbemond independent researcher the Netherlands<br />khosrow fariborzi independent researcher Iran<br />Etienne Bielen independent researcher Belgium<br />Steve Newman independent researcher USA<br />John Hunter independent researcher U.K<br />Jeroen van der Rijst independent researcher The Netherlands<br />Thomas G. Franzel independent researcher USA<br />Bernhard Reddemann independent researcher Germany<br />Leo Sarasúa independent researcher The Netherlands<br />Edward Smith independent researcher USA<br />Gary Meade independent researcher USA<br />Stan Kabacinski independent researcher Australia<br />Jack Dejong independent researcher USA<br />Harry Costas independent researcher Australia<br />Andrei Wasylyk independent researcher Canada<br />Jack Ruijs independent researcher The Netherlands<br />James Marsen independent researcher USA<br />Edgar Paternina independent researcher Colombia<br />Ghertza Roman, Independent_Researcher, Romania<br />Roland Schubert, Independent_Researcher,   Germany<br />Alexandre Wajnberg, Independent Researcher, Skyne, Belgium<br />Dennis H Cowdrick Scientific   Independent_Researcher:   USA<br />Michail Telegin  Independent_Researcher: Russia<br />Robert L Stafford,  Independent_Researcher,   USA   <br />Martin Sach, Independent_Researcher, UK<br />Charles L. Sanders, Independent_Researcher, USA/South Korea<br />Alex Carlson, Independent_Researcher,  United States<br />Lyndon Ashmore, Independent_Researcher,  UK<br />Liedmann, Matthias, Unaffiliated_Scientific_Researcher, Germany<br />Ingvar Astrand, Independent_Researcher, Sweden<br />Olli Santavuori, Independent_Researcher, Finlande<br />Touho Ankka, Independent_Researcher, Finland<br />JR Croca, Independent_Researcher,  Portugal<br />Sol Aisenberg, Independent_Researcher, USA<br />Mustafa Kemal OYMAN, Independent_Researcher,   Turkey<br />Gerard ZONUS, Independent_Researcher,  FRANCE<br />David W. Knight, Independent_Researcher,   USA<br />Marcel Lutttgens, Independent_Researcher,   France<br />Dr Stephen Birch, Independent_Researcher, United Kingdom<br />Abramyan G.L., Independent_Researcher,   Russia<br />Martin Peprnik, Independent_Researcher, Slowakia<br />Van Den Hauwe, PhD, Independent_Researcher,: Belgium<br />Ingvar Astrand, Independent_Researcher, Sweden<br />Daniel Toohey, Independent_Researcher, Australia<br />Jed Shlackman, M.S. Ed. (LMHC, C.Ht.), Independent_Researcher, USA<br />Dr. John Michael Nahay, Independent_Researcher <br />Guido Grzinic, Independent_Researcher, Australia<br /><br /><b>Other Signers</b><br /><br />Charles Weber,USA<br />David Gershon ,USA<br />Peter G Smith ,USA<br />Richard J. Lawrence ,USA<br />Naszvadi László, Hungary<br />Roger W. Browne, USA<br />Bart Clauwens, Netherlands<br />Noah Feiler-Poethke, USA<br />Jonathan Hardy,  UK<br />John S. Kundrat, USA                <br />Vincent Sauve, USA<br />Chris Somers,  Australia<br />Jagroop Sahota, USA<br />Edgar Raab, Germany<br />Gordon Hogenson, USA<br />Burebista Dacia, Romania<br />Christel Hahn, Germany<br />Burebista Dacia, Romania<br />Christel Hahn, Germany<br />Robert Angstrom, USA <br />Norman Chadwick, USA<br />Harley Orr, USA<br />Clive Martin-Ross, UK<br />Alasdair Martin, UK<br />Marcus Ellspermann, Germany<br />Bruce Richardson, USA<br />John Dill, USA<br />Judith Woollard    Australia      <br />Michael Cyrek        USA<br />Randall Meyers    ITA<br />Craig Arend        USA       <br />Onur Cantimur    Turkey  <br />Roland Scheel  France<br />Murat Isik  Turkey<br />Markus Hellebrandt  Germany<br />Mehmet Kara  Turkey<br />Abhishek Dey Das  India<br />D. N. Vazquez  USA<br />Suzan R. Rodenburg  USA<br />Shuming Zhang  Chin<br />Codie Vickers  USA<br />Richard Tobey  USA <br />Elfriede Steiner-Grillmair, Canada<br />Gabriele Manzotti, Italy<br />Michael Wember, USA<br />Fuksz Levente, Romania<br />Seppo Tuominen, Finland<br />Marvin C. Katz, USA<br />Laura Fridley, USA<br />Michael Christian, U.S.A<br />Edgar S. Hill USA<br />Q. John T. Malone USA<br />Michael Bruttel  Switzerland<br />Eric W. LaFlamme USA<br />Robert Diegis, Romania<br />William S. Jarnagin, USA<br />Kevin Glaser, USA<br />Robert Diegis, Romania<br />William S. Jarnagin, USA<br />Kevin Glaser, USA<br />JoAnn Arcuri USA<br />Attila Csanyi USA<br />Pratik Sinha India<br />Donald C. Bull New Zealand<br />Hans Walhout Netherlands<br />Robyn Stewart Australia<br />Tor Johannessen Norway<br />Rick Schmidt USA<br />Terence Watts UK<br />Jody Fulford USA<br />Gene Gordon USA<br />Monica Veloso Alves Brazil<br />Ferdi Prins South Africa<br />Adam Hansil USA<br />Herbert M Watson USA<br />John Patchett UK<br />Jurrie Noordijk, The Netherlands<br />P.S. Phillips, U.S.A<br />JoAnn Arcuri USA<br />Martin Gradwell , ns, United Kingdom<br />Sami Murtomäki, Ns, Finland<br />Anthony Abruzzo , United States<br />Tim Reed, ns, USA<br />Daniel Rijo , ns, USA<br />Ken Couesbouc , ns, France<br />David L. Harrison, United States<br />Kees de Boer<br />Tom HigginsUSA<br />David Calder Hardy, New Zealand<br />Jochen Moerman, Belgium<br />Berend de Boer,  New Zealand<br />Edward E. Rom, USA<br />Jukka Kinnunen, Finland<br />Jerome M. Hall, USA<br />Maria Alvarez, Argentina<br />Paul Chabot, Canada<br />Julia,  Russia<br />Amr Malik: Canada<br />Maureen Bevill, USA<br />Horst Barwinek, Austria<br />Lindsay Smith,  Australia  <br />Richard DeLano, USA. <br />Stefan Landherr, Australia<br />Peter Wilson, USA<br />Gregory Kiser, USA<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Phantom Time Hypothesis","summary":"The Early Middle Ages - also Known as the Dark Ages - is Known by Specialist Historians as One of the Most Problematic Periods in the Study of Human History / Tree Ring Data Seems Missing, Documents Often Turn Out to be Forged, Buildings from this Period Seem Difficult to Track Down, and Some Basic Calendar Algebra Reveals a Very Serious 250-Year Gap / The Outsider Maverick Historian Heribert Illig Proposes that we've been Fooled - this Period of Time Never Actually Happened","image":"https://lh4.googleusercontent.com/-RkKmGeiU99Y/WFxTnJgYyvI/AAAAAAAAJ14/jbi7acHd1Ys49giUMEgCf4QXv3ExS7hNwCL0B/the-phantom-time-hypothesis-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/dZ5tG3BxD1F","publishDate":"2016-12-22T22:29:28.044Z","updateDate":"2017-01-03T01:34:08.991Z","text":"<b>The Phantom Time Hypothesis: The Early Middle Ages - also Known as the Dark Ages - is Known by Specialist Historians as One of the Most Problematic Periods in the Study of Human History / Tree Ring Data Seems Missing, Documents Often Turn Out to be Forged, Buildings from this Period Seem Difficult to Track Down, and Some Basic Calendar Algebra Reveals a Very Serious 250-Year Gap / The Outsider Maverick Historian Heribert Illig Proposes that we&#39;ve been Fooled - this Period of Time Never Actually Happened</b><br /><br />GRAPHIC REFERENCES:<br /><br />[1] This graphic and quotes were pulled from <a href=\"http://dendro.cornell.edu/projects/aegean.php\" class=\"ot-anchor\">http://dendro.cornell.edu/projects/aegean.php</a>, but you&#39;ll observe that the link on their page is currently broken.  Not a problem, we have it!<br /><br />[2] <a href=\"http://www.bearfabrique.org/Catastrophism/illig_paper.htm\" class=\"ot-anchor\">http://www.bearfabrique.org/Catastrophism/illig_paper.htm</a><br /><br />This subject is a true test of my editorical skills.  To get it right, the burden is to keep the interest of the reader sufficient to convey not only the claims being made, but also the reasons to believe them, as well as the larger, lesser-known context for why those reasons might be true in the first place -- which, for me, is the transferable skill -- the wisdom -- which is the most valuable take-away from this discussion.<br /><br />Keeping a person&#39;s interest through all of that to that wisdom is especially challenging on this subject.  But, I will do my best.  Realize that entire books have been written on this subject, and there is no shortage of papers and websites discussing it -- not much of it, honestly, presented in a manner which assumes a short attention span!<br /><br />(NOTE: I am of course aware that Velikovsky has pointed to a 360-day year in earlier times in <i>Worlds in Collision</i>.  I want to note the that Heribert Illig would have definitely been aware of this.  It&#39;s of course possible that the length of the year might have slightly changed over time, but we should also let Illig make his case and do the math ourselves.)<br /><br /><a href=\"http://www.newenglishreview.org/Robert_Wolfe/Calendar_Wars/\" class=\"ot-anchor\">http://www.newenglishreview.org/Robert_Wolfe/Calendar_Wars/</a><br /><br /><b><i>&quot;One of the most ancient and the most fundamental forms of ideological hegemony is control over the calendar.</i></b><i> For example, by dividing all time into Before and After the alleged date of the birth of Jesus Christ, the Christian religion has succeeded in introducing its point of view into the heart of the so-called &#39;civil calendar&#39; now in use in the greater part of the world.&quot;</i><br /><br /><a href=\"http://mysteriousuniverse.org/2014/06/historian-says-the-middle-ages-never-happened-phantom-time-theory/\" class=\"ot-anchor\">http://mysteriousuniverse.org/2014/06/historian-says-the-middle-ages-never-happened-phantom-time-theory/</a><br /><br /><i>&quot;There is an entire sector of conspiracy culture focused on what’s known as alternative history (also known as revisionist history).  That is precisely what it sounds like; folks who look at our collective past and declare &#39;that’s not how it happened!&#39;  The alternative history title covers a huge amount of material, from ancient aliens, Atlantis, biblical history, and planetary science; to Shakespeare, and war history.  If it happened at some point in the past, then someone, somewhere, thinks it happened differently than the establishment says it did.</i><br /><br /><i>Most often this disparity manifests as the claim that the history books have left out something important.  </i><b><i>It’s often said that history is written by the victors, and the obvious logic in that can hardly be denied.  A dead enemy can’t defend his ideologies.</i></b><i>  A great many people are vociferous in their belief that our pre-history is far better known by academics than is generally thought, and in that hidden knowledge lays the truth of countless lost civilizations and technologies.  In some rare cases though, the conspiracy theorist asserts pretty much the exact opposite.</i><br /><br /><i>Enter Heribert Illig.</i><br /><br /><b><i>Little known German historian, writer, and publisher, Heribert Illig is perhaps one of the most eccentric conspiracy theorists out there.  He developed the Phantom Time Theory in the late 1980’s and early 1990’s, eventually elucidating his ideas in several books, none of which has yet been translated to English.  The Phantom Time Theory says that the years AD 614 through 911, a period commonly known in standard history as the Early Middle Ages, never happened.</i></b><i>  Get a grip on those words before moving on.  Illig says that the Middle Ages -- a period that includes the collapse of the Roman Empire, the rise of Islam, the rise (and fall) of the Byzantine Empire, and the Viking Age, among other foundational epochs -- didn’t happen, and were at some point made up by the academic establishment, through a series of blunders </i><b><i>and a heavy reliance on antique documents, which he believes are unreliable.</i></b><br /><br /><i>Illig says that the current year isn’t 2014, but rather is 1739; some 273 years out of date.</i><br /><br /><b><i>The basis of his theory is the discrepancies between the Julian calendar and the current Gregorian calendar. The Julian calendar, introduced by Julius Caesar, was long known to introduce a discrepancy from the tropical year of around one day for each century that the calendar was in use. By the time the Gregorian calendar was introduced in AD 1582, Illig alleges that the old Julian calendar &#39;should&#39; have produced a discrepancy of thirteen days between it and the real (or tropical) calendar. Instead, the astronomers and mathematicians working for Pope Gregory had found that the civil calendar needed to be adjusted by only ten days. From this, Illig concludes that the AD era had counted roughly three centuries which never existed.</i></b><br /><br /><b><i>He also cites a lack of archaeological evidence for the events thought to have occurred during that time, as well as the presence of Romanesque architecture in tenth-century Europe.</i></b><br /><br /><i>Why?  Good question.</i><br /><br /><b><i>Illig claims, most ardently, that the entire Carolingian period, including the existence of Charlemagne, was a fabrication instigated by Otto III, during his campaign against the Byzantine Empire and his successful quest to become the King of Italy and, in turn, the Holy Roman Emperor.</i></b><br /><br /><b><i>As you may imagine, however, the historical community does not support Illig’s theory.&quot;</i></b><br /><br />[...]<br /><br /><a href=\"http://www.bearfabrique.org/Catastrophism/illig_paper.htm\" class=\"ot-anchor\">http://www.bearfabrique.org/Catastrophism/illig_paper.htm</a><br /><br />Anomalous Eras - Best Evidence: Best Theory<br />Toronto Conference, June 28, 29, 30, 2005<br /><br />The Invented Middle Ages<br />By Heribert Illig<br /><br /><i>&quot;The thesis of the &#39;Invented Middle Ages&#39; created quite a stir in the German-speaking world. For years there has been an on-going discussion of this thesis that keeps flaring up again and again, </i><b><i>though it was really less of a discussion than a determined attack by mainstream historians, an attempt to eliminate that which must not be. At present, German medievalists, after several failed attempts at refutation, are no longer willing to react to my arguments. Nevertheless, despite their attempts to ignore me, the debate continues. Two of my books have been translated into Hungarian. But although the general public takes great interest in the subject, the scholars remain silent.</i></b><i> For my presentation here in Canada I decided on the following structure: I will present the steps that led me to results that are totally different from all the scholars who had previously dealt with this period.</i><br /><br /><i>How I arrived at my Thesis</i><br /><br /><i>A brief word on my credentials. I got my doctorate on the subject of the Viennese cultural historian Egon Friedell, but I am not a historian in the narrow sense of the word. I did study economics, mathematics, physics, some art history and Egyptology. My interest in Egyptian culture brought me into contact with Velikovsky&#39;s theses. </i><b><i>Velikovsky&#39;s ideas showed me that the accepted time line has holes like a sieve. In other words: many periods have left so frighteningly little evidence that we must ask: did this particular period ever exist or did it originate on the desks of scholars?</i></b><i> A response to this question was given in the book I cowrote with Gunnar Heinsohn, &#39;Wann lebten die Pharaonen?&#39; (When Did the Pharaohs Live?) This brings me to (Slide I) the first and essential point: Giving up an axiom, i.e. a supposition that nobody has ever doubted. As an example of such an axiom, consider this statement: the accepted time line is God-given and therefore does not need to be checked. </i><b><i>In my study of Ancient Egypt I learned that entire periods may be written up in books, even though they have not left any traces on earth and, more importantly, in the earth. I was thus forewarned when I started looking at the history of the Middle Ages.</i></b><br /><br /><i>[...]</i><br /><br /><i>Whereas questions of ancient Egyptian chronology are not directly linked with our present chronology, it&#39;s a different story with the Middle Ages. </i><b><i>Today we use the Gregorian Calendar, which was defined by Pope Gregory XIII in 1582. It is an improved version of the calendar that Julius Caesar introduced in 45 BC. At that time, the lunar calendar of the Romans was in an utter muddle because corrupt persons had bribed the responsible priests to add another month to the tax year. Caesar solved the problem by inserting three months into the calendar -- this led to a year with 445 days --, by going over to a clear solar year as a basis for the calendar year, and by introducing a clear intercalary rule: every fourth year is an intercalary year with one additional day.</i></b><br /><br /><b><i>This proved to be an excellent system, but not forever because the year was not exactly 365 days and 6 hours long ... From an astronomical point of view, there were 674 seconds too many. This is not even a quarter of an hour per year, but in 128 years this error adds up to a whole day. After 1,282 years, therefore, there are 10 days too many. In 1582 Pope Gregory ordered precisely ten days to be skipped in order to make the day count agree with the celestial situation.</i></b><i> In passing it should be mentioned that the Pope introduced an improved intercalary rule which requires revision only every 2000 years. What is decisive for us is the following:</i><br /><br /><b><i>The ten days that were skipped in October 1582 corrected the mistake that had accumulated in the Julian Calendar over the previous 1,282 years. However, if you deduct these 1,282 years from 1582, you don&#39;t arrive in the year of Caesar&#39;s calendar reform, 45 BC, but in the year AD 300! If he had gone all the way back to Caesar, Pope Gregory would have had to skip 13 days. He did not do so, and yet: the astronomical situation and the calendar agreed. His jump was too short, yet he landed in the right place.</i></b><br /><br /><i>[...]</i><br /><br /><i>What started me off was a phone call from a friend. He was looking into forgeries in the Middle Ages. In 1986 there had been a conference on this subject in Munich ... The final talk was given by Prof Horst Fuhrmann, then president of &#39;Monumenta Germaniae Historica&#39;, the institution that prepares the critical edition of the old and oldest German documents. Fuhrmann talked of &#39;forgeries with anticipatory character&#39;. I quote:</i><br /><br /><b><i>&#39;Sylvester legend, Constantinian Donation, Symmachian Forgeries, Pseudo-Clemens letters, Pseudo-Isidorian Forgeries: let us stop at that list. All these forgeries have the characteristic that at the time they were written, they had hardly any effect. At the time of their creation, they had anticipatory character.&#39;</i></b><br /><br /><i>[...]</i><br /><br /><b><i>Comparing all the biographies, I soon noticed that [Charlemagne&#39;s] achievements would have required the lives of two, three, or four &#39;normal men.&#39;</i></b><i> In 44 of the 46 years of his reign he goes to war. Like a Medici, he has a court of scholars at Aachen that gathers the cleverest Europeans of his time. Depending on whose calculation we use, during the course of his life he traveled the equivalent of two or even three times around the globe. At the same time he was a perfect lawmaker: he formulated more than a hundred decrees, he updated jurisdiction by introducing the jury system; wherever he was he administered justice.</i><br /><br /><i>But he was also active as a folklorist and mythologist, ordering old legends and folk tales to be collected; he was a linguist both for German and for Latin; he ordered -- remember he was illiterate! -- a cleaned-up version of the Bible for he was obviously an exceptional theologian who even conducted ecclesiastical synods himself. He was a grammarian, a founder of schools, of libraries and universities - all of these long before the time when such institutions are first mentioned in Europe. In my book I have collected more than a hundred of the Great Charles&#39; characteristics. This makes an extraordinary list: he was his own minister of agriculture; he was the physical as well as the spiritual ancestor of half of Europe; he was sole ruler to whom omniscience was ascribed; he was a classical philologist, architect, astronomer, builder and so on.</i><br /><br /><i>The conclusion is simple: far too much is ascribed to this one person. How much of it is true? The written sources cannot answer that question, though even while he was alive and before he was crowned emperor they speak of the beacon of Europe and the father of Europe. For the moment, let&#39;s leave aside the written sources and consult the material evidence. For the item Architectural Findings vs. Architectural History, the famous Aachen Palatine Chapel, today&#39;s Aachen Cathedral, is the best example. </i><b><i>For this structure, his most important palace, Charlemagne was not only the patron but, according to some reports, also the architect and building supervisor. Because this building has survived to our time, we can study it thoroughly. In doing so, I found more than 24 building details that -- according to architectural history -- are present here already in perfection, before AD 800. But these architectural features have neither predecessors nor direct successors. All these details had to be rediscovered independently during the subsequent Romanesque Period. This is a riddle of a complexity that does not occur elsewhere.</i></b><br /><br /><i>[...]</i><br /><br /><i>Written sources mention numerous Dark Age buildings in the Frankish region. It has been calculated (not by me but by experts on the documents in question) that there are 1,695 major buildings from the period between 476, the end of the West Roman Empire, and 817, which is three years after the death of Charlemagne. The scholars who came up with this number understood &#39;major buildings&#39; to mean palaces, churches, and monasteries. </i><b><i>When we check the actual number of preserved buildings and ruins, we can be happy if more than 97 percent haven&#39;t disappeared.</i></b><br /><br /><i>The same applies to the period of Charlemagne. He was supposed to have built 65 palaces, and altogether 313 major buildings. Of the palaces, a maximum of five have been preserved. Of the monasteries, not a single foundation exists. We only know the famous monastery plan of St. Gallen but for an architectural drawing this is centuries too early. </i><b><i>Of this number, too, about 97 percent of the buildings have disappeared.</i></b><i> The percentage is probably even higher, since besides the Aachen Palatine Chapel there is hardly a building that is ascribed exclusively to the Carolingians.</i><br /><br /><i>[...]</i><br /><br /><i>First, we collected places named in written documents. We found 2,200 places we called &#39;document places&#39;. But when we started looking carefully for archaeological evidence, we discovered something strange: in only 88 of these document places have archaeologists discovered any remains at all that they ascribe to the Carolingians or to the early Bavarians, called the Agilolfing dukes and their time. </i><b><i>So, here, too, in 96 percent of all possible cases there is nothing to report! It needs to be emphasized: archaeological finds are almost never forged, because nobody secretly puts a foundation in the ground to pretend an old building existed.</i></b><br /><br /><b><i>On the other hand, the number of forged documents is constantly growing. With each new investigation, new forgeries are discovered. The number of genuine documents diminishes all the time. In fact, it&#39;s moving toward zero. If this trend continues, Medieval Studies, an important branch of the historical sciences that puts its trust almost entirely in documents, will soon have lost its reason to exist. Most medieval scholars don&#39;t think much of my thesis, and it&#39;s not hard to see why. If I am right, then the number of genuine documents from the Phantom Era must be zero.&quot;</i></b><br /><br /><a href=\"http://www.philjohn.com/papers/pjkd_h02.html\" class=\"ot-anchor\">http://www.philjohn.com/papers/pjkd_h02.html</a><br /><br />ILLIG&#39;S INVENTED HISTORY<br /><br />On Heribert Illig&#39;s Theory of Invented Time<br /><br />Section Index<br /><br />1. History, Historiography and Time<br />2. In Favor: For Discussion<br />3. Against: Circular Argumentations<br />4. The Appeal of Conspiracy Theories [skipping this section]<br />5. What Remains<br /><br />continued in:<br />Illig the Magician. Re-Examining Illig&#39;s Rhetoric<br />[<a href=\"http://www.philjohn.com/papers/pjkd_h04.html\" class=\"ot-anchor\">http://www.philjohn.com/papers/pjkd_h04.html</a>]<br /><br /><i>&quot;1. History, Historiography and Time</i><br /><br /><i>History is a social science recording and analyzing the past. History is the link to our ancestors, the one we ask for information when we want to go back in time. </i><b><i>Yet history is also historiography, it is history written down, narrated, traded over from one person to another, it is a product produced by human beings, thus subject to interpretation, selection and canonizing. History is also tradition, depending on traces left over for subsequent fixation and analysis.</i></b><i> History can rely both on textual sources as well as material ones, both having advantages and disadvantages.</i><br /><br /><i>History, however, is most of all depending on one crucial instance: Time. History is also chronology -- it is events listed in a static and reliable way, it is a strict logic of before and after.</i><br /><br /><b><i>History is also the history of persons, of persons considered crucial for whatever interpretation&#39;s sake, persons standing out in the abstract concept of national or world history, persons like Alexander, Caesar, Charlemagne, Washington, Lincoln, Kennedy or Gorbachev. History linked to persons both makes it more accessible, more human, for the average recipient, as well as it creates heroes out of ordinary people.</i></b><br /><br /><i>Yet when history cannot anymore be truly a field of academic and scientific research, as it is far too often regarded as tradition, certain stereotypes and simplifications -- and also lies -- will live on and on, although they surely ridicule the academic field behind. </i><b><i>History is a merger between fact and fiction, between actual truth and myth, where nothing really can be seen &#39;as it is&#39; -- and truth and lies are often just a matter of degree.</i></b><i> Yet from this philosophical, perception- and authorship-oriented post-structuralist point of view, this is true for almost everything, following a straight line from the Platonian critique of writing.</i><br /><br /><i>Sometimes this receptional relativism can come down to actual hard choices between the truth, more or less, and outright lies. Is something recorded perhaps wrong, a deliberately false account of something allegedly true? </i><b><i>Especially Medieval history is full of forgeries. Documents have often been created in the aftermath, fixating the status quo in a written way to counter newly-made claims or demands.</i></b><i> A monastery may have &#39;created&#39; a document illustrating the purchase of lands previously in its possession in order to justify its claims. Forgery thus doesn&#39;t have to constitute an outright lie, often its occurrence can be explained through the introduction of writing into a prevaling oral culture. Yet that doesn&#39;t exclude real forgeries, and for a historian, it can be utmost fascinating to discover the &#39;actual truth&#39; behind an event, deconstructing myth and creating history.</i><br /><br /><i>There are small forgeries, large ones, and huge ones. Heribert Illig, a German arts historian, claims to have discovered the ultimate forgery: The invention of more than a quarter millennium in the Middle Ages, somewhere between 700 and 1000 AD. </i><b><i>This would ultimately question the identity of one of the above mentioned icons and heroes of history: Charlemagne, coronated 800 AD, the founder of Europe</i></b><i>, the emperor who restituted and resurrected the Roman title of Emperor for the West, the Emperor who created the working relationship between Roman Church and Imperial rule, a relationship forming Europe till the Reformation.</i><br /><br /><b><i>Illig&#39;s first book presenting his thesis [1] thus mainly deals with deconstructing, or rather, destructing this icon, not quite unconvincingly, however, failing to present the grander scheme, the larger and global scope. This took place in his subsequent book [2], putting the thesis to the test of time and place, taking into account also scientific and academic analyses and aspects.</i></b><i> For him, there can be no doubt: The time in question has never taken place, moving the 21st century into the 18th.</i><br /><br /><b><i>It is unsurprising that such a thesis would create quite a turmoil not only in the academic and scientific community. It is, however, somehow a natural process that there can be theses which may challenge the prevaling point of view. Yet Illig is not part of the established academic and scientific community, he&#39;s an outsider, which makes it different for him to get accepted. Even more so, this is complicated by his manner of presentation, lacking the necessary self-doubt and methodical reflection.</i></b><br /><br /><i>The task of this essay will be to try approaching this topic from a benevolent point of view, ignoring the outsider status of Illig&#39;s, </i><b><i>focussing the discussion not on himself as a person, as it is often, sadly, done,</i></b><i> but on the thesis itself and the way of presentation.</i><br /><br /><i>PJK</i><br /><i>March 31st, 2001</i><br /><br /><i>2. In Favor: For Discussion</i><br /><br /><i>When somebody has been so desperately thrashed by so great many people, you somehow wonder if they shouldn&#39;t be right, but from a critical perspective, you perhaps even more suspect the opposite to be true, or rather, more true at least. </i><b><i>Approaching a thesis as radical as Illig&#39;s may perhaps best be done by imagining beforehand the criticism it may invoke, and put it in perspective. It can come as no surprise that traditional thinking cherishing the Medieval building blocks of Europe would be furiously opposed to his notion that Charlemagne is a character of fiction more than fact. It can come as no surprise, also, that Illig chose to attack precisely that figure in his first book to establish his thesis. Charlemagne has become the godfather of the European Union, the founding icon of the Roman-Franco-Germanic Empire in union with the papacy, the restorator of the West Roman Empire, the one who has finally brought the idea of civilization, i.e. the Roman idea of civilization, to the barbarians of the North. That Charlemagne was a magician, himself barely literate, creating an empire out of nowhere, creating culturally highest forms, like his Palatine Chapel, quasi out of nowhere, and with no real precursors, with everything coming after him being not quite so elaborate, and most obviously, his reign coming out of nowhere and going nowhere. The evidence Illig quotes to illustrate the absurdities in both the common and the academic understanding of Charlemagne is so striking that it does need further attention.</i></b><br /><br /><i>That is not to say I believe him to be entirely right in his assumptions, yet for the sake of argument, let&#39;s just concentrate on a more benevolent perspective for now. The factual basis he collects is striking, to say the least, in both books. </i><b><i>He illustrates quite authoritatively -- which is part of the problem -- how a certain scheme tends to repeat itself within the histories of several nations. First, you have either total chaos or total silence. Then, out of the darkness, a shining hero appears, out of nowhere, pulling the strings and establishing a glorious rule. That having lasted for a short while, the region falls back into a similar apathy like the previous one, until it comes to new glory, only much slower, under a subsequent ruler. All the inventions and achievements of the figure of light in between could very well have been invented beforehand or afterwards, as all his knowledge is lost. The greatness of his &#39;golden age&#39; has to be &#39;recovered&#39; -- which could just be a term for its invention in the first place. Usually, such tales are placed at the beginning of written culture, coming out of an oral culture.</i></b><i> The &#39;Golden Age&#39; described by Ovid, &#39;El Dorado&#39;, variations on a &#39;Paradise&#39; theme, the Tower of Babel; utopian places either loosely or not at all rooted in factual history, but rather illustrating the dreams and political ideology of the people who purported such myths and enlarged them. Sometimes, real conflicts or heroes may have been exaggerated, sometimes, a slow and gradual development was enlarged to make it more telling. Thus the Aeneas myth and the Romulus/Remus tale for Rome, drawing on Homer&#39;s Ilias, itself possibly drawing on a -- probably much less grand -- real war over Troia. Thus also the invention of Lykourgos as the founding king of Sparta. Thus the heroification of people like Napoleon, Washington, Jefferson and Gandhi. What has been done way past, what is being done now, </i><b><i>the mixing of fact with fiction, the &#39;mythification&#39; of history, why shouldn&#39;t it be part of the time in the middle, the Middle Ages? Why should cultural interpretation suddenly cease to exist?</i></b><br /><br /><i>Yet still, there&#39;s something different at work here also, an atrocity perhaps even: Here it is not about the beginning of civilization. It is not about ancient cultures, it is about a time not too remote from our own, a time past the Roman Empire, a time where a writing culture and a culture of literacy already existed. It is not -- in Illig&#39;s understanding -- the mere exaggeration of a factual basis, it is something like a combination of some truth with a whole set of outright lies -- a founding myth serving both an Ottonian and a Byzantine regime, </i><b><i>something of a conspiracy even, a conspiracy to create about 300 years of history to conceal some very embarassing facts in relation to the battle between Christianity and the Islam, to exaggerate one&#39;s historical precursors, to come closer to a magical year 1000, a year of change, a millennium that could bring religious revelation and the apocalypse.</i></b><br /><br /><i>How could that possibly have been done? Manipulating the entire concept of historical time, enforcing a set of new truths which were to replace what was commonly known? Commonly known -- that&#39;s perhaps the key issue at work here. </i><b><i>What we have now, a convenient system of chronology, the BC/AD dating, was probably one of the least common systems around the time of discussion. Instead, there had been a mixture of various chronologies, different not only from state to state, but also containing different chronological traditions within one state. Time was counted by the duration of the reign of a king or pope, an emperor, or a consul, there even was the somewhat artificial construction ab urbe condita, counting from the foundation of the City of Rome, traditionally defined as having taken place in 753 BC. The BC/AD chronological system was to originate much later [3], and was not that widely in use even then, only around the first millennium and after that, it was taken up more widely. That means: If -- I&#39;m talking hypothetically here -- if someone were to have manipulated that system, it would have had to happen before that, not much later. Thus the window of opportunity, as one could call it, was still open. And if you take into account the persons who were actually able to write, the different systems of power in use, the different traditions from region to region, you could perhaps say that -- with some central guidance -- the introduction of a new system of chronologizing events, and the creation of a new writing culture and a new way of writing in the Byzantine Empire, and given the close link between Constantinople and the Ottonian regime, such a manipulation may sound a bit more possible.</i></b><br /><br /><i>The examples and bits and pieces provided by Illig&#39;s meticulous research, especially in the second book, are too striking to not be at least taken under consideration. </i><b><i>Academic thinking does not just come from a civilized exchange of carefully weighed and balanced, unprovocative texts -- it may even more so be driven by some kind of violent outbursts of a very provocative nature.</i></b><i> The scientific and academic process then means to deal with such radical theses and to examine them under great scrutiny. </i><b><i>No matter how provocative -- even insulting -- a thesis may seem, if it comes up with a sufficient body of evidence, even some intelligent speculation, it has to be talked about -- talked about, not just against. Thus any thesis, however strange and perhaps not being academic itself, or coming from an outsider -- can be as valuable as any other, the failure comes not with the thesis as such, it comes with the ignorance that just dismisses possibly valid points for discussion on the grounds of some secondary doubts and due to a traditional look on things.</i></b><br /><br /><i>PJK</i><br /><i>July 19th, 2001</i><br /><br /><i>3. Against: Circular Arguments</i><br /><br /><i>There still remains, somehow, some considerable methodological problem with the approach used by Illig. </i><b><i>Of course, scientific and academic writing mostly originates from having formed a thesis in one&#39;s mind and then trying to either see it confirmed and supported -- or falsified. That&#39;s the normal flow of things.</i></b><i> Yet to remain academical means to approach your topic from both those sides. You may tend towards defending your thesis, you may want to make a provocative argument, that&#39;s fine, and it also allows true dialog to ensue. But presenting your thesis boldly and presenting it one-sidedly, that&#39;s two very different things. Also, you don&#39;t just jump to conclusions the first time you may see them fit, especially not when they would be of such a huge dimension as those drawn by Illig. That&#39;s not only a matter of academic seriosity, it&#39;s also a matter of psychology. If you want to present your thesis, you surely want to communicate something to somebody. It is not just the writer who&#39;s the focus of the text, it&#39;s the reader also -- which means, you have to take into account the possible reactions of a possible audience.</i><br /><br /><i>That means, first of all, being extremely cautious and careful when presenting your facts, your evidence. Communication doesn&#39;t work when it is done in a brutal way, that only leads to friction, to inimical reactions, to ignoring the facts altogether and focussing on external and superficial elements. In that, Illig -- in my judgement -- commits a grave mistake in his first book when he boldly declares that his findings would without doubt not only support but -- even more -- primarily enable his conclusion that the BC/AD chronology is a quarter millennium too long. His first book mainly discusses Charlemagne and his surroundings. He may be very substantial and meticulous in his research, but the data basis is simply too small, too exclusive, too regional to support such a general statement. The findings of his first book can only mean one thing -- the possibility that something might be wrong within Western chronology, and that the evidence for that is so striking that further research -- on a greater scale -- could possibly provide the basis for formulating a theory which then could lead to establishing his thesis. Taken both books and various other studies in his related journal Zeitensprünge together, and putting the thesis with all its implications at the end of that process would not even make a much stronger case, it would also provide his writings with a greater sense of scientific and academic seriosity.</i><br /><br /><i>Why bother? Is it just the concern for a possible audience which would necessitate such a strategy? Is it just something you do for effect, to appease the critics, to bring them &#39;on your side&#39;? Not at all. I may have overstressed the communicative aspect here, but the problem seems to go even deeper. It is the philosphy, the method behind which makes something more academical or less. It&#39;s the honest concern for scientific and academic seriosity. There&#39;s a profound reason for being cautious: The approach chosen by Illig may be seen as a classical case of being caught in the trap of a circular argument. The evidence will manifest itself when the premise is set, the facts speaking in favor of a thesis mentally validated beforehand will be used, the facts to the contrary will be ignored. Research may be biased, but it must never be uni-directional. </i><b><i>The result of research has to be open, you must have accepted the idea that you could be wrong. A healthy dose of self-doubt is maybe the primary ingredient of scientific and academic writing: Otherwise, you might run the danger of getting lost in your narrative, of ignoring possible evidence to the contrary -- unconsciously even -- and thinking in categories of friend and foe, of taking everything personal. Academic writing has to stand beyond that. It has to be a search for the truth, and only for the truth. Personal preferences and overly quick conclusions have no place in such a quest. Truth knows no outsiders, knows no disgrace, knows nothing like personal honor, stubbornness, inconveniences.</i></b><br /><br /><i>That, of course, is true for both sides. It is easy to disqualify Illig&#39;s thesis as a circular argument, but only if you look at the method applied to his books. </i><b><i>But on the other hand, the narrative to the contrary is what is commonly known, it is what can be found in history text books all over the world. Also, the evidence he has found is by far too numerous and revealing than to just dismiss it condescendingly, as it has been done by the academical &#39;establishment&#39; time and again.</i></b><i> Illig may have made some mistakes in the manner of presentation, but he is no Erich von Däniken. Illig cannot be truthfully accused of just being interested in the money and publicity he may get from writing such books. His interest is a deeply serious academical one, his problem may be that his style seems a bit impatient. Maybe that comes from being bashed all the time. </i><b><i>It surely is no sufficient reason to just cling to the traditional historical narrative. That very narrative in itself can be called a circular argument also: It uses a set of variables traded over from century to century, erected upon a not too solid factual basis. In fact, the deeper you dive into the sea of historical narratives, you will recognize how shallow they become. There is a point to Illig&#39;s method. He is rightfully putting his finger on a situation which -- not only academically -- can be said to be very problematic, if not more.</i></b><i> The blind acceptance of historical narrative as fact is no option either. And especially </i><b><i>the period attacked by Illig&#39;s research is one of the most problematic ones in history. There is reason beyond Illig to conclude that something could very well be wrong here, the evidence for that is quite telling.</i></b><br /><br /><i>Thus the alternative to Illig&#39;s discussion can be anything but falling back into assumedly proven paths. </i><b><i>There is no going back to normal after Illig.</i></b><i> That is not to say that his conclusions would have to be correct, neither do I advocate the opposite. As said above, research has to remain neutral, non-teleological. </i><b><i>The outcome cannot be fixed beforehand. A provocation like that put into the game by Illig cannot go unanswered, cannot go academically unanswered. Yet sadly, most replies to his efforts completely lack an academic basis, and are just retributions for an alleged insult to the historical profession. There&#39;s no such thing. If there is an insult, it is the reaction of the academic and scientific community.</i></b><i> That is not to say that Illig would not have any followers, or at least people who&#39;d like to listen to him with an open mind. But scholarship is not unpolitical. </i><b><i>The policy of &#39;established&#39; historiography is to not accept something like that, especially if it is coming from an &#39;outsider&#39;. There&#39;s something wrong with that picture, severely wrong, and the question somehow comes into mind, what the purpose of such a negating could be. The harsher the criticism, the more it has to be taken under close observation. Sometimes, such a harsh reaction means that there could be something wrong indeed.&quot;</i></b><br /><br />[... editorial decision to snip the fourth section, see original site for content if you wish ...]<br /><br /><i>&quot;5. What Remains</i><br /><br /><i>The task of this essay hasn&#39;t been to determine whether Illig would be right or not. That&#39;s not the issue at hand, and neither can this question be answered exhaustively in such a short attempt, nor has there been sufficient research yet. Also, even if someone were to find out that certain astronomical data (which would have to be reliable) could seriously counteract Illig&#39;s time frame, that would still not answer the question. The question isn&#39;t just whether a single person is right or wrong, </i><b><i>the question addresses the very validity of Western-based history.</i></b><br /><br /><b><i>If you look at the issue in detail (and that&#39;s the core of what Illig does) you&#39;ll soon find out that everything is much more complicated than it looks, that the depths of historic data are much more obscured than anyone could possibly imagine. What Illig has dug out is something which used to belong to a very small circle of specialists for special issues within Medieval history. Anyone coming from the outside has to cope with lots of peculiarities and oddities, yet an outsider&#39;s perspective can also be very helpful in broadening the view. That also means investigating the links European history has with other areas. If Illig&#39;s discovery were correct, and approximately 300 years were missing from our history, that doesn&#39;t mean that these years would be missing in other histories in the same time slot, or even at all.</i></b><br /><br /><b><i>What Illig brings up in his second book is mainly a serious attempt to incorporate methods from natural sciences. C14 chronology doesn&#39;t work because the margin of error is too large. Dendro-chronology seems to rather raise more questions than it answers. The only reliable thing seems to be astronomical data. But it seems this field is not as solid either. Thus the only sufficient approach would have to be a very large scope of research, utilizing various fields and methods.</i></b><br /><br /><i>This is precisely what Illig&#39;s approach seems to be most fruitful for. He doesn&#39;t appear to be interested in the traditional conventions of historiography. </i><b><i>He is rather interested in finding evidence to support his thesis, from whatever field.</i></b><i> This method, if it were to be left alone, would incorporate several problems of a circular argument. </i><b><i>Yet if it were to be supported by &#39;regular&#39; scientific and academic research using the same methods, couldn&#39;t that even revolutionize scholarship in general?</i></b><br /><br /><b><i>Today&#39;s historiography, mainly in Europe and especially in Germany, often seems to be too preoccupied with its own traditions and with traditional divisions between the fields of research. These divisions are constructions -- they can be helpful, but they can constitute a serious trap also, if they stand in the way of true, unbiased scientific and academic dialog between all possible fields. There&#39;s really no need why certain methods and approaches should be made something like a corporate identity of one field only. The only thing important is what works, what comes closer to an answer. That sounds very pragmatic, and rightly so, I feel: Scientific research should be interested in the pragma, nothing else. It needs no ideology, no customs, no conventions -- good historic research is something which produces sound scientific and academic results, no matter how strange the approach. The new historiography has to be global, it has to be in dialog with other fields also. No taboos, no stigmatization of heroes or tradition. That&#39;s why Illig matters, that&#39;s why he should be taken so seriously. He has posed a critical question which desperately deserves an answer.</i></b><br /><br /><i>PJK</i><br /><i>July 26th, 2001</i><br /><br /><i>(The entire essay was minimally reworked by September 26th 2001, concerning spelling and minor corrections, while the substance of it was left intact.)&quot;</i><br /><br />continued in: Illig the Magician. Re-Examining Illig&#39;s Rhetoric<br />[<a href=\"http://www.philjohn.com/papers/pjkd_h04.html\" class=\"ot-anchor\">http://www.philjohn.com/papers/pjkd_h04.html</a>]<br /><br />Endnotes<br /><br />[1] Heribert Illig. Das erfundene Mittelalter. Econ: Düsseldorf 1997, 4th ed.<br /><br />[2] Heribert Illig. Wer hat an der Uhr gedreht? Wie 300 Jahre Geschichte erfunden wurden. Econ&amp;List: München 1999.<br /><br />[3] The invention of the starting point for the chronology Anno Domini is attributed to the monk Dionysius Exiguus, around 525.<br /><br />[4] A whole counter-culture has developed in recent years, and is still growing, which -- in its most extreme form -- can be said to be propagating an almost militant resistance to authority of any kind. Though also following other threads, recent militant protests against &#39;globalization&#39; may very well thrive on that argument also. Some kind of cultural groundwork -- as well as grounds for a critical discussion -- has been laid by the television series The X-Files, whose default position has always been a synthesis between the crude scientific and academic and the crude emotional approach. The topic of conspiracy is dealt with par excellence, as it can partly be seen from some of the opening tags, which pretty much describe the general atmosphere of conspiracy theories: &#39;Deny Everything&#39;, &#39;Apology is Policy&#39;, &#39;Deceive, Inveigle, Obfuscate&#39; or &#39;Believe the Lie&#39;, as well as &#39;Trust No One&#39;.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Immanuel Velikovsky","summary":"He May be the Most Controversial Person in all of Modern Science / Many People Make the Mistake of Focusing Entirely Upon his Extraordinary Claims Rather than his Process and Rationale / There is Just as Much to Learn - About the Scientific Community - from the Reactions and Behaviors of Scientists / Even Carl Sagan, Velikovsky's Biggest Critic, Realized by the End of it All that this was a Low Point for the Sociology of Science / Since the Story of what Happened is Rarely Told Properly, the Lessons were Never Actually Learned - which is Why You Need to Read this Version","image":"https://lh3.googleusercontent.com/-GOgo-Vuvj4s/WFq3iMGNhBI/AAAAAAAAJ1E/28-PK5gbz28QX67j2GQPdkfogoDYkX1wgCJoC/w3600-h5040/immanuel-velikovsky-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Dc5ddhsfU2m","publishDate":"2016-12-21T17:11:36.478Z","updateDate":"2017-01-03T01:34:49.421Z","text":"<b>Immanuel Velikovsky: He May be the Most Controversial Person in all of Modern Science / Many People Make the Mistake of Focusing Entirely Upon his Extraordinary Claims Rather than his Process and Rationale / There is Just as Much to Learn - About the Scientific Community - from the Reactions and Behaviors of Scientists / Even Carl Sagan, Velikovsky&#39;s Biggest Critic, Realized by the End of it All that this was a Low Point for the Sociology of Science / Since the Story of what Happened is Rarely Told Properly, the Lessons were Never Actually Learned - which is Why You Need to Read this Version</b><br /><br />GRAPHIC REFERENCES:<br /><br />[1] <a href=\"https://swamphermit.wordpress.com/abrahamic-religions/\" class=\"ot-anchor\">https://swamphermit.wordpress.com/abrahamic-religions/</a><br /><br />[2] James P Hogan, <i>Kicking the Sacred Cow: Questioning the Unquestionable and Thinking the Impermissible</i>, at <a href=\"https://books.google.com/books?id=XPms57v-_w0C&amp;pg=PT125&amp;lpg=PT125&amp;dq=venus+greek+newcomer&amp;source=bl&amp;ots=RY7zuDc5t7&amp;sig=CyGlEmxBEA52Thp1yrrVfChkMF4&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwi315uSw4XRAhVS9mMKHRQ2Bzo4ChDoAQgaMAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=XPms57v-_w0C&amp;pg=PT125&amp;lpg=PT125&amp;dq=venus+greek+newcomer&amp;source=bl&amp;ots=RY7zuDc5t7&amp;sig=CyGlEmxBEA52Thp1yrrVfChkMF4&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwi315uSw4XRAhVS9mMKHRQ2Bzo4ChDoAQgaMAA#v=onepage&amp;q&amp;f=false</a><br /><br />[3] Carl Sagan reflecting, after-the-fact, on the scientific community&#39;s reaction to Velikovsky ...<br /><br /><a href=\"https://www.youtube.com/watch?v=0MlN7iVIuhk\" class=\"ot-anchor\">https://www.youtube.com/watch?v=0MlN7iVIuhk</a><br /><br />(quote occurs at the very end of the clip)<br /><br /><i>&quot;The worst aspect of the Velikovsky affair is not that many of his ideas were wrong or silly or in gross contradiction to the facts.</i><br /><br /><b><i>Rather, the worst aspect is that some scientists attempted to suppress Velikovsky&#39;s ideas.</i></b><br /><br /><b><i>The suppression of uncomfortable ideas may be common in religion or in politics, but it is not the path to knowledge. And there&#39;s no place for it in the endeavor of science.</i></b><br /><br /><b><i>We do not know beforehand where fundamental insights will arise from about our mysterious and lovely solar system. And the history of the study of our solar system shows clearly that accepted and conventional ideas are often wrong -- and that fundamental insights can arise from the most unexpected sources.&quot;</i></b><br /><br />For those who are new to Velikovsky, there exists a widespread tendency to reject everything he has said because it so dramatically diverges from modern scientific theory.<br /><br />But, if Velikovsky turns out to be <b>even partially correct</b> -- which is an increasingly popular stance today -- then we have a very big problem due to the nature of his discovery:<br /><br />If serious catastrophes do indeed extend all the way up to human-historical times, then that suggests that they are wildly more common than the scientific community has understood.  And if the scientific community is wrong on this single point, there is the distinct possibility that something unexpectedly catastrophic may occur in the near future, as in the past ...<br /><br />In other words, this is a mistake of such enormous proportions that it could threaten the existence of all of humanity.  For that reason, we&#39;d all be wise to formulate our own personal opinions on this one by following along with his discoveries and subsequent claims.<br /><br /><a href=\"http://www.bibliotecapleyades.net/ciencia/esp_ciencia_velikovsky14.htm\" class=\"ot-anchor\">http://www.bibliotecapleyades.net/ciencia/esp_ciencia_velikovsky14.htm</a><br /><br />Worlds in Collision: Will the Controversial Theories of Immanuel Velikovsky Be Proven Right?<br /><br />by Laird Scranton<br />New Dawn Special Issue Vol 6 No 1<br /><br />Originally from: <a href=\"http://www.newdawnmagazine.com/articles/worlds-in-collision-will-the-controversial-theories-of-immanuel-velikovsky-be-proven-right\" class=\"ot-anchor\">http://www.newdawnmagazine.com/articles/worlds-in-collision-will-the-controversial-theories-of-immanuel-velikovsky-be-proven-right</a><br /><br />March 2, 2012<br /><br /><i>&quot;Some of the controversies that arise in the world seem to latch on to the public&#39;s imagination and then, for decades or years following, simply will not let go. Such is the character of intrigue that has, for more than sixty years now, swirled turbulently around the personage of Immanuel Velikovsky.</i><br /><br /><i>Velikovsky, who sadly died in 1979, was a Russian-born medical doctor and psychoanalyst.</i><br /><br /><i>He first came into the public eye in the 1920&#39;s for founding the Scripta Universitatis academic journal in Berlin, </i><b><i>and later worked alongside others to establish the Hebrew University in Jerusalem.</i></b><br /><br /><b><i>Velikovsky was an intensely curious man who had been broadly educated in many different fields of study as diverse as science, medicine, philosophy, ancient history and law. He studied psychoanalysis under Sigmund Freud&#39;s acclaimed protégé Wilhelm Stekel.</i></b><br /><br /><b><i>Velikovsky first worked alongside Albert Einstein in Berlin, when Einstein edited mathematical articles in Scripta Universitatis, again in Jerusalem during their efforts to help found the Hebrew University, and later in life as close friends and colleagues at Princeton University.</i></b><br /><br /><i>In 1939, Velikovsky brought his family to New York City, planning to spend the summer engaged in research at Columbia University&#39;s library. </i><b><i>He was compiling a psycho-historical text to outline the many intriguing parallels he had uncovered between the Greek literary character Oedipus and the Egyptian Pharaoh Akhnaton.</i></b><br /><br /><b><i>This research, however, was soon sidelined when Velikovsky uncovered an Egyptian papyrus called &#39;The Admonitions of Ipuwer,&#39; a text that seemed to provide historical confirmation for biblical accounts of the 10 plagues in Egypt at the time of Moses. Intrigued that the biblical account might possibly have foundation in actual historic events, Velikovsky began to seek out other ancient references that might serve to uphold that point of view.</i></b><br /><br /><b><i>Using the techniques of a comparative mythologist, Velikovsky began a comprehensive review of ancient texts from around the world dating from that same time period, and produced a body of supporting evidence that was more substantial than he at first imagined.</i></b><br /><br /><b><i>The ancient texts presented what he saw as a kind of universality of theme relating to reports of global calamity</i></b><i> -- descriptions of,</i><br /><br />- <i>fire raining from the sky</i><br />- <i>violent earthquakes</i><br />- <i>volcanoes erupting</i><br />- <i>displacement of great bodies of water,</i><br /><br /><i>... and similar disasters of seeming mythic proportion.</i><br /><br /><b><i>At the same time he also began a search for references that might point to some real-world agent capable of inflicting the kinds of misfortunes described in the Book of Exodus. He eventually settled on the theoretic close approach of a comet to the Earth as the type of natural event that most closely fits the profile of destructive consequences described in the texts.</i></b><br /><br /><b><i>This tentative conclusion was upheld in his mind by many explicit ancient references to a fearful wandering comet associated with great calamity.</i></b><br /><br /><i>References to this comet were given by different cultures under various names such as Seth and Typhon. </i><b><i>Velikovsky also found himself confused to learn that in some cultures, the names that had been initially assigned to this fearful comet also came later to be associated with the planet Venus.</i></b><br /><br /><b><i>Meanwhile, Velikovsky became aware of the sudden rise at about that same historical period of what seemed to be a global obsession with tracking the motions of Venus.</i></b><br /><br /><b><i>Many different cultures began to keep careful written counts of the number of days between the risings and settings of Venus. Tracking Venus is the likely motive that is cited by some historians as having inspired the Oracle Bone texts -- the earliest form of written record known to exist in China.</i></b><br /><br /><i>For Velikovsky, these facts taken together seemed to implicate Venus as the fearsome agent of terrible events that he believed may have ravaged our planet.</i><br /><br /><i>Velikovsky associated this same period of destruction with,</i><br /><br />- <i>the stupendous eruption of the volcano Thera on the island of Santorini</i><br />- <i>the sudden fall of the Minoan Empire in the Mediterranean</i><br />- <i>the end of the Middle Kingdom in Egypt at around 1500 BCE</i><br /><br /><b><i>In support of this unorthodox thesis, Velikovsky noted that the texts of most ancient cultures prior to 1500 BCE -- most notably those of the Hindus, Babylonians and Egyptians -- refer only to four planets -- Jupiter, Saturn, Mars and Mercury.</i></b><br /><br /><b><i>References to Venus prior to that date are inexplicably given using words and symbols that were traditionally reserved for comets.</i></b><br /><br /><b><i>They describe Venus as appearing &#39;hairy&#39; or refer to its &#39;horns&#39; or &#39;long tail.&#39; (There are references prior to 1500 BCE to goddesses such as the Sumerian Innana, however, the iconography associated with these goddesses typically involve images of comets.)</i></b><br /><br /><i>Two independent but synchronized sets of ancient astronomic records -- one from China and the other from Korea -- describe the first appearance of a supernova on the same observation date and both compare it in size and brightness to </i><b><i>Venus &#39;with its rays.&#39;</i></b><br /><br /><b><i>Furthermore, virtually all ancient cultures went through a period during which they classified Venus alongside the Sun and the Moon, rather than with the planets, based on its brightness. While it&#39;s true that even now, under proper circumstances, the planet Venus can remain visible during the first hours of daylight, there are ancient reports that refer to the brightness of Venus as rivaling that of the Sun.</i></b><br /><br /><b><i>Velikovsky changed the focus of his study from Oedipus to the origins of Venus, and in 1950, MacMillan and Company published his highly controversial book on the subject called Worlds in Collision.</i></b><br /><br /><i>In the book Velikovsky postulated -- based on a wide range of ancient accounts and references -- that the planet Venus must have been formed within the historic memory of mankind as a consequence of the impact of a large astronomic body with Jupiter. This event was recorded in a Greek myth in which Jupiter was said to have swallowed whole a pregnant goddess named Metis, soon after which Athena burst newborn from Jupiter&#39;s head.</i><br /><br /><i>According to Velikovsky, </i><b><i>Venus -- whose name in Greek means &#39;the newcomer&#39;</i></b><i> -- at first &#39;blazed as brightly as the Sun&#39; as it roamed across the sky, far outside the Newtonian bounds of its familiar modern-day orbit.</i><br /><br /><i>He proposed that Venus, in its travels, had wrought considerable havoc within the solar system, that its trajectory had brought it to a near-miss with Earth around 1500 BCE and that Venus had directly impacted Mars.</i><br /><br /><i>This impact caused Mars, in turn, to leave its orbit and to become the catalyst for a second series of close encounters between Mars and the Earth. The worst of these happened, according to Velikovsky, around 750 BCE.</i><br /><br /><i>One serious consequence of this final interaction with Mars, according to Velikovsky, is that it affected the Earth&#39;s orbital period, lengthening it from an ancient 360-day year to our familiar 365-day year, and ejecting Mars into its present orbit.</i><br /><br /><i>Several factors conspired together to help position Velikovsky&#39;s book to become a top seller.</i><br /><br />- <i>The first of these was Velikovsky&#39;s own personal reputation and long time association with high-profile projects and people, which together made him a difficult heretic to simply ignore outright, since Velikovsky already had a well-established reputation.</i><br /><br />- <i>The second was that his theory of an historically recent birth for Venus directly contradicted modern planetary theory and scientific conceptions of proper planetary motion.</i><br /><br />- <i>The third may have come out of the sheer astonishment on the part of traditional scientists that Velikovsky would presume to offer a theory that crossed the traditional boundaries of numerous academic disciplines, and move outside of his own field of expertise into another field entirely -- namely astronomy -- to make his case.</i><br /><br />- <i>The fourth was that he deemed it appropriate to cite as evidence for this theory ancient textual references that, in the estimation of many astronomers of the day, rated somewhere alongside a child&#39;s fairy-tale in scientific evidentiary value. Moreover, </i><b><i>this evidence -- which was drawn from the fields of ancient history and archaeology -- fell far outside of the ability of most astronomers to even attempt to evaluate, let alone refute.</i></b><br /><br /><i>With the help of some very effective advance publicity -- including a condensation of the book that appeared in a popular magazine and advance copies of the book that were sent to several leading astronomers -- the outrage of the astronomers was effectively stoked.</i><br /><br /><i>Their professional outrage helped propel the book to the status of a runaway bestseller.</i><br /><br /><i>The rising popularity of Velikovsky&#39;s book turned the astronomic world on its head. The din of uproar against the heretic Velikovsky approached levels that had not been heard since the Catholic Church&#39;s infamous persecution of Galileo in the mid-1600&#39;s.</i><br /><br /><i>In retrospect, the outrage was understandable:</i><br /><br /><b><i>&#39;In what had by 1950 become an increasingly Darwinian world, Velikovsky&#39;s theory threatened to resurrect a kind of fire-and-brimstone religion that the scientific world had struggled for more than a century to supplant.</i></b><br /><br /><b><i>Likewise, by contradicting the view that all planets must be billions of years old, Velikovsky&#39;s theory threatened to undermine the Uniformitarian views that provided the foundation for Darwinism itself.</i></b><br /><br /><b><i>Darwin&#39;s theory required a stable and unchanging universe to accommodate the imperceptibly slow processes of the evolution it proposed.&#39;</i></b><br /><br /><b><i>Several top astronomers wrote to MacMillan&#39;s management urging the company to block publication of Velikovsky&#39;s book.</i></b><br /><br /><b><i>Dr. Harlow Shapley (then director of the Harvard Observatory) worked behind the scenes to organize colleges and universities in a boycott of MacMillan&#39;s highly profitable textbook division, hoping to financially arm-twist them into dropping the book.</i></b><br /><br /><b><i>MacMillan -- hoping to defuse the boycott without actually bowing to the demands of the astronomers -- took the highly unusual step of transferring its lucrative publishing rights for a bestselling book to Doubleday, one of its competitors who had no stake in the sale of textbooks.</i></b><br /><br /><i>At the time of publication of Worlds in Collision, many aspects of Velikovsky&#39;s theories were flatly characterized as purest nonsense by authorities within the astronomic world. </i><b><i>Later the truth would surface that some of Velikovsky&#39;s harshest critics had never actually read his book prior to making their declarations, but had instead based their critique solely on pre-publication summaries of the book.</i></b><br /><br /><b><i>Certainly Velikovsky&#39;s vision of a young, hot Venus ran counter to the conventional wisdom in 1950, which presumed that Venus had an Earth-like atmosphere and might ultimately prove to be colonizable.</i></b><br /><br /><i>The seemingly acrobatic requirements for the motions of Venus laid out by Velikovsky in his book -- moving first like a comet but then somehow eventually coming to inhabit one of the most circular and regular orbits of all the planets -- appeared to flatly contradict Newtonian laws of motion.</i><br /><br /><i>Carl Sagan pointed out that the great amount of energy required to eject a body the size of Venus from Jupiter would likely have vaporized large portions of Jupiter and left those areas intensely hot, even today.</i><br /><br /><i>Even Einstein, whose natural impulse was to be sympathetic to his friend and colleague, at first sided against Velikovsky, flatly discounting his suggestion that electromagnetic forces must play a significant role in planetary dynamics.</i><br /><br /><i>Velikovsky&#39;s theory, when carefully considered, carries with it a number of logical eventualities or consequences that, if they were not all within the reach of scientists to prove or disprove experimentally in 1950, would surely become testable sometime in the near future.</i><br /><br /><b><i>For example, a geologically-recent birth for Venus would require the planet to be intensely hot. Likewise, it would imply that Venus exhibit a seemingly unevolved set of geological formations.</i></b><br /><br /><b><i>Furthermore, if Venus had roamed the solar system as a rogue astronomic body for centuries then we would expect to find certain anomalies in its orientation and rotation when compared to the other planets.</i></b><i> Surely we would eventually be able to detect if either Mars of Venus had ever suffered a direct impact with a planet-sized body.</i><br /><br /><i>If Venus and Mars had made close approaches to the Earth in ancient times, we should be able to identify chemical, geological or magnetic signatures associated with those events.</i><br /><br /><i>Moreover, Velikovsky himself had provided a long list of his own &#39;prognostications&#39; -- consequential observations that he felt must eventually show themselves to be true, if the facts were to uphold what he saw as the unmovable cornerstones of his theory.</i><br /><br /><i>Soon after publication of the book, certain of Velikovsky&#39;s &#39;prognostications&#39; began to be affirmed, if not always for the precise reasons offered by Velikovsky.</i><br /><br /><b><i>For example, the controversial outlook Velikovsky held on the role of electromagnetism in the interaction of planetary bodies -- the one that had been at first opposed by Einstein -- was upheld by the incidental discovery of radio emissions from Jupiter and acceptance based on work by Van Allen of the existence of a significant magnetic field surrounding the Earth.</i></b><br /><br /><i>By the 1960&#39;s, Velikovsky was considered a credible enough authority on questions of astronomy to be hired by a leading television network to consult and comment during NASA&#39;s live Moon landings.</i><br /><br /><i>In 1974, a symposium of scientists (including Velikovsky) was held in San Francisco to debate Velikovsky&#39;s theories that ended up pitting several leading critics against Velikovsky. </i><b><i>The official &#39;spin&#39; coming out of that conference -- and the impression left on the general public -- was that Velikovsky&#39;s theories had been finally and definitively disproved.</i></b><br /><br /><b><i>However since that time, as new evidence continues to emerge, there has been a persistent nagging tendency for new findings to -- at least outwardly -- appear to uphold many of Velikovsky&#39;s &#39;prognostications.&#39;</i></b><br /><br /><b><i>In recent years when new discoveries are made that could potentially relate to the controversy, these findings are most often presented without official mention of Velikovsky. Instead, they are typically announced bundled with an accompanying new theory, whose net effect is to distance the find from Velikovsky&#39;s controversial theories.</i></b><br /><br /><b><i>For example, when probes to Venus did, in fact, show the planet to be intensely hot -- a key point that Velikovsky had cited as a crucial demonstration of the correctness of his theory -- scientists completely sidestepped the issue by pre-emptively postulating a runaway greenhouse effect to explain the unexpectedly high temperature.</i></b><br /><br /><b><i>When Venus was found to have far fewer impact craters than would be expected for a billions-year-old planet, astronomers again proposed that &#39;unknown geologic forces&#39; must have somehow caused a geologically-recent global resurfacing of Venus, thus wiping away evidence of the craters.</i></b><br /><br /><i>As far-fetched as some of Velikovsky&#39;s proposals may seem at first blush -- especially his often questionable musings on chemical interactions that he presumes to have occurred between the atmospheres of Venus, Mars and the Earth -- </i><b><i>there is a perspective from which we might profit by carefully considering certain aspects of his theory.</i></b><br /><br /><i>For example, his suggestion that a planet might be formed as a consequence of a large impact on a gas giant planet seems as reasonable as either of the two leading traditional theories of planetary creation -- both of which are believed by some astronomers to suffer from serious (perhaps fatal) theoretic difficulties.</i><br /><br /><b><i>Likewise, it is already an accepted part of traditional astronomic theory that our own Moon was formed as the by-product of an impact. Surely it is not unreasonable to think that what can happen on a small scale in our solar system could also happen on a larger scale.</i></b><br /><br /><i>Some of Velikovsky&#39;s critics say that it is unreasonable to think that the wandering orbit of Venus as a comet could have circularized for the planet Venus in such a short period -- </i><b><i>and yet it is well known there are some comets that have apparently achieved circular orbits around our Sun.</i></b><br /><br /><i>Some theories suggest that a comet&#39;s tail can provide the necessary drag to circularize its orbit. Others claim the tidal forces of gravity can cause the orbits to circularize.</i><br /><br /><i>The way Velikovsky&#39;s argument is structured, there are a number of single-question fail-points attached to his scenario.</i><br /><br /><i>For example, if it could be definitively shown that granite exists on Venus (a type of rock that takes millions of years to form) then much of Velikovsky&#39;s theory would simply fall to the ground.</i><br /><br /><i>The same would be true if an archaeologist suddenly turned up an ancient document from prior to 1500 BCE that explicitly referred to Venus as a planet.</i><br /><br /><i>Likewise, if explicit evidence could be produced for the existence of a 365-day year in ancient times, then one of Velikovsky&#39;s key claims would be effectively rebutted.</i><br /><br /><i>My purpose in writing my newest book The Velikovsky Heresies is to help bring Immanuel Velikovsky and the many still-unanswered questions he raised relating to Venus back into the consciousness of the reading public.</i><br /><br /><i>My goal is to remind this audience that -- notwithstanding the symposium held in 1974 -- </i><b><i>an open, ongoing controversy still exists relating to Velikovsky. That controversy continues to be colored by the disturbing suggestion of long-term, politically-motivated manipulation of scientific results.</i></b><br /><br /><i>Another of my main goals is to update the Velikovsky controversy with the latest astronomic evidence relating to Venus, Mars and Jupiter.</i><br /><br /><i>Much of this new evidence is drawn from the wealth of data gathered by recent probes such as the European Space Agency&#39;s Venus Express, and recent first-hand studies that have been conducted in relation to comets.</i><br /><br /><i>One of my original hopes -- unfortunately not yet realized -- was to pinpoint a single fact upon which the controversy could be said to turn -- one essential, illusive bit of evidence that a person could point to as unquestionable proof that Velikovsky&#39;s theory must either be largely correct or specifically and fatally flawed.</i><br /><br /><i>Instead, I believe I have brought new eyes and new evidence to bear on many of the critical questions and criticisms that shape the unique and enduring controversy fostered more than sixty years ago by Velikovsky&#39;s Worlds In Collision.</i><br /><br /><i>LAIRD SCRANTON is an independent software developer from Albany, New York. In addition to The Velikovsky Heresies he has also written a number of books and articles on African and Egyptian cosmology and language, including articles for the University of Chicago&#39;s Anthropology News academic journal and Temple University&#39;s Encyclopedia of African Religion.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"The Debate Over Dendrochronology","summary":"Behind the Public-Facing, Confident Pronouncements of Climate Change Scientists is a More Private Debate between Dendrochronologists and Tree Physiologists Over Whether or Not Dendrochronology is Actually a Science / These Debates Relate to Subjective Interpretations of what an Annual Ring is, Statistical Manipulation of Data to Fulfill Subjective Expectations, the Discarding of Perfectly Good Data Sets when they Contradict Other Accepted Data Sets, and an Over-Focus on the Statistics of Cross-matching Tree Rings, to the Detriment of a More Critical Look at the Physiological Principles of Ring Formation","image":"https://lh3.googleusercontent.com/-7S7bpHnAfKQ/WFmdcI1uRkI/AAAAAAAAJ0A/o2mcty1Pe5gbTKJ6mO09_4hQ_w7txstEgCJoC/w1800-h2520/the-debate-over-dendrochronology-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/JDQ4HXinpRo","publishDate":"2016-12-20T21:07:36.720Z","updateDate":"2017-01-03T01:35:06.164Z","text":"<b>The Debate Over Dendrochronology: Behind the Public-Facing, Confident Pronouncements of Climate Change Scientists is a More Private Debate between Dendrochronologists and Tree Physiologists Over Whether or Not Dendrochronology is Actually a Science / These Debates Relate to Subjective Interpretations of what an Annual Ring is, Statistical Manipulation of Data to Fulfill Subjective Expectations, the Discarding of Perfectly Good Data Sets when they Contradict Other Accepted Data Sets, and an Over-Focus on the Statistics of Cross-matching Tree Rings, to the Detriment of a More Critical Look at the Physiological Principles of Ring Formation</b><br /><br />The discussion in the graphic of Liebig’s Law of the Minimum -- the basis for &quot;Liebig&#39;s barrel&quot; -- comes from ...<br /><br /><a href=\"https://wattsupwiththat.com/2009/09/28/a-look-at-treemometers-and-tree-ring-growth/\" class=\"ot-anchor\">https://wattsupwiththat.com/2009/09/28/a-look-at-treemometers-and-tree-ring-growth/</a><br /><br />Our second testimony to a possible problem comes from the ClimateGate emails ...<br /><br /><a href=\"https://wattsupwiththat.com/2012/01/03/paging-mike-mann-your-dendrochronologist-will-see-you-now/\" class=\"ot-anchor\">https://wattsupwiththat.com/2012/01/03/paging-mike-mann-your-dendrochronologist-will-see-you-now/</a><br /><br />Paging Mike Mann – your dendrochronologist will see you now<br /><br />Anthony Watts / January 3, 2012<br /><br /><i>&quot;Tom Nelson has another Climategate 2 email well worth reading</i><br /><br /><i>Dendrochronologists get spanked by guy with expertise in tree physiology and wood anatomy</i><br /><br /><i>ClimateGate Email 1738&quot;</i><br /><br />Note: when the link works, it&#39;s at <a href=\"http://www.ecowho.com/foia.php?file=1738.txt\" class=\"ot-anchor\">http://www.ecowho.com/foia.php?file=1738.txt</a><br /><br /><i>&quot;&#39;However, there are bounds to dendrochronology, as there are to every field of investigation, and </i><b><i>the discipline has spilled over way outside of those bounds, to the point of absurdity.</i></b><br /><br /><b><i>What troubles me even more than the inexactness attending chronological estimates is how much absolute nonsense -- really nothing but imaginative speculation — about the environment of the past is being deduced from tree rings and published in dendrochronology journals.</i></b><br /><br /><b><i>... but dendrochronology has persistently rejected walking the hard road, that of understanding the fundamental genetic and environmental factors controlling wood formation. As I see it, the peer review process in dendrochronology must be fundamentally flawed to allow such publications.</i></b><i> Physiologist remain to build any real confidence in their ideas of how environmental factors influence tree ring formation, and dendrochronologists therefore are not at all justified in pretending that they do.</i><br /><br /><b><i>The bounds of dendrochronology will be extended, as will confidence in dendrochronological reports, when your group stops pretending that it knows the answers before it has done the needed research.</i></b><i> Again, I am troubled by your group that it shows little humility, no genuine desire to discover the truth.&#39;</i><br /><br /><i>The writer of this email:</i><br /><br /><i>UNB | Faculty of Forestry and Environmental Management | Rod Savidge&quot;</i><br /><br />The rest of this text comes from a PDF document at the following location ...<br /><br /><a href=\"http://www.esf.edu/for/bevilacqua/for496/TreeRingControversy.pdf\" class=\"ot-anchor\">http://www.esf.edu/for/bevilacqua/for496/TreeRingControversy.pdf</a><br /><br /><i>&quot;It all started with this article from NYTimes.com</i><br />_<a href=\"http://www.nytimes.com/2002/11/12/science/life/12CONV.html\" class=\"ot-anchor\">http://www.nytimes.com/2002/11/12/science/life/12CONV.html</a>&quot;_<br /><br /><b>The Interview</b><br /><br />A Writer Leaves History Behind to Celebrate Trees<br />(A Conversation With Tom Pakenham)<br /><br />November 12, 2002<br />By Claudia Dreifus<br /><br />Excerpts:<br /><br /><b>Q.</b> <i>&quot;The dating of trees seems a rather inexact science, is it not?&quot;</i><br /><br /><b>A.</b> <i>&quot;You&#39;d think there&#39;d be plenty of papers written about the age of trees and that people would draw interesting diagrams showing how you would extrapolate from the existing rings across the void and get various dates. </i><b><i>But there&#39;s a deafening silence from scientists about tree age.&quot;</i></b><br /><br /><b>Q.</b> <i>&quot;Is it a myth that you can tell a tree&#39;s age by its rings?&quot;</i><br /><br /><b>A.</b> <i>&quot;If there is a void in the middle of the tree, there will be no rings there to count. You can only make estimates. And some trees don&#39;t have rings, the baobabs, for instance. </i><b><i>Many old trees are extremely hollow.&quot;</i></b><br /><br /><b>The First Letter-to-the-Editor: In Defense of Dendrochronology</b><br /><br /><i>&quot;To the Editor, New York Times</i><br /><br /><i>I was rather horrified by the inaccurate statements about tree-ring dating that you allowed to slip into print in the interview with Thomas Pakenham today. Tree-ring science is an exact science -- none of the data obtained from tree rings would be useful if the dates were inaccurate. </i><b><i>Dendrochronologists don&#39;t say much these days about how old trees are because they are interested in more important questions -- such as &#39;What can the tree rings tell us about our planet&#39;s past?&#39;</i></b><br /><br /><i>You at The New York Times should know something about tree rings. A check on Lexis-Nexis shows that since 1980 you have run more than 100 stories in which the words &#39;tree rings&#39; appear in full text. Some of the stories are irrelevant. But most are not, such as the July 13, 2002, story in which you misspell the name of Neil Pederson at Lamont-Doherty Earth Observatory, or the March 26, 2002, story about a medieval climate warming detected in tree-ring data. I do not remember tree-ring dating being labeled an &#39;inexact&#39; science in stories like that.</i><br /><br /><i>Did Walter Sullivan, who wrote a story about tree rings and drought on September 2, 1980, ever question the &#39;exact&#39; nature of tree-ring dating? He didn&#39;t seem to question it on June 7, 1994, when he wrote a story about ash from Santorini and said that the ash cloud may have &#39;persisted long enough to stunt the growth of oak trees in Irish bogs and of bristlecone pines in the White Mountains of California, producing tightly packed tree rings.&#39; You really do have to know when those rings were laid down before you can associate them with a specific volcanic eruption.</i><br /><br /><i>I tell you what. I am a member of the National Association of Science Writers as well as a working dendrochronologist and occasionally paid-up member of the Tree-Ring Society. If you feel the need for a refresher course on tree-ring dating, I&#39;ll be more than happy to try to introduce you to knowledgeable practioners in you neighborhood, such as Neil Pederson (not Peterson) at Lamont-Doherty Earth Observatory. (It&#39;s actually a local phone call for youse guys.)</i><br /><br /><i>Sincerely,</i><br /><i>Dave Lawrence&quot;</i><br /><br /><b>The Second Letter-to-the-Editor: Dendrochronology as Pseudoscience</b><br /><br /><i>&quot;To the Editor, New York Times</i><br /><br /><i>Further to the message below, I want to assure you that not everyone agrees with the representations by David Lawrence. </i><b><i>As a tree physiologist who has devoted his career to understanding how trees make wood, I have made sufficient observations on tree rings and cambial growth to know that dendrochronology is not at all an exact science. Indeed, its activities include subjective interpretations of what does and what does not constitute an annual ring, statistical manipulation of data to fulfill subjective expectations, and discarding of perfectly good data sets when they contradict other data sets that have already been accepted. Such massaging of data cannot by any stretch of the imagination be considered science; it merely demonstrates a total lack of rigor attending so-called dendrochronology &#39;research&#39;.</i></b><br /><br /><b><i>I would add that it is the exceptionally rare dendrochronologist who has ever shown any inclination to understand the fundamental biology of wood formation, either as regulated intrinsically or influenced by extrinsic factors. The science of tree physiology will readily admit that our understanding of how trees make wood remains at quite a rudimentary state (despite several centuries of research). On the other hand, there are many hundreds, if not thousands, of publications by dendrochronologists implicitly claiming that they do understand the biology of wood formation, as they have used their data to imagine when past regimes of water, temperature, pollutants, CO2, soil nutrients, and so forth existed. Note that all of the counts and measurements on tree rings in the world cannot substantiate anything unequivocally; they are merely observations. It would be a major step forward if dendrochronology could embrace the scientific method.</i></b><br /><br /><i>sincerely,</i><br /><i>RA Savidge, PhD</i><br /><i>Professor, Tree Physiology/Biochemistry</i><br /><i>Forestry &amp; Environmental Management</i><br /><i>University of New Brunswick</i><br /><i>Fredericton, NB E3B 6C2&quot;</i><br /><br /><b>Another Defense</b><br /><br /><i>&quot;And the fun begins</i><br /><br /><b><i>I find myself agreeing with Rod&#39;s statement: &#39;The science of tree physiology will readily admit that our understanding of how trees make wood remains at quite a rudimentary state (despite several centuries of research).&#39; ... and for the most part can&#39;t argue with: &#39;... it is the exceptionally rare dendrochronologist who has ever shown any inclination to understand the fundamental biology of wood formation, either as regulated intrinsically or influenced by extrinsic factors.&#39;</i></b><br /><br /><i>However, and this is a big HOWEVER, I am firmly convinced that the scientific community knows enough about the formation of annual growth rings to apply the dendrochronological method. In fact, dendrochronology is an important tool in the study of tree physiology!</i><br /><br /><b><i>We could take an example from particle physics. Physicists still don&#39;t have the elusive unifying theory, they still don&#39;t know the fine details of what composes matter and energy. That hasn&#39;t stopped the application of nuclear energy.</i></b><br /><br /><b><i>Heck, ask a physicist what electricty is? Forget the simple idea of &#39;a flow of electrons through a conductor&#39; we all learned in Physics 101. Try to get a really comprehensive answer. Yet, where would any of us be with out the practical application of electricty?</i></b><br /><br /><i>Dendrochronology works! There are exceptions to the method where it won&#39;t work. These are readily acknowledged. We can all benefit from learning more about xylogenesis and cambial physiology, but </i><b><i>will that knowledge fundamentally alter the methodology of cross-dating? Or will it improve our interpertation of the information contained in carefully cross-dated tree-ring time-series?</i></b><br /><br /><i>Having taught the &#39;Botanical Basis of Dendrochronology&#39; for several years, I am confident in the botanical foundation of cross-dating and dendrochronlogy. Some interpretation of growth ring timeseries published in the literature may push the limits of our physiological understanding. Again, Rod&#39;s point that more research in cambial physiology and xylogenesis can never be understated! We&#39;ll all benefit from that approach.</i><br /><br /><b><i>Keep in mind, to say all published research in any field is completely accurate is in itself inaccurate. Liberty has been taken with methods and interpretation of data in all fields of research. It&#39;s not bad to question methodology, data, and interpretation. What we need to keep in mind is to &#39;not dispose of the baby with the bathwater.&#39;</i></b><br /><br /><i>That said, and returning to the NY Science Times article, the funny thing is carefully reading the original article. It&#39;s just bad editing or reporting. Either Pakenham didn&#39;t address or understand the question or the reporter mixed-up what Pakenham said. Gee, that&#39;s never happened in press before, has it! Pakenham says &#39;... people would draw interesting diagrams showing how you would extrapolate from the existing rings across the void and get various dates.&#39; The key word here is &#39;void&#39;. He&#39;s talking about hollow trees! We all agree with that. One can only estimate the age of a hollow tree and given the unknown rate of growth, it is near impossible to extrapolate the age from the remaining rings in the outer layer of wood which is decay free. The next QA again is a mismatched question and response. Pakenham&#39;s answer is with regard to hollow trees not wether one can use tree rings to age a tree.</i><br /><br /><i>Who know&#39;s what was discussed in that interview and what actually made it to press. Who know what Pakenham might know or believe about tree rings and dendrochronology? At this point, Pakenham might be wondering what was published vs. what he said in the interview.</i><br /><br /><i>Frank</i><br />=<del>=</del>=<del>=</del>=<del>=</del>=<del>=</del>=<del>=</del>=<del>=</del>=<del>=</del>=<del>=</del>=<del>=</del>=<del>=</del>=<del>=</del>=<del>=</del>=<del>=</del>=<br /><i>Frank W. Telewski, Associate Professor and Curator</i><br /><i>W.J. Beal Botanical Garden</i><br /><i>Michigan State University&quot;</i><br /><br /><b>The Email Exchange</b><br /><br />Email “Conversation” between Rod Savidge (Tree Physiologist) and Hal Fritts (Dendrochronologist)<br />Format: Savidge’s: (PHYS) initial letter<br />Fritts’: (DENDRO) response<br /><br />PHYS: <i>&quot;Dear Hal,</i><br /><i>The nature of genuine scientific endeavour is to discover and explain the truth about nature, no matter how that knowledge impacts the preconceptions of mankind.&quot;</i><br /><br />DENDRO: <i>&quot;No dispute here.&quot;</i><br /><br />PHYS: <b><i>&quot;Over the last century, dendrochronology has set itself up for criticism by persistently shying away from, rather than confronting, the key issues.&quot;</i></b><br /><br />DENDRO: <i>&quot;That is how you allow yourself to see us. Rod, this is a generalization that would irritate any scientist in any discipline. You consider it a fact but it is the myth that you insist on calling your and everyone else&#39;s reality. Please allow your self to consider that you may have some limitations of experience here. You sit at your lofty seat of &#39;God&#39; and make these pronouncements.&quot;</i><br /><br />PHYS: <i>&quot;I give you credit, Hal, for long ago identifying many of the shortcomings of dendrochronological investigation;&quot;</i><br /><br />DENDRO: <i>&quot;Yes, Ron I accepted the criticism at face value but when I got down to doing the nitty gritty of dendrochronological work, I realized how false and biased these criticisms were. They were made by people that had created their own reality which they believed in as you do. Most of them had personal motives to say such things. Glock was fired from the Tree-Ring Lab and was really getting even.&quot;</i><br /><br />PHYS: <b><i>&quot;however, the world is nevertheless entitled to ask what has been done to solve the problems identified decades ago? In my view, almost nothing has been done; the same problems remain today,&quot;</i></b><br /><br />DENDRO: <i>&quot;You perceive them to be present today because you refuse to recognize the checks and balances used by dendrochronologist to assure that false rings and missing rings are located and identified. </i><b><i>They are not located by their anatomical nature as you might think they should be but by a procedure of replication and further replication until there is sufficiently small uncertainty to call it true.</i></b><i> This particular point is a statistical point that I realize you cannot appreciate or accept. All science is based on likelihood and probability. That is why it is so important that things be replicated by other scientists. But you can&#39;t replicate our experiment because you really do not understand what procedure and checks we use.&quot;</i><br /><br />PHYS: <b><i>&quot;and the field of dendrochronology continues to walk the easy road, but now promulgating itself as an exact science. That is not the way of real science at all.&quot;</i></b><br /><br />DENDRO: <i>&quot;We have a lot more scientists that understand what we are doing than you can possibly imagine. You frankly are the exception, not the rule. This is the position you took at Davos and every other contact I have had with you. I have not seen any movement or genuine willingness to learn on your part. As long as you take such a high and mighty position, you will never be convinced because then you would have to admit you, not dendrochronologist have unrealistic views. It is this gap that has to be bridged and it is not dendrochronologist that will be able to bridge it.&quot;</i><br /><br />PHYS: <i>&quot;Any grade school child can count and measure the widths of bands in wood and be led to believe that they are absolutely and inviolately &#39;annual&#39; rings.&quot;</i><br /><br />DENDRO: <i>&quot;A case in point, you actually believe that dendrochronologists are this stupid.&quot;</i><br /><br />PHYS: <b><i>&quot;Taking it the next step, to admitting that, &#39;Well, some are and some aren&#39;t really annual rings, some are false rings&#39;, of course opens you up to the obvious question: &#39;How do you know which is which?&#39; Thus, the &#39;exactness&#39; of dendrochronology begins to be doubted.&quot;</i></b><br /><br />DENDRO: <i>&quot;You and all others that take this high and mighty approach doubt it because you think dendrochronologists are just your grade school children out there counting and measuring widths of bands they &#39;think&#39; are annual rings. Ron, you are so far off on this point we can&#39;t possibly rebut it because what you see is NOT what we are doing and nothing we can say will persuade you because we are only children in your mind.&quot;</i><br /><br />PHYS: <b><i>&quot;Taking it yet one more step, to admitting that, &#39;Well, it could be that some years the tree actually did not produce any wood at all, in which case there would be no annual ring&#39;, introduces a complex problem: &#39;Are there missing rings and, if so, where are they, and how do I integrate plausible missing rings with false rings to correctly age this specimen?&#39;&quot;</i></b><br /><br />DENDRO: <i>&quot;That is precisely the logic all dendrochronologist go through and there are true and tried procedures that we use that will pass any scientific test, if only one will truly look at what is being done.&quot;</i><br /><br />PHYS: <i>&quot;As you well know, Hal, dendrochronology students were led to statistics to solve their problems. Statistics does not have a hotline to the truth, however.&quot;</i><br /><br />DENDRO: <i>&quot;Correct, it is not an answer to all things, but it along with knowledge of physiology and anatomy it has a very important place. I have always taken the position that the biological basis and statistics must be used together, not statistics alone. You saw from my PS that the debate about the importance continues to go on. </i><b><i>I would say to my field that in the absence of a more balanced concern with the tree biology, and statistics we leave ourselves open to critics like you.&quot;</i></b><br /><br />PHYS: <i>&quot;The most it can do is identify the best &#39;guess&#39; from the limited data available and, even more problematic for interpretation, within underlying assumptions. </i><b><i>It seems that some dendrochronologists have set themselves up as gurus; they just know the answers, although they cannot explain them nor support them other than by statistical inference.&quot;</i></b><br /><br />DENDRO: <i>&quot;This is again your perception of what you view as the reality. There are many many dendrochronologists who are very good at understanding physiology. </i><b><i>Others, have a more shallow background.</i></b><i> We are a population with many different viewpoints and I don&#39;t know any one who is not willing to listen to the others viewpoint and to learn from them. All understand the basic procedures and checks if they are true dendrochronologists. </i><b><i>That does not mean that we don&#39;t have some in the field making bad decisions, but these are usually caught when these people try to publish in the field.&quot;</i></b><br /><br />PHYS: <b><i>&quot;As you point out, there is a long history of unconvinced scientists like myself questioning the validity of dendrochronological methodology and deductions.&quot;</i></b><br /><br />DENDRO: <i>&quot;I would not call it a long history at all. There are people from the outside that question. Many of these become convinced when they truly consider and see what we do. I went through this process myself so I so have a little understanding of what is happening.&quot;</i><br /><br />PHYS: <b><i>&quot;Disciples of dendrochronology may want to reflect a moment on how it could be that all of the various subdisciplines based upon the assumption of the &#39;annual&#39; ring have gained such momentum and influence in the face of such doubt and in the absence of the key ingredient needed to ensure that their foundation is solid.&quot;</i></b><br /><br />DENDRO: <i>&quot;Again, we are viewed by you as disciples following the word of the master. If this were really true, then we surely should reflect. But honestly, Ron, we also practice quite a lot of rigor, reflection, questioning etc.&quot;</i><br /><br />PHYS: <i>&quot;What is that key ingredient? It is the desire to know the whole truth, </i><b><i>even if in finding it there should emerge no further reason for the field of study to continue.&quot;</i></b><br /><br />DENDRO: <i>&quot;Ron, the logic here comes from your assumption that you understand the problem completely and nobody else is smart enough to do it.&quot;</i><br /><br />PHYS: <b><i>&quot;I do not see that introspective critical capacity in the dendrochronology literature, rather I see the promulgation of dogma with statistical inference its crutch.&quot;</i></b><br /><br />DENDRO: <i>&quot;Perhaps some reading of the literature starting with basic works early in the century and following the development of ideas would provide you some perspective. We really cannot do that for you, although it might be a good thing to do.&quot;</i><br /><br />PHYS: <b><i>&quot;I see a technology for estimating chronology, not a science. My impression is that disciples of dendrochronology would prefer to remain in darkness than permit any light to illuminate their weaknesses.&quot;</i></b><br /><br />DENDRO: <i>&quot;Ron, consider how you arrived at such a statement. It all comes from your assumption that we are children counting rings. We have much more valuable things to do than sit around counting rings and we do more valuable things too.&quot;</i><br /><br />PHYS: <i>&quot;Hal, you wrote that &#39;We can answer such criticism, but not until we investigate further how the tree responds to its environment and how the tree lays down layers of cells we call the tree ring.&#39;&quot;</i><br /><br />DENDRO: <b><i>&quot;Here you assume that one has to do all the physiology and all the anatomy before one can attempt using tree rings. What you miss, is that there are more circumstances that we cannot date than there are opportunities to data. Dendrochronologist, contrary to what you seem to think, have solid procedures to reject those many cases that cannot be reliably cross-dated. You never allow us this possibility when you say that only I, a physiologists know what I am talking about. How can we answer this??? We never will be able to.&quot;</i></b><br /><br />PHYS: <i>&quot;I agree with that, but you are quite mistaken in saying that &#39;Physiologists outside dendrochronology have little inclination to do it for us as this message reveals.&#39; (Perhaps the &#39;for us&#39; part is partially correct; I prefer to do my research for a greater purpose than simply to please dendrochronologists!)&quot;</i><br /><br />DENDRO: <i>&quot;Making a contribution to science is not (should not be) a matter of being on the inside or outside of a group. Cambial physiologists are few in number, but I would suggest that those attempting such research -- whatever their affiliation or lack thereof -- have made and will continue for many years to make greater contributions to dendrochronology than anyone investigating tree rings.</i><br /><br /><i>Again we are those poor unenlightened children. I hope such statements make you feel good about yourself.&quot;</i><br /><br />PHYS: <b><i>&quot;Let me issue a challenge to the community: Who will be the first to develop a procedure for producing a complete annual ring in a stem piece cultured in vitro? It remains to be done. By doing so, you will have set the stage for determining and differentiating among direct effects of environmental factors on the nature of the annual ring.</i></b><i> Anyone seriously interested should contact me, and I will help you begin to become familiar with the literature.&quot;</i><br /><br />DENDRO: <b><i>&quot;Surely that is a good thing to do, but while you are going about doing that, we have wonderful opportunities to help people manage our earth more kindly and realistically. We think these kinds of questions are equally important, if not more important as we are likely to destroy our planet unless we learn quickly what the trees have to say about our past. Even though you disbelieve that we can read the earth&#39;s history from tree rings, we have our part to play in the scientific world. Yes we need to learn from people like you, but fortunately few scientists consider us as ignorant and out of communication with &#39;true&#39; science as you.&quot;</i></b><br /><br />PHYS: <i>&quot;have a good day,&quot;</i><br /><br />DENDRO: <i>&quot;You know, Rod, I really have more important things to do in taking care of a person I care much about here at home. This is not a good day, as instead of doing that, I feel called upon to somehow challenge you to be a little more tolerant and understanding. To be quite frank, I feel it is not sinking in and that means I have failed. That makes me very sad, indeed.&quot;</i><br /><br />PHYS: <i>&quot;Rod&quot;</i><br /><br />DENDRO: <i>&quot;Regretfully,</i><br /><i>Hal Fritts</i><br />--<br /><i>Harold C. Fritts, Professor Emeritus, Lab. of Tree-Ring Research</i><br /><i>University of Arizona/ Owner of DendroPower</i><br /><i>5703 N. Lady Lane, Tucson, AZ 85704-3905</i><br /><i>Ph Voice: (520) 887 7291&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Problematic Age of Coal","summary":"Conventional Thought Holds that Coal Deposits were Largely Formed 300 Million Years Ago / Since the Halflife of C14 is 5,730 Years, Any C14 that was Present in the Coal at the Time of Formation Should have Long Since Decayed to Stable Daughter Products / But Coal Seams Commonly Exhibit C14 Dates / In the Rush to Explain this Away, Researchers Point to how Easy it is to Contaminate the Dates / But, what if the Coal Actually is Young? / Have We Simply Misunderstood its Cause?","image":"https://lh3.googleusercontent.com/-rGS6xrt3QgE/WFgWlCvAGII/AAAAAAAAJy8/X3375ICgmTwfgp6dqPfz0z3CfJ_c5uuWwCJoC/w7142-h9999/the-problematic-age-of-coal-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Wy6ApTy4KNz","publishDate":"2016-12-19T17:19:23.039Z","updateDate":"2017-01-03T01:35:25.499Z","text":"<b>The Problematic Age of Coal: Conventional Thought Holds that Coal Deposits were Largely Formed 300 Million Years Ago / Since the Halflife of C14 is 5,730 Years, Any C14 that was Present in the Coal at the Time of Formation Should have Long Since Decayed to Stable Daughter Products / But Coal Seams Commonly Exhibit C14 Dates / In the Rush to Explain this Away, Researchers Point to how Easy it is to Contaminate the Dates / But, what if the Coal Actually is Young? / Have We Simply Misunderstood its Cause?</b><br /><br />Pay close attention to the colors used for the references, as this is an approach I have experimented with when it becomes necessary to mix critique from multiple worldviews into one single claim:<br /><br /><b>Red</b> - Mainstream Source<br /><b>Pink</b> - Electric Universe / Catastrophist Source<br /><b>Orange</b> - Creationist Source<br /><br /><a href=\"http://www.detectingdesign.com/carbon14.html\" class=\"ot-anchor\">http://www.detectingdesign.com/carbon14.html</a><br /><br /><i>&quot;There is yet another very interesting problem with C14 dating. Significant amounts of carbon-14 have been detected in specimens previously thought to be millions of years old, to include </i><b><i>coal</i></b><i>, </i><b><i>oil</i></b><i>, and even </i><b><i>carboniferous portions of fossils belonging to dinosaurs</i></b><i> etc.  Of course this would seem to be impossible because of the fact that carbon-14 in any amount cannot theoretically exist beyond 75,000 to at most 100,000 years.  These ancient fossils should have no carbon-14 remaining at all.  </i><b><i>When tested for carbon-14 they should yield an &#39;infinite&#39; age</i></b><i> but they do not because they do in fact have carbon-14 remaining.  What is the explanation of this phenomenon?</i><br /><br /><b><i>Such claims for carbon 14 found in organic material dating in the millions of years are in fact quite common.  Coal is supposed to have formed millions of years ago, and yet all coal has fair amounts of carbon-14. [12]  Fossil wood found in &#39;Upper Permian&#39; rock that is supposedly 250 Ma old still contained significant amounts of carbon-14. [13]  Recently, a sample of wood found in rock classified as &#39;middle Triassic,&#39; supposedly some 230 million years old, gave a carbon-14 date of around 33,720 years.</i></b><i> [14]  The accompanying checks showed that the C14 date was not due to contamination of &#39;modern&#39; carbon-14.&quot;</i><br /><br /><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=818AE22B382E0482CAAFDCD33136D9A4?doi=10.1.1.372.4689&amp;rep=rep1&amp;type=pdf\" class=\"ot-anchor\">http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=818AE22B382E0482CAAFDCD33136D9A4?doi=10.1.1.372.4689&amp;rep=rep1&amp;type=pdf</a><br /><br />Radiocarbon, Vol 31, No 2, 1989, p117-120<br /><br />Problems Associated with the Use of Coal as a Source of C14-free Background Material<br /><br />David C Lowe<br /><br />National Center for Atmospheric Research, Boulder, Colorado 80307<br /><br />[...]<br /><br /><i>&quot;Vogel et al (1987) have provided an excellent analysis of 1) and 2) by separating out the components of contamination introduced during sample combustion and graphitization, as well as the contribution to background by the accelerator system. Some of the background samples tested by Vogel et al (1987) were specimens of anthracite coal which showed C14 concentrations </i><b><i>considerably higher than expected</i></b><i> due to machine background ages and contamination during sample preparation. When background ages in the vicinity of 50 kyr were expected, the coal samples yielded 40-45 kyr. There are many other unpublished accounts by C14 laboratories in which the use of coal as a background test material has been investigated. </i><b><i>In many cases, the samples were found to contain C14, and further studies were discontinued.</i></b><i> The AMS and gas counting facilities, DSIR, in Lower Hutt, New Zealand, eg, have observed apparent ages for coal specimens ranging from 25-40 kyr, and the NSF Accelerator Facility at Tucson, Arizona has determined ages of anthracite samples ranging from 30-40 kyr (AJT Jull, pers commun, 1988).&quot;</i><br /><br />[...]<br /><br /><b><i>&quot;Because coal is formed over geological time scales at depths providing excellent shielding from cosmic rays, its C14 content should be insignificant in comparison to the 14C introduced by even the most careful sample preparation techniques used in C14 dating laboratories. How is it then, that a material, which should show a C14 age indistinguishable from that produced by a combination of machine background and contamination during careful sample preparation, routinely produces a finite C14 age?&quot;</i></b><br /><br /><a href=\"https://cdn-assets.answersingenesis.org/doc/articles/am/v7/n4/ECP-p855-864-Radiocarbon-Dating.pdf\" class=\"ot-anchor\">https://cdn-assets.answersingenesis.org/doc/articles/am/v7/n4/ECP-p855-864-Radiocarbon-Dating.pdf</a><br /><br />[9] R. L. Whitelaw, 1970, Time, life, and history in the light of 15,000 radiocarbon dates, Creation Research Society Quarterly, 7 (1): 56-71.<br /><br /><b><i>&quot;The presence of detectable carbon-14 in fossils, which according to the uniformitarian timescale should be entirely carbon-14-dead, has been reported from the earliest days of radiocarbon &#39;dating.&#39;</i></b><i> For example, a published survey on all the &#39;dates&#39; reported in the journal Radiocarbon up to 1970 commented that </i><b><i>for more than 15,000 samples reported: &#39;All such matter is found datable within 50,000 years as published.&#39; [9]  The samples involved included coal, oil, natural gas, and other allegedly very ancient material.</i></b><i> The reason these anomalies were not taken seriously is because the measuring technique used in the early decades of radiocarbon &#39;dating&#39; had difficulty distinguishing genuine low intrinsic levels of carbon-14 in samples from the background cosmic radiation. Thus, the low carbon-14 levels measured in many samples, which according to their location in the geologic record ought to have had no carbon-14 in them, were simply attributed to the background cosmic radiation.&quot;</i><br /><br /><a href=\"https://cdn-assets.answersingenesis.org/doc/articles/am/v7/n4/ECP-p855-864-Radiocarbon-Dating.pdf\" class=\"ot-anchor\">https://cdn-assets.answersingenesis.org/doc/articles/am/v7/n4/ECP-p855-864-Radiocarbon-Dating.pdf</a><br /><br />[10] P. Giem, 2001, Carbon-14 content of fossil carbon, Origins, 51: 6-30; J. R. Baumgardner, A. A. Snelling, D. R. Humphreys and S. A. Austin, 2003, Measurable 14C in fossilized organic materials: Confirming the young earth Creation-Flood model, in Proceedings of the Fifth International Conference on Creationism, R. L. Ivey, Jr., ed., Pittsburgh, PA: Creation Science Fellowship, 127-147; Baumgardner, 2005, 587-630.<br /><br />[11] A. A. Snelling, 1997, Radioactive “dating” in conflict! Fossil wood in ancient lava flows yields radiocarbon, Creation Ex Nihilo, 20 (1): 24-27; A. A. Snelling, 1998, Stumping old-age dogma: Radiocarbon in an “ancient” fossil tree stump casts doubt on traditional rock/fossil dating, Creation Ex Nihilo, 20 (4): 48-51; A. A. Snelling, 1999, A dating dilemma: Fossil wood in ancient sandstone, Creation Ex Nihilo, 21 (3): 39-41; A. A. Snelling, 2000, Geological conflict: Young radiocarbon dating for ancient fossil wood challenges fossil dating, Creation Ex Nihilo, 22 (2): 44-47; A. A. Snelling, 2000, Conflicting ‘ages’ of Tertiary basalt and contained fossilized wood, Crinum, central Queensland, Australia, Creation Ex Nihilo Technical Journal, 14 (2): 99-122; A. A. Snelling, 2008, Radiocarbon in “ancient” fossil wood, Acts &amp; Facts, 37 (1): 10-13; A. A. Snelling, 2008, Radiocarbon ages for fossil ammonites and wood in Cretaceous strata near Redding, California, Answers Research Journal, 1: 123-144.<br /><br /><i>About seventy AMS carbon-14 measurements that were published in the standard radiocarbon literature between 1984 and 1998 demonstrate that significant levels of carbon-14 are routinely found in organic material. </i><b><i>According to the conventional uniformitarian timescale, these samples should have been entirely devoid of any carbon-14 because they are supposedly older than 100,000 years.</i></b><i> [10] Additionally, AMS radiocarbon analyses were obtained on fossilized wood from Tertiary, Mesozoic, and upper Paleozoic strata that have conventional uniformitarian ages ranging from </i><b><i>32 to 250 million years.</i></b><i> [11] All fossilized wood samples yielded significant quantities of carbon-14, equivalent to radiocarbon &#39;ages&#39; of between 20,000 and 45,000 years. With a half-life of only 5,730 years, after one million years (or 175 half-lives) the amount of carbon-14 expected would be so small as to exclude even a single carbon-14 atom being left from a beginning mass of carbon-14 equal to the mass of the earth itself! </i><b><i>Thus, the presence of any intrinsic carbon-14 in these fossilized wood samples, that are supposed to be 40-250 million years old, represents a profound challenge to the uniformitarian timescale, because the measured carbon-14 limits the ages of these fossilized woods to merely thousands of years.</i></b><br /><br /><a href=\"http://www.talkorigins.org/faqs/c14.html\" class=\"ot-anchor\">http://www.talkorigins.org/faqs/c14.html</a><br /><br />(this link is now apparently dead, but here&#39;s what it used to say ...)<br /><br /><b><i>&quot;conventional thought holds that coal deposits were largely if not entirely formed during the Carboniferous period approximately 300 million years ago.  Since the halflife of carbon-14 is 5,730 years, any that was present in the coal at the time of formation should have long since decayed to stable daughter products.  The presence of C14 in coal therefore is an anomaly that requires explanation&quot;</i></b><br /><br /><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=818AE22B382E0482CAAFDCD33136D9A4?doi=10.1.1.372.4689&amp;rep=rep1&amp;type=pdf\" class=\"ot-anchor\">http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=818AE22B382E0482CAAFDCD33136D9A4?doi=10.1.1.372.4689&amp;rep=rep1&amp;type=pdf</a><br /><br /><b><i>&quot;One suggestion is that radium, which is present in some coals at the sub ppm level, as a decay product of the uranium/thorium series, may produce C14 during an extremely rare decay event</i></b><i> (Rose &amp; Jones, 1984). Jull, Barker and Donahue (1987) have detected C14 from this process in uranium/ thorium ores. Blendowski, Fliessbach and Walliser (1987) however, have shown that the C14 decay mode of Ra226 is only of the order of 10^-11 of the preferred alpha-decay channel to Rn222. Thus, </i><b><i>the amount of C14 produced by such events derived from radium in coal must be considered as insignificant.&quot;</i></b><br /><br />[...]<br /><br /><i>&quot;I would like to suggest a simple explanation for the finite 14C ages observed for many coal specimens, namely microbial and fungal action in coal substrates.</i><br /><br /><i>The action of various kinds of fungi and microbes in coal has been well documented (Paca &amp; Gregr, 1977; Cohen &amp; Aronson, 1987; Greenwell, 1987). Cohen and Gabrielle (1982) first reported that the fungi Polyporus versicolor and Poria montiola could degrade lignite. </i><b><i>The fungus Polyporus versicolor, which is the common species involved in the rotting of wood, incorporates atmospheric CO2 during its growth and thereby introduces C14 into the coal substrate.</i></b><i> Once the coal specimen is contaminated, fungi and microorganisms may be killed by conventional methods such as autoclaving, but the fungal hyphae and waste products, which contain C14 derived from atmospheric CO2, will be almost impossible to remove by standard chemical washing procedures. </i><b><i>Assuming that a sample of coal contains no C14, microbial action only has to result in the deposition of ca 0.1% by weight of modern carbon in the coal to produce an apparent age of 45 kyr for the specimen.&quot;</i></b><br /><br /><i>Primordial Star</i> by Dwardu Cardona, pp 42-46<br /><br /><i>&quot;The belief that coal is formed from the carbonized remains of plants, including trees, is well known.  The accepted theory states that coal-forming plants had once thrived in swampy areas.  However, </i><b><i>at present, when plants growing in swamps succumb, they usually decompose.  On the other hand, that the plants which supposedly went into forming coal did not decompose is claimed to be evidenced by the imprints they left in the resulting coal.</i></b><i>  Images of ferns and the leaves of a multitude of different trees still etched in coal are so perfectly preserved that they can be readily identified for what they are.  It has therefore been surmised that before these fallen denizens of the forests had time to decompose, they were covered by the waters of the swamps in which they had thrived.  But then we find deposits of sand above coal seams which indicate a sedimentary layer that had somehow covered the coal-forming remains.  And because other coal seams are found on top of this sandy deposit, it has been reasoned that the sand acted as the soil on which a new forest grew and thrived before it, too, suffered the same fate as the previous one.</i><br /><br /><b><i>Marine fossils are often found embedded in the layers between coal seams</i></b><i> [1].  An incursion of the sea is then blamed for these deposits but, because coal was then again formed on top of them, the sea must again have retreated.  </i><b><i>The problem here is that in certain areas as much as 400 coal seams have been found interspersed between other layers of sedimentation.</i></b><i>  This, then, would mean that the sea must have inundated the land and again retreated that many times.  And each time, of course, a forest would have to have grown in the very same area.</i><br /><br /><b><i>Another problem is that many of the plants recognizable from their imprints in coal do not presently grow in swamps but on dry ground.</i></b><i>  This then led to a new theory according to which such trees, once fallen, would have been carted away by overflowing rivers.  This could explain why, in some areas, tree trunks have been found embedded in coal seams in an upright position with their roots uppermost.  </i><b><i>It does not, however, explain why other trunks have also been found upright in their original growing position with their roots still in the ground, unrotted and uncarbonized.  In both cases, one is left to wonder why these trunks were not themselves turned into coal.</i></b><br /><br /><i>Yet another objection focuses on the fact that some of these individual coal seams are as much as fifty or more feet thick.  </i><b><i>This would then have required the compression of a plant matter the original thickness of which would have measured thousands of feet.</i></b><i>  And this, according to the number of separate coal seams, would have had to repeat itself at least a hundred times -- all of which begins to stretch one&#39;s credibility.</i><br /><br /><i>[...]</i><br /><br /><i>[E]arlier opinion often stressed that, if anything, the forests which produced the coal could definitely not have burned up [2].  The carbonization of coal deposits is usually attributed to the fact that plants already contain carbon.  A carbon content, however, is not the same as carbonization.  But if coal is derived from the burned remains of plant life, the carbonization of coal is readily explained.  This is lent credibility by the fact that various forests covering thousands of acreage, and calculated to be something like 70,000 years old, have been discovered buried in the peat of the British Fenlands, including Scotland and Ireland.  While these trees are not burned, neither have they turned to coal.  However, to be clear on this, as Wilfred Francis was later to point out, </i><b><i>the carbonization of burned trees could only have occurred if the burning had been checked by flooding before the total destruction of the forests</i></b><i> [3].</i><br /><br /><i>[...]</i><br /><br /><i>But are forest fires really needed?  </i><b><i>Artificial coal has been produced without setting anything on fire, but simply by heating lignin, the substance that binds plant cells together.</i></b><i>  Clays are used to catalyze the conversion of lignin to coal while heating the concoction at </i><b><i>300 F for as short a period as two weeks</i></b><i>.  Higher grades of coal simply require longer heating [4].  Heat -- not even pressure -- seems to be sufficient, at least for the formation of artificial coal.</i><br /><br /><i>[...]</i><br /><br /><i>The Australian Aborigines tell of a mythological giant ancestor who had set so many trees ablaze that he left nothing in his wake but charcoal and a treeless desert.  When Europeans settled the land, the Aborigines showed them the resultant seam of charcoal.  It turned out to be coal [5].  If this constitutes a bona fide eyewitness report of a coal-forming event, the coal in question would have to have been formed during the Holocene, and suddenly at that.</i><br /><br /><i>[...]</i><br /><br /><i>One particular case that might also bear on all this concerns some timbers which were sealed in a Pennsylvania coal mine for the purpose of suffocating a fire.  </i><b><i>When they were examined ten years later, they were found to have been coalified</i></b><i> [6].  Pressure did not play a role.</i><br /><br /><i>[...]</i><br /><br /><i>Theories of coal formation must also account for the fact that dinosaur footprints have been found on the top surface of coal seems in Utah.  The proposed explanation has been that the dinosaurs walked over the soft peat bed from which the coal was later to solidify [7].  But -- as in similar cases -- </i><b><i>how could such marks in a soft material have remained imprinted through the ages despite the weight of the strata which accumulated on top of them?&quot;</i></b><br /><br />[1] See here especially HG Coffin, &quot;Evidence for the Marine Deposition of Coal,&quot; <i>SIS Review</i> IV:2/3 (Winter 1979/80), pp 68-69.<br /><br />[2] See here, for isntance, G McCready Price, <i>The New Geology</i> (Mountain View, California, 1923), p 465.<br /><br />[3] W Francis, <i>Coal: Its Formation and Composition</i> (London, 1961), p 625.<br /><br />[4] <i>New Scientist</i> (September 1, 1983), p 623.<br /><br />[5] A Roberts, <i>Echoes of the Dreamtime</i> (1990), pp 56, 114.<br /><br />[6] MA Cook, <i>Scientific Prehistory</i>, as cited by AN Beal, &quot;A Life&#39;s Work?&quot; <i>Chronology and Catastrophism Review</i> XVI (1994), p 53.<br /><br />[7] <i>Scientific American</i> (September 1989), p 133.<br /><br /><a href=\"http://thunderbolts.info/tpod/2009/arch09/090623longago.htm\" class=\"ot-anchor\">http://thunderbolts.info/tpod/2009/arch09/090623longago.htm</a><br /><br /><i>&quot;There are carbonized trees standing upright in some coal deposits, although how they extend downward through so many &#39;geological ages&#39; is a mystery to paleontologists, since the layers are said to progress through eons of time: 250-500 million years ago.&quot;</i><br /><br /><a href=\"http://www.talkorigins.org/faqs/polystrate/polystrate_trees.html\" class=\"ot-anchor\">http://www.talkorigins.org/faqs/polystrate/polystrate_trees.html</a><br /><br /><b><i>&quot;There is very little, with the exception of terminology, that would be different in a &#39;modern&#39; interpretation of these features</i></b><i>, and Dawson has much more detail on the other sedimentological features found at Joggins that support his interpretation. </i><b><i>Dawson records well over a dozen horizons with large upright trees</i></b><i>, and smaller ones are even more common. The section at Joggins can still be visited today, and is particularly well-known for the small reptile fossils found there (</i><b><i>they often occur inside the upright tree stumps, apparently they fell in the hollow stump</i></b><i>). There are usually a few upright trees exposed on the shore, although the rapid erosion of the 10m+ high cliffs means the exposed examples change every year.</i><br /><br /><i>Given that an &#39;in place&#39; occurrence was convincingly determined by observations made in the 19th century for this and many other &#39;fossil forest&#39; localities, it is surprising that these conclusions have not been recognized by modern &#39;young Earth global flood&#39; [YEGF] creationists as clear evidence of non-global-flood deposition for much of the geologic record. They often hinge their current arguments on the occurrence of upright trees in Yellowstone National Park, point to their volcanic setting, and then point to floating upright trees floating in Spirit Lake near Mt. St. Helens [2], and say, &#39;See? They could be transported during the flood.&#39;. This argument is completely fallacious, because </i><b><i>most &#39;fossil forests&#39; do not occur in volcanic deposits, and do have the fragile roots of the stumps tightly (penetrating into the surrounding sediment, often into a paleosol (fossil soil)</i></b><i> [besides Joggins, see also 3]. One occurrence is even associated with dinosaur footprints on the same surface, on top of a coal seam [4, 5, 6]. The &#39;transported floating upright stumps&#39; model [2] is a complete red-herring that does not apply to the vast majority of &#39;fossil forest&#39; occurrences.</i><br /><br /><i>As for Malone&#39;s &#39;problem&#39; with the &#39;thousands of years&#39; for the tree to remain upright for &#39;slow accumulation&#39; to occur, it is a non-problem -- he is simply interpolating the average depositional rates for an entire formation down to the scale of metres. This is not the correct way to do it, because individual beds can be deposited rapidly (say, sands and mud during a levee breach), and then little deposition can occur for a long time (e.g., a soil horizon), as is observed in modern river floodplain environments where trees commonly occur. In short, he is assuming conventional geologists would interpret the occurrence the simple way he has interpolated -- they do not.&quot;</i><br /><br />[...]<br /><br /><i>&quot;One of the best, and longest-known &#39;fossil forest&#39; occurrences is a locality known as Joggins, in Nova Scotia.  It is Carboniferous in age, and was first described in detail in the late 1800s.  Here is a quote from Dawson 1868 (pp. 179-180) on the nature of the trees at this locality, in a beautiful cliff section over 1km thick:&quot;</i><br /><br /><a href=\"http://www.biodiversitylibrary.org/item/85863#page/228/mode/1up\" class=\"ot-anchor\">http://www.biodiversitylibrary.org/item/85863#page/228/mode/1up</a><br /><br />CHAPTER XII.<br />THE CARBONIFEROUS SYSTEM - Continued.<br /><br />CUMBERLAND COAL-FIELD, continued -- EXPLANATION OF JOGGINS SECTION -- ANIMAL REMAINS OF THE COAL MEASURES<br /><br />Explanatory Remarks on the Joggins Section.<br /><br /><i>&quot;In the section in the preceding chapter the reader will observe tho words &#39;Underclay, Stigmaria,&#39; frequently recurring; and over nearly every underclay is a seam of coal. An underclay is technically the bed of clay which underlies a coal-seam; but it has now become a general term for a fossil soil, or a bed which once formed a terrestrial surface, and supported trees and other plants; because we generally find these coal underclays, like the subsoils of many modern peat-bogs, to contain roots and trunks of trees which aided in the accumulation of the vegetable matter of the coal. The underclays in question are accordingly penetrated by innumerable long rootlets, now in a coaly state, but retaining enough of their form to enable us to recognise them as belonging to a peculiar root, the Stigmaria, of very frequent occurrence in the coal measures, and at one time supposed to have been a swamp plant of anomalous form, but now known to have belonged to an equally singular tree, the Sigillaria, found in the same deposits (Fig. 30). The Sigillaria has derived its name from the regularly arranged pits or spots left by its rootlets, which proceeded from it on all sides. The Sigillaria has been named from the rows of leaf-scars which extend up its trunk, which in some species is curiously ribbed or fluted. One of the most remarkable peculiarities of the stigmaria-rooted trees was the very regular arrangement of their roots, which are four at their departure from the trunk, and divide at equal distances successively into eight, sixteen, and thirty-two branches, each giving off, on all sides, an immense number of rootlets, stretching into the beds around, in a manner which shows that these must have been soft sand and mud at the time when these roots and rootlets spread through them.</i><br /><br /><i>It is evident that when we find a bed of clay now hardened into stone, and </i><b><i>containing the roots and rootlets of these plants in their natural position</i></b><i>, we can infer, 1st, that such beds must once have been in a very soft condition; 2dly, that </i><b><i>the roots found in them were not drifted, but grew in their present positions</i></b><i>; in short, that these ancient roots are in similar circumstances with those of the recent trees that underlie the Amherst marshes.  In corroboration of this, we shall find, in farther examination of this section, that while some of these fossil soils support coals, others support </i><b><i>erect trunks of trees connected with their roots and still in their natural position.&quot;</i></b><br /><br /><a href=\"http://www.earthage.org/polystrate/Fossil_Trees_of_Nova_Scotia.htm\" class=\"ot-anchor\">http://www.earthage.org/polystrate/Fossil_Trees_of_Nova_Scotia.htm</a><br /><br />The &#39;Fossil Forests&#39; of Nova Scotia:<br />A Review of the Literature<br /><br /><i>&quot;Introduction</i><br /><br /><i>For the past 150 years the Nova Scotia strata has been interpreted by most geologists as in situ continental deposits laid down on river flood plains.  According to Dr. John Calder, this interpretation is &#39;too restrictive&#39; and others (such as myself) believe that this view is incorrect.  One of the first things which led me to question this interpretation were the drawings themselves.  It seemed strange to suggest that many of the fossil trees have &#39;extensive root systems&#39; yet the pictures and drawings of them do not.</i><br /><br /><i>The term polystrate fossil [1] is now often used to describe fossil trees that are upright in relation to the surrounding strata.  Such trees quite often traverse multiple layers of horizontally deposited strata: including sandstones, shales, and sometimes even coal seams. [2]  Upright fossil plants range in size from small rootlets, </i><b><i>to trees over 80 feet long</i></b><i>. [3]  Nova Scotia, has perhaps more upright fossil trees and plants than any place on earth.  At Joggins alone, erect trees and plants occur at over 50 &#39;levels&#39; scattered throughout 7,500 feet of strata; erect roots and rootlets occur at many more levels.  Drifted plants and trees also occur in many sections of this strata.  These drift deposits are scattered over more than 10,000 vertical feet of strata. [4, 5, 6]  </i><b><i>Most of the upright fossil trees at Joggins are from 2-15 feet in length.  The longest was 40 feet. [7]  Many, if not most, of the upright fossil trees at Joggins have little or no visible traces of roots. [8]  In addition, many of the larger Stigmaria roots are  missing their rootlets and many, if not most, rootlets are buried individually: unattached to any tree, or larger root.</i></b><i> [8, 9]  Fragile fossils such as leaves are also common in the Joggins and Sidney strata. [10]  Animal tracks, insects, and rain marks are also found, although they are not nearly as common.  The strata at Sydney is also said to be very similar to that of Joggins. [11]</i><br /><br /><i>Obscure Journals and Old Books</i><br /><br /><i>Although there is enough data on fossil trees, tree stumps and roots to perhaps fill a 200-400 page book, much of it is only accessible with access to large University libraries, document provider services, and from books over 100 years old: half of which is in German. [12]  One of the few articles in English that was devoted to this topic was by Prof. Rupke.  In it he remarks that:</i><br /><br /><b><i>&#39;... I am of the opinion that the polystrate fossils constitute a crucial phenomenon both to the actuality and the mechanism of cataclysmic deposition.  Curiously a paper on polystrate fossils appears to be a ‘black swan’ in geological literature.  Antecedent to this synopsis a systematic discussion  of the relevant phenomena was never published.  However, geologists must have been informed about these fossils.  In view of this it seems unintelligible  that uniformitarianism has kept its dominant position.&#39;</i></b><i> [13]</i><br /><br /><i>This was also hinted at by Schrock when he stated that:</i><br /><br /><b><i>&#39;No attempt has been made to compile an extensive bibliography on the subject of buried stumps, trees, and similar structures</i></b><i>, but the following references contain the essential literature: ...&#39; [14]</i><br /><br /><i>And while Schrock&#39;s references were used in the preparation of this paper, they are only a small portion of the available data on this subject.&quot;</i><br /><br />[...]<br /><br />GRAPHIC REFERENCES:<br /><br />[1 / Creationist] <a href=\"http://www.detectingdesign.com/carbon14.html\" class=\"ot-anchor\">http://www.detectingdesign.com/carbon14.html</a><br /><br />[2 / Mainstream] <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=818AE22B382E0482CAAFDCD33136D9A4?doi=10.1.1.372.4689&amp;rep=rep1&amp;type=pdf\" class=\"ot-anchor\">http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=818AE22B382E0482CAAFDCD33136D9A4?doi=10.1.1.372.4689&amp;rep=rep1&amp;type=pdf</a>, more discussion at <a href=\"http://www.talkorigins.org/faqs/c14.html\" class=\"ot-anchor\">http://www.talkorigins.org/faqs/c14.html</a><br /><br />[3 / Creationist] R. L. Whitelaw, 1970, Time, life, and history in the light of 15,000 radiocarbon dates, Creation Research Society Quarterly, 7 (1): 56-71.<br /><br />[4 / Creationist] <a href=\"https://legacy-cdn-assets.answersingenesis.org/assets/pdf/am/v7/n4/ECP-p855-864-Radiocarbon-Dating.pdf\" class=\"ot-anchor\">https://legacy-cdn-assets.answersingenesis.org/assets/pdf/am/v7/n4/ECP-p855-864-Radiocarbon-Dating.pdf</a><br /><br />[5 / Mainstream][6 / Mainstream] <a href=\"http://www.talkorigins.org/faqs/c14.html\" class=\"ot-anchor\">http://www.talkorigins.org/faqs/c14.html</a><br /><br />[7 / Mainstream][8 / Mainstream] <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=818AE22B382E0482CAAFDCD33136D9A4?doi=10.1.1.372.4689&amp;rep=rep1&amp;type=pdf\" class=\"ot-anchor\">http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=818AE22B382E0482CAAFDCD33136D9A4?doi=10.1.1.372.4689&amp;rep=rep1&amp;type=pdf</a>, more discussion at <a href=\"http://www.talkorigins.org/faqs/c14.html\" class=\"ot-anchor\">http://www.talkorigins.org/faqs/c14.html</a><br /><br />[9 / EU] <i>Primordial Star</i> by Dwardu Cardona, pp 42-46<br /><br />[10 / EU] <a href=\"http://thunderbolts.info/tpod/2009/arch09/090623longago.htm\" class=\"ot-anchor\">http://thunderbolts.info/tpod/2009/arch09/090623longago.htm</a><br /><br />[11 / Mainstream][12 / Mainstream] <a href=\"http://www.talkorigins.org/faqs/polystrate/polystrate_trees.html\" class=\"ot-anchor\">http://www.talkorigins.org/faqs/polystrate/polystrate_trees.html</a><br /><br />[13 / Mainstream] Acadian geology the geological structure, organic remains and mineral resources of Nova Scotia, New Brunswick and Prince Edward Island, by John Willam Dawson, from <a href=\"http://www.biodiversitylibrary.org/item/85863#page/228/mode/1up\" class=\"ot-anchor\">http://www.biodiversitylibrary.org/item/85863#page/228/mode/1up</a><br /><br />[14 / Creationist] <a href=\"http://www.earthage.org/polystrate/Fossil_Trees_of_Nova_Scotia.htm\" class=\"ot-anchor\">http://www.earthage.org/polystrate/Fossil_Trees_of_Nova_Scotia.htm</a><br /><br />[15 / Mainstream] <a href=\"http://sustentabilidade.allianz.com.br/?1673/knowledge-blog-green-short-cut-abolish-fossil-fuel-subsidies\" class=\"ot-anchor\">http://sustentabilidade.allianz.com.br/?1673/knowledge-blog-green-short-cut-abolish-fossil-fuel-subsidies</a><br /><br />[16 / Creationist][17 / Creationist] <a href=\"http://www.creationliberty.com/articles/polystrate.php\" class=\"ot-anchor\">http://www.creationliberty.com/articles/polystrate.php</a>﻿<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Dendrochronology","summary":"When it was Observed in the 1950s that there Might be Discrepancies between Radiocarbon and Historical Ages, Scientists Began Publishing Radiocarbon Calibration Curves / These Curves were Derived from the Dating and Counting of Tree Rings - Oftentimes Fallen Trees whose Ring Records Must be Combined Together to Form a Single Chronology / The Problem is that when the Technique was Applied at the End of the 80's to Trees with Known Ages - Trees Still Alive - there was Only 1.3% Agreement per Century Observed between the Ring Patterns","image":"https://lh3.googleusercontent.com/-P5jT9m_wMLE/WFWtfVRzBqI/AAAAAAAAJwk/AzYuqaSm184vSCa_j3AM0mA0xee9S7ExACJoC/w3600-h5040/dendrochronology-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/ZrqygE2BzmP","publishDate":"2016-12-17T21:27:26.597Z","updateDate":"2017-01-03T01:35:39.082Z","text":"<b>Dendrochronology: When it was Observed in the 1950s that there Might be Discrepancies between Radiocarbon and Historical Ages, Scientists Began Publishing Radiocarbon Calibration Curves / These Curves were Derived from the Dating and Counting of Tree Rings - Oftentimes Fallen Trees whose Ring Records Must be Combined Together to Form a Single Chronology / The Problem is that when the Technique was Applied at the End of the 80&#39;s to Trees with Known Ages - Trees Still Alive - there was Only 1.3% Agreement per Century Observed between the Ring Patterns</b><br /><br /><i>Radiocarbon Dating: Interpreting the Past</i><br />Sheridan Bowman<br /><br />[...]<br /><br /><i>&quot;Atmospheric C14 variations</i><br /><br /><i>The work of the early decades</i><br /><br /><i>The assumption of constant C14 concentration was not taken lightly even from the very beginning.  In 1949, James Arnold and Willard Libby published a &#39;curve of knowns&#39; which was a test of the technique, and therefore of the assumption of constant concentration, using known-age samples ranging from about 900 to 4900 years old.  Given the experimental conditions then achievable, there was good agreement, at least for this period, between the theoretical and measured C14 activities versus known age.  </i><b><i>During the 1950s, with advances in techniques for detecting C14, discrepancies increasingly emerged between radiocarbon ages and historical ages for the Egyptian Old Kingdom.  These discrepancies were far from insignificant, the radiocarbon results being several centuries too young.  The validity of the historical ages was of course not proven beyond doubt, and other evidence was sought.</i></b><br /><br /><b><i>Tree rings provided the truly known-age material needed to test the accuracy of the new technique.</i></b><i>  Dendrochronology, the science of using tree rings for dating ... had been developed by A. E. Douglass in America in the early part of the twentieth century for research on past climate.  </i><b><i>By the late 1950s several scientists, notably Hessel de Vries in the Netherlands, were radiocarbon dating rings from trees dated by dendrochronology, and confirming the radiocarbon discrepancy.  It therefore became clear that radiocarbon results would need to be calibrated to convert them to calendar ages.  Since there is no theoretical way of predicting the correction factor, empirical calibration curves were needed to link radiocarbon &#39;age&#39; with known age.</i></b><br /><br /><i>In the 1960s, a continuous tree-ring sequence stretching back some 8000 years was established by Wesley Ferguson, and the first calibration curve using this was published by Hans Suess.  </i><b><i>This curve was partly based on a remarkable tree, the bristlecone pine</i></b><i> ...</i><br /><br /><i>Suess&#39;s curve confirmed that there are indeed major discrepancies between radiocarbon age and calendar age.  This was the first useful calibration curve in that it had a long temporal coverage, </i><b><i>radiocarbon error terms in the order of 1%</i></b><i>, and used </i><b><i>truly known-age material (i.e. tree rings)</i></b><i> for the calendar axis.  It was also the first of many such curves, and their proliferation prior to 1985 has caused almost as many problems as have been solved.  There are now internationally agreed calibration curves for the period back to 2500 BC ...</i><br /><br /><i>Two trends were apparent in Suess&#39;s curve.  First there is a long-term trend that can be described approximately by a sine wave with a period of about 9000 years.  The maximum deviation from true age is about 900 years too recent at the beginning of the fourth millenium BC.  On the other hand, in the middle of the first millenium AD, radiocarbon produces ages too old by a century or so ...</i><br /><br /><i>The second feature takes the form of &#39;wiggles&#39;.  These are superimposed on the main sine wave and are of short calendar duration (a few decades) but can have amplitudes on the radiocarbon axis of a century or so.  Suess, when asked about the line drawn through his data points, said he had used the process of &#39;cosmic schwung&#39;, what we might call &#39;freehand&#39;.  Whether these wiggles really existed or were an artefact of experimental error was the subject of much investigation in the 1970s.  Techniques with higher precision have now been developed and the general validity of these wiggles has been proven by various laboratories, notably those in Belfast, Groningen, Heidelberg and Seattle.&quot;</i><br /><br />[...]<br /><br />Note in that text the quote ...<br /><br /><i>&quot;This was the first useful calibration curve in that it had a long temporal coverage ... and used </i><b><i>truly known-age material (i.e. tree rings)</i></b><i> for the calendar axis.&quot;</i><br /><br />Charles Ginenthal shows us below that this is actually an over-simplification ...<br /><br />Chapter 2: Radiocarbon Concentration Effects<br /><br />(Note that Ginenthal&#39;s &quot;Scientific Dating Methods in Ruins&quot; has in recent years completely disappeared from the Internet -- as though it has somehow become irrelevant.  Think about the implication of that as you read this excerpt from it ...)<br /><br />Scientific Dating Methods In Ruins<br />Charles Ginenthal<br /><br />The Velikovskian Vol 2 No 1 (1994)<br /><br />[...]<br /><br /><i>&quot;DENDROCHRONOLOGY</i><br /><br /><b><i>&#39;I am skeptical about the validity of the counts [of coral growth rings]. The growth lines vary in spacing from a few microns to nearly zero, and it is often difficult to decide whether one should be counted or not. Under these circumstances, it is notorious that total counts come out at what the investigators think they should be. The subjectiveness of such counts is highlighted by a report by R. G. Hipkin, an Edinburgh University geophysicist, that he counted 253 ridges and later 359 ridges in a repeat count of the same specimen.&#39;</i></b><i> [29]</i><br /><br /><i>In order to remove all doubt about the validity of radiocarbon dating, Stiebing informs us that</i><br /><br /><i>&#39;rejection of radiocarbon dating also means rejection of dendrochronology (tree ring dating), </i><b><i>one of the most accurate dating methods yet developed.</i></b><i> A tree ring sequence 7,000 years long has been developed from the long-lived bristlecone pine tree of the southwestern United States ... More recently, a 6,000-year tree ring sequence developed from Irish Oak trees has been radiocarbon tested and its calibration curve generally agrees with that established from the bristlecone sequence. Both sets of calibrated dates generally support the conventional chronology for ancient Egypt and, thus, cannot be reconciled by revisions like those proposed by Heinsohn.&#39; [30]</i><br /><br /><i>In order to match the Egyptian tree ring analysis with bristlecone and Irish Oak tree dating analyses, one would expect to find a tree ring chronology for Egypt and the Eastern Mediterranean that conforms to those of the other regions. However, trees are and were scarce in countries such as Egypt, so that dendrochronologies of these regions have never been developed. The Eastern Mediterranean tree ring chronology goes back no farther in time than 1073 AD. [31]</i><br /><br /><i>Nearly all discussions about dendrochronological studies in catastrophist journals have left out the fact that </i><b><i>climate is only one of several factors affecting tree growth or the production of annual rings.</i></b><br /><br /><b><i>&#39;GROWTH is the cumulative result of all factors acting on the tree over time. Some factors, such as tree species, tree genetics and site quality, are fixed and, therefore, exert a relatively constant influence on the resultant tree growth. Stress factors, such as climate, insects and disease, [and] tree competition ... can change dramatically with time and contribute to radical growth fluctuations.&#39;</i></b><i> [32]</i><br /><br /><b><i>Insect infestation and certain diseases are significant factors affecting tree ring development which are not taken into account.</i></b><i> Certain insects cause defoliation, which can greatly inhibit photosynthesis, particularly during periods of insect population surges. During such periodic insect infestations, trees produce less food and deplete the food stored as starch in their roots. When this stored food is deleted, trees produce significantly narrower tree rings. How does one determine the difference between the narrow tree rings of ancient trees which were caused by periodic epidemic defoliation and those caused by climatic conditions? </i><b><i>Periodic plant diseases also result in narrower tree rings</i></b><i>, particularly those diseases affecting the roots which take in water and minerals for food production. In ancient trees, how does one distinguish between narrow rings caused by periodic root disease and those caused by climate conditions? Even in ideal climates, trees can be affected by insect infestations or by root diseases and nematode attacks. </i><b><i>Tree branch damage during strong, periodic wind storms or heavy snowfalls can also cause problems. After such events, trees must use whatever resources they possess to repair the damage; these resources cannot be utilized for the production of larger tree rings as well. How can one know of all these periodic damages from ancient logs? Trees, like all living organisms, are subject to the vicissitudes of life. None of the previously mentioned problems, connected with ring production, can be distinguished from stresses caused by climate!</i></b><br /><br /><i>Thus, </i><b><i>how does one distinguish between false and true correlations of tree rings in ancient trees, where one cannot know whether the causative factor for these narrower rings is climate, insects, disease, or mechanical damage?</i></b><i> Only a dendrochronologist can determine the difference, </i><b><i>if he or she knows the climate conditions year by year.</i></b><i> Without such knowledge, analysis is simply guess work! This applies, specifically, to the chronology that goes back 2,700 years or more, but must also apply to the long chronology derived from bristlecone pines.</i><br /><br /><b><i>The previously discussed work, carried out by the provincial government of Ontario, Canada, between 1984 and 1990, showed that dendrochronologists were able to do their tree ring analysis only by knowing the climate stresses and the insect and disease stresses, year by year, for each stand of trees. Without this prior knowledge, their analysis would not be reliable.</i></b><br /><br /><i>What is required by this method? John Gribbin explains:</i><br /><br /><b><i>&#39;First, ... the dendrochronologists have to establish that in living trees the [oxygen-16 and oxygen-18] isotopes do not migrate across the tree rings, garbling the temperature record. This can only be done by calibrating the outer rings of living trees against the historical record of temperature variations before using inner rings to find out temperature changes in the more distant past. And among other things, that means throwing out the data from the famous bristlecone pines ... of California where there are no decent records of modern temperatures to establish the calibration.&#39;</i></b><i> [33]</i><br /><br /><i>In essence, the 7,000-year-old California record paraded before us by Stiebing is based on nothing but assumption. </i><b><i>No historical record of temperature variations exists with which to calibrate the outer rings so that one may extrapolate to the inner rings of these trees.</i></b><i> Gribbin tells us that, without this fundamental calibration, the record of the famous bristlecone pines requires that the data collected be thrown out! This is the same record that Stiebing has assured us disproves the revised chronology and supports the established chronology.</i><br /><br /><i>This record cannot be used to support any chronology. To do so requires known historical temperature records for calibration, which do not exist. For anyone to suggest that there is any corroboration for this bristlecone pine chronology requires that the chronology, itself, be calibrated with historical temperature records. It is not!</i><br /><br /><i>The question Stiebing must face is, How can a calibration curve for the American bristlecone pine, which does not correlate with anything, be made to agree with the 6,000-year-old chronology from Irish Oak trees and with ancient, Middle Eastern, historical chronology?</i><br /><br /><i>Is the chronology for the Irish Oak trees any more reliable than that for the bristlecone pines? </i><b><i>According to the Encyclopaedia Britannica, there are two types of records for tree ring chronologies: a fixed record and a floating record. The fixed record is provided by a &#39;freshly cut tree [or living, cored tree] that exposes a record of time in its cross-section of concentric rings ... The starting point is the inner ring and the termination point [is] the outer ring ...</i></b><i> [34]</i><br /><br /><b><i>The floating record is provided when</i></b><br /><br /><b><i>&#39;a tree stump is produced not by cutting but, rather, by natural causes -- such as felling by a heavy wind in the unknown past -- a counting of rings would establish only the age of the tree when it was killed, not the time when the tree began to grow.</i></b><i> The record of ring widths provides a ... chronology of climate variation in either case, but the chronology obtained from the manmade stump is fixed, whereas that from the wind-created stump [has no] anchor in time. It is said to be floating. </i><b><i>Floating chronologies are usually far less reliable than [fixed chronologies].</i></b><br /><br /><i>The foregoing example points to five questions that must be posed in evaluating any method proposed for determining absolute age:</i><br /><br /><i>1. What physical record of time was available?</i><br /><i>2. Is the record complete?</i><br /><i>3. Are the end points of the record clearly discernible?</i><br /><i>4. What point in time corresponds to the terminal point?</i><br /><i>5. Is the rate of buildup or accumulation of the record determinable and, if so, how reliably?&#39; [35]</i><br /><br /><i>With respect to the Irish Oak trees, the physical record available was a tree ring record (answer one). However, the record is incomplete (answer two). The starting, or older, end point cannot be known because the record in incomplete (answer three). We know the time of the present terminal point (answer four). The record, however, is both incomplete and unreliable (answer five) because one cannot know if the different cored trees had narrow rings due to climate, insect infestations, root diseases, or wind and snow damage.</i><br /><br /><i>The Irish Oak tree ring chronology is a floating chronology, not a fixed one. Thus, it is less reliable as a record. The trees were found buried in bogs in different locations in Ireland, having fallen by natural means. </i><b><i>Overlapping, from one bog to another, is required, as with varves in lakes. Also, it is important to know that there are major gaps in the record.</i></b><i> According to Michael G. L. Ballie,</i><br /><br /><b><i>&#39;[o]ne point should be clarified. No one set out to look for gaps ... The gaps are there, however, and show strongly in the fact that they appear in the results of separate workers. So we can suggest that there are periods from which relatively large numbers of long-lived timbers survive, separated by intervals when such timbers virtually do not exist.&#39;</i></b><i> [36]</i><br /><br /><i>These gaps, periods of 100 years or more, are found both in the BC time period and in the AD era. Ballie informs us that</i><br /><br /><b><i>&#39;in theory, oaks should have existed continuously on the bog in order to explain their presence after the gap. Extensive sampling was undertaken and the whole length of each of the chronologies was duplicated with fresh material. No trees were found which would bridge the gap, although numerous specimens fell on either side.&#39;</i></b><i> [37]</i><br /><br /><i>He also states, &#39;Now, obviously, material of this age must exist; oaks did not die out at 900 BC.&#39; [38]</i><br /><br /><i>It is absolutely impossible for trees to not have fallen in some of the separated bogs that were investigated or to have failed in producing a single specimen for long, identical time periods, and then, as if by magic, for the trees at every bog to fall again. The trees could not have all completely vanished into thin air during these identical time periods. </i><b><i>This evidence indicates that there is something fundamentally wrong with the methodology used to compile the Irish Oak tree ring chronology. What seems obvious to me is that the trees from the gaps must have been incorporated into chronological periods where they do not belong!</i></b><i> Before critics raise objections to this basic analysis, I suggest that they explain and demonstrate how it is possible for the gaps to exist at all. I challenge Stiebing and others to explain where the trees from these gaps went! I believe that the Irish Oak tree chronology is unreliable. </i><b><i>Trees simply do not stop falling into bogs at identical times, over extended periods.</i></b><i> They must have fallen and, therefore, must have been incorporated into the chronology at points where they do not belong. This means that the compiled chronology is wrong.</i><br /><br /><i>One more point regarding the Irish Oak tree chronology must be made. These trees are found in mud bogs. One of the methods for correlating tree rings is to find human structures nearby that are known to have been built during or at a certain time. By cross-matching the tree rings of beams in these structures with tree rings from the bogs, it is possible to determine the ages of certain bog trees. However, when one searches the very remote past, one cannot find such well-dated structures; if such are unearthed, their ages cannot be precisely known except by radiocarbon analysis. The problem, in this case, is twofold: First, the manmade structures were found in the earth and the timbers from them that are radiocarbon dated had to have been greatly contaminated by ground water. Second, the bog oaks are lying in earth and water and, thus, any radiocarbon date derived from wood in such an environment must be contaminated by water bearing carbon dioxide. Hence, there is no sure method for precisely corroborating such a very ancient tree ring chronology. Ground water, moving through these bogs, will play havoc with the amount of carbon dioxide that infiltrates the logs. This was found in the organic material from beaches dated by radiocarbon analysis.</i><br /><br /><b><i>The most cogent point that Stiebing makes is that forests of different species of trees, separated from each other by several thousand miles, on different continents, in different climatic environments, will produce tree ring patterns going back several thousand years in which the calibration curves -- which are statistically derived from these separate stands of trees -- will agree with each other.</i></b><i> He also claims that radiocarbon dating of the wood from these trees supports the view that they have generally similar, statistically derived, calibration curves and that these calibrated curves generally support the conventional chronology for ancient Egypt.</i><br /><br /><b><i>If this is correct, then this method should not only work for the dead trees found in the forests of the American southwest and in Ireland, separated from each other by many miles and cross-matched by the investigators at both locations so that they could build their floating chronologies and derive calibration curves, it should also apply to and produce generally similar results with living bristlecone pine trees separated from each other by only a few hundred miles, forming fixed chronologies. This would be the crucial test of Stiebing&#39;s conclusion. If a few stands of living bristlecone pine tree forests, which are separated by a few hundred miles, are cored, then, based on Stiebing&#39;s statement regarding calibration curves for forests separated by thousands of miles, these living trees should also generate tree ring patterns creating calibration curves that generally match each other. This test is fundamental to the concept proposed.</i></b><br /><br /><b><i>The point I wish to drive home is that a fixed chronology derived from living trees, as opposed to a floating chronology for dead trees, can never be in doubt because one is not required to assume that cross-matched pieces of dead tree trunks really match one another chronologically. Since mismatches of dead tree trunks rings are possible, one cannot have perfect confidence in this method. On the other hand, with living trees there is always a complete and undisputed set of tree rings going from the present to the past. The dead trees, which require overlapping pieces of cross-matched cores, are only assumed to give an accurate and complete chronology when carefully integrated. Thus, only if the substantiated, living core chronologies show generally similar patterns from their tree rings can the conventional chronology be supported from this evidence. If, however, the substantiated, living core chronologies emphatically disagree with each other, then dendrochronological dating evidence between forests separated by thousands of miles is worthless.</i></b><br /><br /><b><i>Paula V. Krebs carried out this experimental work on four stands of living bristlecone pine trees in Colorado. Each of these forests was separated from each other by a few hundred miles.</i></b><i> The tree ages, from oldest to youngest, spanned 1,568 years, 843 years, 666 years and 457 years. That is, from oldest to youngest, the trees dated from 403 AD, 1138 AD, 1305 AD and 1514 AD.</i><br /><br /><i>According to Krebs,</i><br /><br /><b><i>&#39;There are a few similarities in the four chronologies, but the dissimilarities are more striking. For the segment common to all four chronologies, AD 1514 to 1970, there are six common diagnostic points [rings that agree with each other on the calibration curves].</i></b><i> These are [for the years] 1645, 1748, 1847, 1851, 1880 and 1902.&#39; [39]</i><br /><br /><b><i>Hence, according to Krebs, this extrapolates so that the &#39;1514 to 1970 average [chronologies show] only 1.3 common diagnostic features per century.&#39;</i></b><i> [40]</i><br /><br /><b><i>This means that, for each 100 years of the four overlapping bristlecone pine tree chronologies, the average percentage of agreements among the calibration curves was only 1.3%, while 98.7% of the curves were in total disagreement. The chronologies derived from their calibration curves disagreed by almost 99%. According to Krebs, these 98.7% &#39;patterns of the chronologies are very diverse ... Each of these chronologies has a distinct pattern which may be attributed to habitat differences and geographic location. The low level of correspondence prohibits the merging of four separate chronologies into one.&#39;</i></b><i> [41]</i><br /><br /><i>What should be obvious at this point is that the chronologies of four living stands of bristlecone pine trees completely contradict the notion that Stiebing has put forth -- that forests separated by even a few hundred miles will be integrated to create a single chronology for any group of trees. Therefore, the conjecture that correlated, overlapping, cross-matched, dead tree parts from regions in the American southwest and Ireland have, in fact, generated similar patterns and calibration curves is extraordinarily dubious. If that were the case, the living trees of the same species, inhabiting the same general environment, would exhibit similar patterns.</i><br /><br /><i>What Stiebing has assumed is that different climatic zones, on different continents, would affect species of trees in a similar manner. </i><b><i>We discovered, instead, that, even in the same climatic zone, on the same continent, in fact, in the same general area and with the same general environment, one tree species was influenced in so diverse a manner that no amount of calibration will ever generate similar curves.</i></b><i> Based on this evidence, it is wrong to suggest that any two regions of the Earth, inhabited by trees that are separated by significant distances, will ever allow corroborative comparison.</i><br /><br /><i>The fundamental test that validates dendrochronology must prove, conclusively, that there is a basic correlation between tree rings subjected to the same climate conditions while growing in the wild, conditions which produced clearly measurable and similar ring patterns. One would expect that trees growing in the same forest, subject to the same topography, sunlight, rainfall, disease, insect infestation and any other conditions, would all exhibit tree rings correlating strongly with one another. During years of drought, one would expect these tree rings to decrease in size. Warm years, with much rainfall, would create thicker tree rings. If we have a drought for five or six years, followed by a dozen good growth years, we would expect to find that, during this 17-year or 18-year period, small rings would be followed by large rings.</i><br /><br /><i>I am speaking of living trees and not of correlated dead ones. This would be the crucial test for the validity of dendrochronology.</i><br /><br /><i>As noted earlier, the Ontario Ministry of the Environment published the results of a study carried out between 1984 and 1990. The study investigated acid rain effects on sugar maple trees, going back to 1900, with three of 11 sites having their earliest growth in 1910, 1920 and 1930. The trees covered a few acres at each site. They had to be on level ground and in full sunlight so as to be subject to exactly the same conditions. Two increment cores were taken, one from the north side of the trees and the other from their south side, mounted on plywood boards and sanded to make the rings stand out more clearly. Phluoroglucinol was used to bring out particularly faint rings. About 10% of the cores had sections so difficult to measure with any degree of confidence that they were not considered. &#39;The cores with the clearest rings were measured first and used as quality control bench marks, against which the less clear tree ring series could be compared.&#39; [42]</i><br /><br /><i>The trees were separated into two groups: one was comprised of healthy trees; the other was comprised of sick trees, or trees in decline. Graphs were made of the ring growth of both groups. </i><b><i>Similarities were noted in the patterns between the two groups, yet great discrepancies were also noted. During some years, while one group of trees had rings that were increasing in size, the other group of trees had rings that were decreasing in size. While the healthy trees producing wide tree rings began to produce narrow ones, the sick trees, originally producing narrow, small rings, began to produce wide, large ones. There was anticorrelation growth between the tree rings of both groups during the same years. Furthermore, anticorrelations were averaged; while some trees in one group produced large tree rings, others in the same group produced much larger ones and while some trees in the other group produced small tree rings, others in the same group produced much smaller ones. This happened often within each of the groups at the 11 sites. The trees were all in the same environment, subjected to the same conditions. Why, then, did anticorrelations occur repeatedly if climate conditions were the same?</i></b><i> Nearly all the conditions were known for this test; therefore, discrepancies can be explained, but not with any degree of certainty or confidence into the remote past.</i><br /><br /><i>The Ontario researchers make the following admission after analyzing the evidence. They state that &#39;even though the tree rings can be measured with considerable accuracy and the data [can be] de-trended (statistically corrected) to minimize the geometry associated with tree age and stand competition, the effect of climate may be significant and remains largely unquantifiable.&#39; [43] </i><b><i>This means that the dendrochronologists, with a complete set of tree ring size data from living trees, were unable to determine the climate and its actual influence on the trees. If this is the case for living trees, why does Stiebing expect anyone to accept this methodology for ancient, sparcely scattered wood from dead trees?</i></b><i> Dead trees from the remote past are correlated by the size and patterns of the growth rings. In ancient structures, we rarely ever encounter complete logs; therefore, ancient correlations between tree rings cannot be known as true! That is, apparently, why the Irish Oak chronology has major gaps in it.</i><br /><br /><i>Why do living trees show anticorrelations between their tree rings, but we are told that dead trees show correlations? Stiebing has stated that dendrochronology is a reliable dating method. The Canadian dendrochronologists finally did show correlation, but only by choosing comparable ring sizes from each group of trees. This proves nothing! A further point that must also be stressed is that we do not know, nor can it ever be known, whether the dead trees chosen for correlations in the ancient past grew on level ground, in full sunlight. On slopes, such trees would have very different growth patterns; however, if they mimicked the pattern of trees in the same or other localities that were hundreds of years older or younger, they would be included in the group that the tree rings resembled most.</i><br /><br /><i>How does a dating method with these dendrochronological problems support the conventional chronology?&quot;</i><br /><br />[...]<br /><br />REFERENCES:<br /><br />[29] S. Warren Carey, &quot;Criticisms of Earth Expansion,&quot; Theories of the Earth and Universe (Stamford, California, 1988), p. 196.<br /><br />[30] William H. Stiebing, Jr., &quot;Heinsohn&#39;s Revised Chronology,&quot; AEON II: 5 (1991): 49.<br /><br />[31] P. I. Kuniholm and C. L. Striker, &quot;Dendrochronological Investigations in the Aegean and Neighboring Regions,&quot; Journal of Field Archeology 14 (1987): 385-398.<br /><br />[32] D. L. McLaughlin, D. E. Corrigan and W. D. McIlveen, &quot;Etiology of Sugar Maple Decline at Selected Sites in Ontario (1984-1990),&quot; report prepared by the Phytotoxicology Section, Air Resources Branch, Ontario Ministry of the Environment, Ontario, Cananda (1992): 94.<br /><br />[33] John Gribbin, Future Weather and the Greenhouse Effect (New York, 1982), p. 124.<br /><br />[34] Encyclopaedia Britannica, Macropaedia, 19 vols. (Chicago, 1982), Vol. 5, p. 501.<br /><br />[35] Ibid., p. 502.<br /><br />[36] Michael G. L. Ballie, Tree Ring Dating and Archeology (Chicago, 1982), p. 222.<br /><br />[37] Ibid., p. 219.<br /><br />[38] Ibid., p. 221.<br /><br />[39] Paula V. Krebs, &quot;Dendrochronology of Bristlecone Pine (Pinus Aristata Engelm) in Colorado,&quot; Arctic and Alpine Research 5: 2 (1973): 149.<br /><br />[40] Ibid.<br /><br />[41] Ibid., pp. 149-150.<br /><br />[42] D. L. McLaughlin et al., op. cit., p. 95.<br /><br />[43] Ibid., p. 96.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Clovis Police","summary":"There is Considerable Danger to Thinking that Radiometric Dates can be Understood in Isolation / Chronologies are Inherently Sociological Phenomena Rooted in the History of Culture / The Theory that the Peopling of the Americas Began with the Clovis - 12,000 Years Ago - is a Case in Point / Numerous Careers have been Destroyed for the 'Heresy' of Publishing Pre-Clovis Artifacts / A Number of Archaeologists have Admitted to Re-burying or Skipping Over Such Finds to Save their Careers","image":"https://lh3.googleusercontent.com/-lGnv2yYEbbA/WFREz9q5JeI/AAAAAAAAJvE/D_hwpKwIOusp24L2EQU7Ld46c0v5inMywCJoC/w7142-h9999/the-clovis-police-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/JtRaDex3d5V","publishDate":"2016-12-16T19:47:33.422Z","updateDate":"2017-01-03T01:36:01.637Z","text":"<b>The Clovis Police: There is Considerable Danger to Thinking that Radiometric Dates can be Understood in Isolation / Chronologies are Inherently Sociological Phenomena Rooted in the History of Culture / The Theory that the Peopling of the Americas Began with the Clovis - 12,000 Years Ago - is a Case in Point / Numerous Careers have been Destroyed for the &#39;Heresy&#39; of Publishing Pre-Clovis Artifacts / A Number of Archaeologists have Admitted to Re-burying or Skipping Over Such Finds to Save their Careers</b><br /><br />Note: Although Ginenthal&#39;s purpose in telling the story of Ales Hrdlicka and the Clovis Police is to cast doubt upon the overkill hypothesis, my motive here is different.  To be clear, I do not defend the validity of any of the proposed dates which follow, and this 1997 text may not be reflective of the current dominant theory for the peopling of the Americas.<br /><br />My purpose here is to instead emphasize a more fundamental lesson -- the way in which the chronology itself is constructed through sociological forces.  The dates subsequently published are selected as right or wrong in accordance with whether or not they provide validation to these historical sociological forces.  The fact of the matter is that we cannot understand the chronology or the dates in full without a look at the sociology of the domains where they are employed.<br /><br />The quote in the small circled icon comes from here ...<br /><br /><a href=\"http://www.slideshare.net/historyteacher38668/his-2213-who-were-the-first-americans-when-did-they-arrive-here-8938083\" class=\"ot-anchor\">http://www.slideshare.net/historyteacher38668/his-2213-who-were-the-first-americans-when-did-they-arrive-here-8938083</a><br /><br />The following excerpt comes from ...<br /><br /><a href=\"http://immanuelvelikovsky.com/Mammoth.pdf\" class=\"ot-anchor\">http://immanuelvelikovsky.com/Mammoth.pdf</a><br /><br />(see original for references until I add them here)<br /><br /><i>The Extinction of the Mammoth</i><br />by Charles Ginenthal<br /><br /><i>The Velikovskian: A Journal of Myth, History and Science</i><br />Quota pars operis tanti nobis committitur?<br /><br />Vol III, Nos. 2 and 3<br /><br />A selection from Chapter 2: The Age of Man in America<br /><br /><i>&quot;Charles Lyell, upon his realization that there had been a recent extinction of the giant animals suggested that the extermination via hunting by man &#39;is the first idea presented to the mind of almost every naturalist.&#39; [33] Eleven years later, Richard Owen claimed that in the Americas, the extinction could have come about through the &#39;appearance of mankind on a limited tract of land not before inhabited.&#39; [34]</i><br /><br /><i>[...]</i><br /><br /><i>The history of the time in which the mammoth lived, as related to man, has a colorful and interesting history which will be disclosed below. </i><b><i>The point I wish to emphasize is that once a powerful and illustrious scientist makes a statement, it often becomes dogma for generations to follow and acts to seriously impede any progress toward solution based on any new evidence.</i></b><i> The following discussion is, therefore, germane not only as science, but as history about the time in which the mammoths lived in the arctic.</i><br /><br /><i>One, therefore, ought to know fairly precisely the dating of man&#39;s arrival in the Americas and the correct date for the extinction; the extinction should be closely timed to the arrival of human hunters. Donald K. Grayson, however, shows, &#39;The timing of Ice Age extinctions is really very poorly understood ... Radio carbon chronologies are bad in North America and worse in Europe.&#39; [44] However, if it could be firmly established that man lived in the Americas for longer than 10,000 to 12,000 years, the synchrony of hunters arriving in America, and the onset and, soon thereafter, extinction or overkill thesis, would fail. </i><b><i>The overkill hypothesis is based, in large measure, on this synchrony which has become archeological dogma over the past 50 years, namely that man has not been in North America prior to about 12,000 years B.P., when he crossed the land bridge at the Bering Strait and then began this slaughter.</i></b><br /><br /><i>&#39;The quest for the origin of the Americans has been going on ever since Columbus ... Although the Bible doesn&#39;t mention them [the Indians], the Pope in 1512 officially declared them to be descendants of Adam and Eve. New England Protestant cleric Cotton Mather disagreed; he believed they were brought here by the devil. Others linked them with the &#39;lost ten tribes&#39; of Israel, with ancient Greeks, Trojans, Egyptians, Norwegians, and the inhabitants of the mythical continents of Atlantis and Mu.&#39; [45]</i><br /><br /><i>But the 12,000 year date was not conceived of until early in the 20th century. According to Roger Lewin,</i><br /><br /><i>&#39;&#39;The origins of American Indians is a topic that has occupied the minds of western scholars for nearly 500 years,&#39; explains Richard Morland of the Canadian Museum in Ottawa. In the late eighteenth century, for instance, Thomas Jefferson became convinced on the basis of archaeological and linguistic evidence that America Indians shared a common origin with northern Asiatics. And the diversity of America Indian languages persuaded him that they had a very long history. </i><b><i>Through Darwin&#39;s time, scholars even suggested that American Indian history went back perhaps as far as 100,000 years. This notion was squashed at the turn of the century by American anthropologists W. H. Holmes and Ales Hrdlicka.&#39;</i></b><i> [46]</i><br /><br /><i>The first hurdle to overcome was the opposition of Ales Hrdlicka, like Cuvier, he said man had not been in America with the mammoth.</i><br /><br /><b><i>&#39;The main authority on these topics ... was the Czech-born anthropologist Ales Hrdlicka, who in 1903 became the Smithsonian Institution&#39;s first curator of physical anthropology. Hrdlicka, a brilliant scientist and a devastating debater, systematically examined and confidently dismissed all evidence offered to show man&#39;s antiquity in the New World. He brushed aside ... discoveries [of human relics found with mammoths or other ancient extinct creatures] by saying that the human relics had accidentally became mixed with those of extinct animals ...</i></b><br /><br /><b><i>&#39;As he grew older his opinions hardened into prejudices, which he proclaimed so vehemently that most scientists thought it would be improper to hold an opposing belief. No one wanted to contradict fiery old Hrdlicka, for when anyone dared to suggest that men had lived in the United States in the time of big extinct mammals, Hrdlicka immediately loosed a barrage of denunciation.&#39; [47]</i></b><br /><br /><b><i>The point I wish to stress is that when a powerful figure like Hrdlicka intimidates professionals in less powerful and less secure positions, then good science cannot be done. And I stress that this is one of the basic problems endemic to modern science. As Robert Silverberg shows, as Hrdlicka,</i></b><br /><br /><b><i>&#39;grew older his opinions hardened into prejudices, and he proclaimed his ideas so vehemently and with such a show of authority that it became professionally dangerous for any scientist to try to contradict them. Only a brave archeologist or a foolish one would dare to tangle with Hrdlicka. One archeologist of the 1920&#39;s warned his pupils, not entirely jokingly, &#39;if you ever find evidence of human life in [America] in a context which is ancient, bury it carefully, but don&#39;t forget about it&#39;.&#39; [48]</i></b><br /><br /><i>A marvelous story about Hrdlicka is told by paleontologist, Louis Leakey:</i><br /><br /><i>&#39;Back in 1929-1930 when I was teaching students at the University of Cambridge ... I began to tell my students that man must have been in the New World [for] at least 15,000 years. I shall never forget when Ales Hrdlicka, that great man from the Smithsonian Institution, happened to be at Cambridge, and he was told by my professor (I was only a student supervisor) that Dr. Leakey was telling students that man must have been in America 15,000 or more years ago. He burst into my rooms -- he didn&#39;t even wait to shake hands,</i><br /><br /><b><i>&#39;Hrdlicka said, &#39;Leakey, what&#39;s this I hear? Are you preaching heresy?&#39;</i></b><br /><br /><i>&#39;&#39;No, Sir!&#39; said Leakey.</i><br /><br /><b><i>&#39;Hrdlicka replied, &#39;You are! You are telling students that man was in America 15,000 years ago. What evidence have you?&#39;</i></b><br /><br /><i>&#39;Leakey answered, &#39;No positive evidence. Purely circumstantial evidence. But with man from Alaska to Cape Horn, with many different languages and at least two civilizations, it is not possible that he was present only the few thousand years that you at present allow.&#39;&#39; [49]</i><br /><br /><i>Leakey was lucky that he survived this encounter. Others were not so lucky. Just imagine the arrogance of Hrdlicka breaking into someone&#39;s rooms without first knocking at the door, the lack of control in this case exhibits disturbed behavior.</i><br /><br /><i>For any reader who believes I am being unduly severe on Hrdlicka, painting him as a raving neurotic. I add this item by C. W. Ceram:</i><br /><br /><i>&#39;He [Hrdlicka] was an odd bird in other respects also. For example, he made it a rule that all his associates had to will their skulls to science, but in his own last will he stipulated that he was to be cremated, his ashes mingled with those of his first wife and deposited in an urn in the Smithsonian Institution.&#39; [50]</i><br /><br /><i>Hrdlicka had no qualms about destroying anyone that he felt endangered his views of the peopling of the New World. According to Lewin, G. Edward Lewis, a doctoral candidate at Yale University on an expedition to the Siwalik Hills of India, found fossils of a possible hominid which he dubbed Ramapithecus, or &#39;Rama&#39;s age.&#39; Hrdlicka,</i><br /><br /><i>&#39;... chose to damn Ramapithecus in the pages of the American Journal of Science ... where Lewis had published his claims for the fossil. In six short pages Hrdlicka tore into Lewis&#39; work, accusing the young man of committing &#39;a series of errors&#39; and reaching an &#39;utterly unjustifiable&#39; conclusion. Ramapithecus, he said, was just an ape [and not a hominid] ...</i><br /><br /><b><i>&#39;Hrdlicka, he [Lewis] says, &#39;thought he was the anointed and elect prophet who had been foreordained and chosen to make such discoveries and demolish the work of anyone else.&#39; Hrdlicka&#39;s paper was somewhat self-contradictory, and says [Elwyn] Simons, [of Yale] &#39;scattered with blunders and naivetes.&#39; &#39;The man didn&#39;t know what he was talking about,&#39; recalls Lewis. &#39;So I could not take the paper&#39;s content seriously, but did take seriously the possibility of his damaging my reputation.&#39;</i></b><br /><br /><i>&#39;As an attempt to salvage his reputation Lewis penned &#39;an unhurried and temperate reply.&#39; The rebuttal never found the printed page, however, because the editor of the American Journal of Science, Lewis&#39; own supervisor, [Richard Swann] Lull, declined to accept it. &#39;They refused to publish it,&#39; says Lewis, &#39;although they admitted that I had written nothing offensive, </i><b><i>because they said Hrdlicka was an important man, and I was a young man whose reputation would be damaged ... inasmuch as the baldly stated facts and courteous comments would make him look like a fool!&#39; Lewis&#39; thesis which is described by [David] Pilbeam [a British anthropologist] as &#39;a very good piece of work&#39; and by Simons as &#39;the best opinion people could reach at the time&#39; -- was never published ...</i></b><br /><br /><i>&#39;Hrdlicka had good reason to want to discredit Lewis&#39; work, says Frank Spencer, a scholar of this period of the history of paleontology and of Hrdlicka in particular. &#39;It has nothing to do with the shape of the jaw,&#39; he suggests. &#39;It had to do with where the jaw came from -- namely, the fringes of Central Asia.&#39; In Hrdlicka&#39;s view, the western part of the Old World was the wellspring of human origins. Everything in his scheme depended upon this, including his ideas on the eventual peopling of the New World. </i><b><i>To have the first hominids appearing in the eastern part of the Old World [so they would arrive in the New World prior to 12,000 B.P.] was therefore simply unacceptable. &#39;So he did a hatchet job on Lewis&#39; work,&#39;</i></b><i> says Spencer.&#39; [51]</i><br /><br /><i>Ceram describes Hrdlicka&#39;s behavior thus:</i><br /><br /><b><i>&#39;Ales Hrdlicka of the Bureau of Ethnology in Washington, blocked all research into the past for a whole generation</i></b><i> ...</i><br /><br /><b><i>&#39;He was ruthless in enforcing his dominance upon younger men</i></b><i> ...</i><br /><br /><i>&#39;Even in 1928, by which time the importance of the Folsom find was clear to everyone, Hrdlicka had the temerity to decree at a meeting of the New York Academy that there could not have been a Paleo-Indian (as we call the people who hunted the now extinct animals). </i><b><i>&#39;With his back to the wall, Hrdlicka was denying everything to maintain his position that man could be anything, anything at all, but not ancient in America,&#39; [Edwin N.] Wilmsen says. But eventually the finds accumulated to such an extent that even the skeptics had to bow to the evidence.&#39;</i></b><i> [52]</i><br /><br /><i>As we will see, the advocates of the overkill theory are just as avid in their denial that ancient man was in the Americas prior to 12,000 years ago and may deny anything, anything at all, to maintain their position.</i><br /><br /><i>I have added this material to show the nature of how scientific debates about anthropology are often carried out by disturbed scholars. Velikovskians are quite familiar with dictatorial types like Hrdlicka. But as we proceed, we will learn Hrdlicka is only one of a long line of angry scientists for whom opposition brings out this kind of behavior.</i><br /><br /><i>In time, however, discoveries of the Clovis people together, with extinct megafauna bones in America, could not be denied, but the conclusion and overall dogma was that this could go back no more than to around 12,000 years B.P. Cracks in this time constraint then began to appear. Of particular importance are two archeological discoveries in North America, that undermine this time constraint. One is at Calico in California, and the other is in a location in the Yukon near the Old Crow River.</i><br /><br /><i>&#39;Two periods of human occupation have been dated at Calico. From about 15,000 to 20,000 years ago the area was inhabited by what [R. D.] Simpson suggests was a hunting-gathering people with more sophisticated tools, including stone flaked on both sides. In deeper layers estimated to be at least 200,000 years old are the simpler flakes of people she says, who probably gathered plants and other foods.&#39; [53]</i><br /><br /><i>To add to this, archeological work carried out in a rock shelter in Meadowcroft, Pennsylvania, J. M. Adovasio and his colleagues reported that </i><b><i>they have 50 correlated consistent dates some of which were derived by employing accelerator mass spectrometry, that place man at that site 14,000 or 14,500 years ago.</i></b><i> [54]</i><br /><br /><i>Grayson describes the Meadowcroft research this way.</i><br /><br /><b><i>&#39;The deepest Meadowcroft deposits appear to date about 31,000 years ago ... Indeed, Meadowcroft is one of the best-dated archaeological sites in the world</i></b><i>, Adovasio having obtained 52 radiocarbon dates for the site, dates that range from 175 years ago at the top ... to 31,000 years ago at the bottom.</i><br /><br /><i>&#39;These dates produce a consistent picture of the accumulation of deposits in Meadowcroft, since, with extremely rare exceptions, as the deposits get deeper, the dates get older, just as should happen.&#39; [55]</i><br /><br /><i>In addition, as far away as Chile, in South America at a site near Monte Verde, strong evidence that it was occupied by humans about 12,000 to 13,000 years ago was uncovered. If ancient man had crossed the Bering Strait 12,000 B.P., they would have taken at a minimum a thousand years to migrate down to Monte Verde. Significantly, </i><b><i>radiocarbon dates at the dig go back 33,000 years</i></b><i>. [56]</i><br /><br /><i>To show the depth of absurdity to which advocates of the idea that man was in the Americas no earlier than 12,000 years ago, I cite Science Frontiers, No. 105 for May-Jun 1996, page 2 titled &#39;Darwinism in Archeology.&#39;</i><br /><br /><i>&#39;Archeologists were initially attracted to Pedra Furada, in northeastern Brazil ... But it is not the rock art that is controversial about Pedra Furada; it is the 595 pieces of quartz selected by French archeologist N. Guidon. These bits of stone closely resemble human-crafted choppers, scrapers, and cutting tools. Indeed, if they had been found in more recent deposits, they would have been judged &#39;man-made&#39; by everyone. </i><b><i>The trouble is that Guidon has dated them at 50,000 B.P. -- a date mainstream archeologists cannot swallow</i></b><i> ...</i><br /><br /><i>&#39;How are the Pedra Furada chipped stoned explained by the mainstream archaeologists? They are geofacts, not artifacts. They were created when quartzite rocks were released by erosion and fell off cliffs to be smashed upon impact below. </i><b><i>Gravity and not the human hand broke the quartz into pieces that just happen to look like prehistoric tools. F. Parenti, a coworker of Guidon, has tried to exorcise the geofact argument, which is used whenever tools are &#39;too old&#39; by showing that the 595 pieces of quartz have characteristics quite unlike those created by natural flaking.</i></b><br /><br /><i>&#39;The doubters are unswayed. You see, despite Parenti&#39;s analysis </i><b><i>there remains a minute chance that a falling rock will fracture into pieces one of which will look human made. Maybe only one falling rock in 10,000 will fracture &#39;unnaturally&#39;; make it one in 10,000,000; it doesn&#39;t matter.</i></b><br /><br /><i>&#39;&#39;Of course, no matter how rare the chances, </i><b><i>given sufficient time and raw material -- Pedra Furada had plenty of both -- nature can magnify even the slimmest odds to the point where geofacts occur in detectable frequencies.&#39;</i></b><br /><br /><i>&#39;In this argument, you see how our title &#39;Darwinism in Archeology&#39; came to be. Random events (rock falls or mutations) plus a sorting mechanism (human selection or natural selection) can produce geofacts or new species. </i><b><i>This sort of explanatory mechanism can, in principle, explain just about anything</i></b><i> (Meltzer, David; &#39;Stones of Contention,&#39; New Scientist, p. 31, June 24, 1995.)</i><br /><br /><b><i>&#39;R. Dennell and L. Hurcombe, two archeologists faced with the geofact problem at their Pakistan dig, tried to solve it experimentally. They deliberately dropped quartzite rocks from heights onto hard surfaces. They concluded,</i></b><br /><br /><b><i>&#39;&#39;While conceding that had we conducted the experiment with a thousand, ten thousand, or a hundred thousand stones, a few might have fractures, we would nevertheless maintain that the chances of any showing multiple, multi-directional flaking and all with bulbs of percussion are as remote as the proverbial monkey typing Shakespeare.</i></b><i> (Dennell, Robin, and Hurcombe, Linda; &#39;Comment on Pedra Furada,&#39; Antiquity, 69:604 1995).&#39;</i><br /><br /><i>No argument by those who refuse to face the mounting evidence that man was in America earlier than 12,000 years ago is too absurd to be excluded. Like all experts, they can twist the evidence to say just about anything they want it to say.</i><br /><br /><i>To damage Guidon&#39;s evidence at Pedra Furada, D. J. Meltzer, et al., who visited the site, still claimed the artifacts were really geofacts. [57] The reply by Guidon, et la., was blistering. They accused their critics of having their facts wrong and being biased because of their allegiance to the 12,000 year people-in-America paradigm. Guidon and Pessis called Meltzer, et al.&#39;s published work &#39;Falsehood or Untruth&#39; and then stated bluntly,</i><br /><br /><i>&#39;The article by Meltzer, et al., (1994) is based on partial data and false information (highlighted below). Its battery of questions takes us by surprise; none of the three colleagues [who criticized our work] came up with these questions during the 1993 meeting -- mounted precisely to generate direct dialogue on the peopling of America. We disagree with their statement, &#39;the comments on Pedra Furada are not offered lightly&#39; (p. 696). The comments are worthless because they are based on partial and incorrect knowledge.</i><br /><br /><i>&#39;We believe that the initial intention of the authors was different; </i><b><i>they got carried away into an exercise in academic style from a fragile scientific base of fragmentary data and with a skepticism born of a subjective conviction.&#39;</i></b><i> [58]</i><br /><br /><i>Jeffrey Goodman&#39;s comments well sums up the dogmatism attached to the dismissal of ancient stone artifacts that challenge the 12,000 year barrier.</i><br /><br /><i>&#39;Even more incriminating evidence against the generally accepted ... theory emerged when it became clear that findings at many sites were challenging the sanctity of the 10,000 B.C. entry date, the &#39;official&#39; date for man&#39;s first appearance in the New World. These new sites suggested much earlier dates, but since they didn&#39;t yield beautifully fashioned pressure flaking points such as the Clovis and Folsom sites had, they were immediately suspect. Instead of spear points these sites contained very crude chipped stone tools such as choppers, scrapers, and simple stone flakes. A much more primitive manufacturer was pictured, and to some, the crudeness of these materials indicted that they weren&#39;t made by man at all, but by nature imitating man via such natural processes as thermal flaking from frost and heat and from rocks tumbling in streams or down hills or being ground up in mud flows, though there weren&#39;t any studies to support this contention. </i><b><i>There simply were no scientific grounds for contending that nature could fashion stones in a way indistinguishable from man. This led one of the authorities in the field to say that &#39;certain archaeologists wouldn&#39;t accept any really crude and early stone tools even if they somehow saw the tools being made with their own eyes.&#39;&#39;</i></b><i> [59]</i><br /><br /><i>In fact, there has been an organized and concerted effort to deny the evidence of man in America prior to 12,000 B.P. by a group William R. Corliss dubs the Clovis Police:</i><br /><br /><b><i>&#39;A new group of law-enforcers has been formed. Although the Clovis Police do not carry guns, they [like Hrdlicka] will make sure that all who stray from the archeological mainstream will be held up for censure. (Does this mean denial of funds, access to journals?) The &#39;law&#39; that the Clovis Police will enforce says that humans did not enter the New World before 12,000 B.P. -- the oldest date attributed to the Clovis people</i></b><i> ... The members of the Clovis squad and their objectives can be found in a recent issue of Science, [by] Marshall, Eliot, &#39;Clovis Counterrevolution&#39; [Vol.] 249 [p.] 738, 1990.&#39; [60]</i><br /><br /><i>The Clovis Police squad&#39;s totalitarian manifesto has evidently borne fruit as presented by R. Bonnischsen of the University of Maine.</i><br /><br /><b><i>&#39;Numerous meritorious grant proposals have been rejected because their goals and objectives were incompatible with entrenched academic opinion ... At least five South American archeologists admitted that they are suppressing pre-12,000-year-old data out of fear that their funds will be cut off by American colleagues who endorse the short -- chronology school of thought.&#39;</i></b><i> [61]</i><br /><br /><i>Warwick Bray, in &#39;The Paleo-Indian Debate,&#39; in Nature shows just how strongly advocates of the view that man could only be in the Americas no earlier than 12,000 years ago quotes anthropologist E. Leach who said of the way they handle data and evidence:</i><br /><br /><b><i>&#39;Justification in terms of scientific methodology is in part self-deception, for when the figures turn out wrong [regarding man in America prior to 12,000 B.P.] the true believer will always shuffle the figures; when contrary evidence turns up he throws doubt upon the credentials of the investigator.&#39;</i></b><i> [62]</i><br /><br /><b><i>Let it be made absolutely clear that some scientists will do anything to one of their colleagues who presents evidence that will overturn the paradigm to which they have invested their professional lives.</i></b><i> As Vine Deloria shows.</i><br /><br /><i>&#39;The most blatant incident concerned Dr. Thomas Lee of Canada. Excavations were made at a site in Canada, Sheguiandah, between 1951 and 1955 by Lee, an anthropologist working at the National Museum of Canada. Preliminary evidence indicated the site might be between 30,000 and 100,000 years old.</i><br /><br /><b><i>&#39;The evidence not only conflicted with accepted doctrine, it would have made it necessary to revise estimates of the stages of North American glaciation. The scientific establishment went after Lee. He lost his position at the museum and some of his papers on the discovery were &#39;lost.&#39; &#39;I was hounded from my Canadian government position by certain American citizens on both sides of the border and driven into eight long years of blacklisting, and enforced unemployment,&#39; Lee wrote.&#39;</i></b><i> [63]</i><br /><br /><i>Michael A. Cremo and Richard L. Thompson describe what happened to Lee thus:</i><br /><br /><i>&#39;In the early 1950&#39;s, Thomas E. Lee of the National Museum of Canada found advanced stone tools in glacier deposits at Sheguiandah, on Manitoulin Island in northern Lake Huron. Geologists John Sanford of Wayne State University argued that the oldest Sheguiandah tools were at least 65,000 years old and might be as much as 125,000 years old. For those adhering to standard views on North American prehistory, such ages were unacceptable. Humans supposedly entered North America from Siberia about 12,000 years ago.</i><br /><br /><i>&#39;Thomas E. Lee complained: </i><b><i>&#39;The site&#39;s discoverer [Lee] was hounded from his Civil Service position into prolonged unemployment; publication outlets were cut off; the evidence was misrepresented by several prominent authors ... ; the tons of artifacts vanished into storage bins of the National Museum of Canada; for refusing to fire [Lee] the discoverer, the Director of the National Museum who had proposed having a monograph on the site published, was himself fired and driven into exile; official positions of prestige and power were exercised in an effort to gain control over just six Sheguiandah specimens that had not gone under cover; and the site has been turned into a tourist resort ... Sheguiandah would have forced embarrassing admissions that the Brahmins did not know everything. It would have forced the rewriting of almost every book in the business. It had to be killed it was killed.&#39;</i></b><i> [64]</i><br /><br /><i>In commenting on Cremo and Thompson&#39;s book, Bradley T. Lepper admits:</i><br /><br /><b><i>&#39;Cremo and Thompson are right about the extreme conservatism of many archaeologists and physical anthropologists. While an undergraduate at a prominent southwest university, I participated in classroom discussions about the claims for a very early [prior to 12,000 B.P.] occupation at the Timlin site (in New York) which had just been announced. The professor surprised me when she stated flatly that if the dates were correct then it was &#39;obviously not a site&#39; [of early man]. The dismissal of the possibility of such an ancient site without an examination of the data or even a careful reading of the published claim is dogmatism ...&#39;</i></b><i> [65]</i><br /><br /><i>A typical description of how this suppressive behavior is exerted is related to the work of George Carter of Texas A and M University who dug up an</i><br /><br /><i>&#39;... early stone tool industry reminiscent of the European eoliths ... discovered ... in the 1950&#39;s at the Texas Street excavation in San Diego. At this site, Carter claimed to have found hearths, and crude stone tools at levels corresponding to the last interglacial period some 80,000-90,000 years ago. </i><b><i>Critics scoffed at these claims, referring to Carter&#39;s alleged tools as products of nature, or &#39;cartifacts,&#39; and Carter was later publicly defamed in a Harvard [University] course on &#39;Fantastic Archeology.&#39; However, Carter gave clear criteria for distinguishing between his tools and naturally broken rocks, and lithic experts such as John Witthoft have endorsed his claims.</i></b><br /><br /><b><i>&#39;In 1973, Carter conducted more extensive excavations at Texas Street and invited numerous archeologists to come and view the site firsthand. Almost none responded. Carter stated: &#39;San Diego State University adamantly, refused to look at work in its own backyard.&#39;</i></b><br /><br /><i>&#39;In 1960, an editor of Science, the journal of the American Academy for the Advancement of Science, asked Carter to submit an article about early humans in America. Carter did so, </i><b><i>but when the editor sent the article out to two scholars for review they rejected it.</i></b><br /><br /><i>&#39;Upon being informed of this by the editor Carter replied in a letter, dated February 2, 1960: </i><b><i>&#39;I must assume that you had no idea of the intensity of feeling that reigns in the field. It is nearly hopeless to try to convey some idea of the status of the field of Early Man in America at the moment. But just for fun: I have a correspondent whose name I cannot use for though he thinks that I am right, he could lose his job for saying so. I have another anonymous correspondent who as a graduate student found evidence that would tend to prove me right. He and his fellow students buried the evidence. They were certain that to bring it in would cost them their chance for their Ph.D.&#39;s. At a meeting, a young professional approached me to say, &#39;I hope you really pour it on them. I would say it if I dared, but it would cost me my job.&#39; At another meeting, a young man sidled up to say, &#39;In dig x they found core tools like yours at the bottom but didn&#39;t published them.&#39;&#39;</i></b><i> [66]</i><br /><br /><i>The authors go on to show on the same page and the next:</i><br /><br /><i>&#39;The inhibiting effect of negative propaganda on the evaluation of Carter&#39;s discoveries is described by archeologist Brian Reeves, who wrote with his coauthors in 1986: &#39;Were actual artifacts uncovered at Texas Street, and is the site really Last Interglacial in age? ... </i><b><i>Because of the weight of critical &#39;evidence&#39; preserved by established archeologists, the senior author [Reeves], like most other archaeologists, accepted the position of the skeptics uncritically dismissing the sites and the objects as natural phenomena.&#39; But when he took the trouble to look at the evidence himself, Reeves changed his mind.&#39;</i></b><br /><br /><i>Furthermore, on an NBC television special, The Mysterious Origins of Man, presented in New York City in March 1996, it was revealed that archeologist Virginia Steen McIntyre had an experience very similar to Lee&#39;s. She had been digging in Mexico in 1966 and found human artifacts at a deep level of the excavation. These were dated by two dating methods, uranium and crystal. McIntyre had expected a 20,000 year age, but both dating methods gave a much greater age. </i><b><i>Rather than deny the facts uncovered, she naively felt the unvarnished evidence should be presented via publication to the scientific world. What followed was that she was fired from her job, the site was closed and then filled in and she has been blacklisted from work in her field.</i></b><br /><br /><i>Commenting on the behavior of the establishment, Cremo and Thompson say of the Steen-McIntyre affair:</i><br /><br /><b><i>&#39;The anomalous findings ... resulted in personal abuse and professional penalties, including withholding of funds and loss of job, facilities and reputation for Virginia Steen McIntyre. Her case opens a rare window into the actual social processes of data suppression in paleoanthropology, processes that involve a great deal of conflict and hurt.</i></b><br /><br /><i>&#39;A final note -- we ourselves once tried to secure permission to reproduce photographs of the Hueyattaco artifacts in a publication. </i><b><i>We were informed that permission would be denied if we intended to mention the &#39;lunatic fringe&#39; date of 250,000 years.&#39;</i></b><i> [67]</i><br /><br /><i>As reported in The New York Times by John Noble Wilford, Anna C. Roosevelt has found evidence in a cave in Brazil of a people unrelated to the Clovis who were in South America at the same time that the ancestors of the Clovis people settled in the United States. </i><b><i>Dr. Betty J. Meggers, an influential archeologist at the Smithsonian Institution, has dogmatically and resolutely denied this evidence. Because Dr. Roosevelt is a descendant of President Theodore Roosevelt, she cannot be kicked out of the profession without considerable uproar and deep embarrassment to it. Her work received</i></b><br /><br /><b><i>&#39;... a scorching review in 1992 in The Journal of Field Archeology [at which] Dr. Meggers complained of its &#39;polemical tone&#39; and &#39;extravagant claims.&#39;</i></b><br /><br /><b><i>&#39;On all sides archeologists are distressed that the conflict between Dr. Roosevelt and Dr. Meggers has descended into a name-calling feud. Dr. Roosevelt has repeatedly charged Dr. Meggers with using her influence to block those who do not share her views from working in the region.&#39;</i></b><i> [68]</i><br /><br /><i>This is not related only to the early peopling of the Americas but also to the lifestyles of these early people who apparently were not hunters of the megafauna but &#39;may have consumed more plants, fish and small game than had been assumed. [69] Jeff Hecht of New Scientist, describes Roosevelt&#39;s work thus:</i><br /><br /><i>&#39;An ancient campsite in a Brazilian cave is forcing anthropologists to rethink their ideas about the history of human settlement in the Americas.</i><br /><br /><i>&#39;The traditional views that all native Americans descended from a group called the Clovis people who crossed the Bering Strait some 11,500 years ago ... But an international team headed by Anna Roosevelt of the Field Museum in Chicago has found that people living in the Amazon at about the same time as Clovis culture had a different lifestyle and used quite different tools ...</i><br /><br /><i>&#39;... although nobody has found Clovis tools in South America, most early human sites in the continent have been found in the dry or temperate zones that the Clovis people seemed to favour -- leading anthropologists to assume that the first South Americans were from Clovis stock.</i><br /><br /><i>&#39;Roosevelt decided to look for human artifacts in tropical rainforests, which Paleo-Indian researchers have rarely explored. After reading 19th century accounts of stone tools and cave paintings in the Amazon, she teamed up with South American anthropologists to locate a cave at Monte Alegre [Brazil], several hundred kilometers from the present mouth of the Amazon which contained a wealth of human artifacts ...</i><br /><br /><i>&#39;Evidence of early humans includes rock paintings and debris in 2.5 metres [8 feet] of soil on the floor of the cave. Stone spearheads found in the cave differ from Clovis tools: They are triangular and have backwards-facing barbs ...</i><br /><br /><i>&#39;In addition, the researchers excavated wood fragments, charcoal, remains of fruit, nut shells, and the remains of many different animals, including birds, mammals, amphibians, fish and shellfish. The oldest debris dates from 11,145 years ago. Roosevelt says that the remains &#39;point ... to broad-spectrum foraging,&#39; rather than the hunting associated with Clovis culture.</i><br /><br /><i>&#39;Many other specialists had already questioned the traditional view that the earliest Americans were big game hunters. &#39;They must have been foraging as well,&#39; says John Hoffecker of the Argonne National Laboratory in Illinois. He notes that larger animal bones are preserved better than those of smaller animals, while plant remains rarely survive.&#39; [70]</i><br /><br /><i>The evidence thoroughly contradicts the hunting hypothesis on two levels. First, it shows that people were in South America about 350 years after Clovis people supposedly crossed the Bering Strait. One would have expected them to be in South America, at the very least, 1,000 years later. </i><b><i>If they descended from Clovis people, why didn&#39;t they use the same tools? After all, Clovis tools did not change greatly while they inhabited North America. This suggests these Amazonians were not descendants of the Clovis.</i></b><br /><br /><b><i>The reasons, therefore, for the behavior of those who oppose Roosevelt&#39;s research, especially those who support the hunting hypothesis, is understandable. Roosevelt&#39;s research is destroying their life&#39;s work and they simply don&#39;t know how to get rid of her without showing the world that their science is not science but institutionalized dogma.</i></b><br /><br /><i>Not only does Dr. Roosevelt&#39;s work show that Amerindians were not extensively big-game hunters, but after a half century of obstruction, the 12,000 year wall is beginning to crumble. As reported by John Noble Wilford, Monte Verde was occupied by man at least 13,000 years ago! [71] This will lead, I believe, to finally destroying the Clovis Police&#39;s mandate and in a few years, the admission will be made that man was in America long before the blitzkrieg hunting theory allows and will deeply undercut it and kill it. According to the report, the people at Monte Verde were also not extensively big-game hunters.</i><br /><br /><i>&#39;... a group of 20 to 30 people occupied Monte Verde for a year or so. They lived in shelters covered in animal hides. They gathered berries in spring, chestnuts in fall, and also ate potatoes, mushrooms, and marsh grasses. They hunted small game, and also ancestors of the llama, and sometimes went down to the Pacific, 30 miles away, for shellfish. They were hunters and gatherers.&#39; [72]</i><br /><br /><i>Ward says, &#39;The new South American record [from Brazil] shows people living in a jungle region ... subsisting on berries, fruit, and other items, just as many local people today.&#39; [73]</i><br /><br /><i>This evidence of the age of the site and the hunter gathering behavior of the people is a second double blow to the hunting hypothesis. First, animals in America coexisted with man long before the extinction occurred. Second, the Indians were not extensively big game hunters. It will be interesting to see what those advocates of the overkill blitzkrieg concept will summon to grab their theory out of the jaws of this evidence. Reputations and the life work of a great many hunter school doyens are on the line.</i><br /><br /><i>A perfect example of how control is exerted by a senior researcher in anthropology on a younger is described by Kenneth Good with David Chanoff in their book, Into the Heart. Good, as a Ph.D. candidate, was sent into South American jungles to evaluate his mentor&#39;s thesis which was in debate. His mentor, Napoleon Chagnon, had claimed that a tribe of people, the Yanamama, were a wild, uncontrolled, fierce, murdering people. Marvin Harris, of Columbia University, had strongly opposed this concept of people being inherently violent, like the ancient hunters. Good was sent by Chagnon to document evidence that they were indeed killers. </i><b><i>What Good discovered was that the evidence would not support Chagnon&#39;s thesis and what followed led to a confrontation in which Chagnon suggested that Good get a job doing other work &#39;Because you&#39;re not going to get into any other anthropology department. I&#39;ll see to that,&#39; he said.</i></b><i> [74]</i><br /><br /><i>To combat some of these allegations and many others I have presented above about the outrageous and reckless behavior of scientists, Jo Wodak and David Oldroyd, in a review of Cremo and Thompson&#39;s book cited above, claim:</i><br /><br /><b><i>&#39;We concede that the science of paleoanthropology does not have an unblemished record, though that does not mean that it is riddled with fraud. It is true, however, that it is a branch of science that has been characterized by passionate controversies, and it has been peculiarly prone to the effect of the theory-ladenness of observations. It is also subject to large stories (&#39;scenarios&#39;) have to be told on the basis of often limited or slender evidence.&#39;</i></b><i> [75] [However Schiller admits]</i><br /><br /><i>&#39;... reconstructing the history of early man in America is in some respects a more speculative business than it is in Africa, and interpretations are more divergent. </i><b><i>Controversies among anthropologists tend to be more numerous, more spirited, and sometimes downright nasty. The opinions expressed about their colleagues by some of the experts I interviewed verged on insult, and differences of opinion are so bitter that there are those who will not sit at the same table with others. It makes for lively, professional meetings.&#39;</i></b><i> [76]</i><br /><br /><b><i>Given the prior evidence outlined of behavior that is academically and scientifically indefensible, it is rather clear that even when the actions of senior researchers are exposed, those within the field are constitutionally incapable of facing the facts. If these forms of behavior aren&#39;t addressed, then conditions can only become worse and inquisitorial conduct will continue to flourish.</i></b><br /><br /><i>Now all this behavior is not as unusual as it has been made out to be. </i><b><i>Most scientists have great trouble dealing with such totalitarian actions and refuse to believe their beloved science is different than that of other human institutions. But these are workers who have never attempted to present evidence that thoroughly contradicts the reigning paradigm. Those who have challenged these deeply held theoretical constructs soon find that they are persona non grata in their field and become the subject of all the mechanisms of coercion, suppression and hostility expressed in politics.</i></b><br /><br /><b><i>Yet Paul S. Martin, the leading exponent of the overkill hypothesis, still claims, &#39;If humans lived in the New World more than 12,000 years ago, there would be no secret about it.&#39;</i></b><i> [77] How on Earth can one believe that these advocates of the hunting hypothesis believe or care anything about the tenets of good science when one can observe Lycenkoism being carried out in broad daylight and with the scientific press and the popular press being so unwilling to raise a chorus of denunciation to the public regarding these matters? </i><b><i>By the lack of clear and forthright denunciation regarding this affront to science and decency the press&#39;s lack of vigor becomes a form of tacit collusion.</i></b><br /><br /><i>As anyone with sense can see, science and academia can act just as totalitarian and fascist as any dictatorial institution or regime. The true reason for this type of behavior meted out to Lee and McIntyre and others is that their finds pointed to the concept that man was in America in Mexico and Canada much earlier than accepted and destroyed the theory that man reached the middle Americas only 12,000 years go.</i><br /><br /><i>But then in 1961, parts of human skeletons were uncovered in water laid sand in a valley in Alberta, Canada, which geologists claimed were at least 18,000 years old. [78] The bones of an infant of about four months old were found, including parts of the scapula, a collarbone, a fragment of femur, two ribs and two smashed vertebrae. These fossil remains were buried beneath 33 feet of alluvium which had a sheet of glacial till above it on the surface. The only way to interpret this was to suggest the child died and was either buried slowly or rapidly by alluvium. Then the ice sheet advanced and covered over the area with glacial till. The geologist, A. McS. Stalker, gave the date for the fossil as 30,000 to 40,000 years B.P., or perhaps 20,000 years B.P. The point that must be stressed is that &#39;Each advance [by the ice sheet] is marked by a later of till.&#39; [79] </i><b><i>Thus, the burial occurred prior to the last glacial advance.</i></b><br /><br /><b><i>To get around this impossible situation, the bones were radiocarbon dated to about 4,000 years old. [80] What was omitted about the date offered was that the bones had been kept in preservatives loaded with carbon-14 which would lower the age of the date. Although there are doubts about this date, nearby at a place called Medicine Hat, human stone artifacts were found and they, too, dated from between 17,000 to 20,000 years B.P. Naturally, these artifacts were dismissed as being caused by nature.</i></b><br /><br /><i>L. Krishtalka, a critic of the overkill theory, goes so far as to accuse the proponents of that concept of willfully culling the data.</i><br /><br /><b><i>&#39;Their selective acceptance of only &#39;good&#39; dates -- those that fit the model (for example, dates of human beings in North America no older than 12,000 yr B.P., and those for mammoths no younger than 10,000 yr B.P.) -- may play fast and loose with the evidence that doesn&#39;t fit.&#39;</i></b><i> [81]</i><br /><br /><b><i>What all this displays is that evidence is playing less and less of a role in this scientific debate. The tactics involved are clearly aimed at winning by techniques that smack of intimidation and propaganda, the same behavior evidenced by Hrdlicka&quot;.</i></b><br /><br />[...]<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"Willard Libby's Nobel Fraud","summary":"A Crucial Requirement for the Radiocarbon Dating Technique is that the Same C14 Concentration Must Exist in All Parts of the Biosphere / Today We Know that C14 Concentrations are Significantly Below Normal in the Polar Regions Whilst there are 2 Very Distinct Above-Normal Latitudinal Bands Centred Around 30° N and 30° S / What is Remarkable is that a Comparison of Libby's Published Dataset Shows that the Data Mysteriously Aligns with this Necessary Assumption from 1949 to 1951","image":"https://lh3.googleusercontent.com/-kKA3qUHDutE/WFDhq84I0UI/AAAAAAAAJtg/PZ30smihRSsrForX5egaxxyiAdE-TJ4JwCJoC/w7142-h9999/willard-libbys-nobel-fraud-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Uq5Sr5beF5o","publishDate":"2016-12-14T06:08:02.686Z","updateDate":"2017-01-03T01:36:39.509Z","text":"<b>Willard Libby&#39;s Nobel Fraud: A Crucial Requirement for the Radiocarbon Dating Technique is that the Same C14 Concentration Must Exist in All Parts of the Biosphere / Today We Know that C14 Concentrations are Significantly Below Normal in the Polar Regions Whilst there are 2 Very Distinct Above-Normal Latitudinal Bands Centred Around 30° N and 30° S / What is Remarkable is that a Comparison of Libby&#39;s Published Dataset Shows that the Data Mysteriously Aligns with this Necessary Assumption from 1949 to 1951</b><br /><br />GRAPHIC REFERENCES:<br /><br />[1] <a href=\"http://cdiac.ornl.gov/epubs/ndp/ndp057a/ndp057a.htm\" class=\"ot-anchor\">http://cdiac.ornl.gov/epubs/ndp/ndp057a/ndp057a.htm</a>, search in paper on &quot;measurements by latitude&quot;<br /><br />[2] <a href=\"http://www.sciencemag.org/content/109/2827/227\" class=\"ot-anchor\">http://www.sciencemag.org/content/109/2827/227</a>, Age Determination by Radiocarbon Content: World-Wide Assay of Natural Radiocarbon, <i>Science</i>, 04 <b>Mar 1949</b>: Vol. 109, Issue 2827, pp. 227-228, DOI: 10.1126/science.109.2827.227<br /><br />[3] This <b>1951</b> paper is not, to my knowledge, available for download online, but these are the values which Libby continued to publish in numerous other future papers, like here: <a href=\"http://etler.com/docs/OSTI/ACC0338.pdf\" class=\"ot-anchor\">http://etler.com/docs/OSTI/ACC0338.pdf</a><br /><br />[4] <a href=\"https://malagabay.wordpress.com/2014/05/26/carbon-14-the-big-fudge/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/26/carbon-14-the-big-fudge/</a><br /><br />[5] <a href=\"http://scienceline.ucsb.edu/getkey.php?key=2901\" class=\"ot-anchor\">http://scienceline.ucsb.edu/getkey.php?key=2901</a>, University of California Santa Barbara<br /><br />THE STORY:<br /><br /><a href=\"https://malagabay.wordpress.com/2014/05/16/carbon-14-libbys-ring/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/16/carbon-14-libbys-ring/</a><br /><br /><b><i>&quot;One of the great achievements of Post-Normal Science after the Second World War was the establishment of Radiocarbon Dating as academically acceptable Settled Science.</i></b><br /><br /><b><i>This accomplishment was strategically initiated by Willard Libby when he asked for mainstream assistance with the validation of his pioneering Radiocarbon Dating technique.&quot;</i></b><br /><br /><b>The History of the Radiocarbon Dating Technique</b><br /><br /><i>Interpreting the Past: Radiocarbon Dating</i> (1990)<br />by Sheridan Bowman<br /><br />(This short booklet is the perfect primer for a layperson seeking a review of the technical details and limitations of the radiocarbon dating technique.  If you feel that you already know this technique in sufficient detail to understand why Libby needs a global constant C14 concentration, feel free to skip ahead ...)<br /><br /><i>&quot;The existence of radiocarbon in nature was predicted before it was detected.  Nevertheless, this prediction was sufficient for an American scientist called Willard Libby to perceive the basis of a dating method.  The theoretical aspects were formulated in the mid 1940s when Libby was Professor of Chemistry at the University of Chicago.  In 1946 he published a paper suggesting that radiocarbon might exist in living matter.  One year later, a single-page paper appeared in the journal Science in which Ernest Anderson and Libby, together with collaborators in Pennsylvania, summarised the first detection of radiocarbon in material of biological origin.  They showed that methane collected from the Baltimore sewage works had measurable radiocarbon activity, whereas methane manufactured from petroleum did not, and the implications of the findings for dating of carbonaceous materials were noted.  These first experiments required enrichment of the radiocarbon in the sample to make it easily detectable.  By 1949, when Libby and Anderson (now joined in Chicago by James Arnold) published results of a world-wide assay of radiocarbon, enrichment was no longer necessary.  </i><b><i>The assay showed the contemporary level of radiocarbon in wood to be the same globally.</i></b><i>  The paper also contained the first two results of measurements on archaeological samples.  The end of 1949 saw the publication of radiocarbon results on several samples of known age, and the publication of measurements of unknowns shortly followed.  As Libby himself later recalled, success was by no means a foregone conclusion: </i><b><i>he and his colleagues persevered through a &#39;period of two or three years of secret research when we believed that the notion of radiocarbon dating was beyond reasonable credence&#39;</i></b><i>.  A remarkable vision had been turned into an invaluable tool, and for his work on radiocarbon Libby was awarded the Nobel prize for chemistry in 1960.&quot;</i><br /><br /><b>The Radiocarbon Dating Hypothesis</b><br /><br /><i>&quot;Basic principles</i><br /><br /><i>Provided the reader has some familiarity with scientific terminology, the principles of radiocarbon dating can be fairly briefly stated and readily understood.  </i><b><i>However, in practice, various factors must be taken into account which can affect radiocarbon concentration in specific environments or organisms.</i></b><br /><br /><i>Carbon has three naturally occurring isotopes, that is, atoms of the same atomic number but different atomic weights.  These are designated C12, C13 and C14 in scientific notation, the letter C being the symbol for elemental carbon and the isotopes having atomic weights 12, 13 and 14 respectively.  They do not occur equally: carbon consists of 99% of C12, 1% of C13, but only about one part in a million million of modern carbon is C14.  Unlike C12 and C13, C14 is unstable and therefore radioactive, though only weakly.  Hence the name &#39;radiocarbon&#39; for this isotope which, because of its scientific designation, is also called &#39;carbon fourteen&#39; ...</i><br /><br /><i>The really unusual characteristic of C14 is tht it is continually being formed.  This occurs in the upper atmosphere (strictly the lower stratosphere and upper troposphere) by the interaction of neutrons produced by cosmic rays with nitrogen atoms.  C14 is therefore one of a small number of &#39;cosmogenic&#39; nucleides.  After formation, the C14 atoms rapidly combine with oxygen to form carbon dioxide which is chemically indistinguishable from carbon dioxide containing either of the other carbon isotopes.  </i><b><i>This carbon dioxide mixes throughout the atmosphere, dissolves in the oceans and, via the photosynthesis process and the food chain, enters all plant and animal life, known collectively as the biosphere.  Under certain circumstances, in particular if the production rate is constant, there is a dynamic equilibrium between formation and decay, and therefore a constant C14 concentration in the atmosphere.  Thus in principle there is a constant C14 level in all living organisms.</i></b><br /><br /><i>When a plant or animal dies, it ceases to participate in carbon exchange with the biosphere and no longer takes in C14.  Were C14 stable, its concentration would remain constant after death, but since it is not, the level falls at a rate that is determined by the law of radioactive decay ...&quot;</i><br /><br /><b>The Necessary Assumptions of the Radiocarbon Dating Technique</b><br /><br /><i>... The usefulness of any dating technique requires that it be applicable to the materials commonly found on archaeological sites and have a sufficiently low error term to allow temporal differentiation.  </i><b><i>Equally fundamental is the need for the method to be globally applicable</i></b><i>; it should also be valid at all, or a good range of, periods in the past.  In the case of radiocarbon this in turn requires a global level of C14 in the atmosphere that has not changed with time; in addition, the biosphere should be in equilibrium with the atmosphere.  Theoretically, these equilibria exist under certain circumstances, and at this stage it is perhaps appropriate to review some of the assumptions made, explicitly or implicitly, in setting out the basis of the radiocarbon dating method:</i><br /><br />- <i>The atmosphere has had the same C14 concentration in the past as now; this in turn assumes constant production, constant and rapid mixing, exchange and transfer rates, as well as constant sizes of reservoirs.</i><br />- <i>As a corollary of this, the biosphere has the same overall concentration as the atmosphere and therefore it is assumed that there is rapid mixing between these two reservoirs.</i><br />- <b><i>The same C14 concentration exists in all parts of the biosphere.</i></b><br />- <i>The death of a plant or animal is the point at which it ceases to exchange with the environment.</i><br />- <i>After ceasing exchange, the C14 concentration in a plant or animal is only affected by radioactive decay.</i><br /><br /><i>None of these assumptions is strictly correct, beyond a rough first approximation ...&quot;</i><br /><br /><b>How Libby and Anderson Sought to Validate the Assumption of a Consistent Global Concentration of Biosphere C14</b><br /><br /><a href=\"http://www.nobelprize.org/nobel_prizes/chemistry/laureates/1960/libby-lecture.pdf\" class=\"ot-anchor\">http://www.nobelprize.org/nobel_prizes/chemistry/laureates/1960/libby-lecture.pdf</a><br /><br /><b><i>&quot;The plan was to measure living materials from various places on earth and to see whether they had the same radiocarbon content per gram of carbon.</i></b><i> These data on the natural abundance of radiocarbon in the earth were presented by E. C. Anderson for his doctoral thesis at the University of Chicago. </i><b><i>They show no appreciable differences even though they come from places varying in latitude from near the South Pole to near the North Pole (Table 2) . At the present time, ten years later, no evidence for variation has been found</i></b><i> except in areas of extensive carbonate deposits where the surface waters may carry a considerable amount of old carbon dissolved, and thus reduce the carbon- 14 level below the world-wide average for the biosphere-atmosphere ocean pool as a whole. </i><b><i>Fortunately, such conditions are relatively rare</i></b><i> and generally easily recognized.&quot;</i><br /><br /><b>What We Know Today, In Hindsight</b><br /><br /><a href=\"https://malagabay.wordpress.com/2014/05/25/carbon-14-lifting-the-veil/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/25/carbon-14-lifting-the-veil/</a><br /><br /><b><i>&quot;The observational data [which is ignored by the mainstream] clearly indicates that Carbon 14 concentrations are significantly below normal in the Polar Regions whilst there are two very distinct above normal latitudinal bands centred around 30° N and 30° S.</i></b><br /><br /><b><i>This pattern can be very clearly identified in the data collected during a round-trip cruise to Antarctica made by the Research Vessel Andenes during 1989-90.&quot;</i></b><br /><br />(See Figure 1)<br /><br /><i>&quot;Sadly, when it comes to Carbon 14, the mainstream has a long tradition of drawing a veil over inconvenient data and ignoring the Carbon 14 variations that are associated with latitude, altitude, climate and species ...</i><br /><br /><i>... This pattern was very clearly confirmed [for the Southern Hemisphere – denoted by the asterisks in the image below] in a paper published in 1949 by Libby, Anderson and Arnold.&quot;</i><br /><br />(Compare Figures 2 &amp; 3, Differences in Figure 4)<br /><br /><a href=\"https://malagabay.wordpress.com/2014/05/26/carbon-14-the-big-fudge/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/26/carbon-14-the-big-fudge/</a><br /><br /><b><i>&quot;Comparing the 1949 and 1951 versions of the table of samples it is clear that many of the assay values subsequently used by Willard Libby had increased by over 20% and in the case of the seal oil by 50%.</i></b><br /><br /><b><i>Additionally, the versions from 1951 onwards are cherry picked values from the original 1949 version of the table of samples and the sea shells from Florida are totally excluded.</i></b><br /><br /><b><i>The net effect of these selections was to falsely support Libby’s assertion that the new normal average of 15.3 decays per minute per gram of carbon was globally applicable.&quot;</i></b><br /><br /><a href=\"http://www.osti.gov/cgi-bin/rd_accomplishments/display_biblio.cgi?id=ACC0338&amp;numPages=17\" class=\"ot-anchor\">http://www.osti.gov/cgi-bin/rd_accomplishments/display_biblio.cgi?id=ACC0338&amp;numPages=17</a><br /><br />Radiocarbon Dating, Memories and Hopes – 1972 – W. F. Libby<br />Department of Chemistry and Institute of Geophysics and Planetary Physics,<br />University of California, Los Angeles<br /><br /><i>&quot;The simultaneity principle states that two samples taken from any place in the world for any past epoch will give the same date.</i><br /><br /><i>This of course, follows from the principle of good mixing described previously, i.e., </i><b><i>Dr Anderson’s result that the same concentration of 14C in natural radiocarbon is found all over the world and in different life forms.&quot;</i></b><br /><br />These unexplained corrections to Libby&#39;s dataset are not mentioned in Sheridan Bowman&#39;s 1990 technical summary of the radiocarbon technique, <i>Interpreting the Past: Radiocarbon Dating</i>.  In fact, it&#39;s not clear that they are <b>ever</b> mentioned in any technical overview of the technique, nor that any explanation has ever been put forward to justify the corrections.<br /><br />Further, this is not even the only problem with Libby&#39;s &quot;proof&quot; for radiocarbon dating mentioned at the Malaga Bay site.  The site dedicates a number of pages to Libby which appear to corroborate that the technique has been a problem from its very inception.  My own objective was to make sure that this one single claim of a correction is fully understood, so I leave it to the reader to decide how much more they wish to know ...<br /><br /><b>Carbon 14 - Libby&#39;s Luck</b><br /><a href=\"https://malagabay.wordpress.com/2014/05/20/carbon-14-libbys-luck/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/20/carbon-14-libbys-luck/</a><br /><br /><b>Carbon 14 - Cookbook</b><br /><a href=\"https://malagabay.wordpress.com/2014/05/21/carbon-14-cookbook/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/21/carbon-14-cookbook/</a><br /><br /><b>Carbon 14 - Willard&#39;s World</b><br /><a href=\"https://malagabay.wordpress.com/2014/05/22/carbon-14-willards-world/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/22/carbon-14-willards-world/</a><br /><br /><b>Carbon 14 - Lifting the Veil</b><br /><a href=\"https://malagabay.wordpress.com/2014/05/25/carbon-14-lifting-the-veil/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/25/carbon-14-lifting-the-veil/</a><br /><br /><b>Carbon 14 - The Big Fudge</b><br /><a href=\"https://malagabay.wordpress.com/2014/05/26/carbon-14-the-big-fudge/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/26/carbon-14-the-big-fudge/</a><br /><br /><b>Carbon 14 - Seeing the Light</b><br /><a href=\"https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2014/05/31/carbon-14-seeing-the-light/</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"The Cherry-Picking of Radiocarbon Dates","summary":"Laboratories and Researchers have Repeatedly Admitted that Radiocarbon Dates are Selected from a Scattered Dataset to Fit the Established Timeline / Around Half of All Dates are Literally Thrown Away / Nobody Keeps Track of them / It's a Way to Give  Observed Facts a Twist in the Direction which Agrees with Pre-existing Theories without Commiting Overt Fraud / Put Simply, \"Radiocarbon Dating is Not Employed to Test Theories, but to Support them\"","image":"https://lh3.googleusercontent.com/-5dKCv6gxl6s/WE7Qkx8HpzI/AAAAAAAAJr4/NlXMrNgxCgM_8ofZT3cfezY2g3uL9xDrwCJoC/w3600-h5040/the-cherry-picking-of-radiocarbon-dates-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/aJrn3cbWw3P","publishDate":"2016-12-12T16:30:47.987Z","updateDate":"2017-01-03T01:36:58.467Z","text":"<b>The Cherry-Picking of Radiocarbon Dates: Laboratories and Researchers have Repeatedly Admitted that Radiocarbon Dates are Selected from a Scattered Dataset to Fit the Established Timeline / Around Half of All Dates are Literally Thrown Away / Nobody Keeps Track of them / It&#39;s a Way to Give  Observed Facts a Twist in the Direction which Agrees with Pre-existing Theories without Commiting Overt Fraud / Put Simply, &quot;Radiocarbon Dating is Not Employed to Test Theories, but to Support them&quot;</b><br /><br />The quote in the graphic sources to ...<br /><br /><a href=\"https://books.google.com/books?id=m3RDCwAAQBAJ&amp;pg=PT205&amp;lpg=PT205&amp;dq=%22Like+any+good+archaeologist,+I+will+ignore+the+dates+that+do+not+fit%22&amp;source=bl&amp;ots=2SUTBtRN2W&amp;sig=CgkQtp9vPKff5Qgnd8Tv4tWgPBs&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjm0vC2h-_QAhVCzGMKHZktCRkQ6AEIGzAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=m3RDCwAAQBAJ&amp;pg=PT205&amp;lpg=PT205&amp;dq=%22Like+any+good+archaeologist,+I+will+ignore+the+dates+that+do+not+fit%22&amp;source=bl&amp;ots=2SUTBtRN2W&amp;sig=CgkQtp9vPKff5Qgnd8Tv4tWgPBs&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjm0vC2h-_QAhVCzGMKHZktCRkQ6AEIGzAA#v=onepage&amp;q&amp;f=false</a><br /><br />Note: Where a quote is available online, I will attempt to directly link to it.  Some quotes only appear in print.  You will notice that many of the dates source to the work of Charles Ginenthal, who was sometimes referred to as &quot;Immanuel Velikovsky&#39;s librarian&quot;.  Prior to the Creationists, the Catastrophists were the world&#39;s most vocal critics of this technique.<br /><br />[1] <a href=\"http://www.allaboutarchaeology.org/carbon-dating-2.htm\" class=\"ot-anchor\">http://www.allaboutarchaeology.org/carbon-dating-2.htm</a><br /><br />Robert E. Lee, &quot;Radiocarbon, Ages in Error,&quot; <i>Anthropological Journal of Canada</i>, Vol. 19, no. 3, 1981, p.9<br /><br /><i>&quot;The troubles of the radiocarbon dating method are undeniably deep and serious.  Despite 35 years of technological refinement and better understanding, the underlying assumptions have been strongly challenged ... It should be no surprise, then, that </i><b><i>fully half of the dates are rejected.  The wonder is, surely, that the remaining half comes out to be accepted.  There are gross discrepancies, the chronology is uneven and relative, and the accepted dates are actually selected dates.&quot;</i></b><br /><br />[2] <a href=\"https://books.google.com/books?id=W8VACwAAQBAJ&amp;pg=PT161&amp;lpg=PT161&amp;dq=Some+archaeologists+refuse+to+accept+radiocarbon+dates.+The+attitude+probably,+in+the+early+days+of+the+new&amp;source=bl&amp;ots=thPc_lWfeB&amp;sig=6wJ5LeGkzJ2zupIeaBq2GzR3uvQ&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwix-Yeg5-7QAhUD82MKHcD5DLwQ6AEIHTAA#v=onepage&amp;q=Some%20archaeologists%20refuse%20to%20accept%20radiocarbon%20dates.%20The%20attitude%20probably%2C%20in%20the%20early%20days%20of%20the%20new&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=W8VACwAAQBAJ&amp;pg=PT161&amp;lpg=PT161&amp;dq=Some+archaeologists+refuse+to+accept+radiocarbon+dates.+The+attitude+probably,+in+the+early+days+of+the+new&amp;source=bl&amp;ots=thPc_lWfeB&amp;sig=6wJ5LeGkzJ2zupIeaBq2GzR3uvQ&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwix-Yeg5-7QAhUD82MKHcD5DLwQ6AEIHTAA#v=onepage&amp;q=Some%20archaeologists%20refuse%20to%20accept%20radiocarbon%20dates.%20The%20attitude%20probably%2C%20in%20the%20early%20days%20of%20the%20new&amp;f=false</a><br /><br />David Wilson, <i>The New Archaeology</i> (NY 1974), p.97<br /><br /><i>&quot;Some archaeologists refuse to accept radiocarbon dates.  The attitude probably, in the early days of the new [radiocarbon] technique was summed up by Professor Jo Brew, Director of the Peabody Museum at Harvard, </i><b><i>&#39;If a C14 date supports our theories, we put it in the main text.  If it does not entirely contradict them, we put it in the footnote.  And if it is completely &#39;out of date&#39;, we just drop it.&#39;&quot;</i></b><br /><br />[3] <a href=\"https://books.google.com/books?id=W8VACwAAQBAJ&amp;pg=PT168&amp;lpg=PT168&amp;dq=ages+are+always+subject+to+interpretation,+and+radiocarbon+dates+are+often+ignored+or+dismissed+as+a+%27bad+date%27+if+they+do+not+fit+an+a+priori+hypothesis&amp;source=bl&amp;ots=thPc_lX6iE&amp;sig=boeGFZkB2J5OvLHTb-AuNhLHEbM&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjy5-jj5-7QAhUGwWMKHeNsBx4Q6AEIGzAA#v=onepage&amp;q=ages%20are%20always%20subject%20to%20interpretation%2C%20and%20radiocarbon%20dates%20are%20often%20ignored%20or%20dismissed%20as%20a%20%27bad%20date%27%20if%20they%20do%20not%20fit%20an%20a%20priori%20hypothesis&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=W8VACwAAQBAJ&amp;pg=PT168&amp;lpg=PT168&amp;dq=ages+are+always+subject+to+interpretation,+and+radiocarbon+dates+are+often+ignored+or+dismissed+as+a+%27bad+date%27+if+they+do+not+fit+an+a+priori+hypothesis&amp;source=bl&amp;ots=thPc_lX6iE&amp;sig=boeGFZkB2J5OvLHTb-AuNhLHEbM&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjy5-jj5-7QAhUGwWMKHeNsBx4Q6AEIGzAA#v=onepage&amp;q=ages%20are%20always%20subject%20to%20interpretation%2C%20and%20radiocarbon%20dates%20are%20often%20ignored%20or%20dismissed%20as%20a%20&#39;bad%20date&#39;%20if%20they%20do%20not%20fit%20an%20a%20priori%20hypothesis&amp;f=false</a><br /><br />T. A. Thompson, G. S. Fraser, G. Olyphant, &quot;Establishing the altitude and age of past lake levels in the Great Lakes,&quot; <i>Geological Society of America Abstracts with Programs</i>, Vol. 20, No. 5, (1988), p.392.<br /><br /><b><i>&quot;Relative [radiocarbon] ages are always subject to interpretation, and radiocarbon dates are often ignored or dismissed as a &#39;bad date&#39; if they do not fit an a priori hypothesis.&quot;</i></b><br /><br />[An a priori hypothesis is one that is generated prior to a research study taking place.]<br /><br />[4] <a href=\"http://immanuelvelikovsky.com/Mammoth.pdf\" class=\"ot-anchor\">http://immanuelvelikovsky.com/Mammoth.pdf</a><br /><br />Paul A. Colinvaux, &quot;The environment of the Bering land bridge,&quot; <i>Ecological Monographs</i>, Vol. 34, (1964), p.314.<br /><br /><i>&quot;In this respect, Colinvaux admitted that in his own research </i><b><i>he rejected over thirty percent of the radiocarbon dates he knew of in order to &#39;erect a chronology which is compatible with the known late Pleistocene and recent.&#39;&quot;</i></b><br /><br />[5] This quote is online, but it is only reprinted in creationist materials ...<br /><br />Gwen Schultz, <i>Ice Age Lost</i>, p.28.<br /><br /><i>&quot;With respect to the problem of accuracy of radiocarbon dating, Schultz frankly admits:</i><br /><br /><i>&#39;... </i><b><i>radiometric dating techniques, nor any other, can supply dates that are acceptable at face value.  Every date needs verification by other means, and then it still carries a question mark.</i></b><i>  Radiometric dates are always stated with a plus-or-minus number of years to allow for statistical error.  The test processes are complicated and tedious, and human error is an ever-present possibility ... In radiometric dating, for instance: An unearthed fragment of bone or wood did not lie in a sealed box all through the years.  Soil, organisms, roots, and other matter that could affect the dating have been in contact with it; ground water has dissolved and carried away material that should figure in the measuring and brought to that spot material from elsewhere.  </i><b><i>Just a speck of contaminant throws the computed date off.&#39;&quot;</i></b><br /><br />[6] This one only appears in this text ...<br /><br /><i>The Extinction of the Mammoth</i>, Charles Ginenthal, The Velikovskian: A Journal of Myth, History and Science, Vol III, No&#39;s 2 and 3<br /><br /><b><i>&quot;The lesson to be learned is that radiocarbon dating is not employed to test theories, but to support them.</i></b><i>  As Dr. Roger Wescott told me when I spoke with him by telephone about this on April 17, 1997, radiocarbon always gives a scattered set of dates.  The theorists then pick the ones that they believe to be correct.  In essence, no evidence from Carbon-14 dating can be fully incorporated by the establishment if it fails to support the dogma of these researchers.  They simply do not believe it.&quot;</i><br /><br />[7] <a href=\"https://books.google.com/books?id=Pz78tSwRAaUC&amp;pg=PA226&amp;lpg=PA226&amp;dq=%22the+investigator+is+first+asked+what+date+he+will+accept+for+the+material+he+brings+to+be+dated%22&amp;source=bl&amp;ots=G0c6mW_ykH&amp;sig=NDrqxebzk5WVtJVOhBLOU5HMMw4&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiu1NPe7-7QAhVN8WMKHfKiDFQQ6AEIIDAB#v=onepage&amp;q=%22the%20investigator%20is%20first%20asked%20what%20date%20he%20will%20accept%20for%20the%20material%20he%20brings%20to%20be%20dated%22&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=Pz78tSwRAaUC&amp;pg=PA226&amp;lpg=PA226&amp;dq=%22the+investigator+is+first+asked+what+date+he+will+accept+for+the+material+he+brings+to+be+dated%22&amp;source=bl&amp;ots=G0c6mW_ykH&amp;sig=NDrqxebzk5WVtJVOhBLOU5HMMw4&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiu1NPe7-7QAhVN8WMKHfKiDFQQ6AEIIDAB#v=onepage&amp;q=%22the%20investigator%20is%20first%20asked%20what%20date%20he%20will%20accept%20for%20the%20material%20he%20brings%20to%20be%20dated%22&amp;f=false</a><br /><br />&quot;Scientific Dating Methods in Ruins - Charles Ginenthal&quot;, from J. Ogden, &quot;The Use and Abuse of Radiocarbon,&quot; <i>Annals of the New York Academy of Science</i> 288 (New York, 1977): 173.<br /><br /><i>&quot;J. Ogden, the director of a radiocarbon dating laboratory in Wesleyan University in Ohio, stated that </i><b><i>the investigator is first asked what date he will accept for the material he brings to be dated; then, when a figure is obtained that comes near this date, it is duly reported -- together with tolerance values -- to make the test appear honest.&quot;</i></b><br /><br />[8] <a href=\"https://books.google.com/books?id=m3RDCwAAQBAJ&amp;pg=PT205&amp;lpg=PT205&amp;dq=%22archaeologists+and+laboratory+technicians+began+to+hammer+out+the+exact+history+of+the+earliest+Americans%22&amp;source=bl&amp;ots=2SUTBtLL6U&amp;sig=X6h7EJ4Nfi-xS4Nah-jrQcjxauA&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjorLu88O7QAhUOS2MKHaQRDIYQ6AEIGzAA#v=onepage&amp;q=%22archaeologists%20and%20laboratory%20technicians%20began%20to%20hammer%20out%20the%20exact%20history%20of%20the%20earliest%20Americans%22&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=m3RDCwAAQBAJ&amp;pg=PT205&amp;lpg=PT205&amp;dq=%22archaeologists+and+laboratory+technicians+began+to+hammer+out+the+exact+history+of+the+earliest+Americans%22&amp;source=bl&amp;ots=2SUTBtLL6U&amp;sig=X6h7EJ4Nfi-xS4Nah-jrQcjxauA&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjorLu88O7QAhUOS2MKHaQRDIYQ6AEIGzAA#v=onepage&amp;q=%22archaeologists%20and%20laboratory%20technicians%20began%20to%20hammer%20out%20the%20exact%20history%20of%20the%20earliest%20Americans%22&amp;f=false</a><br /><br />&quot;Scientific Dating in Ruins - Charles Ginenthal&quot;, originally from 13. Frank C. Hibben, &quot;Radioactive Times,&quot; The Lost Americans, revised and updated (New York, 1968), pp.139-140.<br /><br /><i>&quot;Frank C. Hibben also discussed the process of radiocarbon dating.  After outlining several problems associated with using this method, he stated that &#39;[e]ven with these drawbacks and pitfalls ... archaeologists and laboratory technicians began to hammer out the exact history of the earliest Americans.  </i><b><i>The dates badly out of line were disregarded.&#39;&quot;</i></b><br /><br />[9] <a href=\"https://books.google.com/books?id=m3RDCwAAQBAJ&amp;pg=PT205&amp;lpg=PT205&amp;dq=%22Like+any+good+archaeologist,+I+will+ignore+the+dates+that+do+not+fit%22&amp;source=bl&amp;ots=2SUTBtLN6X&amp;sig=QL-6KvKIqZxBcHFQQK1nH0mICtE&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwis-aqd8e7QAhVC12MKHevcABwQ6AEIGzAA#v=onepage&amp;q=%22Like%20any%20good%20archaeologist%2C%20I%20will%20ignore%20the%20dates%20that%20do%20not%20fit%22&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=m3RDCwAAQBAJ&amp;pg=PT205&amp;lpg=PT205&amp;dq=%22Like+any+good+archaeologist,+I+will+ignore+the+dates+that+do+not+fit%22&amp;source=bl&amp;ots=2SUTBtLN6X&amp;sig=QL-6KvKIqZxBcHFQQK1nH0mICtE&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwis-aqd8e7QAhVC12MKHevcABwQ6AEIGzAA#v=onepage&amp;q=%22Like%20any%20good%20archaeologist%2C%20I%20will%20ignore%20the%20dates%20that%20do%20not%20fit%22&amp;f=false</a><br /><br />&quot;Scientific Dating Methods in Ruins - Charles Ginenthal&quot;, originally from 14. Ron Willis, Info Journal 3 (1973): 1-7.<br /><br /><i>&quot;Regarding this same point, Ron Willis stated that &#39;[t]here are anomalous dates in the series [of dates] which do not fit.  </i><b><i>This is common in the C-14 process.  Like any good archaeologist, I will ignore the dates that do not fit.&#39;&quot;</i></b><br /><br />[10] <a href=\"https://books.google.com/books?id=W8VACwAAQBAJ&amp;pg=PT170&amp;dq=%22Human+bias+influences+radiocarbon+chronologies+far+more+than+is+acknowledged%22&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiiwfWA8u7QAhVLImMKHQzpD3MQ6AEIHTAA#v=onepage&amp;q=%22Human%20bias%20influences%20radiocarbon%20chronologies%20far%20more%20than%20is%20acknowledged%22&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=W8VACwAAQBAJ&amp;pg=PT170&amp;dq=%22Human+bias+influences+radiocarbon+chronologies+far+more+than+is+acknowledged%22&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiiwfWA8u7QAhVLImMKHQzpD3MQ6AEIHTAA#v=onepage&amp;q=%22Human%20bias%20influences%20radiocarbon%20chronologies%20far%20more%20than%20is%20acknowledged%22&amp;f=false</a><br /><br />Thomas W. Stratford Jr, “How Old Is It?” <i>Discovering Archaeology</i>, vol. 2, no. 1 (Jan/Feb. 2000), p.45<br /><br /><b><i>&quot;&#39;Human bias influences radiocarbon chronologies far more than is acknowledged.  While human subjectivity should not be a factor, it may be one of the most intransigent problems in radiocarbon dating ...&#39;&quot;</i></b><br /><br />[11] <a href=\"https://books.google.com/books?id=W8VACwAAQBAJ&amp;pg=PT209&amp;lpg=PT209&amp;dq=%22simply+because+this+appears+to+be+a+more+scientific+way+of+approaching+archaeology.+It+isn%27t,+of+course.+You+can+tell+lies+with+numbers+just+as+easily+as+you+can+with+words%22&amp;source=bl&amp;ots=thPc_lZflx&amp;sig=5KSxgiYcJoKxwBy65WB0sFjRMQs&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjY74HW8u7QAhVF62MKHeYQDhgQ6AEIGzAA#v=onepage&amp;q=%22simply%20because%20this%20appears%20to%20be%20a%20more%20scientific%20way%20of%20approaching%20archaeology.%20It%20isn%27t%2C%20of%20course.%20You%20can%20tell%20lies%20with%20numbers%20just%20as%20easily%20as%20you%20can%20with%20words%22&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=W8VACwAAQBAJ&amp;pg=PT209&amp;lpg=PT209&amp;dq=%22simply+because+this+appears+to+be+a+more+scientific+way+of+approaching+archaeology.+It+isn%27t,+of+course.+You+can+tell+lies+with+numbers+just+as+easily+as+you+can+with+words%22&amp;source=bl&amp;ots=thPc_lZflx&amp;sig=5KSxgiYcJoKxwBy65WB0sFjRMQs&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjY74HW8u7QAhVF62MKHeYQDhgQ6AEIGzAA#v=onepage&amp;q=%22simply%20because%20this%20appears%20to%20be%20a%20more%20scientific%20way%20of%20approaching%20archaeology.%20It%20isn&#39;t%2C%20of%20course.%20You%20can%20tell%20lies%20with%20numbers%20just%20as%20easily%20as%20you%20can%20with%20words%22&amp;f=false</a><br /><br />David Wilson, op.cit., p. 277, <i>The New Archaeology</i> (NY 1974), p.97<br /><br /><i>&quot;&#39;If one looks at ... [the] new [radiocarbon] method ... simply as [an] alternative approach ... or, if you like, alternative tool ..., then it seems to me that one can get a great deal out of ... [it] as long as one always bears in mind ... [it is a] method and not an end ... and there is a danger of the method being used wrongly in the sense that it is applied to data which ... [is] not suitable for it.  </i><b><i>At worst ... you use [this] method ... simply because this appears to be a more scientific way of approaching archaeology.  It isn&#39;t, of course.  You can tell lies with numbers just as easily as you can with words.&#39;&quot;</i></b><br /><br />[12] Not apparently accessible online ...<br /><br />R. E. Lee, &quot;Radiocarbon Ages in Error,&quot; <i>Anthropological Journal of Canada</i>, (1981) 19 (3):9, p.27.<br /><br /><i>&quot;As was pointed out quite directly by R. E. Lee, </i><b><i>the dates derived by radiocarbon &#39;are actually selected dates.  Although some of these dates are based on bone collagen which is easily contaminated, not all are.&#39;&quot;</i></b><br /><br />[13] <a href=\"https://books.google.com/books?id=9x3VBwAAQBAJ&amp;pg=PA48&amp;lpg=PA48&amp;dq=Data+are,+in+fact,+used+in+model+construction+%28the+models+are+fitted+to+the+data+observed%29+and+only+those+dimensions+of+the+data+supporting+model+construction+are+considered&amp;source=bl&amp;ots=sYuANZ68aj&amp;sig=EKg_2bbuupoDVv68VZYA2FqboU0&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwixwev_8-7QAhVP22MKHXNbAzQQ6AEIGzAA#v=onepage&amp;q=Data%20are%2C%20in%20fact%2C%20used%20in%20model%20construction%20%28the%20models%20are%20fitted%20to%20the%20data%20observed%29%20and%20only%20those%20dimensions%20of%20the%20data%20supporting%20model%20construction%20are%20considered&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=9x3VBwAAQBAJ&amp;pg=PA48&amp;lpg=PA48&amp;dq=Data+are,+in+fact,+used+in+model+construction+(the+models+are+fitted+to+the+data+observed)+and+only+those+dimensions+of+the+data+supporting+model+construction+are+considered&amp;source=bl&amp;ots=sYuANZ68aj&amp;sig=EKg_2bbuupoDVv68VZYA2FqboU0&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwixwev_8-7QAhVP22MKHXNbAzQQ6AEIGzAA#v=onepage&amp;q=Data%20are%2C%20in%20fact%2C%20used%20in%20model%20construction%20(the%20models%20are%20fitted%20to%20the%20data%20observed)%20and%20only%20those%20dimensions%20of%20the%20data%20supporting%20model%20construction%20are%20considered&amp;f=false</a><br /><br />Jean-Philippe Regaud, Jan F. Simek, &quot;Paleolithic Prehistory in France,&quot; <i>The Pleistocene Old World Regional Perspectives</i>, Olga Soffer ed. (New York, 1987), pp.48-49.<br /><br /><i>&quot;The very same kind of accusation was made by two researchers, Jean-Philippe Regaud and Jan F. Simek.</i><br /><br /><i>&#39;Recent prehistories are post hoc constructs attempting to account for the observed record. (C.F. Binford, 1982).  Almost by definition, this accommodation process suffers from a lack of empirical sufficiency, that is, allows no systematic model testing ... </i><b><i>Data are, in fact, used in model construction (the models are fitted to the data observed) and only those dimensions of the data supporting model construction are considered</i></b><i> ...&#39;&quot;</i><br /><br /><i>&#39;This is particularly distressing when prehistorians actually acknowledge such problems but do not take them into account.&#39;&quot;</i><br /><br />[14] <a href=\"https://books.google.com/books?id=8OWQTJP8oFkC&amp;pg=PA111&amp;lpg=PA111&amp;dq=%22This+disarrangement+of+radiocarbon+ages+suggests+that+the+delta+sediments+are,+to+some+degree,+composed+of+recycled+sediment%22&amp;source=bl&amp;ots=nrmtxuygzL&amp;sig=VlIhUV_VlDF_PfFSVkva7Fsm5Kg&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwis0NXD9e7QAhVK02MKHa0KBlQQ6AEIGzAA#v=onepage&amp;q=%22This%20disarrangement%20of%20radiocarbon%20ages%20suggests%20that%20the%20delta%20sediments%20are%2C%20to%20some%20degree%2C%20composed%20of%20recycled%20sediment%22&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=8OWQTJP8oFkC&amp;pg=PA111&amp;lpg=PA111&amp;dq=%22This+disarrangement+of+radiocarbon+ages+suggests+that+the+delta+sediments+are,+to+some+degree,+composed+of+recycled+sediment%22&amp;source=bl&amp;ots=nrmtxuygzL&amp;sig=VlIhUV_VlDF_PfFSVkva7Fsm5Kg&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwis0NXD9e7QAhVK02MKHa0KBlQQ6AEIGzAA#v=onepage&amp;q=%22This%20disarrangement%20of%20radiocarbon%20ages%20suggests%20that%20the%20delta%20sediments%20are%2C%20to%20some%20degree%2C%20composed%20of%20recycled%20sediment%22&amp;f=false</a><br /><br />&quot;Scientific Dating Methods in Ruins&quot;, Charles Ginenthal, originally from 15: 15. D. S. McCulloch, &quot;Quaternary Geology of the Alaskan Shore of the Chukchi Sea,&quot; The Bering Land Bridge, ed. David M. Hopkins (Stamford, California, 1967), p.110.<br /><br /><i>&quot;Let us see, nevertheless, just how data is handled when a series of strata give highly discordant dates.  In The Bering Land Bridge, D. S. McCulloch described a core taken from the Chukchi Sea, off the coast of Alaska:</i><br /><br /><i>&#39;Six radiocarbon ages have been determined for organic carbon throughout the core.  </i><b><i>These ages, which range from 4,390 ± 210 to 15,500 ± 800 years BP, are so disarranged</i></b><i> that no consecutive dates are juxtaposed and the oldest age determination is from the 2.88 to 3.40-meter interval. [The oldest date in the deposit, which should have been found at the very bottom of the core, was found toward the top.]  This disarrangement of radiocarbon ages suggests that the delta sediments are, to some degree, composed of recycled sediment.&#39; </i><b><i>By disregarding the 15,500 ± 800-year age as being inconsistent with the other radiocarbon ages and by assuming that contamination had made the remaining dates too old</i></b><i> by about the same amount that the radiocarbon age (4,390 ± 210 years BP) near the top of the core exceeds usual surface sediment dates, [J.S.] Creager and [D.A.] McManus conclude that the delta was formed about 12,000 years ago.&quot;</i><br /><br />[15] Only reprinted in Creationist and New World Order conspiracy materials ...<br /><br />William N. Irving and C. R. Harington, &quot;Upper Pleistocene Radiocarbon-dated Artefact from the Northern Yukon,&quot; <i>Science</i> 179 (January 26, 1973): 335.<br /><br /><i>&quot;Footnote 5.</i><br /><br /><i>W. Blake, Jr. (personal communication) has advised us that, in view of the discrepancy between these two dates (GSC 730-1 and GSC 730-2), new, small pieces of wood were taken from the same sample, burned to liberate the carbon dioxide, and then the radioactivity of the resulting gas was counted for just a few hours.  </i><b><i>When it became apparent that the radioactivity was equivalent to a date of about 4000 years, the counting was stopped</i></b><i>, because by this time it was clear that the samples were mixed, containing some old wood and some young wood (a mixture of which had produced the 14,390-year date).&quot;</i><br /><br />[16] <a href=\"http://www2.clarku.edu/~piltdown/map_expose/scie_enthus_pilt.html\" class=\"ot-anchor\">http://www2.clarku.edu/~piltdown/map_expose/scie_enthus_pilt.html</a><br /><br />Cremo, Thompson, <i>Hidden History of the Human Race</i>, op. cit., p.189.<br /><br /><i>&quot;&#39;There is, however, another more insidious and pervasive kind of cheating -- the routine editing and reclassifying of data according to rigid theoretical preconceptions.&#39;</i><br /><br /><i>&#39;Vayson de Pradenne, of the Ecole d&#39;Anthropologie in Paris, wrote in his book Fraudes Archeologiques (1925): </i><b><i>&#39;One often finds men of science possessed by a pre-conceived idea, who, without committing real frauds, do not hesitate to give observed facts a twist in the direction which agrees with their theories.</i></b><i>  A man may imagine, for example, that the laws of progress in prehistoric industries must show itself everywhere and always in the smallest details.  Seeing the simultaneous presence in a deposit of carefully finished artifacts and others of a coarser type, he decides that there must be two levels: the lower one yielding the coarser specimens.  He will class his finds according to their type, not according to the stratum in which he found them.  If at the base he finds a finely worked impelement he will declare there has been accidental penetration and that the specimen must be re-integrated with the site of its origin by placing it with items from the higher level.  He will end with real trickery in the stratigraphic presentation of his specimens; trickery in aid of preconceived ideas, but more or less unconsciously done by a man of good faith whom no one would call fraudulent.  The case is often seen, and if I mention no names it is not because I do not know any.&#39;</i><br /><br /><b><i>&#39;This sort of thing goes on not just in the British Museum, but in all museums, universities, and other centers of paleoanthropological research the world over.  Although each separate incident of knowledge filtration seems minor, the cumulative effect is overwhelming, serving to radically distort and obscure our picture of ... antiquity.&#39;&quot;</i></b><br /><br />[17] <a href=\"http://rogerswebsite.com/ah/TheExtinctionoftheMammoth.pdf\" class=\"ot-anchor\">http://rogerswebsite.com/ah/TheExtinctionoftheMammoth.pdf</a><br /><br />Emil W. Haury, <i>The Hohokam, Desert Farmers and Craftsmen</i>, 1964-1965, (Tucson, 1976), p.333.<br /><br /><i>&quot;An excellent example which illustrates how the paradigm determines what is found in radiocarbon testing is the 32 radiocarbon assays made on Hohokam artifacts as delineated by Emil W. Haury.</i><br /><br /><i>&#39;... It would be an understatement to record that the [radiocarbon] results were in agreement with each other.  The opposite is the case.  </i><b><i>The task of sorting out those dates that appear usable from those that are obviously incorrect and justify one&#39;s selections is not simple.</i></b><i>  It is unthinkable, however, that dates for the Vahki Phase materials as far apart as 425 ± 115 B.C. and A.D. 1020 ± 120, or for the Sacaton Phase of A.D. 900 ± 100 and 1820 ± 110 can all be correct.  </i><b><i>These discrepancies force a choice.  To do otherwise would land us in a chronological nightmare.&#39;</i></b><br /><br /><i>&#39;The reason or reasons for these disparities may be many, ranging from the selection, collection, and recording of samples in the field, to contamination, and analytical errors in laboratory processing, </i><b><i>and even to the assumptions on which isotope dating is based</i></b><i>.  It is not my intent to try to determine where the problem lies.  However, I firmly believe that in making a qualitative judgment about the value of dates, an intimate knowledge of field problems and of the nature of the cultural complexity under study is fundamental to the decision ... My dependence on certain radiocarbon dates and rejection of others will not be pleasing to everyone, but these judgments must be made.  </i><b><i>If the complications arising from the establishment of a chronology ... have taught us anything, it is that we are far from reaching a finite level of expertness in the art of dating ...&#39;&quot;</i></b><br /><br />[18] This one&#39;s hard to track down ...<br /><br />D. Q. Bowen, Quaternary Geology, <i>A Stratigraphic Framework for Multidisciplinary Work</i>, (New York, 1978), pp. 7-8.<br /><br /><i>&quot;Indeed it could be said that force-fitting of the pieces into a preconceived pigeon-hold classifications is what is almost a way of life for the Quaternary worker ... Tendencies to oversimplify in this way lead to new discoveries being forced into pigeon-holed classification.  </i><b><i>Such arbitrary methods tend to perpetuate an illusion of security and precision in an apparently repeated confirmation of the original model.  This tendency to confirm discoveries from limited amounts of data has been called The Reinforcement Syndrome.&#39;&quot;</i></b><br /><br />[19] Another one that has yet to appear on the Internet ...<br /><br />&quot;Scientific Dating Methods in Ruins&quot;, Charles Ginenthal, original from 20: <i>Robert Silverberg, Clocks for the Ages</i> (New York, 1971), pp. 205-206.<br /><br /><i>&quot;According to Robert Silverberg:</i><br /><br /><i>&#39;Carbon-14 dating provided a firm underpinning for New World chronology.  In 1951, [W] Libby and his colleague, J.R. Arnold, published the first Folsom Date: 9,928 ± 350 years for a charred bison bone found with Folsom points near Lubbock, Texas.  Other Folsom dates soon followed, most of them placing these people between 10,000 and 12,000 years ago.  Charcoal from a hunting camp of the Yuma-point people in Arizona registered 10,500 ± 300 years.&#39;</i><br /><br /><b><i>&#39;Sandia Cave&#39;s lowest occupied level yielded a date of about 20,000 years, though it was not considered entirely trustworthy.  Even more surprising were carbon-14 ages of 27,000, 38,000, and 40,000 years for the three sites in California and Texas.  These findings, too, came under strong challenge; unable to believe such high readings, many archaeologists suggested that ground conditions must have caused the samples to lose some of their carbon-14 over the course of time, resulting in misleading analyses.&#39;</i></b><br /><br /><i>Of course, if these artifacts were dated to less than or between 10,000 and 12,000 years ago, we can be sure that there would never have been a single challenge to the findings.  </i><b><i>Only those radiocarbon dating findings contradicting accepted chronology are contaminated, not those supporting the theory.  This, I suggest, is not science.&quot;</i></b><br /><br />[20] Another one only cited by Creationists ...<br /><br />Charles A. Reed, &quot;Animal Domestication in the Prehistoric Near East,&quot; <i>Science</i> 130 (1959): 1630.<br /><br /><b><i>&quot;A last difficulty, and at the moment one of the most frustrating, is the failure of the radiocarbon (C14) technique to yield dates of certain dependability.  Although it was hailed as the answer to the prehistorian&#39;s prayer when it was first announced, there has been increasing disillusion with the method because of the chronological uncertainties (in some cases, absurdities) that would follow a strict adherence to published C14 dates.</i></b><i>  This is not to question the validity of the physical laws underlying the principle used, or the accuracy of the counters now in operation around the world; </i><b><i>the unsolved problem, instead, seems to lie in the difficulty of securing samples completely free from either older or younger adherent carbon.  At least to the present, no kind or degree of chemical cleaning can guarantee one-age carbon, typical only of the time of the site from which it was excavated.</i></b><i>  Wht bids to become a classic example of </i><b><i>&#39;C14 irresponsibility&#39;</i></b><i> is the 6000-year spread of 11 determinations for Jarmo (3), a prehistoric village in northeastern Iraq, which, on the basis of all archeological evidence, was not occupied for more than 500 consecutive years.&quot;</i><br /><br />[21] Commentary on former quote ...<br /><br />&quot;Scientific Dating Methods in Ruins&quot;, Charles Ginenthal<br /><br /><b><i>&quot;Here, Reed admits that a strict adherence to publish C-14 dates would lead to absurd results, then labels as irresponsible the publication of dates that, from an a priori standpoint, do not fit the theory.  If all materials can become contaminated and this is well known, on what basis is one date truly uncontaminated and another one contaminated?  According to Reed, this cannot be determined.  Contamination makes radiocarbon dating unreliable.&quot;</i></b><br /><br />[22] It&#39;s unfortunate that this paper has disappeared from the Internet, as it contains so many important quotes ...<br /><br />&quot;Scientific Dating Methods in Ruins&quot;, Charles Ginenthal<br /><br /><b><i>&quot;The investigators have chosen, once again, only the dates that agree with their assumptions.  Out of hundreds of dates, they have rejected all but a small fraction and have done so by assuming that the overwhelming number that do not agree are all contaminated.</i></b><i>  They only use wood samples that originate from &#39;stratigraphically significant material.&#39; When water with biological carbon-14 inundates a shoreline, it works its way into wooden debris.  Thus, any selection process has to be arbitrary.&quot;</i><br /><br />[23] Here is the quote that Ginenthal is talking about, the last remaining online mention is from a Creationist paper ...<br /><br />D. M. Mickelson et al., &quot;The Last Wisconsin Glacial Record of the Laurentide Ice Sheet in the United States,&quot; <i>Late-Quaternary Environments of the United States</i>, ed. H. E. Wright, Jr., (Minneapolis, Minnesota, 1983), Vol. I, pp.12-13.<br /><br /><i>&quot;We know of several hundred radiocarbon dates from the area and time range being considered here, but only 27 have been used in our chronology.  All but wood dates have been rejected because of unresolvable contamination problems, and only wood dates that seem to be from stratigraphically significant materials have been used.&quot;</i><br /><br />[24] <a href=\"https://www.newscientist.com/article/mg12316841-900-unexpected-errors-affect-dating-techniques/\" class=\"ot-anchor\">https://www.newscientist.com/article/mg12316841-900-unexpected-errors-affect-dating-techniques/</a><br /><br />Andy Coghlan, &quot;Unexpected errors affect dating techniques,&quot; <i>New Scientist</i>, (September 30, 1989), p.26.<br /><br />Here is a quote of Andy talking about that article.  This quote is not accessible online, but the link above refers to the same exact study ...<br /><br /><i>&quot;What I pointed out in that article is that radiocarbon was put to a basic test and failed, on a massive scale, to show itself to be what the true believers suggest it is.  Briefly, &#39;Britain&#39;s Science and Engineering Research Council ... commissioned a trial that compared the accuracy with which 38 [radiocarbon] laboratories around the world dated artifacts of known age.  </i><b><i>Of the 38, only seven produced results the organizers considered satisfactory.&#39; The facts are that none of the laboratories got the exact date, while 31 of the 38 of the world&#39;s leading radiocarbon testing facilities were in error, some by many hundreds to thousands of years, even with plus and minus limits added to their dates.  That is, only 18.5 percent gave results near enough to be accepted, while 81.5 percent were dead wrong.  For those who advocate such a method I ask: What is the basis of their confidence, given this recent overwhelming categorical failure to pass such a basic test?&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Age of the Earth","summary":"Darwinian Evolution Needs an Old Earth to Work / This May Explain Why - of the Many Methods that Could be Used to Estimate the Earth's Age - the Radioactive Decay of Certain Metallic Elements, Sometimes Referred to as the Uranium-Lead Method, is Cherry-Picked as the \"Correct\" Approach / But, there is a Very Serious Problem with the Technique which We Know Should Tip the Age Calculation Towards an Old Earth - as Well as Confirmation that the Problem is Real / The Problem is Simply Ignored because a Creationist Revealed it","image":"https://lh3.googleusercontent.com/-X9b8MIjyHGc/WE37qUmmF1I/AAAAAAAAJrI/9iDqJP3-mEYNGiq8vc9hJ1z0at3qgEeGQCJoC/w1800-h2520/the-age-of-the-earth-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/4wej9zi8NYT","publishDate":"2016-12-12T01:22:56.398Z","updateDate":"2017-01-03T01:37:14.784Z","text":"<b>The Age of the Earth: Darwinian Evolution Needs an Old Earth to Work / This May Explain Why - of the Many Methods that Could be Used to Estimate the Earth&#39;s Age - the Radioactive Decay of Certain Metallic Elements, Sometimes Referred to as the Uranium-Lead Method, is Cherry-Picked as the &quot;Correct&quot; Approach / But, there is a Very Serious Problem with the Technique which We Know Should Tip the Age Calculation Towards an Old Earth - as Well as Confirmation that the Problem is Real / The Problem is Simply Ignored because a Creationist Revealed it</b><br /><br />Good critiques of dating techniques are curiously hard to find.  The most technical articles tend to be written by Creationists, which is a non-starter for many people.  Only a handful of these critiques provide a broad survey of the problems for non-technical audiences, and most are by now several decades old.  Richard Milton&#39;s 1997 book -- <i>Shattering the Myths of Darwinism</i> -- doesn&#39;t suffer from any of these common defects.  Milton dedicates a full 37 pages to the subject.  It is almost surely the most devastating critique of the practice of geological dating ever written, and it definitively reopens the big question of how old the Earth is.<br /><br />(Amazon.com reviews, btw, are worth checking out ... <a href=\"https://www.amazon.com/Shattering-Myths-Darwinism-Richard-Milton/product-reviews/0892818840/ref=cm_cr_getr_d_paging_btm_1?ie=UTF8&amp;reviewerType=avp_only_reviews&amp;showViewpoints=1&amp;sortBy=helpful&amp;pageNumber=1\" class=\"ot-anchor\">https://www.amazon.com/Shattering-Myths-Darwinism-Richard-Milton/product-reviews/0892818840/ref=cm_cr_getr_d_paging_btm_1?ie=UTF8&amp;reviewerType=avp_only_reviews&amp;showViewpoints=1&amp;sortBy=helpful&amp;pageNumber=1</a>)<br /><br /><a href=\"https://www.krusch.com/books/evolution/Shattering_Myths_Of_Darwinism.pdf\" class=\"ot-anchor\">https://www.krusch.com/books/evolution/Shattering_Myths_Of_Darwinism.pdf</a><br /><br /><i>Shattering the Myths of Darwinism</i> (1997)<br />by Richard Milton<br /><br />(p19-56)<br /><br /><b><i>&quot;Let us ask what evidence we have for the age of the Earth and what grounds we have for accepting that evidence.</i></b><br /><br /><b><i>The importance of this question ... lies in the fact that an Earth of immense age is indispensably necessary to the neo-Darwinist theory because genetic mutation and natural selection are processes that are conceived of as working very slowly over hundreds of millions of years.</i></b><i> If the Earth were only a few million years old then there simply would not have been enough time for natural selection to work. Whether we liked it or not, we would be compelled to seek a fresh explanation for the origin of living species.</i><br /><br /><i>On this fundamentally important question, the Natural History Museum and all other modern authorities are in complete agreement. The Earth is 4,600 million years old. What is more, </i><b><i>different periods of the Earth&#39;s history have been characterized by the formation of different kinds of rock containing the fossil remains of distinctive kinds of creature.</i></b><i> These different periods have also been dated to give what is usually referred to as the Geological Column of the Earth&#39;s history. The column is reproduced on page 69.</i><br /><br /><i>By referring to the geological column anyone can tell the age of a rock or fossil that he or she finds. For instance, England&#39;s white cliffs consist of chalk dating from the end of the Cretaceous period, which, the column tells us, dates from 65 million years ago.</i><br /><br /><i>The dates attached to the geological column have been arrived at and refined over the past century or so. The most recent evaluation, and the one quoted in Natural History Museum publications, is that of Van Eysinga published in 1975. [12] This scheme (which is the source of Figure 1) is closely similar to that used in most museums and universities since the early decades of this century, and is based on the pioneering work of Arthur Holmes in the United Kingdom and Henry Faul in the United States. Some minor disagreements may exist among geologists but a very wide measure of agreement exists over the big issue that the earliest rocks of the column are around 4 billion years old, and over most of the details in Figure 1, for example that the Cretaceous period began around 140 million years ago and ended around 65 million years ago.</i><br /><br /><i>When I began to research this question a little more closely I uncovered a puzzle. Those experts I referred to and the authoritative textbooks I consulted all told me that modern dating has been accomplished by using radioactive methods and hence was an absolute dating method of a far higher order of accuracy than all previous methods -- most of which relied on calculations involving one or more relative factors. These relative dating methods had relied on such factors as the increasing salinity of the oceans, or the Earth&#39;s rate of cooling, and are now considered unreliable. Radioactive dating, though, is used to date the rocks and the fossils they contain directly and hence was welcomed as an absolute method.</i><br /><br /><b><i>The puzzle arises because radioactive dating techniques can be applied only to volcanic rocks that contain some radioactive mineral -- the primary rocks of the Earth&#39;s crust. But the geological column consists of sedimentary rocks -- rocks formed from sediments laid down on the beds of ancient seas and composed of particles of those primary rocks. So, of course, any age determination made using these particles will be the same as that of the primary rocks from which they were derived. In some common sedimentary rocks, such as chalk or limestone, there are not even particles of the primary rocks present and so radioactive dating cannot be used at all.</i></b><i> Happily for English men and women, the white cliffs of Dover are not radioactive.</i><br /><br /><i>In The Age of the Earth published by the Institute of Geological Sciences, the position is succinctly explained by John Thackray:</i><br /><br /><b><i>&#39;The only sediments which can be dated directly are those in which a radioactive mineral is formed during diagenesis [laying down] of the sediment</i></b><i>, such as the rather uncommon illite shales and glauconitic sandstones; other sediments give only the age of the parent rock from which the mineral grains that make them up are derived.&#39; [13]</i><br /><br /><i>How then did Holmes, Faul, and Van Eysinga arrive at the dates attached to the sediments of the geological column?</i><br /><br /><i>&#39;The Institute of Geological Sciences explains:</i><br /><br /><b><i>Where lavas or volcanic ashes are interbedded with a sediment of known stratigraphic age, then a date may be given to that stratigraphic division. Where an igneous rock intrudes one sedimentary unit and is blanketed by another, then the sediments may be dated from the igneous rock by inference. The rarity of such cases, together with analytical error inherent in age determination, mean that isotopic ages are unlikely to rival or replace fossils as the most important means of ... correlation.&#39;</i></b><br /><br /><i>It turns out that what has been dated by radioactive decay methods is not the sedimentary rocks or fossils themselves but the isolated intrusion into them of igneous or primary rocks, usually as volcanic material. This has been a rare and purely fortuitous process and one that is unreliable -- so rare and so unreliable that the Institute of Geological Sciences thinks it unlikely to replace or even rival fossils as a method of dating. Nor is this all, for the method depends in turn on a further chain of inference. For </i><b><i>the geological column of Van Eysinga is nowhere to be found in nature. It is an imaginary structure that has been synthesized from comparing a stratum of rock in one part of the world with a similar looking stratum in another part of the world</i></b><i> ...</i><br /><br /><i>Naturalists themselves are often confused in their knowledge of this question. Gavin de Beer, for example, director of the British Museum of Natural History from 1950 to 1960, wrote in the introduction to the museum&#39;s guide to evolution, published in 1970,  that the rocks forming the geological column had been dated by radioactive methods:</i><br /><br /><i>&#39;Estimates of time based on disintegration of radioactive material enable various levels of evolutionary lineages to be dated and the time measured during which certain changes have occurred, thereby providing quantitative evidence of evolution rates and the duration times of genera and species.&#39; [14]</i><br /><br /><b><i>This claim, which is universally believed and taught in schools and universities throughout the world, is entirely false. And when Darwinists speak of absolute dating of the geological column and the fossils it contains by radioactive methods they are quite mistaken, there is nothing absolute about it. In fact the method ought to be referred to as &#39;comparative dating,&#39; because it dates the sedimentary rocks by inference alone through their relationship to the rare samples of igneous or primary rocks that are being dated.</i></b><br /><br /><i>When I pursued this question a little further, I found that there is in reality another factor that has been used to arrive at the age of the geological column and the fossils it contains -- conjecture. </i><b><i>This process crept into geological dating at a very early stage when Charles Lyell, the nineteenth century&#39;s most prominent geologist and Darwin&#39;s mentor in geological matters, attempted to date the end of the Cretaceous period by reference to how long he thought it would have taken the shellfish (whose fossils are found in later beds) to have evolved into their modern descendants.</i></b><i> Lyell estimated that the Cretaceous ended 80 million years ago -- not too far from today&#39;s accepted figure of 65 million, plus or minus 3 million.</i><br /><br /><i>According to Harold Levin of Washington University, </i><b><i>&#39;By comparing the amount of evolution exhibited by the marine molluscs in the various series of the Tertiary System with the amount that had occurred since the beginning of the Pleistocene Ice Age, Lyell estimated that 80 million years had elapsed since the beginning of the Cenozoic.&#39;</i></b><i> [15]</i><br /><br /><i>Levin adds that, &#39;He came astonishingly close to the mark.&#39; </i><b><i>In fact, it is not at all astonishing when you know that today&#39;s accepted date has been derived not from an absolute, independent source but from conjectures including Lyell&#39;s.</i></b><br /><br /><b><i>The kind of surmise used to supplement the relative dates yielded by radioactive dating includes assumptions about the rates at which sediments are laid down on the bottoms of lakes, seashores, and ocean floors; estimates of the rates at which forests are turned into coal deposits; and estimates of the rates at which certain very long-lived families of creatures might have evolved. But although these conjectures are embodied in the modern view of the age of geological deposits, they are rarely if ever disclosed in geological or biological textbooks, and they are rarely exposed to debate.</i></b><br /><br /><i>Curiously, too, no geologist seems to have checked out the geological column dates with an electronic calculator on a commonsense basis. Let us go back to the illustration of the column in Figure 1 and look again at the thickness of the rocks in each period compared with the length of time assigned to those periods. Note </i><b><i>that there is a remarkable consistency between assigned age and thickness of deposit.</i></b><i> For instance the Cretaceous period is said to have lasted 65 million years and is 15,000 meters thick -- an average annual rate of deposition of 0.2 millimeters. Now look at the Silurian period: this, too, yields an average rate of deposition of about 0.2 millimeters per year -- as does the Ordovician, the Devonian, the Carboniferous, and the rest. It is only when we come to relatively modern times in the Cenozoic era that rates of deposition vary much, and here they appear to speed up slightly.</i><br /><br /><i>This is a very remarkable finding. One naturally expects Uniformitarian geology to favor uniformity, but this is too much of a good thing. Throughout widely changing climatic conditions, advancing and retreating oceans, droughts, and Ice Ages, </i><b><i>the rate of sedimentation appears to remain amazingly constant regardless throughout the thousands of millions of years that are said to have elapsed</i></b><i>. The presumed rate of deposition itself -- about the thickness of a human hair in a year -- is a matter looked at in more detail later. But it is worth pausing in passing to note that such a slow rate would be quite incapable of burying and fossilizing entire forests, dinosaurs, or even a medium-sized tadpole.</i><br /><br /><i>Of course, all these sediments, with their time capsule contents of fossilized creatures from the past, were laid down long after the Earth was formed and long after the decisive event took place in the chain of evolution -- the origin of life itself in ancient seas. It is the rock from which those later sediments were derived -- the primary bedrock of the Earth&#39;s crust -- in which we are chiefly interested if we wish to date the Earth.</i><br /><br /><i>The key question remains: How old is the Earth? And to examine the answer that has come to be accepted on this score, we must look more closely at radioactive methods of dating.</i><br /><br /><i>CHAPTER 4</i><br /><i>The Key to the Past?</i><br /><br /><i>IN THE YEARS FOLLOWING THE SECOND WORLD WAR, American chemist Willard Libby made a discovery that won him the Nobel prize for chemistry, which revolutionized the study of the Earth&#39;s prehistory, but which ultimately was to provide unexpectedly disconcerting evidence on the age of the Earth itself.</i><br /><br /><i>Libby&#39;s discovery was the now-famous radiocarbon method of determining the age of organic remains, which gave archeologists their first practical tool for routinely dating the past. At the time of its discovery and its first application to archeological sites around the world in 1949, the radiocarbon method appeared to confirm that humankind&#39;s past was indeed of great antiquity and that geologists and evolutionists had been perfectly justified in continually pushing further back in time the dawn of humanity.</i><br /><br /><i>Field archeologists in the 1950s, applying the new power given them by chemistry, confidently assigned absolute dates to early human prehistoric settlements with a precision that must have astounded their teachers of a generation before. The city of Jericho was said to have been a thriving human settlement 11,000 years ago, while Neolithic sites in Russia and Africa were dated as being well over 50,000 years old. The author of Encyclopaedia Britannica&#39;s article on prehistoric Mrica, for instance, says &#39;Radiocarbon dating suggests that the Earlier Stone Age may have lingered on until about 55,000 B.c.&#39;</i><br /><br /><i>The readiness of science today to accept a great antiquity for the Earth and humankind contrasts sharply with the attitude of scientists little more than a century ago. This radical change in outlook involved the overthrow of the old geological belief in a catastrophic origin for the rocks of the Earth&#39;s crust and its replacement by the modern uniformitarian theory -- the idea that the rocks have formed slowly over millions or billions of years.</i><br /><br /><i>At the time that Darwin set sail for South America in the Beagle in 1831, the Earth&#39;s age was reckoned merely in thousands of years, and not many thousands at that. One well-known early attempt to date the Earth is that of Archbishop James Ussher of Armagh, a noted Bible scholar who deduced through careful analysis of biblical texts that the Earth was created in 4004 B.C.. The Archbishop&#39;s finding was published in 1650 and soon after was added as a marginal notation to the Book of Genesis in the Authorized Version of the Bible where it remained until Victorian times, and can still be found occasionally today.</i><br /><br /><i>A contemporary of the Archbishop, Dr. John Lightfoot, Master of St Catherine&#39;s College and Vice Chancellor of Cambridge University, was able to endorse this date and indeed refine it with astounding precision. &#39;Man was created by the Trinity,&#39; wrote Dr. Lightfoot, &#39;on October 23rd 4004 B.C. at nine o&#39;clock in the morning.&#39; </i><b><i>As Ronald Millar has pointed out, only a Cambridge Vice Chancellor would have the audacity to assign the date and time of the creation to the beginning of the academic year.</i></b><i> [1]</i><br /><br /><i>A number of the influential geologists in Darwin&#39;s day were also clergymen whose religious views strongly influenced their scientific beliefs. This religious complexion to geology in the eighteenth and early nineteenth centuries -- an otherwise flourishing era for rationalist thinking in science -- influenced theories of rock formation and the age of the Earth in two important ways.</i><br /><br /><b><i>First, widespread acceptance of the biblical creation story contained in Genesis meant that the cleric-geologists neglected to question how the Earth began or how life originated because they believed they already had the answers to these questions. And second, the creation story constituted a ready-made theory to accommodate all their scientific observations (often meticulously detailed) thereby stifling the formation of any new theory when they discovered new evidence in the field.</i></b><br /><br /><i>When these researchers found thousands of feet of compacted mudlike sediments containing the bones of dead animals, the discovery was taken as clear evidence of Noah&#39;s flood described in the Bible. Hence the prevailing geological theory of the pre-Darwinian era was that of catastrophism -- the doctrine that the rocks of the Earth&#39;s crust were formed more or less simultaneously as a result of a divinely ordained Great Flood.</i><br /><br /><i>Some of the attempts by pre-nineteenth century geologists to fit their observations to biblical teaching appear obviously contrived and rather absurd from our perspective today. Swiss naturalist Johann Scheuchzer, who discovered some early vertebrate remains of a salamander around 1720, exhibited them widely as the remains of the imaginatively named Homo diluvii testis -- Man, a witness to the flood. (Some believe that Scheuchzer was seeking to turn an honest copper or two with his discovery, in which case we must blame the gullibility of his customers rather than the inadequacy of eighteenth century science.)</i><br /><br /><b><i>In general, though, the observations of nature made at this time were models of scientific accuracy and would do credit to any modern researcher. Unfortunately when the theory of catastrophism fell into disrepute after Darwin, many of the observations of the cleric-geologists were rejected as religiously inspired prescientific thinking: observations that did indeed support a catastrophic origin for many rocks. ... one observation of this type that was well known in Darwin&#39;s day may be mentioned now by way of example: the occurrence of &#39;graveyards&#39; of millions of land-dwelling (not marine) creatures who suffered death simultaneously.</i></b><i> [2]</i><br /><br /><b><i>Darwin and his supporters realized at an early stage that their theory demanded vast reaches of geological time to support the supposed microscopic changes in form from one generation to another.</i></b><i> Equally, evolutionists stood in need of a geological basis for this great antiquity -- a mechanism that worked slowly and gradually rather than one that worked suddenly and all at once. They rejected catastrophism and instead found the mechanism they sought in an idea taking shape among the new generation of secular geologists who asserted that sedimentary rocks (that is, fossilbearing rocks) were formed slowly by the same processes that can be seen on the ocean bottom today: the deposition of silt and sand that becomes cemented and compacted over millions of years to form successive strata of rock.</i><br /><br /><i>Under the reassuring-sounding label of uniformitarianism these ideas were actively promoted by secular geologists like James Hutton and later Charles Lyell, who was Darwin&#39;s coach on geological issues. </i><b><i>The uniformitarian doctrine is summed up in the famous phrase &#39;the present is the key to the past&#39;</i></b><i> -- a concept eagerly accepted by Darwinists as ready-made for their theory and one expounded on at length in Lyell&#39;s Principles of Geology, the primary geological work of the century, published between 1824 and 1833.</i><br /><br /><b><i>The important point to note here is that it was the imperative need for great antiquity that deposed catastrophism, rather than any new scientific discoveries or observations; it was a new way of looking at things, not a new piece of knowledge. But, superficially, the change in view seemed to be a shift away from naive belief in biblical tales of creation and flood, and toward a newly established scientific viewpoint. And those who continued to argue the case for a catastrophic origin of rocks were seen as merely making a last-ditch attempt to rescue the religious doctrine of the creation as told in Genesis.</i></b><br /><br /><b><i>Darwinists needed time, and lots of it: uniformitarians had the geological theory that demonstrated great antiquity. Geologists needed a firm foundation for the relative dating and correlation of the many sediments piled one on another in the past-the many strata of the geological column: Darwinists were able to supply the key to the stratigraphical succession of the rocks by comparative anatomy of the fossils contained in those strata, interpreted along evolutionist lines. Thus an unusual academic interdependence sprang up between the two sciences that continues to this day. A geologist wishing to date a rock stratum would ask an evolutionist&#39;s opinion on the fossils it contained. An evolutionist having difficulty dating a fossil species would turn to the geologist for help. Fossils were used to date rocks: rocks were used to date fossils.</i></b><br /><br /><i>A modern example of paleontologists using fossils to date rocks in a circular way is provided by one of the most famous of all North American dinosaur discovery sites: the rocks at Como Bluffs, Wyoming. I only regret that this example involves one of today&#39;s most innovative researchers, Robert Bakker of the University of Colorado. It was at Como Bluffs in the 1870s and 1880s that paleontologists such as Edward Cope and O. C. Marsh discovered more than 120 new species of dinosaur, including diplodocus and stegasaurus. The many strata exposed in the steep cliff at this seminal site have subsequently yielded many more specimens and they are still worked today by scientists from many universities.</i><br /><br /><i>&#39;Of the site, Robert Bakker says:</i><br /><br /><i>At a place like Como Bluffs you have layer after layer -- it&#39;s like getting a burst of frames from a motion picture of how the dinosaurs came, flourished and went extinct. At any one place in the world, you don&#39;t have the whole history of dinosaurs, in fact you don&#39;t have the whole history of one family of dinosaurs, you just have a little burst of fossils.</i><br /><br /><b><i>We don&#39;t yet have radioactive beds that can give us a nice hard number [on the age of the deposit]. But by comparing the fossils we get at the bottom of the section and at the top, it&#39;s about 10 million years. So all of this history is played out roughly over about 2 million dinosaur generations, 10 million chronological years.&#39;</i></b><br /><br /><b><i>Ironically, not only is there no radioactive basis for the dating of Como Bluffs, there is, as Robert Bakker says, not even a complete history of a single dinosaur family at the site. Yet we are given the confident assertion concerning the number of dinosaur generations and the number of years to which this sequence is equivalent, with no solid physical basis. No other scientific discipline would be permitted even to consider such procedures</i></b><i>, but when paleontologists date rocks by means of fossils, they do so with the authority of Charles Darwin himself.</i><br /><br /><b><i>This circular process ought to have aroused suspicion, if not among its practitioners then among scientists of related disciplines. In fact it went unremarked and unchallenged because the discovery and introduction of methods of dating based on radioactive decay in the early years of this century appeared amply to vindicate the Darwinist-uniformitarian view and to justify their interdependence.</i></b><br /><br /><b><i>In the last two decades, however, further research into these technical methods of dating has revealed a number of worrying inconsistencies in the now orthodox view of the Earth&#39;s age: radioactive dating techniques are far less reliable than was previously thought; the Earth could be much younger than has been supposed by Darwinists; and nothing like the billions of years required by evolution theory have elapsed since the Earth&#39;s formation.</i></b><br /><br /><i>The first clue that something may be amiss with the view of uniformitarian geology and its claim for an old Earth came paradoxically from the technique that seemed most to support that view -- Willard Libby&#39;s radiocarbon dating method. To appreciate exactly why the radiocarbon technique has had such unexpected consequences, it is necessary first to look at just how the technique was supposed to work.</i><br /><br /><i>Radiocarbon -- radioactive carbon 14 -- is a form of carbon created in the upper atmosphere by the bombardment of cosmic particles from space. As radioactive carbon dioxide it permeates the atmosphere and passes into the bodies of plants and animals through the food chain. To any plant or animal, carbon 14 is indistinguishable from the common carbon (carbon 12) which occurs naturally on Earth. </i><b><i>Radiocarbon is relatively rare, so of the total amount of carbon in the body of a plant or animal only a minute fraction is radiocarbon. What makes this tiny fraction useful for dating, argued Libby, is that the proportion of radiocarbon is the same for all living animals and plants the world over, and something that can readily be measured.</i></b><br /><br /><i>Radiocarbon begins to decay as soon as it is formed. When a quantity of radiocarbon is produced in the atmosphere, half of that amount will have decayed away (becoming nitrogen gas) in some 5,700 years. Half the remainder will decay in a further 5,700 years, and so on, until an immeasurably small residue remains. Once a plant or animal dies, it ceases to take in radiocarbon from the &#39;terrestrial reservoir&#39; or outside world, so the amount of radiocarbon in its body begins to dwindle through decay while the ordinary carbon remains unchanged. So, 5,700 years after a tree dies, it contains only half the proportion of radiocarbon to common carbon that exists in a living tree, and in the living world in general. After a total of 11,400 years, or two half-lives, it will  contain only one quarter the proportion in the outside world, and so on. </i><b><i>After about five half-lives, or roughly 30,000 years, only an immeasurably small residue remains and so the radiocarbon test is only good for dating remains younger than this natural &#39;ceiling.&#39;</i></b><br /><br /><i>To date an organic find (the test only works, of course, on the remains of once-living things, such as bones in a Neolithic burial, or Roman fence posts) it is only necessary to measure the amount of remnant radioactive carbon with a suitable counter and hence deduce when the specimen ceased to take in radiocarbon -- when it died.</i><br /><br /><i>The great value of the test is that only a tiny fragment of an irreplaceable papyrus or rare skull is needed because it is the proportion of radiocarbon to ordinary carbon that is measured and compared with the proportions that exist in the terrestrial reservoir or living world today. In the end the whole technique rests, therefore, on knowing with some precision the ratio of radiocarbon to common carbon in the terrestrial reservoir today, and it was for making these measurements as well as developing the dating technique that Libby was awarded the Nobel prize.</i><br /><br /><i>There is just one further factor of some importance for the test to work properly: </i><b><i>the standard mix of radiocarbon to ordinary carbon in the terrestrial reservoir must always have been the same throughout the lifetime of the test subject and in the years since its death.</i></b><i> Take the case of archeologists setting out to determine the age of a Neolithic woman whose burial chamber they discover. If there had been a lot more carbon 14 around during the life of this early woman, the reading from her bones will be falsely inflated -- she will appear a much more recent burial than she really was. Had there been a lot less radiocarbon around during her life, then the reading will appear falsely diminished and she will appear much older.</i><br /><br /><b><i>At the time that Libby and his co-workers were developing the new technique, in the 1940s, they had every reason to believe that the amount of carbon 14 in the world could not possibly have varied during the time that humankind had been on Earth simply because the Earth is of immense age, some 4,600 million years old. This great age stamps the radiocarbon technique with the seal of  respectability because of what Libby called the &#39;equilibrium value&#39; for the radiocarbon reservoir.</i></b><br /><br /><i>After the Earth was formed and acquired an atmosphere, there would be a 30,000 year transition period during which carbon 14 would be building up. </i><b><i>At the end of that period, the amount of carbon 14 created by cosmic radiation will be balanced by the amount of carbon 14 decaying away to almost zero. To use Libby&#39;s terminology, at the end of 30,000 years, the terrestrial radiocarbon reservoir will have reached a steady state.</i></b><br /><br /><b><i>Since the Earth, according to uniformitarian geology, is many, many times older than the 30,000 years needed to fill up the reservoir, then radiocarbon must unquestionably have attained equilibrium billions of years ago, and must have been constantly so throughout the few million years allotted to human history. To test this essential part of the theory, Libby made measurements of both the rate of formation and the rate of decay of radiocarbon. He found a considerable discrepancy in his measurements indicating that, apparently, radiocarbon was being created in the atmosphere somewhere around 25 percent faster than it was becoming extinct. Since this result was inexplicable by any conventional scientific means, Libby put the discrepancy down to experimental error.</i></b><i> [3]</i><br /><br /><i>During the 1960s, Libby&#39;s experiments were repeated by chemists who had been able to refine their techniques after a decade or so of experience. The experiments demand almost heroic measures since the amounts of radiation involved are very small (only a few atomic disintegrations per second) and because of the need to screen out all other sources of radiation that would contaminate the result. </i><b><i>The new experiments, though, revealed that the discrepancy observed by Libby was not merely experimental error -- it did exist. It was found by Richard Lingenfelter that &#39;There is strong indication, despite the large errors, that the present natural production rate exceeds the natural decay rate by as much as 25 percent</i></b><i> ... It appears that equilibrium in the production and decay of carbon 14 may not be maintained in detail.&#39; [4]</i><br /><br /><i>Other researchers have confirmed this finding, including Hans Suess of the University of Southern California, writing in the Journal of Geophysical Research [5] and V. R. Switzer writing in Science. [6]</i><br /><br /><i>Melvin Cook, Professor of Metallurgy at Utah University, has reviewed the data of Suess and Lingenfelter and has reached the conclusion that the present rate of formation of carbon 14 is 18.4 atoms per gram per minute and the rate of decay 13.3 atoms per gram per minute, </i><b><i>a ratio indicating that formation exceeds decay by some 38 percent</i></b><i>. [7]</i><br /><br /><i>The meaning of this discovery is described as follows by Cook: </i><b><i>&#39;This result has two alternate implications: either the atmosphere is for one reason or another in a transient build up stage as regards Carbon 14 ... or else something is wrong in one or another of the basic postulates of the radiocarbon dating method.&#39;</i></b><br /><br /><b><i>Cook has gone one step further by taking the latest measured figures on radiocarbon formation and decay and calculating from them back to the point at which there would have been zero radiocarbon. In doing so, he is in effect using the radiocarbon technique to date the Earth&#39;s own atmosphere. And the resulting calculation shows that, using Libby&#39;s own data, the age of the atmosphere is around 10,000 years!</i></b><i> [8]</i><br /><br /><i>To anyone who, like me, was brought up on a diet of uniformitarian geology and Darwinian theory and to any high-school pupil or college student who opens a standard geology textbook, the suggestion that life on Earth may have a history as short as 10,000 years inevitably appears preposterous. Surely, the radiocarbon method has been tested against artifacts of known age and has been thoroughly vindicated? Surely the technique has been widely adopted in archeology with excellent results? And surely any fundamental flaw in the methods would have been discovered years ago?</i><br /><br /><i>It is perfectly true that radiocarbon dating has been tried on objects whose age is independently known from archeological sources and scored some impressive early successes. One of the very first artifacts to be tested was a wooden boat from an Egyptian pharaonic tomb whose age was independently known to be 3,750 years before the present. Radiocarbon assay produced the date of between 3,441 and 3,801 years, a minimum error of only 51 years. But after this promising start, the method quickly ran into difficulties. </i><b><i>Anomalous dates were produced from later assays that showed that some living things may interact with parts of the reservoir that have been anomalously depleted of carbon 14 and thus appear to be much older than they really are.</i></b><br /><br /><b><i>In one of the most recent cases of anomalous dating, rock paintings found in the South African bush in 1991 were analyzed by Oxford University&#39;s radiocarbon accelerator unit which dated them as being around 1,200 years old. This finding was significant because it meant the paintings would have been the first bushman painting found in open country. However, publicity of the find attracted the attention of Joan Ahrens, a Capetown resident, who recognized the paintings as being produced by her in art classes and later stolen from her garden by vandals. The significance of incidents such as this is that mistakes can only be discovered in those rare cases where chance grants us some external method of checking the dating technique. Where no such external verification exists, we have simply to accept the verdict of carbon dating.</i></b><br /><br /><i>The position resulting from these anomalous discoveries was summarized by Hole and Heizer in their Introduction to Prehistoric Archaeology:</i><br /><br /><b><i>&#39;For a number of years it was thought that the possible errors ... were of relatively minor consequence, but more recent intensive research into radiocarbon dates, compared with calendar dates, shows that the natural concentration of Carbon 14 in the atmosphere has varied sufficiently to affect dates significantly for certain periods. Because scientists have not been able to predict the amount of variation theoretically, it has been necessary to find a parallel dating method of absolute accuracy to assess the correlation between Carbon 14 dates and the calendar.&#39;</i></b><br /><br /><i>The parallel dating method turned to in order to assess radiocarbon dating involves that strange tree the bristlecone pine, which grows at high altitudes in the mountains of California and Nevada and is the oldest living thing on Earth -- some specimens said to be 5,000 years old.</i><br /><br /><i>The bristlecone pine has been exploited by Charles Ferguson of Arizona University to develop the science of dendrochronology -- dating by tree rings. The tree is useful here because it lives to a great age and certain &#39;signature&#39; sequences of tree rings are said to be characteristic of specific years before the present, enabling a younger tree to be correlated with older trees (including dead ones) to stretch the tree-ring chronology further and further back. Cross-dating from one core sample to another by means of such signatures enabled Ferguson to construct a master chronology that spans a total of 8,200 years before the present. This has been used to check up on radiocarbon dating variations.</i><br /><br /><b><i>Hans Suess of the University of California in San Diego has radiocarbon dated the bristlecone pine samples of the master chronology and from this a table of deviation has been drawn up which in theory allows the inaccuracies of the radiocarbon method to be corrected for up to around 10,000 years ago.</i></b><br /><br /><i>Radiocarbon dating&#39;s inventor Willard Libby did not at first think that large deviations were possible. </i><b><i>&#39;When we developed the radiocarbon dating method,&#39; he said, &#39;we had no choice than to assume that the cosmic rays had remained constant, though obviously we hadn&#39;t the slightest evidence that this was so. But now we know what the variations were.&#39;</i></b><br /><br /><i>Hans Suess was able to show precisely how variations in the amount of cosmic radiation changed the amount of radiocarbon in the atmosphere and </i><b><i>his table indicates that by about 5,000 B.C., radiocarbon-derived dates are around 1,000 years too young.</i></b><br /><br /><b><i>&#39;Whatever the source of radiocarbon,&#39; says Libby, &#39;it mixes very rapidly with life on earth so we have a firm belief that the calibrations with the bristlecone pine apply worldwide.&#39;</i></b><br /><br /><i>Are archeologists happy with this result? In fact they appear rather confused by it. Before the bristlecone pine amendments, the dates given by radiocarbon dating had confirmed the widely held belief of diffusionists -- that culture had spread from Egypt and the Middle East via Mycenae and Crete westward into Europe and then Britain. However, the new chronology indicates that, for instance, the island of Malta was carving spiral decorations and erecting megalithic structures before the supposed cradle civilizations further east. Many archeologists are unhappy about this, but the chronology now has the authority of both Libby and the dendrochronological corrections of Suess&#39;s bristlecone pine deviation tables.</i><br /><br /><b><i>A further difficulty has more recently been introduced into the controversy because the fundamental principle on which dendrochronology is based -- that a tree ring forms each year -- has been  questioned.</i></b><i> R. W Fairbridge, writing on dendrochronology in Encyclopaedia Britannica&#39;s entry on the Holocene epoch says:</i><br /><br /><i>&#39;As with Palynology, certain pitfalls have been discovered in tree-ring analysis. </i><b><i>Sometimes, as in a very severe season, a growth ring may not form. In certain latitudes, the tree ring&#39;s growth correlates with moisture, but in others it may be correlated with temperature. From the climatic viewpoint these two parameters are often inversely related in different regions.&#39;</i></b><i> [9]</i><br /><br /><b><i>It is also possible for two tree rings to grow in a single year, when growth begins in spring but is later arrested by a period of unseasonal frosts and later starts up again.</i></b><br /><br /><b><i>These climatic variations presumably mean that a fresh set of correction tables will be needed to modify the bristlecone pine dates, although no one has yet devised a method of calibration for such tables.</i></b><i> But whatever the outcome of the debate between archeologists and radiocarbon chemists, the key question for chemistry is how to explain the observed discrepancy between the rate of production of carbon 14 and its rate of decay in the atmosphere. </i><b><i>Cook has suggested that one possible explanation of the discrepancy is that the atmosphere is still in nonequilibrium because the required 30,000 years have not yet elapsed since it was first formed.</i></b><br /><br /><b><i>Adherents of the old-earth theory have responded first by seeking to minimize the discrepancy -- claiming that it is &#39;around 10 percent&#39; when it is really as great as 38 percent -- and second by saying that the proportion of radiocarbon in the terrestrial reservoir may fluctuate over time and that we are currently going through a build-up phase. There is no scientific evidence to support this view</i></b><i> but to someone who already believes in an old earth, the conclusion seems self-evidently more reasonable.</i><br /><br /><i>But what reasonable alternative could there be? How could the Earth possibly be merely thousands of years old? How could science have gone so far wrong?</i><br /><br /><i>CHAPTER 5</i><br /><i>Rock of Ages</i><br /><br /><i>ONE DAY, MORE THAN TWENTY YEARS AGO, I picked up an apparently dull geology textbook and found my attention arrested by a single sentence. The book was called Prehistory and Earth Models and was by the professor of metallurgy at Utah University, Dr. Melvin Cook. [1] Cook, a physical chemist now in his eighties, is a world expert on high explosives and his textbook on explosives for mining is still a classic work of reference. Professors of metallurgy do not usually stir up trouble in the academic world, but what I had read in his geology book was more explosive than any text on TNT.</i><br /><br /><i>In his preface Cook wrote: </i><b><i>&#39;An attempt to publish a manuscript giving direct evidence for the short-time chronometry of the atmosphere and oceans entitled &#39;Anomalous Chronometry in the Atmosphere and Hydrosphere,&#39; not unexpectedly nor without some cause, met with considerable opposition and was not published.&#39;</i></b><br /><br /><i>Who on earth had prevented Dr. Cook from publishing his paper? I wondered. And what could a metallurgy professor have to say that was so heretical that someone wanted to prevent its publication? I found that his book contained scientific evidence and reasoned argument which showed that something was terribly wrong with the orthodox scientific view of methods of dating. </i><b><i>The most widely used methods, such as uranium-lead and potassium-argon, had been found to be seriously flawed, not merely in practice but in principle. In addition, the methods yielded dates so discordant as to make them unreliable.</i></b><br /><br /><b><i>Cook showed for example that if you used the uranium-decay method on the rocks of the crust you got the conventionally accepted age of over four thousand million years. But if you used the selfsame method on the atmosphere, you got an age of only a few hundred thousand years. He also showed that the entire amount of &#39;radiogenic&#39; lead in the world&#39;s two largest uranium deposits could be entirely modern. Clearly something was wrong.</i></b><br /><br /><b><i>When I dug deeper, I found that Cook was not a lone voice. Other papers by scientists in reputable scientific journals expressed similar doubts and findings. Funkhouser and Naughton at the Hawaiian Institute of Geophysics used the potassium-argon method to date volcanic rocks from Mount Kilauea and got ages of up to 3 thousand million years -- when the rocks are known to have been formed in a modern eruption in 1801. McDougall at the Australian National University found ages of up to 465,000 years for lava in New Zealand that is independently known to be less than 1,000 years old.</i></b><br /><br /><b><i>I eventually came to the alarming realization that although radioactive decay is the most stable source of chronometry we have today, it is badly compromised as a historical timekeeper, because it is not the rate of decay that is being measured but the amount of decay products left. For this reason, all radioactive methods of geochronometry are deeply flawed and cannot be relied on with any real confidence in this application.</i></b><br /><br /><i>At the end of the last chapter, I asked, How could science have gone so far wrong? The answer turns out to be that it is not science which has gone wrong, merely those scientists seeking to defend a single idea -- Darwinian evolution. </i><b><i>Science has proposed many methods of geochronometry -- measuring the Earth&#39;s age -- all of which are subject to some uncertainties, for reasons I shall describe in a moment. But of these many methods, only one technique -- that of the radioactive decay of uranium and similar elements -- yields an age for the Earth of billions of years. And it is this one method that has been enthusiastically promoted by Darwinists and uniformitarian geologists, while all other methods have been neglected.</i></b><br /><br /><b><i>So successful has this promotional campaign been that today almost everyone, including scientists working in other fields, has been led to believe that radioactive dating is the only method of geochronometry worth considering, and that it is well-nigh unassailable because of the universal constancy of radioactive decay. In fact, none of these widely held beliefs is supported by the evidence.</i></b><br /><br /><i>To appreciate how and why radiometric methods are flawed, first look a little more closely at the problems which confront the geologist attempting to measure the Earth&#39;s age.</i><br /><br /><i>All methods of measuring time, whether for domestic or scientific purposes, rely on the same basic principle: monitoring the rate of some constant natural process. Today our most sophisticated chronometric methods involve the rate at which a quartz crystal vibrates when an electric potential is applied to it, and the rate at which radioactive elements decay -- said to be the most constant source of all.</i><br /><br /><i>But having some readily available process to measure is not enough by itself. </i><b><i>To measure elapsed time accurately we must be sure that the process does in fact remain constant, even when we are not watching. You must know the starting value of the clock -- how much water was in your water clock to begin with or how tall your candle was before it was lit. And you must be sure that some external factor cannot interfere with the process while it is in operation, for instance, that a temporary power cut does not stop your electric clock while you are out walking your dog.</i></b><br /><br /><i>All these conditions apply to measuring time today. When it comes to the science of geochronometry, the process we choose will have started in prehistoric times, which we have no method of directly observing and verifying. This means we must make sure as far as possible that our three conditions were met in the past as well as in the present -- and it is here that our problems begin.</i><br /><br /><i>Suppose, for instance, we were to take the increasing salinity of the oceans as a means of finding out how old the Earth is (a method actually proposed in 1898 by Irish geologist John Joly). On the face of it this is a promising method, since it can be assumed that initially the oceans consisted of fresh water, and the present-day accumulation of salt is due to erosion of land masses by rainfall and the subsequent transport of dissolved salt into the seas by way of the world&#39;s rivers. Even more encouraging is the fact that the rate of erosion of the land by rainfall is surprisingly constant each year -- about 540 million tons of salt a year. All that would be necessary is to measure the present-day concentration of salt in the sea (32 grams per litre); calculate from this the total amount in all the oceans (about 5 x 1016 tons); and divide this total by the annual amount of salt deposited to get the age of the Earth in years. [2]</i><br /><br /><i>Using this method, Joly came up with an age of 100 million years. Unfortunately, when we apply the three conditions mentioned earlier to this method its shortcomings quickly become obvious. </i><b><i>First, we cannot be sure that the annual runoff of dissolved salt has always been constant.</i></b><i> Indeed there is good reason to suppose that climatic conditions have been very different in the past -- with ice ages and major droughts for instance -- and these conditions might have had an effect that is incalculable.</i><br /><br /><b><i>Second, we cannot be quite sure that there was zero salt in the sea to begin with.</i></b><i> Initially, some salt might have been present, though no one can say how much, if any. (Recent research in the Atlantic suggests that salts may have been extruded into ocean basins from the molten magma beneath the crust.) </i><b><i>And third, it turns out that an apparently constant process is interfered with by external factors.</i></b><i> Large amounts of salt are recirculated into the atmosphere, and recent evidence suggests that the salt in the sea might actually be in a steady state -- as fast as salt is deposited in the sea, it is picked up in the air and redeposited on land again. A large quantity of salt is evaporated by biological processes and still more is incorporated into bottom sediments through chemical processes, spoiling our &#39;clock.&#39;</i><br /><br /><b><i>All methods of measuring the age of the Earth are subject, to some extent, to the same defects -- quite simply, no one was there at the time to check up on our three criteria.</i></b><i> The technique used by uniformitarian geologists to arrive at the tremendous age of 4,600 million years for the Earth is usually referred to simply as the &#39;uranium&#39; or &#39;uranium-lead&#39; method. Sometimes it is popularly referred to merely as radioactive or radiometric dating. The technique in question covers a family of methods involving the radioactive decay of a number of different metallic elements with very long half-lives (they stay radioactive for very long periods). These elements include uranium and its sister element thorium, which both decay into helium and lead; rubidium, which decays into strontium; and potassium, which decays into argon and calcium.</i><br /><br /><i>The basic principle is this: over very long periods of time uranium spontaneously decays into lead and helium gas. The rate of decay is remarkably constant. The atoms of the uranium are unstable and periodically throw out an alpha particle, which is the nucleus of an atom of helium. It is impossible to tell in advance when any particular atom will break apart in this way since the process occurs at random. But in any substantial mass of the mineral there will be many billions of atoms, and with very large numbers of events the &#39;law of large numbers&#39; operates to produce a statistically predictable result.</i><br /><br /><b><i>The important part of the theory is that the kind of lead into which uranium eventually decays is chemically distinctive from common lead already present in the rocks, and is referred to as radiogenic lead, a daughter product of the decay process. Common lead is an isotope called lead 204, while the decay product of uranium 238 is lead 206. In order to date a rock deposit a sample is taken and the amount of radioactive uranium, together with the amount of radiogenic lead it contains, is accurately assayed in the laboratory. Since the rate of decay is known from modern measurements, it is possible to calculate directly how long the uranium has been decaying -- how old the deposit is -- by how much radiogenic lead it has turned into.</i></b><br /><br /><i>The half-life of uranium 238 (one of the principal isotopes used) has been calculated to be 4,500 million years. To take a simplistic example, if the assay showed that a deposit was composed of half uranium 238 and half its daughter product lead 206, then one would draw the conclusion that the deposit was 4,500 million years old. (This, incidentally, is the average figure that is found for the Earth&#39;s crust although the figure is arrived at by extrapolation rather than direct measurement.)</i><br /><br /><i>On the face of it, uranium decay seems an ideal method of geochronometry, and above scientific suspicion. But, as in the case of radiocarbon dating, research in recent decades has begun to cast serious doubts on its reliability.</i><br /><br /><i>The first criterion for any method of geochronometry is that we must know the starting value of the process we are measuring; we must have a point of departure, or reference point, from which to make our calculations. On the face of it, uranium decay fulfills this requirement since the type of lead which results is said to be uniquely formed as a by-product of this process. </i><b><i>If radiogenic lead -- lead 206 and lead 207 from uranium, and lead 208 from thorium -- really is uniquely formed as the end product of disintegration, then it is perfectly reasonable to suppose, as adherents of radioactive dating do, that there was zero radiogenic lead in the rocks of the Earth&#39;s crust when they first formed, and so we have a reliable starting point for our calculations.</i></b><i> The same argument can be used to make us reasonably certain that no radiogenic lead could have intruded into the rocks by some other means, thus distorting the effects of the decay process.</i><br /><br /><b><i>But things are by no means as simple as they seem when investigated a little more closely. Cook has suggested there is another, and quite separate, mechanism by which common lead can be transmuted into a form which, on assay, will be indistinguishable from &#39;radiogenic&#39; lead. This transmutation can occur through the capture of free neutrons -- atomic particles with enough energy to transmute common lead into so-called radiogenic lead. Where, though, could such a source of free neutrons be found? The answer is in a radioactive ore deposit such as uranium, where they occur through spontaneous fission!</i></b><br /><br /><b><i>In other words, the very process being measured can be moonlighting at another job. As well as spontaneously decaying into radiogenic lead, it is also making available a supply of particles which are simultaneously converting common lead into another isotope which, on being assayed, will be indistinguishable from a radiogenic product of alpha decay. Significantly, this is a mechanism that would tip our measurements in favor of an &#39;old&#39; Earth. Too much &#39;radiogenic&#39; lead would lead us to imagine that the process has been going on for much longer than it actually has.</i></b><br /><br /><b><i>In the neutron capture process, the isotopic values of lead would be systematically changed: lead 206 would be converted into lead 207, and lead 207 into lead 208. Interestingly, lead 208 usually constitutes more than half the lead present in any given deposit. This is normally interpreted as meaning that thorium, the parent element of lead 208, was very common in the deposit in question, although it could also be interpreted as indicating that free neutron capture is a far more important process in lead isotope formation than radioactive decay.</i></b><br /><br /><b><i>In Prehistory and Earth Models, Cook examined the lead content of two of the world&#39;s largest uranium ore deposits -- in Zaire and Canada. He found that they contained practically no Thorium 232. However they do contain significant amounts of lead 208. This could have been derived only from lead 207 by neutron capture, says Cook, while all the so-called radiogenic lead can be accounted for on the same basis and the mineral deposits could be essentially of modern origin.</i></b><i> [3]</i><br /><br /><b><i>Because Cook is a creationist as well as a scientist and because creationists have used Cook&#39;s findings as ammunition for their cause, strenuous attempts have been made by some scientists, such as G. Brent Dalrymple, a geologist with the U.S. Geological Survey, to discredit him and his research. [4] So far, however, neither Dalrymple nor any other dating advocate has offered a satisfactory explanation for the finding that there is practically no thorium 232 in the world&#39;s two largest uranium deposits, but that there are significant quantities of lead 208.</i></b><br /><br /><b><i>Dalrymple and others have asserted that the level of free neutrons available is too low to be capable of causing any significant change in the ratio of lead isotopes in deposits such as these. But if that assertion is correct then it becomes impossible to account on any rational basis for the quantities of lead 208 in Zaire and Canada.</i></b><br /><br /><i>So uranium decay fails the most important criterion for a reliable method of geochronometry. But it also fails a second criterion -- that we must be reasonably sure no outside agency can interfere with the smooth running of our chosen process. Uranium does not naturally occur in metallic form but as uranium oxide. This material is highly soluble in water and is known to be moved away from its original deposit in large quantities by ground waters. The type of effect this has on dating is unpredictable since some parts of a mineral deposit can be unnaturally enriched while others are unnaturally depleted.</i><br /><br /><b><i>There is one further discovery relating to uranium dating that is of considerable relevance to attempts to measure the age of the Earth. As mentioned earlier, the final disintegration products of the decay process are two, not only lead but also helium gas. Like the lead which results from the decay process, the helium is also a radiogenic daughter product with an atomic weight of 4. In fact almost the entire amount of helium in the Earth&#39;s atmosphere is believed to be radiogenic helium, formed during the decay process throughout most of the Earth&#39;s history.</i></b><br /><br /><b><i>Now, if the uranium-lead dating technique were reliable, then the amount of this radiogenic helium in the atmosphere would yield a date for the Earth&#39;s age consonant with that yielded by measuring the amount of radiogenic lead in the crust. In fact, the dates are so different as to be irreconcilable.</i></b><br /><br /><b><i>If the Earth were 4,600 million years old, then there would be roughly 10,000 billion tons of radiogenic helium 4 in the atmosphere. Actually, there are only around 3.5 billion tons presentseveral thousand times less than there should be (0.035 percent to be precise).</i></b><br /><br /><i>Writing in Nature on the &#39;mystery&#39; of the Earth&#39;s missing radiogenic helium, Melvin Cook says,</i><br /><br /><i>&#39;At the estimated 2 x 10^20 gm uranium and 5 x 10^20 gm thorium in the lithosphere, helium should be generated radiogenically at a rate of about 3 x 10^9 gm/yr. Moreover the (secondary) cosmic-ray source of helium has been estimated to be of comparable magnitude. Apparently nearly all the helium from sedimentary rocks and, according to Keevil and Hurley, about 0.8 of the radiogenic helium from igneous rocks, has been released into the atmosphere during geological times (currently taken to be about 5 x 10^9 yr). Hence more than 10^20 gm of helium should have passed into the atmosphere since the &#39;beginning.&#39; Because the atmosphere contains only 3.5 x 10^15 gm helium 4, the common assumption is therefore that about 10^20 gm of helium 4 must also have passed out through the exosphere, and that the present rate of loss through the atmosphere balances the rate of exudation from the lithosphere. &#39;[5]</i><br /><br /><b><i>Cook says that uniformitarian geologists have attempted to explain this discrepancy by assuming that the other 99.96 percent has escaped from the Earth&#39;s gravitational field into space -- but this process has not been observed.</i></b><br /><br /><i>G. Brent Dalrymple has rebutted Cook&#39;s claim by suggesting a mechanism that might account for the missing helium 4. In his 1984 Reply to &#39;Scientific Creationism,&#39; Dalrymple says,</i><br /><br /><i>&#39;Banks and Holzer (12) have shown that the polar wind can account for an escape of 2 to 4 x 10^6 ions/cm^2 .sec of helium 4, which is nearly identical to the estimated production flux of (2.5 ± 1.5) x 10^6 atoms/cm^2.sec.&#39; [6]</i><br /><br /><i>There are two things that make Banks and Holzer&#39;s findings unsuitable for the purposes to which Dalrymple tries to fit them. The first is that </i><b><i>the figure he cites for escape may be great enough to account for the production whose figures he gives, but that is only because he has selected a low estimate for production</i></b><i>. In reality the escape rates he cites are not remotely great enough to account for the amount of helium 4 that must have been created and lostremember we are looking for more than 10^20 grams of missing helium. </i><b><i>This means that if the Earth really is 4,500 million years old, then its atmosphere would have to lose helium at a rate somewhere around 10^16 atoms/cm2.sec., or some ten orders of magnitude faster than Dalrymple&#39;s figure, to account for the missing helium.</i></b><br /><br /><b><i>The second objection is that the figures he uses come from a time (nearly 30 years ago) when most space scientists assumed that the Earth was moving through the vacuum of space -- that the atmosphere was surrounded by nothing but empty space. At that time it was believed that light hydrogen and helium atoms would either escape or be dislodged into the void.</i></b><br /><br /><b><i>More recent studies have suggested that far from losing helium, the atmosphere may actually be gaining quantities of this gas. As it orbits the Sun, the Earth moves not through empty space but through a thin solar atmosphere, which consists principally of hydrogen and helium resulting from nuclear processes within the Sun. Measurements in the upper atmosphere have suggested that the Earth is gaining helium by this means.</i></b><br /><br /><i>In his 1987 book Gaia: A New Look at Life on Earth, space scientist James Lovelock writes,</i><br /><br /><i>&#39;The outermost layer of the air, so thin as to contain only a few hundred atoms per cubic centimeter, the exosphere, can be thought of as merging into the equally thin outer atmosphere of the sun. </i><b><i>It used to be assumed that the escape of hydrogen atoms from the exosphere gave the Earth its oxygen atmosphere. Not only do we now doubt that this process is on a sufficient scale to account for oxygen, but we rather suspect that the loss of hydrogen atoms is offset or even counterbalanced by the flux of hydrogen from the sun&#39;</i></b><i> [7]</i><br /><br /><b><i>Of course, Lovelock is writing about hydrogen not helium. However, helium is four times heavier than hydrogen and it is plentiful in the Sun&#39;s atmosphere since it is the principal product of the Sun&#39;s nuclear fusion process. If hydrogen is not lost but gained, then the same will be true for helium.</i></b><br /><br /><i>If we take the measured amount of helium 4 in the atmosphere and apply the radioactive dating technique to it, says Cook, we find that the calculation yields an age for the Earth of around 175,000 years. This procedure fails our criteria of reliability in that the possible acquisition of helium 4 from outside upsets the process.</i><br /><br /><b><i>The only conclusion that can be safely drawn from the discordance between the uranium-lead and uranium-helium dates is that this form of radioactive dating is unreliable.</i></b><br /><br /><i>What about the dating techniques based on other radioactive elements referred to earlier? </i><b><i>The methods based on decay of potassium to argon and rubidium to strontium are also subject to some of the defects already described, as well as having specific problems of their own.</i></b><br /><br /><i>Potassium minerals are commonly found in many rocks. Potassium 40 decays by capturing an electron and turning into the gas argon 40, with a half-life of 1.3 billion years.</i><br /><br /><i>Advocates of the potassium-argon method claim that the argon gas that results from the decay of potassium 40 remains trapped in the crystal structures of the mineral in which it forms -- &#39;like a bird in a cage,&#39; to use Brent Dalrymple&#39;s phrase -- and accumulates through the ages, thus acting as a clock when the stored daughter isotope is released and measured.</i><br /><br /><b><i>The potassium-argon method is suspect because the end product used for assay, argon 40, is a very common isotope in the atmosphere and the rocks of the Earth&#39;s crust. Indeed, argon is the twelfth most abundant chemical element on Earth and more than 99 percent of it is argon 40.</i></b><i> There is no physical or chemical way to tell whether any given sample of argon 40 is the residue of radioactive decay or was present in the rocks when they formed. Moreover, as argon is an inert gas that will not react with any other element, its atoms will always be trapped in the crystal structures of minerals whether it is radiogenic in origin or not. Cook has calculated that even if the Earth were five billion years old, no more than 1 percent of the argon 40 currently present on Earth could be a radiogenic daughter product and it is thus highly probable that some of the argon 40 in all potassium minerals has been derived directly rather than as a result of decay. [8]</i><br /><br /><i>So, if radiogenic argon 40 is like &#39;a bird in a cage,&#39; then it is a cage that already contains birds of the same feather, from which it is indistinguishable.</i><br /><br /><b><i>The possibility of anomalous inclusion of argon is not merely conjecture but is borne out by numerous studies of volcanic rocks that have resulted in false dates. Even modern volcanic lava formed in recent historical times has been dated as up to 3 billion years old by the potassium-argon method.</i></b><br /><br /><i>According to Noble and Naughton of the Hawaiian Institute of Geophysics:</i><br /><br /><b><i>&#39;The radiogenic argon and helium contents of three basalts erupted into the deep ocean from an active volcano (Kilauea) have been measured. Ages calculated from these measurements increase with sample depth up to 22 million years for lavas deduced to be recent. Caution is urged in applying dates from deep-ocean basalts in studies on ocean-floor spreading.&#39;</i></b><i> [9]</i><br /><br /><b><i>A similar study of Hawaiian basaltic lava actually dating from an eruption in 1801, near Hualalei, came up with potassium-argon dates ranging from 160 million years to 3 billion years. [10] In 1969, McDougall of the Australian National University measured the ages of lava in New Zealand and got an age of 465,000 years whereas the carbon dating of wood included in the lava showed it to be less than 1,000 years old.</i></b><i> The suspected reason for the anomalous ages was the incorporation of environmental argon 40 at the time of the eruption, and the inheritance of argon 40 from the parent magma.</i><br /><br /><i>As well as the anomalous inclusion, or gain, of argon 40, </i><b><i>it is also possible for mineral samples to become anomalously depleted of the gas if the rocks from which the sample comes have been heated after formation</i></b><i>, for instance by further volcanic activity. Such disturbed samples will yield incorrect dates if a simple accumulation clock method is applied to them.</i><br /><br /><b><i>Dating advocates, such as Dalrymple, accept that potassium-argon methods can be flawed but claim that they know the occasions on which the results are correct and when they are incorrect: &#39;Like all radiometric methods, the potassium-argon method does not work on all rocks and minerals under all geologic conditions. By many experiments over the past three decades, geologists have learned which rocks and minerals act as closed systems and under what geologic conditions they do so.&#39;</i></b><i> [11]</i><br /><br /><b><i>The problem with this widely held belief is that there is no truly independent means of verifying the age of any given sample (other than the very exceptional cases mentioned above). And the experiments to which Dalrymple refers have consisted solely of rejecting dates that seem wrong while accepting those that seem right, &#39;seem&#39; in this context meaning in line with uniformitarian expectations, thus compiling a database of self-fulfilling predictions.</i></b><br /><br /><b><i>Radiogenic strontium -- strontium 87 -- occurs in rocks as a result of decay of radioactive rubidium. However, this technique is again complicated by the fact that strontium 87 also occurs both as a daughter product of radioactive decay and as a commonly occurring element in its own right. Typically, rocks contain ten times more common strontium 87 than radiogenic strontium 87. Rubidium-strontium is also suspect because it is subject to exactly the same neutron capture process as uranium-lead. This time it is strontium 86 that can be transformed to strontium 87.</i></b><br /><br /><b><i>Most disconcerting of all is the fact that these various methods of dating commonly produce discordant ages for the same rock deposit. Where this occurs, a &#39;harmonization&#39; of discordant dates is carried out -- in other words, the figures are adjusted until they seem right. The chief tool employed to harmonize discordant dates is the simple device of labeling unexpected ages as anomalous and, in the future, discarding those rock samples that will lead to the &#39;anomalous&#39; dates. This practice is the explanation of why many dating results seem to support each other -- because all samples that give ages other than expected values are rejected as being &#39;unsuitable&#39; for dating.</i></b><br /><br /><i>If radioactive dating is seriously flawed as claimed here, why is it so enthusiastically embraced by dating scientists and so readily accepted by their academic colleagues?</i><br /><br /><i>On the face of it, radioactive dating is the most accurate source of chronometry available. Indeed, our most trustworthy timepieces are atomic clocks: clocks regulated by precisely the same processes used in dating techniques. And because radioactive decay is the most stable process known, then it appears that methods of geochronometry based on radioactive decay must themselves be the most accurate methods.</i><br /><br /><i>This widely held view fundamentally misrepresents the true nature of radioactive decay geochronometry. The accuracy of such techniques is not only critically dependent on the constancy of the rate of decay, but it is even more critically dependent on the accurate assay of the residue of the decay process -- how much argon 40 is left or how much strontium 87 is left -- and how that residue is distinguished from the nonradiogenic argon 40, or strontium 87, that occurs naturally in the same rocks.</i><br /><br /><i>This issue has nothing to do with how constant radioactive decay processes may be: it is purely a human problem in measurement. If the scientist conducting the experiment fails to measure the residue accurately, the age he gets will be distorted by an unknown number of years.</i><br /><br /><i>But how is it possible that dozens of scientists around the world involved in dating techniques could all be misled on such fundamental matters? How could so many scientists be wrong?</i><br /><br /><b><i>I believe there are at least four ways in which dating scientists could mislead themselves: ways that may be transparent to them, and which could lead them to obtain comparable results apparently independently.</i></b><br /><br /><b><i>First, there is the untestable error. When errors in radiometric dates are pointed out by critics, advocates of the method usually dismiss such criticism on the grounds that errors are very rare in comparison with the thousands of dates that are not found to be incorrect. This is a misleading argument because the overwhelming majority of dates could never be challenged or found to be flawed since there is no genuinely independent evidence that can contradict those dates. The reason why known anomalies are very rare is simply because independent evidence is very rare.</i></b><br /><br /><b><i>What is alarming is that in the very few cases of truly independent evidence we have -- such as Mrs. Ahrens&#39;s rock paintings, and the volcanic lavas in Hawaii and New Zealand -- the measured dates are spectacularly wrong. The response of radioactive dating advocates is to reject the few cases of independent verification as aberrations, and to prefer instead their theory purely because of its internal consistency, principally that it fits with a belief in an old Earth. In doing so, they are rejecting the only real independent check available.</i></b><br /><br /><i>Second, there is the phenomenon of &#39;ballpark&#39; thinking. This is exemplified by the error that was made in the curvature of the mirror for the Hubble space telescope. The error was not discovered by normal inspection processes, even in one of the world&#39;s best-equipped laboratories, because it was so big -- more than a centimeter out -- that it was outside the range that anyone was mentally prepared to check on. Had it been a millionth of a meter out, it would have been spotted at once.</i><br /><br /><b><i>Ever since Charles Lyell estimated that the end of the Cretaceous was 80 million years ago, the accepted value has been in this ballpark. Any dating scientist who suggested looking outside the ballpark, at 20 million years or 10 million or 5 million, would be looked on as a crackpot by his colleagues. More significantly, perhaps, he would not be able to get any funding for his research.</i></b><br /><br /><b><i>A third potential source of error is the phenomenon of&#39;intellectual phase-locking.&#39; It is not widely realized that the published value of physical constants often varies. Before it was settled internationally by definition, the measured value for the velocity of light varied considerably, as did the gravitational constant and Planck&#39;s constant. One reason for such variation is that all scientists make experimental errors that they have to correct. They naturally prefer to correct them in the direction of the currently accepted value thus giving an unconscious trend to measured values. This group thinking has even been given a name: &#39;intellectual phase-locking.&#39;</i></b><br /><br /><b><i>Fourth, there are powerful professional pressures on scientists to conform to a consensus. Dating geologists are offended by the suggestion that their beliefs can or would influence the dates obtained.</i></b><i> Yet nothing could be easier or more natural. Take for example a rock sample from the late Cretaceous, a period which is universally believed to date from some 65 million years ago. Any dating scientist who obtained a date from the sample of, say, 10 million years or 150 million years, would not publish such a result because he or she will, quite sincerely, assume it was in error. On the other hand, any dating scientist who did obtain a date of 65 million years would hasten to publish it as widely as possible. </i><b><i>Thus the published dating figures always conform to preconceived dates and never contradict those dates. If all the rejected dates were retrieved from the waste basket and added to the published dates, the combined results would show that the dates produced are the scatter that one would expect by chance alone.</i></b><br /><br /><i>Dating scientists have looked for a technique that would enable them to eliminate the problems of the simple accumulation clock method caused by inclusion or depletion of daughter isotopes. They believe that they have found such a technique in the idea first proposed by L.O. Nicolaysen of Witwatersrand University in 1961 and which is usually called the Isochron technique.</i><br /><br /><b><i>Geologists said to themselves, if we can find a way of using not just a single isotope, but of linking together several isotopes, and if we get a concordance of ages when we measure the linked group, then we can have a high level of confidence that the age we obtain is real and not a disturbed date. The main reason for believing this is that both of the two disturbing phenomena -- inclusion or depletion of daughter isotope -- will affect the different isotopes in a rock sample differentially, so they can no longer be made to lie on the same straight line when their ages are plotted on an Isochron graph.</i></b><br /><br /><b><i>On the face of it, the Isochron technique solves the basic problem of the simple accumulation clock method. In reality, it solves it only in a single limiting case -- the case where all daughter isotopes are measured with perfect accuracy. If there is any systematic reason why the assay of the daughter isotopes is flawed, then the Isochron method is worse than useless -- it is actively misleading, because it will cause geologists to place a high level of confidence in results that are actually false.</i></b><br /><br /><b><i>But, of course, the whole problem with radiometric methods is the difficulty of making accurate assays of the daughter isotopes coupled with the fact that there are a number of pressures compelling geologists to arrive at certain acceptable target dates and reject unacceptable dates in their published results.</i></b><br /><br /><b><i>In reality the apparent concordance of some of the dates derived by Isochron radiometric techniques is an artifact of two influences: the selection of &#39;suitable&#39; rock samples for assay and the rejection of &#39;unsuitable&#39; samples; and the selection of only some dates for publication and the nonpublication of others as being erroneous.</i></b><br /><br /><i>That the Isochron technique does not, in practice, provide the high level of confidence that some geologists attribute to it can be seen in the case history examined later in this chapter.</i><br /><br /><i>In evaluating the strength of the evidence and arguments against radiometric dating, the sticking point for many reasonable people is that a great age for the Earth -- in the region of 4,500 million years -- seems securely arrived at, whatever lesser problems may remain to be ironed out in radiometric dating techniques. Yet, as Melvin Cook has pointed out, the Earth may be made of materials that are 4,500 million years old and yet still have been formed relatively recently. Even if dates for meteorites and other celestial bodies such as the Moon and Mars could reliably tell us the age of the materials comprising the solar system, they still cannot tell us when the Earth itself was formed.</i><br /><br /><b><i>No part of this book has attracted such heated and vigorous rebuttal as this chapter on the flawed nature of radiometric dating. Advocates of radiometric dating have said that it is wrong of me to charge that discordant dates can be derived for the same deposit by different radiometric methods, wrong to say that such discordant dates are harmonized in the laboratory and wrong to say that dating scientists would be confused by the anomalous presence, or absence of, for example, nonradiogenic argon 40. One critic wrote to say that it is &#39;dishonest&#39; of me to include examples such as the modern Hawaiian lavas. &#39;This is the sort of thing that is allowed for in radiometric dating,&#39; he told me indignantly. Another critic wrote and told me that the use of &#39;Isochron&#39; techniques for radiometric dating ensures that spurious dates would be eliminated and lead to a high level of confidence in radiometric dates.</i></b><br /><br /><b><i>These beliefs are no doubt sincerely held, but to show just how misguided they are, let me give a brief summary of one episode -- involving some of the world&#39;s most distinguished isotope-dating laboratories -- that embraced all the dating errors referred to above, despite every precautionary measure and attention to detail.</i></b><br /><br /><i>Paleontologists have made many important discoveries of human bones and tools at Lake Turkana (formerly Lake Rudolph) in Kenya. Among the deposits in which important finds have been made are those marked by a layer of volcanic ash or tuff identified by Kay Behrensmeyer of Harvard and which has become known as the KBS (Kay Behrensmeyer Site) Tuff.</i><br /><br /><i>From as long ago as 1967, when Richard Leakey began making finds there, it became important to try to date the KBS Tuff. Although it is volcanic and hence promising for the potassium-argon method, the deposit is not &#39;juvenile&#39; or original but has been transported by water and laid down as a sedimentary rock. It thus contains some foreign material including much older particles that could give an anomalous date -- a fact which geologists who have dated it have recognized and which they have dealt with by selecting suitable juvenile particles to date.</i><br /><br /><i>In 1969, F. J. Fitch of Cambridge and J. A. Miller of Birkbeck College, London, dated the KBS Tuff as &#39;very close to 2.6 million years old.&#39; [12] This had important implications later because when Richard Leakey found a very rare human skull below the KBS Tuff, he was able to say that it was found below rock that was &#39;securely dated&#39; at 2.6 million years ago. [13]</i><br /><br /><i>In 1976 Nature carried a second article by Fitch, Miller, and Hooker. They had refined their 1969 date using a more accurate constant of decay and found an age of 2.42 million years ago. In the same paper, the authors referred to &#39;a small programme of conventional total fusion potassium-argon age determinations on East Rudolf pumice samples undertaken at Berkeley.&#39; [14]</i><br /><br /><i>The experiments they referred to were conducted by G. H. Curtis and colleagues at the University of California at Berkeley who, using potassium-argon dating, came up with dates of 1.6 and 1.82 million years for the KBS Tuff -- a discrepancy with Fitch&#39;s results ranging from half a million years to close to a million years. [15]</i><br /><br /><i>Commenting on the discordant dating, Fitch said, &#39;Potassiumargon apparent ages in the range 1.6 - 1.8 million years obtained from the KBS Tuff by other workers are regarded as discrepant, and may have been obtained from samples affected by argon loss.&#39;</i><br /><br /><b><i>What is especially interesting about these results is that both teams used Isochron methods -- the methods that are claimed to ensure mistakes cannot be made simply because of anomalous loss or gain of argon, as in the Hawaiian lavas. Thus Fitch was alleging that the Berkeley team had got their sums wrong precisely because they failed to allow for argon loss -- the very fault that my critic assured me was &#39;the sort of thing that is allowed for in radiometric dating.&#39;</i></b><br /><br /><i>Perhaps because the issue of discordance had become public, Fitch went even further in his Nature paper and disclosed that the Berkeley group reported &#39;scatter&#39; in their dates ranging from 1.5 to 6.9 million years, a range large enough to cast some doubts on the accuracy of their work. By comparison, in their own experiments, Fitch and his colleagues claimed much lower &#39;scatter&#39; in apparent ages ranging from 0.5 to 2.4 million years implying that their measurements were more accurate. [16]</i><br /><br /><i>The controversy was brought to a close in 1981 by an argon-40-to-argon-39 study by Ian McDougall of the Australian National University, giving a date of 1.88 million years. </i><b><i>As this was halfway between the two previous discordant studies, the combatants decided to call it a day -- even though it meant they were both wrong by a large margin.</i></b><i> [17]</i><br /><br /><b><i>In his paper McDougall frankly confessed that &#39;conventional potassium-argon, argon-argon and fission track dating of pumice clasts within this tuff have yielded a distressingly large range of ages.&#39;</i></b><i> [18]</i><br /><br /><i>Indeed, McDougall went even further than this rare emotive statement, because he revealed that the &#39;scatter&#39; referred to by Fitch was in reality even greater than that of Curtis. Fitch and Miller actually reported results of ages ranging from 0.52 to 2.64 million years for one set of samples and ages from 8.43 to 17.5 million years on another sample before eventually settling on their 2.6-million-year date.</i><br /><br /><i>McDougall concluded, &#39;On the basis of the large scatter in the ages and the small proportion of argon-40 in the gas extracted from the anorthoclase concentrates, I suggest that the results are analytically less precise than given by these authors.&#39;</i><br /><br /><i>In the restrained diplomatic language of science, this is the equivalent of one scientist whacking another over the head with the sort of club that Lake Turkana Man was probably using on his enemies anywhere between 0.5 and 17.5 million years ago.</i><br /><br /><b><i>One aspect of this affair that deserves special attention is that all the scientists dating the formation started by selecting rocks they thought were the right age and discarding samples which seemed wrong. No one doubts that this is done honestly and intelligently. But the question must be asked: How do dating scientists know in advance which are the right rocks and which the wrong rocks? What scientifically led them to reject dates of 0.5 million years or 17.5 million years in favor of 2.6 million?</i></b><br /><br /><b><i>The answer that dating adherents give is that any scientist would exclude the few extreme measurements and settle for the majority of figures that are clustered together in a straight line or &#39;plateau&#39; when the results are plotted graphically. But, of course, had they measured the salinity of the oceans as a means of geochronometry (a method which as we saw earlier is known to be flawed) they would have found the same kind of &#39;plateau&#39; grouping for most results, regardless of where they took their seawater samples, because the method itself is systematically flawed. The majority of their dates would have been in the range of 100 million years because that is what the current salt content and annual erosion figures indicate. Constancy of results is not an indicator of correctness when the method itself is defective.</i></b><br /><br /><b><i>The truth is that, to those who dated the KBS Tuff, the chosen date of 2.6 million years seemed to be more &#39;reasonable&#39; than 0.5 million or 17.5 million. And the word reasonable in this context can be interpreted only as meaning consistent with uniformitarian and Darwinist beliefs on dating.</i></b><i> The objection to this viewpoint is that &#39;being reasonable&#39; is not an acceptable substitute for scientific measurement and proof.</i><br /><br /><b><i>The fact is that presently it is impossible to say with any confidence how old the Earth is, beyond the obvious fact that it predates the calendar of human history.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Controversy Channel","summary":"Science Historian John Burnham Suggests that Sports Journalism has Become the Most Recognizable Model for Science Journalism / \"... Insignificant Items Given Emotional Appeal, Rapidly Becoming Known to Everyone Who Shared the Mass Media World and then Passing into Oblivion\" / What Would Happen to Our Rate of Innovation if We Took the Analogy to its Logical Conclusion? / What if We Adapted the Sports Television Channel Format to the Debates that Rage Every Day, Currently Behind the Scenes, at the Very Edges of Scientific Discovery? / Is it a Groundbreaking or Simply Malformed Idea?","image":"https://lh3.googleusercontent.com/-8MG4uWOlnVk/WEx3IgZUNUI/AAAAAAAAJqI/Wj7uiUcFSmohqRYSbMAKvZAQGOsu1s5EQCJoC/w7142-h9999/the-controversy-channel-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/TQLCcetDXjw","publishDate":"2016-12-10T21:44:40.347Z","updateDate":"2017-01-03T01:37:49.489Z","text":"<b>The Controversy Channel: Science Historian John Burnham Suggests that Sports Journalism has Become the Most Recognizable Model for Science Journalism / &quot;... Insignificant Items Given Emotional Appeal, Rapidly Becoming Known to Everyone Who Shared the Mass Media World and then Passing into Oblivion&quot; / What Would Happen to Our Rate of Innovation if We Took the Analogy to its Logical Conclusion? / What if We Adapted the Sports Television Channel Format to the Debates that Rage Every Day, Currently Behind the Scenes, at the Very Edges of Scientific Discovery? / Is it a Groundbreaking or Simply Malformed Idea?</b><br /><br /><b>Our Sports-Based Culture</b><br /><br />Across the world, every day, people are sitting in front of televisions watching sports games, game highlights, reporting on recent games, and sports experts pontificating in a half-circle before a camera on the successes and failures of sports teams and players.  It&#39;s amazing that so much can be said about these games without people losing interest.<br /><br />It&#39;s a tremendous amount of activity that happens across the world.  To be sure, friendships have been lost over sports disagreements.  Marriages have surely collapsed.  And many students have become convinced that they can succeed in life without actually studying or even paying heed to the universe which science is slowly unraveling around them.<br /><br />Our cultural infatuation with sports today is maybe testament to our short attention spans in that we can be entertained by sports without knowing too much detail about the teams, players or even -- to some extent -- the rules.  We can experience an immediate reaction to the action on the field; there&#39;s very little actual effort or barrier to immediate enjoyment of these games ... hence their widespread popularity.<br /><br />I&#39;ve played sports myself, and I recognize that they served an important role in my own education insofar as they taught me as a child how to work on a team -- a very important lesson!<br /><br />But, as an adult, I&#39;ve honestly struggled with games more broadly.<br /><br />And perhaps that is in some ways a good thing.<br /><br />There can be no doubt that sports help many people to escape from what may otherwise be a mundane existence.  But, at the end of the game, those who have mundane existences have not truly altered their mundane situation one bit.  The world within which the game occurs remains almost completely unaffected.<br /><br />And that&#39;s part of the reason why I&#39;ve traditionally had a problem with most types of games -- because they&#39;re just too removed from the realities of my own life to heavily invest my time.  I want to live an interesting life -- one which results in some sort of exciting change which takes the world to a new, exciting place.<br /><br />There must be others out there who feel the same, but I&#39;ve yet to see somebody confirm it.  As far as I can tell, I&#39;m in a small minority of people who just don&#39;t see any point to games.  I prefer the real world.<br /><br />But, when I go to a bar and look up, what I see is a basketball game.  Or, when I hang out with friends, they ask me if I watched the baseball game last night.  Or, when I go home for the holidays, my parents have left the football game on the television all weekend long.  I&#39;ve long since learned to not wear the local team shirt, for fear that a well-intentioned fan might start talking to me about &quot;the game&quot;.<br /><br /><b>An Alternative Future</b><br /><br />When I watch these sports shows, there&#39;s a hypothetical future which every time comes to mind.<br /><br />Say you are sitting in your apartment on a Sunday afternoon.  You flip on the television, and just as the hundreds of times before, there&#39;s a half-circle of manicured pundits in expensive Armani suits blabbing away.  There&#39;s a ticker at the bottom of the screen ticking off numbers.  There are brief interviews by journalists &quot;on the field&quot;.  And there are more in-depth stories that tell the emotional life stories of those involved ...<br /><br />... with one difference ...<br /><br />This is not a sports channel.<br /><br />The pundits are talking about scientific controversies.<br /><br />The ticker at the bottom of the screen informs the viewers of the current wagers for a variety of upcoming science mission results on the science futures market.<br /><br /><b>The Science Futures Market</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/im3GayFgEGr\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/im3GayFgEGr</a><br /><br />The journalist &quot;on the field&quot; is interviewing people directly involved with the mission, as well as critics of textbook theory who are explaining why their wagers will defy the odds.<br /><br />And the emotional life stories remind us of how far we&#39;ve come to this new (hypothetical) reality from a former era where science mavericks were en masse ridiculed.<br /><br /><b>Science Journalism as Sports Journalism</b><br /><br />John Burnham seems to think that at least in one sense, science journalism is already halfway there ...<br /><br />I have to admit that I did not entirely come up with this odd idea myself.  It comes from Burnham&#39;s surprising analysis in <i>How Superstition Won and Science Lost: Popularizing Science and Health in the United States</i> ...<br /><br /><b>The Science Journalist</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/4JJvZu18D9L\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/4JJvZu18D9L</a><br /><br /><i>&quot;A structural element in news reporting also intensified the sensationalism issue in science popularization throughout the twentieth century.  Journalistic reports almost always started with conclusions and implications and then were filled out with supporting detail -- </i><b><i>exactly the opposite format from the restrained scientific report in which conclusions and discussions came only after the methodology and findings were fully set forth. The news format therefore put great emphasis on the journalists&#39; interpretations. The whole problem was exacerbated by the fact that news items tended to be short -- in contrast to the longer magazine format -- and so journalists cut the scientific context at the same time that they played up attention-grabbing elements and products.</i></b><i> Again, a combination of sensationalism with the briefly stated facts removed science news substantially from older standards of responsible popularizing.</i><br /><br /><b><i>What was new in twentieth-century popularizing of science and health, then, was not sensationalism but the bits-and-pieces format</i></b><i>, and the early to mid-century timing of the coming of journalists and their format explains when and why they had such an impact.  Journalism itself, as Michael Schudson has pointed out, developed, around the 1930s especially, a strategy to strive for objectivity, whereby fact and opinion were separated -- opinion to go into signed columns and objective fact into the news columns. This development, Schudson shows, was the end product of the nineteenth-century belief in objective reality, when the daily flow of events came to constitute a reality in itself. For most news, then, the interpretive signed column served to provide a context, so that the endless flow of bits and pieces was at least given a place within the socio-political world of the journalists.</i><br /><br /><i>For science and health, however, only half of this journalistic strategy was effective. By and large, columnists in those fields, when such columns existed, merely multiplied facts -- as typically, and best known, in the health columns of newspapers or in Science Service syndications. </i><b><i>Such columnists did not provide context. And ordinary journalistic columnists were not competent to act the part of men of science. The only interpreters of science who had any impact, then, were the advertisers. *The result was that sports journalism became the most recognizable model for all reporting, including science and health -- a kaleidoscope, as Gilman Ostrander puts it, &#39;a jarring and distracting montage of unrelated pictures and news items and advertisements, all competing for attention and most of them immediately forgotten.&#39;</i></b><i> Science, health, and psychology were incidental elements in this twentieth-century torrent of items, and when they occurred in a sensationalistic guide, they became just one of the &#39;tremendous trifles,&#39; media fads that Frederick Lews Allen described as characteristic of the 1920s -- </i><b><i>insignificant items given emotional appeal, rapidly becoming known to everyone who shared the mass media world and then passing into oblivion.&quot;</i></b> (236-237)<br /><br /><b>A Future We Could Choose to Have</b><br /><br /><b>The Controversies of Science collection has to-date covered around 40 ongoing controversies.</b><br /><br />For those who have followed along, I am betting there is a sense with at least some of you that at least some of these controversies will eventually upset their textbook counterparts.<br /><br />There remain many more we&#39;ve yet to get to.  But, the sad fact is that most of these challenges remain completely unknown to the scientific community.  There of course is no controversy channel, just as there currently is no legal science futures market (at least in the United States ... yet).  And without any infrastucture which makes the controversial cutting edge of science interesting and meaningful to the public, we should be real about our prospects for progress on these innovation bottlenecks.<br /><br />Some would say that all we can do is wait for scientists to die.<br /><br />I find that grim and the least creative course of action possible to us: Let&#39;s do nothing ... and wait.<br /><br />If &quot;the game&quot; was an actual debate about actual science, then when &quot;the game&quot; is over, the world would have undergone some identifiable change.  With each &quot;game&quot;, we&#39;d in theory at least be a step closer to a more scientific future.  If it was to culturally catch on, we cannot even in our current reality begin to grasp the new pace of innovation which might transpire.  All sorts of things which had been assumed to be a hundred years off might suddenly look possible on much shorter timescales.<br /><br />But, the current reality is that the ongoing progress in science today occurs out of our own ability to participate in it, behind the closed doors of experts fully invested in the old ideas.  They direct science according to their own, older conception of where it should go.<br /><br />And we sit and we passively wait for an exciting scientific future.  And in the meantime, there are plenty of &quot;real&quot; games to watch on tv.<br /><br />Go team!<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Controversy Campaign","summary":"If we were Culturally on the Verge of an Impending Paradigm Change, how would we Know? / A Critical Review of the History of Science Journalism Shows that Modern Science Journalism Obscures Our Ability to See them Coming / We can Fix this Problem by Creating Rewards for the Successful Prediction of Anomalous, Unexpected Scientific Data Using a Science Futures Market / The Role of the Controversy Campaign is to Meaningfully Inform these Wagers","image":"https://lh3.googleusercontent.com/-D3SeOqMqlYE/WExP8hcoi-I/AAAAAAAAJpM/dOd8CnnBx_QocLfh3RomcfJ7gwfVe8EMQCJoC/w7142-h9999/the-controversy-campaign-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/a1Rwz9XXNQ7","publishDate":"2016-12-10T18:59:03.015Z","updateDate":"2017-01-03T01:38:06.139Z","text":"<b>The Controversy Campaign: If we were Culturally on the Verge of an Impending Paradigm Change, how would we Know? / A Critical Review of the History of Science Journalism Shows that Modern Science Journalism Obscures Our Ability to See them Coming / We can Fix this Problem by Creating Rewards for the Successful Prediction of Anomalous, Unexpected Scientific Data Using a Science Futures Market / The Role of the Controversy Campaign is to Meaningfully Inform these Wagers</b><br /><br /><b>Have you ever wondered if we are on the verge of a paradigm change in the sciences?</b><br /><br />How would you know?  Probably most people expect that an announcement would appear in the newspapers, after having gone through peer review.<br /><br />But, is that what a paradigm change would actually look like?<br /><br /><b>A dramatic change in the way we view the world around us will of course generate winners and losers in the academic community</b>; science journalists would be caught in the middle, unsure of which side is correct and perhaps concerned of burning connections they&#39;ve carefully cultivated within the scientific community.<br /><br /><b>The Trouble with Science Journalism</b><br /><br />The last time this happened was in the early 20th century with the rise of Relativity and quantum mechanics; this proved to be such a challenging time for science journalism that the entire industry underwent significant changes in how they report on science which persist to this day.<br /><br />Let&#39;s revisit John C Burnham&#39;s critique of the history of science journalism from <i>How Superstition Won and Science Lost: Popularizing Science and Health in the United States</i> ...<br /><br /><b>The Science Journalist</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/4JJvZu18D9L\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/4JJvZu18D9L</a><br /><br /><b>Science Gets Complicated</b><br /><br /><b><i>&quot;One reason that nineteenth-century evangelical popularization of science receded in the twentieth century was that what scientists had to communicate -- beyond image and attitude -- grew much more difficult to put into popular form after 1900. At one time, the task of popular science had been to clear up mysteries.</i></b><i> Indeed, as late as 1924, Slosson, the voice of Science Service, was still denouncing false beliefs and commercial distortions by saying that &#39;the test of real science&#39; was &#39;its honesty&#39; in attacking &#39;the real mysteries of nature.&#39;  Explanation was for him, as it had been earlier for &#39;men of science,&#39; the essence of popularizing. </i><b><i>But by the midcentury period, popularizers of all varieties were bemoaning the fact that pure science had become so complicated that they could hardly explain it. The science of the new century did not necessarily confirm the gospel of certain progress, unity, and reductionism of an earlier day.</i></b><i> Biologist Francis Sumner of the University of California in 1937 summarized the dilemma for example of those who read about the new physics: </i><b><i>&#39;There would seem to be a vast inconsistency between the traditional notion of the man of science, with his uncompromising insistence on evidence and his lofty scorn of guesses and unproved assumptions, and the quasi-mystic who tells us all these strange things about space and infinity and who describes with such assurance the detailed intricacies of an infinitesimal world forever beyond the range of human observation.&#39;</i></b><br /><br /><b><i>While innumerable newspapermen of the 1920s tried to explain relativity, and in later years other baffling ideas, or to explain why something could not be explained, only a few scientists attempted to draw conclusions for the public from the unsettling findings of the new physics and other mathematical or abstract studies. Popularizers of every stripe found themselves instead portraying confusion. They therefore often had to readjust what they were doing, because they could no longer appeal to the paradigmatic act of explaining the mystery. They were, rather, being called upon to explain the confusion, which was particularly difficult without the vision of ultimate unity that earlier popularizers had enjoyed and exploited.</i></b><i> The newspapers were not to blame, instead the editor of the New York Times in 1935: &#39;The function of the newspaper is primarily to report what the leading scientists do and say.  If they contradict each other, and there is confusion, the newspapers merely picture the confusion and do not create it.&#39;</i><br /><br /><b><i>The result of this challenge to the unity of science was to encourage popularizers to redouble the emphasis that had been developing on portraying the results, rather than the ideas, of science.</i></b><i> The Science Year Book published in the 1940s, for example, usually had &#39;aviation&#39; as a major category and consisted almost entirely of articles about applied science developments.  As early as 1903, W.S. Franklin, a physicist at Lehigh, observed: </i><b><i>&#39;Everything that appears in the name of science in our newspapers and magazines relates only to results. Have any of you seen in our newspapers or popular magazines any detailed description of the principles and methods used by Marconi in his wireless telegraphy?&#39;</i></b><i> More than half a century later, Palmer Wright, a Dow Chemical chemist, wondered if popularizers should not build on applied science for understanding. </i><b><i>&#39;The popular thrust,&#39; he observed, &#39;is toward the what, not the why of science.&#39;</i></b><br /><br /><i>So it was that popularized science in the twentieth century continued as in the nineteenth to emphasize progress -- </i><b><i>but progress now in terms more exclusively of the &#39;applications mankind can make from [the] marvelous findings&#39; of &#39;pure science,&#39;</i></b><i> as a 1926 writer put it.  But the new context of progress had further implications.  Any facts at all could be digested into the popular science of results; </i><b><i>they needed no further context than that they were part of the advance of science. Thus a discovery at any level of science reported in the 1920s was a fact and a part of the progress of science;</i></b><i> so, too, was a machine of the 1930s or a cure of the 1940s a fact and a part of the progress of science.  This approach to popularizing was compatible with the rise of general science in the schools, in which teachers emphasized application and turned away from the work of researchers that was abstract and unpopular. </i><b><i>Even in the 1960s, reported Howard E. Gruber, &#39;high school teachers generally approach[ed] science teaching as a matter of conveying science as established facts and doctrines&#39; rather than &#39;science teaching in which science [was] treated as a way of thought.&#39;</i></b><i> In the more general popularizing of the twentieth century, audiences would have been lucky to get doctrines in addition to lists of products.&quot;</i> (p214-216)<br /><br />Burnham&#39;s critique seems to challenge our simplistic notions of what a paradigm change would look like.  <b>In a world where science journalists have already been reporting on the what rather than the why for a full century, the knowledge structures constructed in the minds of the consumers of that information do not trace back to reasons to believe; they trace back to an authority -- perhaps even as vaguely defined as just &#39;scientists say&#39;</b><br /><br />This is a world where we are dependent upon the experts for any announcement of our new paradigm; <b>yet, the experts are the very people who stand to lose the most by such an announcement.</b><br /><br />So, what is the answer to our question ...<br /><br /><b>What would a paradigm change actually look like?</b><br /><br />The correct answer is that, assuming you&#39;re not an actual academic with some insight into the inner world of academia, <b>there&#39;s no real guarantee that you&#39;d actually see our new paradigm emerging at all.</b><br /><br />Let that thought settle in.  Feel it.  Think about what it means to live in a time where if there was some exciting new scientific era just around the corner, we&#39;d never know it.<br /><br />If you can get the seriousness of this problem, then you already understand the motive for the invention of the controversy campaign.<br /><br /><b>The Controversy Campaign</b><br /><br />Consider the precedent set by Wikipedia ...<br /><br /><i>Building Successful Online Communities: Evidence-Based Social Design</i>, by Robert E. Kraut and Paul Resnick<br /><br /><i>&quot;WikiProjects, where groups of editors in Wikipedia organize to improve articles in a defined domain area, use goals organized into what they call &#39;Collaborations of the Week.&#39;  These encourage project members to work on specific high-priority articles.  The project chooses one or two articles under its purview and advertises that editors should improve them during a defined time period (typically a week).  These collaborations of the week are highly successful.  They cause project members in particular to triple the work they do on the designated articles during the collaboration period (see figure 2.7).  The motivational effects of these goals also spill over, causing project members to do ore project-related work generally, editing articles beyond those listed in the collaboration of the week.  In particular, people who have participated in collaborations of the week then go on to engage in what organizational scholars call &#39;organizational citizenship behavior,&#39; those behaviors vital to group functioning that aren&#39;t explicit parts of people&#39;s job descriptions.  For example, in Wikpedia, in contrast with tasks like editing the main body of the articles, citizenship behaviors include fighting vandalism, maintenance work, and clean-up work. (Wilson, Straus, and McEvily 2006).&quot;</i><br /><br />A controversy campaign might look somewhat similar -- but with some interesting twists ...<br /><br />A campaign on a controversy site would be used to fill out the controversy graph on a particular topic of upcoming relevance.  The controversy graph is the map of all pertinent information related to a debate: crucial concepts, questions, claims, even speculations, arguments and their rebuttals, models, assumptions and the big picture of the clash of worldviews.  This is not simply a collection of raw resources; it&#39;s things like annotations, which very quickly focus a person onto which part of that resource is relevant for some claim.  The idea is, simply put: <b>How can we eliminate the costly investment in time and research necessary that some future person must do in order to achieve the understanding that we &#39;now&#39; have?</b><br /><br />A controversy campaign, in other words, would fill out the controversy on both sides with a &quot;map&quot;.<br /><br /><b>Campaigns to Support Wagers</b><br /><br />But, there&#39;s one more reason why we&#39;d want to direct the entire community to a particular upcoming controversy: to support the creation of well-informed wagers on a science futures market.<br /><br /><b>The Science Futures Market</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/im3GayFgEGr\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/im3GayFgEGr</a><br /><br />Recall that a science futures market is a form of decision market where people can wager bets on well-crafted, specific, time-limited scientific predictions.  Science futures markets have the potential to expose scientific dogma that&#39;s become embedded within science journalism.  They can help us to identify impending paradigm changes, as <b>they reward those who are paying attention sufficient to understand that the data is deviating from the carefully-managed message we get from mainstream science.</b><br /><br />In other words, the science futures market solves the problem that&#39;s been created by the science journalism reporting.  It removes this obstacle that stands between the public and some impending hypothetical paradigm.<br /><br />Let&#39;s say that a mission is about to collect its first data, and we want to support the creation of wagers on some known controversy which relates to this mission.<br /><br />There would be a special focus on that controversy during a defined period of time -- a controversy campaign. During this campaign, relevant specialists would on occasion be paid by the social network to make themselves available.<br /><br />The thing about this, of course, is that their time would be limited. People would be invited to speak with the expert in order to establish lines of argumentation, get clarity on some confusing concept, elicit hidden assumptions in the domain&#39;s institutional approach, and so on.  We&#39;ve covered what this sort of Socratic questioning looks like here ...<br /><br /><b>The Critical-Analytical Vocabulary</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/1TsgUccCTBr\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/1TsgUccCTBr</a><br /><br /><i>&quot;Richard Paul, founder and director of research for the Foundation for Critical Thinking, once told me a story about a short flight he took in a private aircraft that contained only himself and the pilot. This pilot also happened to be a biology teacher at the University of Minnesota, a subject Paul knew little about. Paul, being inquisitive, began questioning the pilot on the nature of biology. He began using the tools of critical thinking to probe the foundations of the field, asking questions like ‘what is the purpose of biology?’ ‘what are some of the main assumptions biologists make?’ ‘what are some key questions biologists routinely consider?’ ‘what kinds of information do biologists pursue, and how do they determine its quality?’ ‘what are the most fundamental concepts necessary for understanding biological thought?’ ‘how do biologists tend to view the world?’ ‘what are some inferences biologists make as a result of this viewpoint?’ After about 30 minutes in which Paul continually questioned the pilot Socratically (by following the implications of each answer given), the pilot stopped the process. He said ‘you know, what’s really interesting about the questions you’re asking is that some of them would be answered in the first day of an introductory biology course, and some of them could be the focus of PhD dissertations’.&quot;</i><br /><br />The social network would be set up to permit people to rate each others&#39; questions and questioning styles.<br /><br />Think carefully about what is going on here.<br /><br /><b>This is regular laypeople directly participating in the process of scientific discovery and analysis by aggregating information and argumentation -- essentially doing research -- on important, challenged scientific qestions.</b><br /><br />If ever there was a system which could teach critical thinking and authentically thinking like a scientist ... not just thinking what scientists think, but true, deep, purposeful scientific thinking ... this is it.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Critical-Analytical Vocabulary","summary":"The Large Majority of Teachers Believe that they are Developing Critical Thinking within their Students / Yet, When Asked about their Process, it becomes Clear that there Typically is None / The Critical-Analytical Vocabulary Provides a Starting-Point Process for Any Layperson to Critically and Rapidly Understand a Domain of Science through Direct Inquiry / Dr. Cosgrove's Intro to the Technique Shows that it could Also Act as a Standard on a Social Network for Evaluating a Person's Learning Capabilities if it Was Worked into the Site's Voting and Annotation Infrastructure","image":"https://lh3.googleusercontent.com/-BaVMBo_uiBQ/WEhy2fyYGII/AAAAAAAAJnQ/Dgorgf04XIgUhA02klDpsp2e25HjuHe_ACJoC/w7142-h9999/the-critical-analytical-vocabulary-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/1TsgUccCTBr","publishDate":"2016-12-07T20:37:16.060Z","updateDate":"2017-01-03T01:38:55.116Z","text":"<b>The Critical-Analytical Vocabulary: The Large Majority of Teachers Believe that they are Developing Critical Thinking within their Students / Yet, When Asked about their Process, it becomes Clear that there Typically is None / The Critical-Analytical Vocabulary Provides a Starting-Point Process for Any Layperson to Critically and Rapidly Understand a Domain of Science through Direct Inquiry / Dr. Cosgrove&#39;s Intro to the Technique Shows that it could Also Act as a Standard on a Social Network for Evaluating a Person&#39;s Learning Capabilities if it Was Worked into the Site&#39;s Voting and Annotation Infrastructure</b><br /><br />In April of 2014, I attended the iAnnotate conference at San Francisco&#39;s Fort Mason Center.  One of the speakers was Dr. Rush Cosgrove of the Critical Thinking Institute.  His talk on how to teach critical thinking with annotations played some part in my own evolving notions of an annotation library, but so many things have happened since then, that the paper he handed me became lost in my stack of post-its and social network sketches.<br /><br />Today, I brought a little bit of order to those notes, and I ran into the paper.  I immediately read it.  It seems to me that this is not something which a person can -- without effort, initially -- just pick up.  The design for our information system should act to focus us upon these aspects of critique and thought, and it will take some experimention to understand what will work.<br /><br /><a href=\"https://s3.amazonaws.com/criticalthinking/Critical_Analytical_Vocabulary.pdf\" class=\"ot-anchor\">https://s3.amazonaws.com/criticalthinking/Critical_Analytical_Vocabulary.pdf</a><br /><br />The Critical-Analytical Vocabulary as a Common Academic Language<br /><br />Dr. Rush Cosgrove<br />Assistant Director of Research<br />Foundation for Critical Thinking<br /><br /><i>&quot;(This article was written for an edited work recently released in paperback: Critical Thinking and Higher Order Thinking: A Current Perspective, Editor: Michael F. Shaughnessy, Eastern New Mexico University, Portales, New Mexico)</i><br /><br /><i>Preface</i><br /><br /><b><i>Richard Paul, founder and director of research for the Foundation for Critical Thinking, once told me a story about a short flight he took in a private aircraft that contained only himself and the pilot. This pilot also happened to be a biology teacher at the University of Minnesota, a subject Paul knew little about. Paul, being inquisitive, began questioning the pilot on the nature of biology. He began using the tools of critical thinking to probe the foundations of the field, asking questions like ‘what is the purpose of biology?’ ‘what are some of the main assumptions biologists make?’ ‘what are some key questions biologists routinely consider?’ ‘what kinds of information do biologists pursue, and how do they determine its quality?’ ‘what are the most fundamental concepts necessary for understanding biological thought?’ ‘how do biologists tend to view the world?’ ‘what are some inferences biologists make as a result of this viewpoint?’ After about 30 minutes in which Paul continually questioned the pilot Socratically (by following the implications of each answer given), the pilot stopped the process. He said ‘you know, what’s really interesting about the questions you’re asking is that some of them would be answered in the first day of an introductory biology course, and some of them could be the focus of PhD dissertations’.</i></b><br /><br /><b><i>This is an example of highly effective intellectual communication. By questioning in a disciplined manner, using a small set of universal analytical tools, Paul had begun to enter into the logic, the system, of biology. In just twenty minutes, he was beginning to ask questions like a biologist, to think like a biologist.</i></b><br /><br /><i>This essay explores the idea of intellectual communication and its improvement through the development of a shared academic language based on these and other universal analytical and evaluative concepts.</i><br /><br /><i>Intellectual Communications and Miscommunications</i><br /><br /><i>Intellectual communications abound in academia. Researchers must communicate the status of their work at numerous stages: during initial grant applications, when obtaining permission or access, within publications and to reviewers, in presentations to other academics or to the public. Teaching involves communicating to students through syllabi and assignment descriptions, during class and in office hours or tutorials; and again to administrators, department heads, or deans regarding publications as well as teaching successes and struggles, progress and development. Students, for their part, must communicate to professors their puzzlement and confusions during class time and after, as well as the extent of their understanding during exams and tests.</i><br /><br /><i>Yet many of these communications are not successfully received and understood by their intended audience, leading to much wasted time and energy. For instance, students are often confused about what is being asked of them. Indeed, recent broad-scale studies (see e.g., Casner-Lotto and Benner, 2006; and especially Arum and Roksa, 2011) support prominent scholars (see e.g., Bloom, 1988; Bok, 2006; Ramsden, 2007) who have long argued that many students complete their undergraduate degree without learning much at all, and with very little development of their critical capacities. Taken as an aggregate of professor and student time across the nation (or world, if you like), this represents hundreds of thousands of hours of ‘wasted’ time each year.</i><br /><br /><i>And, of course, misunderstanding is not exclusive to students. The overwhelming majority of teachers find it hard to identify objective standards for assessing thought (Paul, Elder, and Bartell, 1997; Thomas, 1999) and to communicate them to their students (for review, see Black and William, 1998, and; James et al., 2006). </i><b><i>One implication of this is that much evaluation of students’ written work is grounded in highly subjective and personal criteria, such as the degree of ‘flair and sparkle’</i></b><i> (see, e.g. Paul, 1995).</i><br /><br /><i>Analysis and Evaluation: Fundamental to Successful Communication</i><br /><br /><b><i>Successful communication depends upon effective analysis and evaluation of thought. That is, in every case in which humans are confronted with something requiring intellectual processing (an essay, a lecture, an assignment, a research publication, etc.), they must break it down to focus on individual parts (analysis) and then they must determine the extent to which those parts make sense or have validity (evaluation).</i></b><br /><br /><i>This can be seen clearly when applied to written work: </i><b><i>there is no way to take in an entire book or a page or a paragraph all at once. Operating simultaneously during analysis is the process of evaluation: once readers have determined the meaning behind a section they’ve focused on, they then begin to judge the extent to which that meaning is valuable or true. As you critically read this chapter, you are doing these precise things -- analyzing what you are reading and assessing it for quality, taking it apart to understand it, deciding what to accept and what to reject, relating the ideas within it to other ideas you already have about education and learning, and so on.</i></b><br /><br /><i>Of course, there are many forms of analysis and evaluation. I’ve just discussed one: argument analysis. We might also analyze and evaluate this essay grammatically: noting some interesting uses of punctuation and a bizarre combination of British and American spelling. Or perhaps from a gender studies perspective: noting an absence of gendered pronouns in examples being used.</i><br /><br /><i>Subject Specific Forms of Analysis and Evaluation</i><br /><br /><i>In fact, there are myriad forms of analysis and evaluation. </i><b><i>Each discipline has at least one, and many have more. These subject specific forms are often highly specialized and unique: biologists are interested in the extent to which an experiment has been controlled and can be repeated; historians are not. Art critics focus on brush stroke and use of color; psychologists don’t. Basketball coaches care about wrist flick and arm extension. Poets think about word choice and rhyme structure. Anthropologists are concerned with interpretation bias.</i></b><br /><br /><b><i>Though this is not often made explicit, much of our success or failure within academia is due to our ability or inability to become conversant in these scholarly languages and dialects, and thus to communicate in appropriate ways to colleagues, mentors, and students. In other words, people are considered skilled in a subject, not usually by measuring the number of ‘facts’ they can regurgitate (though this often forms the basis of low-level assessment) but by the extent to which they can read and interpret (i.e. analyze and evaluate) texts or other intellectual communications in the field, and then produce unique and creative syntheses which are clear, accurate, and logical (among other important criteria); conditions which are tested at higher levels of examination, such as PhD dissertations.</i></b><br /><br /><i>Subject specific forms of analysis and evaluation are important and useful. </i><b><i>They represent systems which serve to ensure (or at least improve) quality of thought within the discipline.</i></b><i> They have developed slowly over time, often in response to previous abuses which were recognized as problematic (e.g. the creation of ethical guidelines in medical research due to unethical actions taken by researchers), or to correct common mistakes in the field (e.g. not controlling for variables in the sciences and social sciences).</i><br /><br /><b><i>However, subject-specific forms of analysis and evaluation are also limited in important ways: because each is unique, it is not transferrable to other fields or disciplines; because each is complex, often requiring the absorption of a large vocabulary, students must spend many weeks or years reading and working in a particular field before they develop skill enough to contribute to its discourse, and most will never attain nor are interested in attaining such a level [2]. Further, these specialized languages necessarily exclude those who do not speak them.</i></b><br /><br /><i>This exclusion tends to increase as one gets closer to primary research; </i><b><i>much of what is published is readable by only a small fraction of humanity, which renders impotent many important insights and implications for human action and societal development</i></b><i> (for instance, how many fundamental and fairly simple yet powerful ideas have you studied which are not widely understood or employed, with negative implications for individuals or society?) </i><b><i>These important insights are not successfully communicated to students and the public because they require knowledge and skill in specific forms of analysis and evaluation, which many readers lack.</i></b><br /><br /><b><i>Fortunately, underneath these specialized forms of discourse, and fundamental to effective functioning within any discipline, lies a universal set of analytical and evaluative concepts; a set which is sometimes called the ‘critical-analytical vocabulary’. This vocabulary, being based in common language (e.g. English, French, Japanese, Arabic), is accessible to all.</i></b><i> In other words, it forms the basis of a vocabulary that can be shared by all people in human societies; it can be developed and expanded in an effort to improve the efficiency and success of intellectual communications wherever they exist.</i><br /><br /><i>The next two sections briefly explore some of these analytical and evaluative tools, and the manner in which they implicitly operate in human thinking, wherever and whenever humans think.</i><br /><br /><i>Universal Analytical Language</i><br /><br /><i>I will now make an assertion which may seem controversial, but bear with me for a moment: despite the unlimited potential manifestations of human thinking, there are universal elements of thought which are always present, and which are therefore always subject to inquiry. Further, </i><b><i>if one understands that disciplines do not exist as bodies of collected facts but as forms of thinking about the world in specific directions, one will see that each subject can be probed at a fundamental and powerful level through its system of thought.</i></b><br /><br /><i>Think back to the story which begins this essay. How was Paul able to enter into a subject about which he knew very little in such a short time? The answer lies in his understanding of human thinking and therefore human thinking regarding specific subjects: he knew that certain structures form the basis of every discipline, and so knew he had a ready set of analytical questions with which he could investigate this new (to him) system of thought (biology).</i><br /><br /><i>Paul and Elder call these fundamental structures the ‘Elements of Thought’, and generally place them into a circle diagram to emphasize the non-linear nature of the relationships between and among them. These structures are not proceduralized, but are based on principles which can be ordered in many possible ways for many possible purposes. To my mind, they form a starting point for the development of a universal analytical language:&quot;</i><br /><br />[See Large Diagram]<br /><br /><b><i>&quot;Paul and Elder argue that the elements of thought are present wherever human thinking is present, and therefore suggest a minimal set of questions which could be asked while performing any intellectual analysis.</i></b><i> One can, for example, question the purpose of studying history or of car buying. One might seek out information for preparing for a nursing exam as well as for making a political decision. There are implications for being a highly skilled teacher just as there are for being a loving and supportive parent. Furthermore, these elements interact in the mind in an integrated way. </i><b><i>If your purpose changes (teaching elementary students versus teaching college students), then so too will your questions, and therefore the information you seek, the conclusions you come to or the inferences you make, etc.</i></b><br /><br /><b><i>Another way to consider the universality of these structures can be seen in the absurdity of their negation; that is, it would be unintelligible to say of one’s own reasoning that it is without a purpose, asks no questions, is based on no information, leads to no implications, embodies no point of view, begins with no assumptions, employs no concepts, and comes to no conclusions.</i></b><i> Further, to claim that these structures are not universal is a conclusion (inference) which is based on some information and in response to a problem; it employs concepts filtered through the lens of particular assumptions contained within a point of view; it seeks to accomplish a goal (purpose) and leads to implications and consequences. </i><b><i>In other words, to negate them is to use or presuppose them -- and thus to prove that they are implicit in reasoning.</i></b><br /><br /><i>In an academic subject, these elements can be applied on multiple levels. To introduce students to a subject, for example, one might begin with a discussion of the elements of reasoning at the heart of the subject, as in: some important purposes of engineering are ... some key questions engineers ask are ... engineers tend to gather the following types of information ... engineers make the following types of inferences ... engineering is founded on certain assumptions regarding the nature of the world, such as ... some key concepts central to engineering, without which one would not be able to understand it are ... some implications of skilled engineering reasoning versus unskilled engineering reasoning might be ... engineers tend to view the world as follows ... One might substitute any subject or human activity for &#39;engineering&#39; above (e.g. history, anthropology, teaching a course, teaching an individual class, reading an essay or speech, etc. etc.) and these analytical tools will, I believe, prove valid and useful.</i><br /><br /><i>Of course, these are not the only possible universal analytical concepts. One theoretician, Gerald Nosich (2009), has proposed a ninth: </i><b><i>context</i></b><i>. </i><b><i>He presents all nine together by putting a box around the circle of elements, with the word ‘context’ in the corners, implying that human thinking always exists within a particular context.</i></b><i> There are possibly more structures which are universal in their application, and as the field of critical thinking continues to emerge it will be important for more scholars to contribute their own ideas and suggestions regarding universal as well as subject specific forms of analysis and evaluation. Linda Elder makes such an appeal in her contribution to this collection.</i><br /><br /><i>Universal Evaluative Language</i><br /><br /><i>In addition to these analytical tools lies a group of fundamental intellectual standards which are essential, to varying degrees, in every discipline. Paul and Elder (2002) offer the following list of what they term &#39;essential intellectual standards.&#39;&quot;</i><br /><br />[See Small Diagram]<br /><br /><i>&quot;One can easily see the usefulness of this list to teaching and learning, and indeed to thinking generally. </i><b><i>But Elder and Paul point out that the above is far from complete, and have explored and developed a more extensive, but still not exhaustive, conception of intellectual standards in the Thinkers’ Guide to Intellectual Standards</i></b><i> (2009). For example, one can readily see the importance of intellectual standards such as sufficiency, validity, reasonability, consistency and so on.</i><br /><br /><i>People skilled in adhering to intellectual standards can determine the quality of intellectual communications by asking the following types of questions, contextualized, for example, when reading an article: &#39;To what extent are the central parts of this argument clear; to what extent are they muddy or vague (clarity)?&#39; &#39;How accurate is the information used in this report (accuracy)?&#39; &#39;Is there an adequate amount of detail in examples used(precision)?&#39; &#39;Does the author deal with the complexities in the issue, or is the issue treated superficially (depth)?&#39; &#39;Has the author considered alternative viewpoints, or given an overly narrow account (breadth)?&#39; &#39;To what extent is this specific example pertinent to the argument (relevance)?&#39; &#39;How important is this issue (significance)?&#39; &#39;Is the argument coherent, or does it have internal inconsistencies (logic)?&#39; &#39;to what extent is the author using manipulative language or other intellectual trickery to convince the reader that the argument is sound (fairness)?&#39;</i><br /><br /><b><i>Of course, the standards require some degree of interpretation during contextualization.</i></b><i> That is, accuracy in a biology experiment is not the same as accuracy in interpreting an essay. The same goes for depth: one might consider multiple societal forces and their effects on a particular event in the study of history or one might look for multiple variables leading to mental illness when reasoning psychologically. In some cases it may seem that one or more standard is not relevant to a discipline at all (e.g. ‘breadth’ in architecture); however, in my experience this is usually resolved upon deeper reflection [3] (e.g. ‘have you considered the viewpoints of those who will be living or working in the building? Of the builders? Of the neighbors?).</i><br /><br /><i>As with the elements of reasoning, arguments against the importance of intellectual standards in human thought are implicitly based on those very criteria. For instance, they imply at minimum that the argument is clear and accurate. Further, can you imagine a professor or professional saying &#39;what we want in our field is people who routinely think unclearly, inaccurately, imprecisely, superficially, without regard to complexity, narrow-mindedly, illogically, and unfairly&#39;?</i><br /><br /><i>Analytical and Evaluative Fluency</i><br /><br /><i>This analytical and evaluative language is intuitive (few would argue against the importance of their usage). </i><b><i>Yet how fluent are we in its use or in the use of any analytic and evaluative language? Extant research is not inspiring. For example, in the studies conducted by Paul, Elder, and Bartell (1997) and Thomas (1999), only 8% of professors interviewed were able to enumerate any intellectual criteria required of students [4], and over 90% of faculty could not distinguish between an inference and an assumption, or between an inference and an implication. It may come as no surprise, then, that the new and highly visible report Academically Adrift (Arum and Roksa, 2011), finds that students’ ability to effectively analyze and evaluate is generally poor, and that these skills do not increase much during their college years (during which 45% of students demonstrate no statistically significant gains in critical thinking).</i></b><br /><br /><i>Thus, teachers’ good intentions are often not successfully communicated to students. Let us look at some brief excerpts from research I conducted at the University of Oxford which demonstrate this miscommunication between teachers and students (Cosgrove, 2011; 351):</i><br /><br /><i>RC: and when you have your students critique the other arguments, what kinds of criteria do you see them using?</i><br /><br /><i>Tutor B: Well I think that’s much more ad hoc. They tend to assess in terms of what they agreed and disagreed with. That’s probably less helpful ... it tends to be more sort of, &#39;well you know I agree with x. y, z, but I disagree with a, b, c&#39; ...</i><br /><br /><i>RC: So you don’t actually say &#39;ok when you’re critiquing this person, you need to use these criteria&#39;?</i><br /><br /><i>Tutor B: No but I think I should do [pause] just thinking about it [pause] now you ask it, I probably should say &#39;look, you know, what do you think are the criteria that I use? You should use the same sorts of things&#39; ... but obviously your implicit point is right in that they should do it with criteria.</i><br /><br /><i>Accordingly, </i><b><i>undergraduate responses to the question of what criteria they use in intellectual evaluation exhibited considerable confusion and anxiety</i></b><i>:</i><br /><br /><i>Student G: I find it really hard to read someone’s essay and critique it. I don’t know why, it’s like impossible -- it’s like gibberish I don’t know why! ... But in the end I just kind of [go] through the plan of [an] essay and then just underneath in a different color pen, just say like whether I think this is a good or bad idea, but I think that’s a bit sort of childish.</i><br /><br /><i>Student F: yeah well you often just get a -- it sounds really like stupid but it’s almost just sort of what you think sounds right. It’s almost like an impulse. It’s almost an impulse decision. It’s just what seems more convincing ...</i><br /><br /><i>We can see here frustration on both sides: by the tutor for lack of critical thinking from students, and by students for not knowing how to engage in critical thinking; and all for lack of an explicit understanding of the tools of analysis and evaluation.</i><br /><br /><i>Finally, even much research activity (even by PhD’s; even graduates of the world’s most respected institutions) suffers from improper or poorly conducted analysis and evaluation; thus the need for extensive peer review and the rejection of much submitted work.</i><br /><br /><i>Recap</i><br /><br /><i>In sum, my argument is that effective intellectual pursuit requires effective communication, which in turn depends upon effective engagement in analysis and evaluation. Further, though forms of analysis and evaluation differ from subject to subject, there are universal forms which are fundamental to all disciplines; and that the groundwork for this ‘critical-analytical vocabulary’, or shared academic language, has been established by Paul and Elder in the intellectual constructs termed the ‘Elements of Thought’ and the ‘Universal Intellectual Standards’. Finally, despite our good intentions, we humans are not, with few exceptions, fluent or disciplined in the language of analysis and evaluation, universal or otherwise.</i><br /><br /><i>Immediately following is an exploration of some possibilities which explicit adoption of a common academic language might create.</i><br /><br /><i>What would adoption of a common academic language look like in an ideal world?</i><br /><br /><i>Let us now consider a hypothetical university in which virtually all students and faculty are fluent in a universal analytical and evaluative language, focusing for the moment on important implications for teaching and learning. In this imaginary institution the logic of each course is identified from the very beginning in the syllabus: its purposes and key questions, core concepts and main sources of information, important assumptions and implications, as well as the central perspective (or perspectives) which will be explored or developed. Students, being sensitive to the elements of thought, are readily able to read and digest the basic logic of the course, even if they have never before studied the subject (as Paul did with biology, using the elements of thought). Students come to class with questions about these fundamental structures -- how are they different from or similar to those within subjects and fields which they already understand? What unique structures will be considered during the course?</i><br /><br /><i>Teachers are ready to respond to these questions and are comfortable helping students make connections with other disciplines, since their own fluency in this common academic language has allowed them to probe the structures of surrounding as well as distant fields of understanding (again, as Paul did). Further, they make clear their intentions with the course: the grounds upon which students will be evaluated; the skills and dispositions they will be expected to exhibit or develop. Students understand these communications, as they are accustomed to discussing analysis and evaluation and possess well-developed vocabularies for both; they are knowledgeable about these processes generally, as well as experienced in their engagement in multiple forms.</i><br /><br /><i>Consequently, the quality of student papers and student thinking is generally high. Though differences certainly still exist, few student papers contain wildly irrelevant or flagrantly inaccurate statements. </i><b><i>Students are cognizant of what to look for in their own writing and, by the time they graduate, have years of experience of explicit and informed self-reflection (itself entailing analysis and evaluation). Due to this experience, these students take constructive critique well, and understand that their writing and reasoning can always improve.</i></b><br /><br /><i>Students graduate with fundamental and long-term understandings of the basic logic of various disciplines. They are explicitly aware of both universal as well as subject specific forms of analysis and evaluation, and they are ready to apply these to further study or in work settings; finally, students are flexible and experienced in learning new systems, so they can more readily enter into whatever novel and/or unique forms of analysis and evaluation they encounter throughout their professional and personal lives.</i><br /><br /><i>What does this look like in reality?</i><br /><br /><i>The above ideal may never be achieved, certainly not in the near-future. However, schools and universities across the United States and beyond are beginning to take critical thinking more seriously, and are working to integrate its analytical and evaluative language into their courses and curricula. One such institution is currently attempting to infuse the ‘elements of thought’ and ‘universal intellectual standards’ across the curriculum. Some faculty are attesting to powerful change, and to the benefits of a shared academic vocabulary. For example, consider this highly reflective and self-aware comment from one professor after being introduced to the elements and standards and using them in her classroom:</i><br /><br /><i>&#39;I think that for decades I have given my students many opportunities to engage in critical thinking, and I have modeled critical thinking in class discussions. </i><b><i>But I don&#39;t think I can claim ever to have taught critical thinking in a systematic way.</i></b><i> [The Elements of Thought and Intellectual Standards] give me a way to share a critical thinking vocabulary with students and to chart their progress. I know and can tell my students exactly what I am looking for.&#39; -- Spring 2008 Pilot Program Participant, Department of English</i><br /><br /><i>The first sentence represents the thoughts of the vast majority of teachers, 97% of whom claim critical thinking to be of primary importance and who further claim to be developing it in their students (Gardiner, 1995; Paul, Elder, and Bartell, 1997; Thomas, 1999). Of course, most teachers believe they are teaching for critical thinking and most probably are providing some opportunity for students to think critically; </i><b><i>however, because the overwhelming majority of faculty (roughly 80%) are not fluent in the language of analysis and evaluation, and so do not discuss analytical and evaluative concepts explicitly with their students, most of this opportunity is wasted, the time instead filled with uncritical discussion and reaction.</i></b><br /><br /><i>After being introduced to the ‘elements’ and ‘standards’, this professor began to integrate them into her course in a systematic way. On the next page is an example of how she used this language to communicate with her students more explicitly and clearly regarding the analysis and evaluation of historical texts. Notice that she has added one subject-specific analytical concept: ‘techniques’.</i><br /><br /><i>There is some indication that explicitly communicating this language to students can lead to improved student reasoning. For example, contrast the student responses from Oxford undergraduates regarding evaluation with this response from a student four years their junior, but who has been introduced explicitly to the intellectual standards (Cosgrove, 2010):</i><br /><br /><i>RC: so as you can see from the stuff that you’ve read, what I’m focused on is critical thinking -- has your teacher talked to you about critical thinking?</i><br /><br /><i>(Students laughing, lots of &#39;yeahs&#39;)</i><br /><br /><i>Student: yeah he loves his critical thinking. The concepts are wide right. Like &#39;breadth, how wide does the argument go? Depth, how deep does it go? Specific, precision, accuracy&#39; all of that, all the time. (laughing)</i><br /><br /><i>RC: so what does he have you do with these ideas?</i><br /><br /><i>Student: just so when you write, and also when you read. So when you read a source &#39;how deep does this go? Is this just skimming the surface or is it a deep in-depth opinion?&#39; when you write &#39;are you just writing briefly or are you writing deep points?&#39; and accuracy is something, on exams you need to be accurate, so that’s a good thing. Other ones, such as breadth, so how wide do you cover, do you look at different points of view? Or are you being quite limited in your thinking and the way your arguing? So he’s always saying like &#39;don’t forget!&#39; he loves them, he loves them! (laughing)</i><br /><br /><i>This response is far more elaborate and precise than any response from the Oxford undergraduates, as well as 90% of the teachers interviewed by Paul, Elder, and Bartell (1997) and Thomas (1999). Of course, it is imperfect, and we have no evidence that this student uses these ideas effectively when reading or writing. However, he has become explicitly aware of some fundamental and powerful evaluative language and seems to be interested and engaged with the ideas. Further, he is at minimum aware of their potential use, and so is better positioned to employ them in his thinking.</i><br /><br /><i>These are but a few of the ways in which the Elements of Thought and Universal Intellectual Standards are currently being used to improve intellectual communications between teachers and students. For example, some resources can be found in the handbooks provided by the Foundation for Critical Thinking, which contain dozens of sample course designs and assignments from every major subject for k-12 education (see, e.g. Paul et al., 2008). Further guidance can be found in Gerald Nosich’s (2009) Learning to Think Things Through: A Guide to Critical Thinking Across the Curriculum.</i><br /><br /><i>Some Implications for Research</i><br /><br /><i>Of course there are broader implications beyond teaching and learning of a shared academic language. One important direction is that of interdisciplinary research. Currently, much research is confined within a specialized area, and there are relatively few examples of crossdisciplinary research; yet such investigations often produce the most groundbreaking advancements. Further, as world problems are increasingly recognized to be multi-dimensional, researchers are finding it necessary to take a multi-disciplinary approach; an approach which includes team members from different backgrounds who possess varied knowledge and skill sets. One such example is climate change research, which necessitates integrating insights from such disparate disciplines as ecology, chemistry, climatology, biology, meteorology, and physics. Another example is that of drug rehabilitation, which again requires insights from psychology, history, neuroscience, and sociology.</i><br /><br /><i>If effective research is to be done regarding climate change or drug rehabilitation, must individuals then become experts in all these fields? </i><b><i>Even if we had the inclination and potential, the time required makes this highly impractical. Yet the critical-analytical vocabulary offers the possibility for experts to communicate complex ideas in a universal language which is understood by all members of a research team.</i></b><i> Such communication could, would, and should, be done creatively; yet a simple way to start would be to produce the ‘logic’ (again, which includes all the elements of thought) of a given aspect of the project to all participating researchers. Thus, each would be able to grasp the basic ideas and findings fairly quickly, and could then integrate them into their own specialized thinking and planning.</i><br /><br /><i>A similar approach (using the elements of thought to structure a summary of research) could be taken in the presentation of findings to the public: by using common language which is more accessible, researchers might better bridge the gap that often separates academic research from public understanding, acceptance, and implementation.</i><br /><br /><i>Summary and Conclusion</i><br /><br /><i>To summarize: if we look at the academic world as intense with intellectual communications, we see that much is lost in translation between people skilled in different forms of analysis and evaluation. Some of this difference is necessary and helpful; however much of it is counterproductive. I see in the critical-analytical vocabulary of the English language [5] the potential for an academic lingua Franca which could significantly improve communications between teachers, students, researchers, and the public. This article has laid out some of these possibilities.</i><br /><br /><i>What, then, are some important implications of this argument? The first is that we need to think and talk about analytical and evaluative language more explicitly in general. Every field should discuss and consider the important forms of analysis and evaluation central to that discipline, and these should be communicated to students clearly and routinely. Indeed, the ‘Assessment for Learning’ and ‘Learning how to Learn’ (Black and William, 1998; James et al., 2006) projects encouraged faculty to do just this and achieved significant learning improvements as a result.</i><br /><br /><i>More immediately, and perhaps more easily, individuals and communities should increase their fluency in the universal language of analysis and evaluation. If we are to communicate effectively with each other, if we are to understand thoughts and ideas that are presently beyond us, we must improve our ability to analyze and evaluate those communications. The ‘elements of thought’ and ‘universal intellectual standards’ provide us with resilient tools to improve our efforts in this direction. If this vocabulary seems to you intuitive, begin to experiment with it more explicitly in your intellectual communications. Then, finally, after a rich diversity of applications of these elements and standards to a range of intellectual systems, judge for yourself whether they contribute to the power and enrichment of your students’ (and your own) intellectual work.&quot;</i><br /><br />Footnotes<br /><br />[2] That is, most students of most classes will not become professionals in that particular field.<br /><br />[3] This statement is not meant to be all-encompassing.<br /><br />[4] The Paul et. al. (1997) study was randomized and included 120 faculty drawn from 28 public and 29 private colleges and universities across California, encompassing prestigious universities such as Stanford, Cal Tech, USC, UCLA, UC Berkeley, and the California State University System. The Thomas study repeated this protocol with secondary teachers in a San Diego school district with, in the author’s own words, ‘virtually identical results’.<br /><br />[5] The English language concepts and principles can be constructed in parallel forms in all natural languages. The works of the Foundation for Critical Thinking, for example, have been translated into 11 languages so far: Spanish, Dutch, Arabic, Chinese, Japanese, Korean, French, Greek, Polish, Thai, and Turkish.<br /><br />References<br /><br />Arum, R. and Roksa, J. (2011) Academically Adrift: Limited Learning on College Campuses. Chicago: University of Chicago Press.<br /><br />Black, P. and William, D. (1998). Inside the Black Box. London: GL Assessment.<br /><br />Bloom, A. (1988). The closing of the American mind.. NY, NY: Simon &amp; Schuster.<br /><br />Bok, D. (2006). Our underachieving colleges: a candid look at how much students learn and why they should be learning more. Princeton, NJ: Princeton University Press.<br /><br />Casner-Lotto, J. and Benner, M. (2006). Are they really ready to work? Employers’ perspectives on the basic knowledge and applied skills of new entrants to the 21st century U.S. Workforce. Conference Board, Inc.<br /><br />Cosgrove, R. (2010). ‘Critical Thinking: Lessons from a Continuing Professional Development Initiative in a London Comprehensive Secondary School’. Unpublished Masters Thesis. University of Cambridge, Faculty of Education.<br /><br />Cosgrove, R. (2011). ‘Critical Thinking in the Oxford Tutorial: A Call for an Explicit and Systematic Approach’. Higher Education Research and Development, 30(3), 343-356.<br /><br />Gardiner, L. (1995) Redesigning higher education: producing dramatic gains in student learning. ASHE-ERIC Higher Education Report Volume 23, No. 7. Washington, DC: The George Washington University, Graduate School of Education and Human Development.<br /><br />James, M., Black, P., McCormick, R., Pedder, D., and William, D. (2006). Learning How to Learn, in Classrooms, Schools and Networks: aims, design and analysis, Research Papers in Education, (21)2, 101 – 118.<br /><br />Nosich, G. (2009) Learning to Think Things Through. Upper Saddle River, NJ: Prentice Hall.<br /><br />Paul, R. (1995) ‘Why Students – and teachers – don’t reason well’. in Paul, R. (ed) Critical Thinking: How to Prepare Students for a Rapidly Changing World. Dillon Beach, CA: Foundation for Critical Thinking Press.<br /><br />Paul, R. and Elder, L. (2002) Tools for Taking Charge of your Learning and your Life. Upper Saddle River, NJ: Prentice Hall.<br /><br />Elder, L., and Paul, R (2009). The Thinker’s Guide to Intellectual Standards. Dillon Beach, CA: Foundation for Critical Thinking Press.<br /><br />Paul, R., Elder, L., Bartell, T. (1997) California Teacher Preparation for Instruction in Critical Thinking: Research Findings and Policy Recommendations. State of California, California Commission on Teacher Credentialing.<br /><br />Paul, R. Binker, A.J.A., Martin, D., and Andamson, K. (2008). Critical Thinking Handbook: High School. Foundation for Critical Thinking Press: Dillon Beach, CA.<br /><br />Ramsden, P. (2007). Learning to Teach in Higher Education. Routledge Falmer, London.<br /><br />Thomas, P. (1999). Critical thinking instruction in selected greater Los Angeles High Schools. Unpublished dissertation. Azusa Pacific University.<br /><br />Williamson, J. (1991) The Greensboro Plan: Infusing Reasoning and Writing into the K-12 Curriculum. Dillon Beach, CA: Foundation for Critical Thinking Press<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"The Anti-Pattern of Settled Science","summary":"Anti-Patterns are - Oftentimes Common - Patterns in Software Development that are Considered Bad Programming Practice / The Academic Practice of Completely Ignoring \"Settled Science\" Claims is Perhaps the Most Dangerous Anti-Pattern Practiced by the Scientific Community Today / Academic Reformers Should Reconsider the Normalization of this Risk-taking Behavior, because it has the Potential to Lead the Entire Scientific Community to Spend Decades Asking the Wrong Questions","image":"https://lh3.googleusercontent.com/-ktfrraA2sXQ/WEcGbIdRYPI/AAAAAAAAJk0/hl2wX-VhkxI6Q3Q0ek9QSe4MP6Mh0FmtQCL0B/the-anti-pattern-of-settled-science-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/i6e3YHua8z1","publishDate":"2016-12-06T18:43:01.773Z","updateDate":"2017-01-03T01:38:25.586Z","text":"<b>The Anti-Pattern of Settled Science: Anti-Patterns are - Oftentimes Common - Patterns in Software Development that are Considered Bad Programming Practice / The Academic Practice of Completely Ignoring &quot;Settled Science&quot; Claims is Perhaps the Most Dangerous Anti-Pattern Practiced by the Scientific Community Today / Academic Reformers Should Reconsider the Normalization of this Risk-taking Behavior, because it has the Potential to Lead the Entire Scientific Community to Spend Decades Asking the Wrong Questions</b><br /><br />Let&#39;s reason through a very simple sequence of events, which I&#39;ll intentionally keep hypothetical and vague since the lesson is bigger than any one particular instance of the problem.  But be aware that I am basing the logic on actual observation of a number of historical examples ...<br /><br />Imagine that you have a theory which starts to dominate in a domain that exists at the edge of our empirical abilities.<br /><br />Over time, observational and/or experimental capabilities increase and it becomes apparent that our &quot;best&quot; idea is not very predictive.<br /><br />Yet, the scientists keep the dominant theory alive, as they are prone to doing, with ad hoc hypotheses added to the larger theoretical structure.<br /><br />Other competing ideas which have already been considered have been ruled out.  They are the &quot;settled science&quot; claims ...<br /><br />But not everybody agrees.  Buried deep within the mound of scientific journals is a plea by a tiny minority to pay attention to this alternative -- to them equally valid -- way of explaining the data.  They point to their own system of interoperating ideas which base upon a very different set of assumptions and hypotheses.<br /><br />From each side of the debate, it is as though the other is speaking a foreign language.  The minority claims fall on deaf ears, perhaps even evoking ridicule.<br /><br />And yet, the minority plainly sees that this alternative idea can point to a number of successes in predicting the recent anomalous observations.<br /><br />But, the scientific community fails to notice because their favored framework includes a number of mistaken concepts and propositions which now act as &quot;blockers&quot; to learning the numerous components to the alternative framework.<br /><br />Now, operating under the assumption that the theoretical structure is only wrong in the details -- yet sound near the base of their theoretical structure -- the scientific community directs the most sophisticated scientific and mathematical tools known to mankind towards a potentially endless, fruitless search for &quot;new science&quot; at the edges of their own framework.<br /><br />They have embarked upon a journey to prove the textbooks, and since they collectively agree on the &quot;settled&quot; claims, this search is now extremely constrained in the types of questions it can ask.<br /><br />This pattern appears to characterize a surprisingly large number of historical scientific failures -- some of which are probably ongoing at this very moment, of course, because it&#39;s not yet recognized as a common anti-pattern in academic research.<br /><br />Notice that the mistaken approach exhibits three crucial sub-components:<br /><br />(1) <b>The treatment of former competing claims as &quot;settled science&quot;.</b>  Rather than placing these ideas into a special state where they are routinely checked against new observations, settled science claims are completely ignored.<br /><br />(2) <b>The tendency to stop learning of alternative inferences which originate from alternative scientific frameworks, once a believable one is found.</b>  Mel Acheson refers to this as &quot;finder&#39;s fallacy&quot; at <a href=\"https://www.thunderbolts.info/wp/2016/08/17/finders-fallacy/\" class=\"ot-anchor\">https://www.thunderbolts.info/wp/2016/08/17/finders-fallacy/</a>.  I think the term &quot;conceptual blocker&quot; is also instructive in that it reminds us that some concept or proposition is blocking the learning of alternatives.  In terms of the psychology, the Village Venus Effect reminds us of how those individual tendencies can scale to the larger scientific community.  It occurs through our systems of education when those systems become highly homogenized ...<br /><br /><b>The Village Venus Effect</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/cToYviP8nEX\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/cToYviP8nEX</a><br /><br />(3) <b>The idea that we can simplistically rule things out in the sciences sufficient to proceed, only forward -- never backwards -- in the manner of Sherlock Holmes.</b><br /><br /><b>Science - It&#39;s Not Just a Process of Elimination</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/MEBFnq984RD\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/MEBFnq984RD</a><br /><br />The combination of all three of these anti-patterns into one single anti-pattern constitutes risk-taking behaviors.  These risk-taking behaviors have become so normalized by the scientific community that few even notice that the entire structure is vulnerable to collapse -- <b>or even that there exists a significant risk at all</b>.  Each sub-process can be observed to be a problem on its own, without any need to consider the others.  But when they become combined into one single approach, as has occurred in a number of historical instances, this is how science can spend decades asking the wrong questions.<br /><br />It all starts with this notion of &quot;settled science&quot;.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Rejection of Radio Waves from Space","summary":"When Radio Waves were First Observed Coming from Space, Astronomers Assumed it was Either a Hoax or a Mistake / There are Some Important Lessons Here / Many Research Scientists 'Know' So Much that their Chance of Making a Lucky or Creative Discovery May be Severely Curtailed / \"If We Know Too Much, Our Vision is Sometimes Narrowed to the Point Where New Opportunities are Not Seen\" / \"Radio Engineers Didn't Care Where the Radio Waves Came From, and the Astronomers ... 'Could Not Dream Up any Rational Way by which the Radio Waves Could be Generated'\" / Thus, Radio Astronomy Was for Many Years Caught between Two Disciplines","image":"https://lh4.googleusercontent.com/-pBn7e3v9o0I/WETYN9qEm_I/AAAAAAAAJjg/vLw4k2TJBdUr-oMEl85SR64rn08bq2I5ACL0B/the-rejection-of-radio-waves-from-space-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/XWnQdjJBNyr","publishDate":"2016-12-05T03:00:15.013Z","updateDate":"2017-01-03T01:39:40.348Z","text":"<b>The Rejection of Radio Waves from Space: When Radio Waves were First Observed Coming from Space, Astronomers Assumed it was Either a Hoax or a Mistake / There are Some Important Lessons Here / Many Research Scientists &#39;Know&#39; So Much that their Chance of Making a Lucky or Creative Discovery May be Severely Curtailed / &quot;If We Know Too Much, Our Vision is Sometimes Narrowed to the Point Where New Opportunities are Not Seen&quot; / &quot;Radio Engineers Didn&#39;t Care Where the Radio Waves Came From, and the Astronomers ... &#39;Could Not Dream Up any Rational Way by which the Radio Waves Could be Generated&#39;&quot; / Thus, Radio Astronomy Was for Many Years Caught between Two Disciplines</b><br /><br /><i>The Invisible Universe: The Story of Radio Astronomy</i><br />Gerrit L. Verschuur<br />Second Edition<br /><br /><i>&quot;1.1. A Little History</i><br /><br /><i>In 1886, Heinrich Hertz accidentally constructed the first radio transmitter and receiver. In a darkened lecture theater at the Technical College in Karlsruhe, in Germany, Hertz had set up an experiment to test what happened when an electrical current flowed in an open circuit (that is, a circuit with a gap in it). </i><b><i>As he explained the setup to his wife, Elisabeth, he switched on a spark generator, used to produce current, and one of them noticed a simultaneous spark that flashed in an unrelated piece of equipment at some distance away from his main experimental apparatus. Whoever noticed it first, Heinrich or Elisabeth, is unknownto us, but it was Heinrich who made the leap of curiosities that underscore the nature of scientific research. Hertz asked &#39;Why?&#39; and started a systematic search for an answer.</i></b><br /><br /><b><i>Eighty years later historians of science would report that Hertz was at least the sixth physicist to see this odd effect, but he was the first to follow up on his key question.</i></b><i> He proceeded to design a series of brilliantly simple experiments, one after another, in search of an answer. He was able to show that an invisible form of radiation, which he called &#39;electric waves,&#39; carried energy through intervening space. Hertz was also able to demonstrate that the electric waves were a phenomenon very similar to light. In fact their speed through the air was the same as that of light. Today we know that both light and Hertz’s &#39;electric waves&#39; are forms of electromagnetic radiation (see Appendix A.2). Over time, the Hertzian waves (a name used very early in the 20th century) came to be called radio waves. Their frequency is measured in cycles per second, now called Hertz (Hz) ...</i><br /><br /><i>Hertz died tragically at the young age of 35 of blood poisoning from an infected tooth. If he hadn’t, he surely would have won a Nobel Prize in Physics for his discovery.</i><br /><br /><i>After showing that radio waves behave much as light does, except that they are utterly invisible, Hertz did not ask how far they might travel through space. That was left to Guglielmo Marconi, the Italian physicist who performed a series of obsessively creative experiments to prove that radio waves could travel enormous distances and even pass through rock. He was wrong in this latter belief, but he did show that a radio signal could traverse the Atlantic Ocean. The reason that the radio waves made it across despite the curvature of the earth was because the earth’s atmosphere is surrounded by an electrically conductive layer known as the ionosphere and radio waves bounce off that layer to be reflected across the ocean. That wouldn’t be understood until decades later. Meanwhile, Marconi was happy to know that radio waves did go all the way around the earth and it was not long before that ships at sea could signal one another and to their home ports using radio waves. By 1912 the infamous sinking of the Titanic spread awareness that radio transmitters could send an SOS far and wide.</i><br /><br /><b><i>Marconi did wonder whether there might be radio waves reaching earth from space but his equipment would not reveal the existence of the wondrous invisible universe for the same reason that he could signal across the Atlantic. At the low radio frequencies that Marconi used, the reflecting ionosphere not only allows radio signals to bounce around the curvature of the earth, it also prevents radio waves from space from reaching the earth’s surface. Those that do arrive from space are reflected back. (Only if their intrinsic frequency is higher than about 20 MHz such radio waves do reach the ground unimpeded, but then it was not known very much about building receivers at such frequencies.)</i></b><br /><br /><i>1.2. The Birth of Radio Astronomy</i><br /><br /><i>Karl Guthe Jansky, the father of radio astronomy, was employed at Bell Laboratories, which, in 1927, introduced the first transatlantic radiotelephone. For a mere $75 one could speak for three minutes between New York and London, but the radio links were terribly susceptible to electrical interference. The first system operated at the extraordinarily low frequency of 60 kHz (that is, at the very long wavelength of 5 km) and in 1929 a change was made to short waves whose frequencies were in the range of 10–20 MHz. </i><b><i>But the new telephone links were still susceptible to electrical disturbances of unknown nature, which plagued the connections. Jansky was assigned the task of locating the source of the interference.</i></b><i> To carry out his studies he built a rotating antenna (Figure 1.1) operating at 20.5 MHz and by 1930 began taking regular observations. </i><b><i>In 1932 he reported that local and distant thunderstorms were two sources of the radio noise and a third source was &#39;a very steady hiss-type static, the origin of which is not yet known.&#39;</i></b><br /><br /><b><i>During the following year he was unambiguously able to demonstrate that the source of the signals was outside the earth and presented a report entitled &#39;electrical disturbances apparently of extraterrestrial origin.&#39; And so radio astronomy was born.</i></b><br /><br /><i>Just imagine this: When Jansky became convinced he had picked up radio waves from space he enjoyed what few people ever experience -- the thrill of discovery -- seeing something no one had ever seen before. That is part of the reward, the joy, and excitement of doing scientific research.</i><br /><br /><i>Fifty years later, at the National Radio Astronomy Observatory in Green Bank, West Virginia, distinguished radio astronomers gathered to celebrate the anniversary of Jansky’s discovery. </i><b><i>A report entitled &#39;serendipitous discoveries in radio astronomy&#39; grew out of that meeting and it presents the human side of the birth and growth of this science.</i></b><br /><br /><i>&#39;Serendipity&#39; is a term coined by Horace Walpole, the writer and historian, who used it to refer to the experience of making fortunate and unexpected discoveries, according to the fairy tale about the three princes of Serendip (an old name for Ceylon). </i><b><i>Serendipitous discoveries are those made by accident, but also by wisdom; however no one can make an accidental discovery unless that person is capable of recognizing that something of significance is occurring. Jansky was such a person.</i></b><br /><br /><i>In January 1934, in a letter to his father, Jansky wrote:</i><br /><br /><i>&#39;Have I told you that I now have what I think is definite proof that the waves come from the Milky Way? However, I’m not working on the interstellar waves anymore.&#39;</i><br /><br /><i>His boss had set him to work on matters of more immediate concern, matters, which were:</i><br /><br /><i>... not near as interesting as interstellar waves, nor will it bring near as much publicity. I’m going to do a little theoretical research of my own at home on the interstellar waves, however.</i><br /><br /><i>Jansky did not take interest in his new discoveries to the point of building his own antenna so as to pursue his explorations over the weekends. Jansky’s boss, who ruled with an iron hand, was later to encourage him to write another report, and </i><b><i>in 1935 Jansky interpreted the sky waves as coming from the entire Milky Way. But he did not know why and suggested that either a lot of stars were contributing or something in interstellar space was the cause. He realized that if the waves were due to stars he should have picked up the sun. As observed from the surface of the earth the Milky Way happens to reach its maximum brightness in the radio band close to Jansky’s chosen frequency. It is brighter at still lower frequencies, but those radio waves do not penetrate the ionosphere. Furthermore, the ionosphere experiences daily changes of its characteristics and in the daytime blocks out the sun’s radio emission. Thus Jansky’s antenna was blind to its radiation. The mid-1930s were also a time of sunspot minimum, which meant that the ionosphere was transparent to 20 MHz at night. Had Jansky been observing at sunspot maximum, the ionosphere, whose reflecting properties vary with time of day and season as well as sunspot cycle, would have blocked out all 20-MHz radio waves from space and he would not have discovered the signals from the Milky Way.</i></b><br /><br /><i>Jansky failed to pursue his discoveries any further because there were other projects to be done and &#39;star noise could come later&#39; he was told by his employers. It was to be years before significant follow-up work began. A few astronomers in the United States and Europe had become aware of Jansky’s work, but any plans to look more closely at his discoveries had to be shelved when World War II broke out. </i><b><i>In any event, most astronomers knew absolutely nothing about radio receivers and antennas, so how could they get involved? Optical astronomers were just not equipped with the skills necessary to tinker with radio sets. After the war it was mostly radio physicists who launched the new science, and they had to learn astronomy in the process.</i></b><br /><br /><i>After he made his discovery, Jansky wrote to the famous physicist, Sir Edward Appleton:</i><br /><br /><b><i>&#39;If there is any credit due to me, it is probably for a stubborn curiosity that demanded an explanation for the unknown interference and led me to the long series of recordings necessary for the determination of the actual direction of arrival.&#39;</i></b><br /><br /><b><i>Such stubborn curiosity is the hallmark of good scientists.</i></b><i> Jansky trusted his data and continued his measurements for confirmation. His persistence led to the discovery that the source of the static lay in the astronomical heavens.</i><br /><br /><b><i>The story of radio astronomy is replete with apparently amazing cases of fortuitous discoveries, but such discoveries [require] more than good luck. They require a prepared mind and dedicated effort to follow up on what might at first have seemed to be a preposterous new observation. What would have been more preposterous, at least back in 1933, than to learn that radio waves were reaching the earth from all sorts of strange astronomical objects, and even from the beginnings of time and space.</i></b><br /><br /><i>[...]</i><br /><br /><i>2.1. Caught between Two Disciplines</i><br /><br /><i>In 1933 John Kraus, then at the University of Michigan, attempted to detect the sun by using a searchlight reflector to focus the radio waves. He failed because the receiver was not sensitive enough. This was the first use of a reflector-type radio telescope. At the Serendipity meeting, Kraus stated that </i><b><i>meaningful accidental discovery occurs only as the result of &#39;being in the right place with the right equipment doing the right experiment at the right time.&#39; Another noted astronomer, R. Hanbury Brown, added that the person should &#39;not know too much,&#39; otherwise the discovery might not be made!</i></b><br /><br /><b><i>This summarizes a very interesting phenomenon. Many research scientists, especially the theoretically inclined, &#39;know&#39; so much that their chance of making a lucky or creative discovery may be severely curtailed. If we know too much, our vision is sometimes narrowed to the point where new opportunities are not seen. Jansky knew a little astronomy, but not enough for it to get in his way and cause him to reject the possibility that radio waves originating in the cosmos might be real.</i></b><br /><br /><b><i>Grote Reber, a professional engineer and radio ham in his spare time, was one of the few people who recognized the interesting implications of Jansky’s discovery. Reber was certainly not hampered by any astronomical prejudices about whether or not the cosmic radio waves could exist. Instead, he was interested in verifying their existence and followed up on Jansky’s work.</i></b><i> To this end, Reber built the world’s first steerable radio dish antenna (Figure 2.1) in his backyard and mapped the Milky Way radiation during the period 1935–1941. Figure 2.2 shows an example of Reber’s data. He pointed out that </i><b><i>the new field of radio astronomy was originally caught between two disciplines. Radio engineers didn’t care where the radio waves came from, and the astronomers</i></b><br /><br /><b><i>... could not dream up any rational way by which the radio waves could be generated, and since they didn’t know of a process, the whole affair was (considered by them) at best a mistake and at worst a hoax.</i></b><br /><br /><i>The very essence of research is that once an observation is made it requires some understanding and interpretation in order to formulate a plan for making further observations. </i><b><i>It was initially very difficult for astronomers, entirely ignorant of radio technology, to interpret or understand the significance of Jansky’s or Reber’s epoch-making discoveries.</i></b><br /><br /><i>Jesse Greenstein, of Caltech, one of the few astronomers who did get involved before World War II, summed up the dilemma confronting the astronomer of those prewar days:</i><br /><br /><b><i>I did not say that the radio astronomy signals would go away someday, but I didn’t know what next to do.</i></b><br /><br /><i>How could anyone know what next to do? </i><b><i>The mystery of where the radio waves originated was a profound one, not easily solved. Significant new technologies had to be combined with astronomical knowledge in order to carry out radio astronomical research. If the science was to flourish, either astronomers had to learn about radio engineering or radio engineers had to learn astronomy. The new science therefore grew slowly.</i></b><i> The intrusion of World War II may have speeded up its growth because of the intense research in radar techniques, which led to the very rapid development of precisely those types of radio antennas and receivers that the radio astronomers were to require for theirwork. After the war those dishes and receivers became freely available as war surplus equipment.</i><br /><br /><i>[...]</i><br /><br /><i>2.4. Who Could Have Guessed?</i><br /><br /><b><i>In the beginning of 1950, the radio discoveries had barely made an impression on &#39;normal&#39; astronomers.</i></b><i> The new breed of radio astronomers had discovered radio waves from the Milky Way and the Sun, and had managed to locate several radio sources that were optically identified. However, the picture appeared very confusing. Nearby galaxies, such as M31 in Andromeda, were at best faint radio sources while very distant galaxies, such as Virgo A (M87) and Cygnus A, were powerful emitters of radio signals. Centaurus A was associated with the galaxy NGC5128 and was clearly not at all well behaved because it showed a dark dust lane crossing in front of an elliptical galaxy not expected to contain dust. At the time, no accepted theory existed to explain their radio emissions. In fact, around 1950</i><br /><br /><b><i>... radio astronomers were greatly impressed by the almost total lack of connection between radio observations and the visual sky. It did not seem impossible then that there were two separate kinds of celestial objects, each requiring distinct research techniques.</i></b><br /><br /><i>With regard to the problem of explaining the existence of the newly discovered radio sources, Greenstein commented:</i><br /><br /><b><i>No rational explanation that explains the weak (radio) emission from the brightest nearby galaxy, the Andromeda Nebula, can also apply to the faint distant radio source Cygnus A. You have to break down the prejudice that the world is pretty much as you know it, and begin to think of a world which is not like the world you understand.</i></b><br /><br /><b><i>Breaking through preconceived notions is something that has frustrated many a scientist (as well as philosopher, politician, or lay person).</i></b><i> Who, at that time, could possibly have guessed at the amazing scenario that now accounts for the cosmic radio waves. </i><b><i>Radio signals from the Milky Way are produced by cosmic ray electrons spiraling around magnetic fields stretched out in space between the stars. In the 1930s and 1940s no one knew that interstellar space contained cosmic ray electrons or that there were magnetic fields between the stars. At the time, cosmic rays were defined as protons (but not electrons) from space that struck the earth continuously.</i></b><i> Cosmic ray physicists didn’t concern themselves too much about the origin of the cosmic rays, nor did they know what happened to the electrons. Those researchers were mainly interested in studying the composition and physical properties of the particles that did reach their detectors. The absence of electrons was noted, but who would have thought that the electrons didn’t reach the earth because they had wasted their energy radiating radio signals in interstellar space.</i><br /><br /><b><i>After the World War II, Enrico Fermi proposed that cosmic ray electrons could be accelerated in interstellar space, provided magnetic fields were present, but it wasn’t until 1951 that evidence for such fields was obtained through the observation of the polarization of starlight by dust grains aligned by those fields. Later, when supernova remnants (the remains of exploded stars) were recognized to be strong sources of radio waves and when their polarization was measured, astronomers did realize that cosmic rays originated in supernovae. (Polarization refers to a preferred plane of vibration of the incoming radio waves, for example, horizontal or vertical or at some angle in between.) The cosmic ray electrons, spiraling about magnetic fields, cause the supernova remnants to shine. The electrons then leak out into space and ultimately cause the entire Milky Way to glow with radio energy. Who, back in 1950, could ever have dreamt up something so outrageous?</i></b><br /><br /><i>2.5. Identity Crisis</i><br /><br /><i>During the 1950s more radio sources were discovered and cataloged, and arguments raged as to what the new data meant. The first generation of large radio dishes, the 250-ft diameter Jodrell Mark I (1957), and the NRAO 300-ft (1962) were years from completion. Receiver technology was still relatively crude and internally generated receiver noise hampered efforts to detect faint radio signals from space. (Receiver noise is generated because elements of the radio receiver are themselves at some finite, and in those days, quite high temperature.) </i><b><i>By the late 1950s the science appeared to be in a state of relative confusion.</i></b><i> David O. Edge and Michael J. Mulkay, who have traced the early development of radio astronomy in their book, Astronomy Transformed, observed that by 1958:</i><br /><br /><b><i>&#39;... we are ... at a time of maximum uncertainty and confusion in the history of work on radio sources. Agreement between the two major groups engaged in survey work (in Australia and Cambridge) is minimal, and the status of many of the observations is radically in doubt.&#39;</i></b><br /><br /><i>Argument raged and regarding the general state of radio astronomical knowledge at the time these authors ask:</i><br /><br /><b><i>&#39;... what was achieved, by 1958 ...? A handful of optical identifications, of an odd assortment of objects, ‘normal’ and ‘abnormal’; a suggested mechanism for radio emission from some of these (this being very largely the work of optical astronomers and theorists); a growing realization (many having already realized it quite early in the fifties) that the majority of radio sources must be extragalactic ... ; catalogs of sources numbering, for all the hopes, merely hundreds, and those still the subject of controversy; some (but not many) radio diameters, spectra, and a few polarization measures; cosmological claims radically in doubt; source counts in complete disarray ...&#39;</i></b><br /><br /><i>In this quote, the term &#39;source counts&#39; refers to the number of radio sources observed at ever fainter and fainter levels. At the time these counts were believed to be a potential holy grail that would allow us to understand whether the universe began with a bang or existed forever (see Chapter 13), an expectation that was never realized.</i><br /><br /><b><i>Radio astronomers, who were not considered by traditional stargazers to be astronomers until the late 1950s and early 1960s</i></b><i>, had clearly stumbled into a new universe, an invisible universe. </i><b><i>Much like a blind man has to learn his way around his world, the radio astronomer not only had to develop new ways of sensing what was out there, but also had to invent methods for communicating what it was they were discovering. After careful discussion with optical astronomers, the radio astronomers attempted to infer whether their observations related to something known to other astronomers, or whether they were sensing a completely different universe.</i></b><br /><br /><i>The authors of Astronomy Transformed suggested that radio astronomy went through several stages. The first stage began with the discovery of radio waves from objects like the sun and the early exploration of these discoveries. There was a sharing of information between several groups and by the end of this stage (early 1950s) it was recognized that there were several astronomical lines of inquiry involved:</i><br /><br /><i>&#39;During the ensuing stage radio astronomers publish increasingly in optical journals, join optical (astronomy) societies ... and come to hold joint conferences with optical astronomers. Essentially a bond is formed with the ‘real’ astronomers. The radio technology is developed so that good data, which make sense and are repeatable, are generated.&#39;</i><br /><br /><i>2.6. An Epoch of Discovery</i><br /><br /><b><i>It wasn’t until the 1960s that the bond with &#39;real&#39; astronomers began to be forged on a large scale</i></b><i>, following stunning new discoveries made possible by enormous improvements in receiver technology and the construction of large reflector-type radio dishes and interferometers with ever greater baselines, the distance between their individual dishes. These contributed to the radio astronomer’s ability to measure radio source positions with greater accuracy, sufficient to force the attention of the general community of optical astronomers. </i><b><i>The days when an old-timer at a meeting of the Royal Astronomical Society in London was overheard to ask &#39;What is this new-fangled wireless astronomy?&#39; were past</i></b><i> ...&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"The Lesson of the Cracking of Enigma","summary":"There's a Subtle, Important Lesson in the WWII Story / When a Problem Seems Insurmountable, We Need to Reconsider the Traditional, Direct Approach, and Instead Scrutinize Our Process for Inefficiencies / A Similar Moment is Coming for the Traditional Academic Approach to Science","image":"https://lh3.googleusercontent.com/-VafHM32WaYk/WER7yezKFVI/AAAAAAAAJi8/ezzrxemJp2cOe6-ZdaLTvsxUy8wlaRUJACJoC/w1800-h2520/the-lesson-of-the-cracking-of-enigma-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Qd6aZV2ASvR","publishDate":"2016-12-04T20:25:53.551Z","updateDate":"2017-01-03T01:39:20.349Z","text":"<b>The Lesson of the Cracking of Enigma: There&#39;s a Subtle, Important Lesson in the WWII Story / When a Problem Seems Insurmountable, We Need to Reconsider the Traditional, Direct Approach, and Instead Scrutinize Our Process for Inefficiencies / A Similar Moment is Coming for the Traditional Academic Approach to Science: We can Construct an &quot;Information Machine&quot; as a Reaction to the Critiques of Modern Science which can Make Our Discussions of Science More Efficient / The First Step is to Create a Survey of Every Critique and Controversy we can Get Our Hands On</b><br /><br /><b>The Cracking of Enigma</b><br /><br /><a href=\"http://www.iwm.org.uk/history/how-alan-turing-cracked-the-enigma-code\" class=\"ot-anchor\">http://www.iwm.org.uk/history/how-alan-turing-cracked-the-enigma-code</a><br /><br /><i>&quot;Until the release of the Oscar-nominated film The Imitation Game in 2014, the name ‘Alan Turing’ was not very widely known. But Turing’s work during the Second World War was so crucial that Winston Churchill acknowledged his role, saying that Turing made the single biggest contribution to Allied victory. Who was Turing and what did he do that was so important?</i><br /><br /><i>Mathematician</i><br /><br /><i>Alan Turing was a brilliant mathematician. Born in London in 1912, he studied at both Cambridge and Princeton universities. He was already working part-time for the British Government’s Code and Cypher School before the Second World War broke out. In 1939, Turing took up a full-time role at Bletchley Park in Buckinghamshire -- where top secret work was carried out to decipher the military codes used by Germany and its allies.</i><br /><br /><i>Enigma and the Bombe</i><br /><br /><i>The main focus of Turing’s work at Bletchley was in cracking the ‘Enigma’ code. The Enigma was a type of enciphering machine used by the German armed forces to send messages securely. Although Polish mathematicians had worked out how to read Enigma messages and had shared this information with the British, the Germans increased its security at the outbreak of war by changing the cipher system daily. This made the task of understanding the code even more difficult.</i><br /><br /><i>Turing played a key role in this, inventing -- along with fellow code-breaker Gordon Welchman -- a machine known as the Bombe. This device helped to significantly reduce the work of the code-breakers. From mid-1940, German Air Force signals were being read at Bletchley and the intelligence gained from them was helping the war effort.&quot;</i><br /><br /><b>Science&#39;s Information Machine</b><br /><br />It takes some reading to achieve, but for those who take the time to listen to the critics and &quot;heretics&quot; of modern science, they will eventually experience the formation of a tacit sense for how science, in practice, progresses -- or more usefully, how it <b>resists</b> progression.<br /><br />This is not to pretend that studies of the sociology of science do not already exist.  They do.  The problem is that they build upon a philosophical tradition of analyzing <b>former</b> rather than <b>active</b> controversies -- and in the spirit of specialization, the science studies critiques have failed to specifically connect the dots from failures in theory back to failures in process.<br /><br />Careful, sustained inspection of ongoing scientific controversies reveals that the &quot;anti-patterns&quot; of scientific research can oftentimes be traced back to some aspect of the process for publishing or selecting scientific information -- its rewards, discussion and/or vetting. And the message of those problematic processes is that they have oftentimes haphazardly <b>evolved</b>.<br /><br />Even as the rise of technology has forced the reconsideration and reformulation of most business models in existence, our information systems in service of science have largely managed to escape much radical change.  Surely one of the reasons is that unlike business, science is funded by government; thus, if there exists any inefficiences in these two worlds, the short-term need to remain profitable will predictably drive change in business before it will in science.<br /><br />But, consider as well the history of what has happened: The widespread adoption of the peer review process of course preceded the emergence of computer networking -- leaving us with the important observation that our processes for extracting wisdom from the scientific community -- through groups of specialist experts -- have yet to experience a radical redesign in the light of &quot;free&quot; access to global information.  Some of science&#39;s information systems do utilize the Internet, but these systems were never designed as a reaction to critiques.  They are essentially simplistic attempts to mine the crowd for information which root in a tradition of ignoring challenges to textbook theory.<br /><br />Thus, we have good reason to suspect an impending profound change in science&#39;s relationship to information.<br /><br />Now, there has been some debate within the academic community over whether or not the recent movie, <i>The Imitation Game</i>, accurately conveys Turing&#39;s role in the invention of the computer.  That debate is worth checking out, but it could be a technical distraction from a more fundamental lesson about the use of a computer to crack Enigma.<br /><br />The lesson is simply stated as:<br /><br /><b>When a problem seems insurmountable, the proper approach is to &quot;go to process&quot;.</b><br /><br />It&#39;s a crucial lesson because it will eventually also prove to apply to the larger enterprise of science itself.  And what will prove quite humbling for the scientific community in due time -- once we have the benefit of hindsight -- is that it will take an outsider to fix it.  The ceaseless threat of publish-or-perish essentially ensures that academics will remain heads-down, refusing to stop what they&#39;re doing long enough to fix any fundamental process issues which publicly surface.  Rather telling is that when I do rarely succeed in evoking a response from actual academics to critiques of their process <b>by other scientists and academic whistleblowers</b>, they tend to ironically respond that <b>I</b> do not understand &quot;how science works&quot;.<br /><br />Probably many people who have followed the Controversies of Science collection are not fully tuned into the implications of what we can do with this understanding that is being created. There is an opportunity here to construct a parallel world of scientific discourse which is fundamentally <b>designed</b> to be reactive to all of the critiques of modern science.<br /><br />Nobody&#39;s ever done this before.  In fact, it&#39;s only become possible in very recent times -- since the rise of the Internet.<br /><br />Part of the point of the wisdom-of-crowds posts to this collection was to demonstrate the importance of <b>designing</b> our communication infrastructure to service science&#39;s need to remain innovative.  The intent was to show how easy it is for an innovative system to slip into bureacratic groupthink habits.  Innovation should never be a circumstantial aspect of an information system -- yet that is exactly what has happened with today&#39;s academic community: Genius is assumed to be rare and uncrackable by any specific methodology.  Where it happens by chance to emerge, we perhaps have a dim chance of understanding it by slicing up and analyzing the genius&#39; brains (it&#39;s surreal).<br /><br />The mainstream approach to wisdom and genius is absurd in the way it takes for granted the mechanics of the current academic system, bestowing peer review as the best system we can conceive, and then labeling the rare innovator as a remarkable achievement of that very system.  <b>It seems to never dawn upon many people today that the rarity of innovation within the sciences is in fact the product of this very system.</b><br /><br />The infrastructures we currently use to publish, discuss and vet our ideas in the sciences today were not actually <b>designed</b> to innovate at the globally connected scale, since they exhibit no visibility into innovation&#39;s long tail.  Peer review is a rejection of wisdom from the crowd because it&#39;s fundamentally a filter for expertise.<br /><br />We do of course have a crucial need for both filters and experts, but the lesson of the Enigma is that when problems seem insurmountable, we must scrutinize our process for inefficiencies.  Now combine that with the lesson of scientific controversies -- that what sometimes separates culture from some new innovation is the experts themselves.<br /><br />It should be apparent that although we of course rely heavily upon expertise every day, it&#39;s a mistake to <b>only</b> rely upon experts when it comes to claims which have become contested.  We should re-design science&#39;s information systems to also extract wisdom from the crowd -- never straying so far from the pre-existing body of critiques and controversies of science that we lose touch with the reasons for why we are doing it.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Rejection of Epigenetics","summary":"During the Mid-20th Century, Evolutionary Science Underwent a \"Dogmatic Hardening,\" Focusing Excessively on Natural Selection / The Story of Paul Kammerer is a Case in Point / Kammerer was Hailed in 1923 by the NY Times as \"the Second Darwin\" / Kammerer Claimed to Observe the Inheritance of Acquired Traits / He Committed Suicide Just One Month After he was Accused of Fraud / \"The Embarrassing Fact Remains that Mainstream Science Spent More than Half a Century Doubting the Reality of Any Special Mechanisms by which Inheritance Could be Directly Modified by the Environment\"","image":"https://lh3.googleusercontent.com/-SSVclrT6cM0/WEODExomErI/AAAAAAAAJho/ELaKFjttoVwofet7TP857XppaSOnQECxgCJoC/w7142-h9999/the-rejection-of-epigenetics-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/EibeZRc6xyp","publishDate":"2016-12-04T02:45:06.198Z","updateDate":"2017-01-03T01:40:25.311Z","text":"<b>The Rejection of Epigenetics: During the Mid-20th Century, Evolutionary Science Underwent a &quot;Dogmatic Hardening,&quot; Focusing Excessively on Natural Selection / The Story of Paul Kammerer is a Case in Point / Kammerer was Hailed in 1923 by the NY Times as &quot;the Second Darwin&quot; / Kammerer Claimed to Observe the Inheritance of Acquired Traits / He Committed Suicide Just One Month After he was Accused of Fraud / &quot;The Embarrassing Fact Remains that Mainstream Science Spent More than Half a Century Doubting the Reality of Any Special Mechanisms by which Inheritance Could be Directly Modified by the Environment&quot;</b><br /><br /><a href=\"http://phys.org/news/2016-10-re-examination-paul-kammerer-scientific-fraud.html\" class=\"ot-anchor\">http://phys.org/news/2016-10-re-examination-paul-kammerer-scientific-fraud.html</a><br /><br />Re-examination suggests Paul Kammerer&#39;s scientific &#39;fraud&#39; was a genuine discovery of epigenetic inheritance<br /><br />October 31, 2016<br /><br /><b><i>&quot;Paul Kammerer committed suicide in 1926 after being accused of fraud in his famous experiments of &#39;inheritance of acquired traits&#39; with the midwife toad. A new study shows how recent advances in molecular epigenetics and re-examination of his descriptions suggest the experiments were actually authentic.</i></b><br /><br /><b><i>The alleged scientific fraud by Paul Kammerer is perhaps one of the most controversial mysteries in the history of biology. In the early 20th century, he was a famous evolutionary scientist, hailed as a &#39;new Darwin&#39; in the pages of the New York Times. His experiments provided impressive evidence that environmental life experiences could have a direct, inheritable effect on progeny, as maintained by his intellectual predecessor Lamarck, and by Darwin himself.</i></b><i> In one of his most famous experiments, Kammerer had shown how a normally terrestrial species, the midwife toad, could be made to live and mate in water when kept in an artificially heated environment. These modified &#39;water&#39; toads laid eggs that grew into toads with an innate preference to live and mate in water, even when raised in normal, unheated environments. In successive generations of water toads, Kammerer reported that male toads developed nuptial pads on their fingers. These are rough, dark-colored thickenings of skin that are usually absent in midwife toads, but present in other water-loving species, which use them to grasp females during copulation. Additionally, Kammerer crossed one of his modified &#39;water&#39; males with a normal, untreated land female, obtaining 100% water toads in the first generation, and about three-quarters water toads in the second generation. Thus, modified traits were being inherited according to Mendel&#39;s rules of genetics, the same that most of us were taught in high school.</i><br /><br /><b><i>In 1926, fellow scientist G.K. Noble examined a fixed and worn-out specimen of an experimental &#39;water&#39; toad. Noble found that the fingers had been injected with india ink to create an artificial resemblance to nuptial pads. A huge scandal ensued, placing Kammerer&#39;s entire scientific legacy into question. Kammerer committed suicide shortly after, and although he left letters maintaining that his results were authentic, many interpreted the suicide as an admission of guilt. Thus, Paul Kammerer became a symbol of scientific fraud, as well as a powerful story about the ultimate failure and scientific invalidity of studies in the &#39;inheritance of acquired traits.&#39; This view of Kammerer has continued to be the mainstream opinion</i></b><i>, although some voices have defended his innocence. Soviet contemporaries did not accept the fraud accusations, and even made a popular propaganda film, &#39;Salamandra&#39; (1928), that narrated how Kammerer (who was a leftist jew) had been framed by his enemies in the wake of Austrian Nazism. In the 1970&#39;s, the renowned journalist Arthur Koestler wrote a famous book, &#39;The case of the midwife toad,&#39; which pointed out several problems with the fraud accusations, including the fact that the specimens had been observed by several experts, who should not have been easily fooled by crude ink injections.</i><br /><br /><b><i>It is no secret that even today, the &#39;inheritance of acquired characteristics&#39; is often treated as an impossibility, supposedly discarded by experiments such as the amputation of the tail of mice during successive generations, which never leads to mice being born without tails. It was argued that no special mechanism existed by which environmental change could directly modify inheritance, and that every apparent case could be ultimately explained by indirect effects of natural selection and conventional genetics. But these views started to change drastically since the 1990&#39;s, along with the progress in techniques to study molecular genetics. These uncovered several molecular mechanisms, such as DNA methylation, that could directly change inheritance in response to the environment. The modern field of epigenetics studies those changes in gene expression that do not involve a mutation, but are nevertheless inherited in absence of the signal or event that initiated the change. During the 21st century, experiments in mice have reported such inheritable modifications, identifying the relevant genes that have been altered by epigenetic mechanisms.</i></b><br /><br /><i>In light of these recent ground-breaking discoveries, a team of scientists from the University of Chile (Alexander Vargas), the Technical University of Munich (Quirin Krabichler) , and Linköping University (Carlos Guerrero-Bosagna) have re-examined Kammerer&#39;s descriptions of the midwife toad experiments, and have published their conclusions yesterday in the Journal of Experimental Zoology. The situation is perhaps analogous to detectives analysing a journal that describes numerous murders in great detail: There are many opportunities to find out whether it fits with reality, or is merely a work of fantasy. If some of the alleged data in Kammerer&#39;s papers were entirely extraordinary, beyond known phenomena or explanatory mechanisms, we should remain cautious. If, in turn, science offers experimental evidence that matches Kammerer&#39;s reports down to specific details, we may consider a greater likelihood of their authenticity.</i><br /><br /><i>The team concluded that Kammerer&#39;s descriptions are completely within the realm of possibilities uncovered by modern epigenetics. This in itself is an important alternative to fraud and the alleged impossibility of the experiments. However, they additionally confirmed a very special detail about the experiments: Dominance in hybrid crosses would depend on the sex of the toads, such that if Kammerer now crossed a land male with a water female, the first generation would be 100% land toads, and the next, ¾ land toads. </i><b><i>Such switches in dominance according to sex (also known as &#39;parent of origin effects&#39;) have been known to scientists for a long time, but have only recently been understood to result from epigenetic mechanisms. Kammerer and other Lamarckians never placed any importance on this detail, which was mentioned as little more than a curiosity. However, since epigenetic mechanisms produce these effects in several modern experiments, it provides a very specific resemblance, that strongly suggests the authenticity of the midwife toad experiments. There is no good reason why Kammerer would invent such a detail if his descriptions were a mere work of fantasy.</i></b><br /><br /><i>The new in-depth study of Kammerer&#39;s descriptions has also clarified several important details about the experiments, which are shown to favor an epigenetic mechanism over other discussed alternatives. Recent claims that Kammerer would provide changing and unreliable accounts of his experiments are also conclusively discarded by the new study. Gunter Wagner, an evolutionary geneticist and professor at Yale University, calls the new study an &#39;exciting read&#39; and </i><b><i>&#39;the most in depth analysis of Paul Kammerer&#39;s primary publications to date, and the only one that has been performed in the light of recent biological knowledge on epigenetics.&#39;</i></b><br /><br /><i>Why did so many of Kammerer&#39;s contemporaries disbelieve the experiments? Most likely, they were conceptually unprepared to understand the results, many of which are puzzling and hard to make sense of without a modern knowledge of epigenetic molecular mechanisms. An especially important contemporary of Kammerer was William Bateson, who coined the word &#39;genetics.&#39; Bateson described the results of Kammerer&#39;s hybrid crosses as &#39;most astounding,&#39; but doubted their reality, arguing that until they were clearly demonstrated and confirmed, &#39;we are absolved from basing broad conclusions on his testimony.&#39;</i><br /><br /><i>Indeed, Kammerer had a unique way of thinking for his time. While most of his contemporaries would split into opposite &#39;Mendelian&#39; and &#39;Lamarckian&#39; bands, he combined the experimental principles of both schools of thought. He was also unusual among Lamarckians in that he did not believe that the inheritance of acquired traits was necessarily progressive or beneficial, but could also produce neutral or detrimental traits.</i><br /><br /><i>The new paper by Vargas, Krabichler and Guerrero-Bosagna coincides with the very recent publication of a new book in German, &#39;Der Fall Paul Kammerer&#39; (&#39;The case of Paul Kammerer&#39;) by journalist Klaus Taschwer. </i><b><i>This book provides abundant new historical data on the strange circumstances surrounding the discovery of the ink-injected nuptial pads, strongly suggesting that Kammerer was indeed framed. Combined with the scientific approach of Vargas et al., there is now more evidence than ever that suggests the authenticity of the midwife toad experiments.</i></b><i> Conclusive evidence, however, can only come from renewed experimentation with the midwife toad. New molecular techniques would allow a quick assessment of epigenetic mechanisms, and important advances have already been made that demonstrate epigenetic inheritance and parent of origin effects in other amphibians. </i><b><i>To further encourage new research in the midwife toad, Vargas et al have included a model of epigenetic mechanisms that can explain Kammerer&#39;s results through specific assumptions, that can be experimentally tested.</i></b><i> Renewed research in the midwife toad may not only give closure to one of the most intriguing mysteries in the history of biology, but it is also likely to generate important scientific advances on how epigenetic mechanisms work in evolution and adaptation.</i><br /><br /><i>Modern epigenetics may clear Kammerer&#39;s name, and is already carrying out what is likely to be his unfulfilled contribution to science. In an ironic final paragraph of his paper on hybrid crosses, Kammerer alludes to the fact that Mendel himself was overlooked in his lifetime: &#39;May my work fare better than its great predecessor written by Mendel; may it find its useful application in science and economy before the author&#39;s death and before the passing of many decades!&#39;&quot;</i><br /><br />Here&#39;s a sampling of some critique of Kammerer from 1975, which I will follow with a far more recent update that you will observe contradicts many of the claims made here.  We can learn a lot about science by observing the behaviors of scientists who end up losing their debates ...<br /><br />Behavior Genetics, Vol. 5, No. 2, 1975<br />The Case of The Case of the Midwife Toad<br />Lester R. Aronson<br /><br />Received 6 Mar. 1974 Final lO June 1974<br /><br /><i>&quot;The facts concerning Kammerer&#39;s case are reviewed. Koestler&#39;s view that Kammerer was unjustly accused is presented and shown to be incorrect.</i><br /><br /><i>KEY WORDS: midwife toad; Koestler, A.; Kammerer, P.</i><br /><br /><i>De mortuis nil nisi bonum.</i><br /><br /><i>One day in 1938, shortly after I came to The American Museum of Natural History as an assistant to Dr. G. Kingsley Noble, Chairman of the Departments of Herpetology and Experimental Biology, a young colleague whom I had just befriended remarked most profoundly, &#39;Noble, you know, is the man who exposed Kammerer.&#39; Although this notorious scandal had reached its climax 12 years before with the suicide of the then illustrious Professor Paul Kammerer, </i><b><i>I was familiar with this amazing story, for it was regularly told to biology students as an object lesson. In essence, falsifying evidence is just about the worst sin that a scientist can commit since such actions threaten to destroy the very heart of the scientific system. During the intervening years, Kammerer, his writings, and his theories have fallen into oblivion. In a recent, informal poll of gradtiate students and young instructors including one assistant curator of amphibians and reptiles, I found that none had even heard of the Kammerer affair.</i></b><br /><br /><i>From material that I have examined over the years, including Noble&#39;s &#39;Kammerer file,&#39; I gather that many if not most biologists were convinced that Kammerer was guilty, that Noble had performed an important service to science by exposing him, that </i><b><i>the famous geneticist William Bateson was clearly justified in regularly expressing, in the pages of the prestigious professional journal Nature, his intense skepticism concerning the validity of Kammerer&#39;s many experiments</i></b><i>, and that the Lamarckian doctrine concerning the hereditary fixation of structures and processes acquired during an individual&#39;s lifetime lacked valid scientific support. </i><b><i>Arthur Koestler would change all of these views and more. Since he is a superb writer with a fine reputation and since his book, The Case of the Midwife Toad (1971), is modeled after a &#39;whodunnit&#39;</i></b><i> and reads better than the best of them, there is no question that he has successfully rescued Kammerer from the graveyard of forgotten people. In the course of this resurrection Kammerer becomes the righteous and courageous victim of an &#39;inhuman establishment&#39;; Bateson becomes a totally unscrupulous villain who stooped habitually to falsehood and subterfuge in his unjustified attacks on Kammerer; Noble is a clever &#39;ruffian&#39; who was nevertheless duped into believing that he had caught Kammerer red-handed; Hans Przibram, Director of the Vienna Biologisches Versuchsanstalt, where Kammerer performed his experiments, is pictured as a &#39;benevolent and saintly&#39; man beyond reproach; the inheritance of acquired characteristics is revived as a valid and useful theory in evolutionary biology; </i><b><i>and, most important, the scientific community is characterized as basically irresponsible and corrupt.</i></b><br /><br /><i>Koestler writes with great confidence. There is no question that he examined much of the evidence carefully. Most of the statements in his book are documented, and long passages are quoted, especially from letters and articles in Nature. He examined personal correspondence of Bateson and obtained much private information from various sources, especially from Kammerer&#39;s only daughter, Laeerta (named after a genus of lizards), and from Bateson&#39;s only surviving son, Gregory (named after Gregor Mendel). Presented in this authoritative framework, Koestler&#39;s claims and charges should not be taken lightly. </i><b><i>They come, unfortunately, at a time when the antiscience movement is growing (Newell, 1974) and many young people are searching for arguments to reject the system although they have no realistic alternatives. I am afraid that this book will give them only false support and false hopes.</i></b><i> But first the story, which Koestler presents succinctly and clearly.</i><br /><br /><i>The European midwife toad, known to scientists as Alytes obstetricans, lays its eggs on land. The sticky eggs cling to the male, who carries them around until they hatch. Most other frogs and toads spawn in water and this keeps the eggs from becoming adhesive. Like other tailless amphibians, the male Alytes clasps the female&#39;s body with its forelimbs during copulation and the eggs are fertilized as the female expels them. During the spawning season, the males of many amphibians develop spiny, black areas, called nuptial pads, on the outer surfaces of the forelimbs (Noble, 1931). These assist the male in maintaining his clasp, especially when the female&#39;s skin is wet and slippery. When spawning takes place on land, however, the female&#39;s skin is dry, and, as might be expected, the male Alytes does not develop these pads. </i><b><i>By keeping the surrounding temperature high, Kammerer induced Alytes to spawn in water. Although these &#39;water eggs&#39; did not adhere to the male, Kammerer succeeded with great difficulty in raising a small number to maturity. He bred these offspring in water also (Kammerer, 1913a). In this way he raised five generations of water-bred Alytes. In the third generation he noticed faint beginnings of nuptial pads, and these became more pronounced with each subsequent generation. Kammerer claimed that the pads had become acquired hereditary characters. Since the terrestrial midwife toad must have evolved from an aquatic form that originally had pads, Kammerer (1919b) later agreed that the occurrence of pads could be explained as a case of atavism, meaning reversion to the ancestral condition. He emphasized that this experiment did not provide substantial evidence for Lamarckian theory. Referring to his Alytes experiments, Kammerer (1923a) wrote &#39;In my opinion it is by no means a conclusive proof of the inheritance of acquired characters.&#39; The real evidence, he said, came from his experiments with the sea squirt Ciona (Kammerer, 1923a, 1924b). Nevertheless, because of its uniqueness, the midwife toad experiment received a great deal of attention and was widely and incorrectly cited as Kammerer&#39;s crucial test of Lamarckian theory. Koestler, however, blames the undue attention given to this noncritical experiment on the evil machinations of William Bateson, who sensed that it would be Kammerer&#39;s undoing.</i></b><br /><br /><b><i>Between 1919 and 1926 the Controversy raged, mostly in the pages of Nature. Apparently only a very few scientists ever saw any of the Alytes specimens with nuptial pads, because, according to Kammerer (1919b), they appeared only during the breeding season and he hesitated to preserve his subjects since he needed all his precious few animals for breeding further generations. This scarcity of preserved material became an important item in the controversy, and Koestler accepts Kammerer&#39;s explanation. It seems to me that if Kammerer really wanted to preserve his evidence he could have done so by first using the males for breeding and then preserving them with their pads still present.</i></b><br /><br /><b><i>When Kammerer (1923a) gave his famous lectures at Cambridge and London, he brought with him just one preserved water-bred Alytes. This specimen had a large nuptial pad on the left forelimb.</i></b><i> Most of the right pad had been removed previously (Bateson, 1923a; Kammerer, 1923a) for microscopic examination (Koestler mixes this up and says that the left pad had been removed). It is not at all clear whether there ever had been additional preserved evidence, but Koestler implies that there was by noting that Przibram&#39;s Versuchsanstalt had been badly neglected during World War I and preserved material in it may have been destroyed. Also, Dr. Paul Weiss, the renowned embryologist, who was an assistant at the time in the Versuchsanstalt, does recall, in a letter to Koestler, a second specimen with &#39;an equally aberrant &#39;pigment spot.&#39;&#39; In England, the famous specimen was superficially examined by a number of scientists including Bateson. After Kammerer left England, Bateson apparently pondered further about the unusual location of the blackened area on thepalm of the hand, which he described as &#39;a piece of thickened blackish-brown skin&#39; (Bateson, 1923a). He therefore asked Przibram to send the specimen to him for further examination (Bateson, 1923b). Przibram refused, stating that he could not take the responsibility of entrusting the sample to anyone else (Przibram, 1923).</i><br /><br /><b><i>Near the middle 1920s, G. Kingsley Noble joined the controversy. In photographs of stained microscopic sections published by Kammerer, Noble noticed that certain glands in the pad did not look right. Then in 1926 he visited the institute in Vienna and examined the famous specimen. He soon discovered that there was something very wrong (Fig. 1). He and Przibram (Kammerer had already left the Institute) agreed to study the pad independently and they both concluded that the black coloring was nothing but India ink.</i></b><i> These conclusions were based on independent gross and histological examination of the pad as well as chemical analyses of the black pigment. At first, Noble and Przibram planned to publish a joint paper in Nature describing their shocking discovery, and I found this manuscript in Noble&#39;s files. </i><b><i>Later they disagreed because Noble felt that other discrepancies suggested to him that the pad was never geniune while Przibram held that it was originally a genuine pad that had been doctored with ink at a later date. They therefore published their findings independently (Noble, 1926; Przibram, 1926a). A month later, Kammerer shot himself. In a farewell letter to Przibram (1926b), he alleged that an unknown person must have injected the ink.</i></b><br /><br /><i>It is a bit ironic that 3 years before, in 1923, Kammerer had written a long letter to Dr. Henry Fairfield Osborn, President of The American Museum, pleading for a job. In a brief reply, Osborn urged Kammerer to meet with G. K. Noble and discuss the job situation with him. Kammerer apparently met with Noble, and I wonder now whether this meeting stirred Noble&#39;s curiosity about Kammerer&#39;s research.</i><br /><br /><i>Kammerer also wrote a letter to the Moscow Academy of Science a few days before his death. A translation of this letter published in Science (Kammerer, 1926) and reprinted in two books by Conway Zirkle (1949, 1959) reads in part </i><b><i>&#39;After having read the [Noble&#39;s] attack I went to the Biological Experimental Institute for the purpose of looking over the object in question. I found the statements of Dr. Noble completely verified. Indeed, there were still other objects (blackened salamanders) upon which my results had plainly been &#39;improved&#39; post mortem with India ink. Who, besides myself, had any interest in perpetuating such falsifications can only be very dimly suspected.&#39;</i></b><i> Przibram (1926c) also mentions &#39;the existence of other doctored specimens ...&#39; Although Koestler presents a glowing description of the important salamander experiments, he never once mentions that the evidence in these studies had also been tampered with. Could it be that Koestler was not aware of the letter in Science and had seen only Wettstein&#39;s (1926) brief quotation from it in the Neue Freie Presse?</i><br /><br /><i>Koestler has gone to great lengths to convince his readers that a genuine pad was present when the specimen was demonstrated in England in 1923. </i><b><i>He even enlisted the support of the well-known biochemist Holger Hyden, who ran some tests on Alytes showing that the India ink would not remain black when the subjects were preserved for more than a few weeks in a variety of preservatives. This seems to me to have been a useless exercise, for even without the mentality of Sherlock Holmes I can easily imagine the ink being injected on more than one occasion. Koestler asks why the eminent scientists of England who examined the specimen failed to recognize the deception, while a few years later Noble discovered fraud almost immediately.</i></b><i> But did they?</i><br /><br /><i>Dereck Freeman (1972) in a letter to the New Scientist has called attention to a book by Dr. H. Graham Cannon (1959) in which he describes a meeting in England of Kammerer, McBride, Bateson, and himself during April 1923. Bateson, Cannon says, examined the specimen with a hand lens and remarked, &#39;It looks to me like a spot of ink.&#39; Perhaps this observation, rather than Bateson&#39;s evil inclinations, explains his desire to have the specimen returned to England for further study. Koestler omits all references to this episode in his book, but in reply to Freeman he implies that Cannon was in his dotage and did not remember correctly events occurring 36 years earlier. Koestler (1972a) cites other errors in Cannon&#39;s book to bolster his claim. Considering, however, Bateson&#39;s (1923a) open and continued skepticism concerning the pad, the remark that Cannon recalls seems entirely plausible. Eight months after Kammerer&#39;s death, Przibram (1926e) reviewed in Nature all the evidence, especially photographs of the pad and Kammerer&#39;s changing descriptions of them. Przibram came to the conclusion that the pad must have been altered between 1919 and 1922 (before the Cambridge and London lectures, contrary to Koestler&#39;s main argument), and also that Kammerer was aware that the pad looked different in 1923. Koestler dismisses these arguments in like manner, attributing them to Przibram&#39;s &#39;disturbed state of mind&#39; which was &#39;hopelessly muddled.&#39; Koestler&#39;s selective bias is also reflected in his treatment of Alma Mahler-Werfel&#39;s biography (1958). He includes the episode in which Kammerer fell madly in love with her and threatened to shoot himself on Gustav Mahler&#39;s grave unless she married him. On the other hand, he omits her account of her experiences as Kammerer&#39;s assistant: &#39;I kept very exact records. That too annoyed Kammerer. Slightly less exact records with positive results would have pleased him more.&#39; I see this statement as an important clue suggesting that Kammerer may have encouraged his assistants, perhaps unintentionally, to provide the kind of data that he wanted. But Koestler, I am sure, would dismiss Alma&#39;s remark, as can be seen by referring to the following passage from his book: &#39;she got all her facts wrong and wrote with undisguised venom.&#39;</i><br /><br /><i>Koestler considers a number of hypotheses for the suicide. Kammerer was scheduled to leave shortly for Moscow, where he had accepted a Chair in Genetics and was scheduled to build an institute for experimental biology. His theoretical position on the inheritance of acquired characteristics seemed to fit in very well with Marxist philosophy, and the Russians were not at all disturbed by the accusation of fraud -- it was just a capitalist plot. Perhaps, says Koestler, Kammerer just could not leave Vienna, or live permanently in Russia; perhaps his latest conquest, Grete Wiesenthal, would not leave with him for Moscow; perhaps he mourned the loss of his collections at the institute; perhaps a technician had deceived him for one of several reasons. Koestler, however, settles on a political explanation: </i><b><i>Kammerer was a socialist and atheist who talked favorably of the USSR. The Nazi movement was gaining strength and an unidentified enemy who knew of Noble&#39;s pending visit injected the ink so that Kammerer would be caught.</i></b><br /><br /><i>Koestler is convinced that Kammerer himself did not commit the fraud, but this was not the opinion of a number of biologists at the time. A letter found in Noble&#39;s file from the renowned geneticist of Columbia University, Thomas Hunt Morgan, dated October 11, 1926, reads in part &#39;Kammerer has done one more dirty trick in trying to put the fraud over on to his assistants. Remember that this is not the first time, either, that he has been caught, and all responsible people I think will draw the same conclusion.&#39; It is not possible for me to say what other &#39;dirty tricks&#39; Morgan had in mind. Perhaps he was thinking of the sensational news stories that Kammerer engendered during his American lecture tour. Morgan maintained that Kammerer was at least partly responsible for this sensationalism (see Koestler, 1971, p. 92). From a revealing handwritten note to Noble, dated November 15, 1926, from the Viennese biologist Professor Franz Werner, we read in broken English, &#39;You have no doubt done a good work to have unveiled the true nature of Kammerer at least to science. We have never had confidence to him at all, and myself, who has known him still as a boy, knows best, how many lies he has put in the world during his life. But he was a very able and intelligent man and it is a pity that he has not made a better use of the rich gifts of nature, induced, as he always was by the desire of being spoken of, of being the object of enthusiastic admiration of half-grown girls and boys and of the people of his own race, the newspapers of which have much contributed to his rising. Now he is the third of the Biolog. Versuchsanstalt convicted of foul things -- Koppanyi and Finken having been already done.&quot;</i><br /><br /><b><i>The innocence or guilt of Kammerer is far from settled</i></b><i>, and Przibram, despite his &#39;saintliness,&#39; may have been grossly negligent concerning the affairs of his institute. It seems best, therefore, to follow my opening proverb and &#39;say nothing but good of the dead.&#39; But this dictum should apply equally to Bateson, whose actions can just as easily be interpreted as the search for truth as diabolical machinations. On the other hand, calling Noble a &#39;ruffian&#39; on the hearsay evidence of Gregory Bateson is patently false and libelous, although it is true that while Noble endeared himself to some colleagues he antagonized others. Gregory, on the other hand, is obviously so bitter that he even maligned his father: &#39;He was certainly not a nice man whenever the inheritance of acquired characteristics was mentioned.&#39;</i><br /><br /><b><i>Koestler spends much time extolling the virtues of Lamarckian theory and emphasizes the impossibility that chance mutations could have produced the wonderful adaptations that are so commonplace in the world of nature. While these arguments may impress laymen, to the biologist they reveal Koestler&#39;s total lack of understanding of modern concepts of evolution. That recent advances in genetics and molecular biology seem to mesh so well with Mendelian concepts does not impress Koestler, whereas the fact that no biochemical mechanism (indeed, no system of any kind) has been forthcoming to explain how acquired characteristics change the genetic constitution does not bother Koestler one bit.</i></b><br /><br /><i>Koestler&#39;s attack on science and scientists occurs on two fronts. First, he says that frauds are commonplace in science but few scientists are troubled by them. He cites Ronald Fisher&#39;s (1936) accusation that Mendel&#39;s original data must have been doctored that the ratios of his peas are too close to expectancy. But Koestler neglects to say that Sewall Wright (1966) and others have explained the close conformity as the result of unintentional biases which usually occur in measurements. In Mendel&#39;s time, the need for &#39;double-blind&#39; experiments had not yet been realized and no techniques existed for performing them. Kammerer&#39;s (1924a,b) most extensive research dealt with reproductive and color differences in species of European salamanders. In his long critique of Kammerer&#39;s experiments, Cunningham (1928) noted that when Kammerer crossed two naturally occurring forms of Salamandra maculosa he reported an exact 3:1 ratio in the F2 (231 forma typica to 77 forma taeniata) (Kammerer, 1913b). These striking data, which may also have been influenced by unintentional experimental biases, apparently escaped Koestler&#39;s attention.</i><br /><br /><b><i>Equally serious is Koestler&#39;s repeated charge that the scientific community has been irresponsible in not repeating Kammerer&#39;s main experiments.</i></b><i> In the first place, several unsuccessful attempts to do so have been made by various people. Munro Fox tried unsuccessfully to repeat the &#39;crucial&#39; sea squirt study on the inheritance of acquired characters, but Koestler treats this controversy very badly. He attempts to protect Kammerer in a most devious manner and fails to cite Fox&#39;s (1924) definitive report. While Koestler cites the Ciona experiment as Kammerefs strongest evidence favoring the inheritance of acquired characters, he does not explain how amputating sea squirt syphons for several generations differs in essence from amputating mice tails, which he ridicules in an earlier chapter. Kammerer (1924b), on the other hand, was aware of this difficulty and attempted to circumvent it by explaining that the sea squirt responds to mutilation by adaptively regenerating a new syphon, while mice do not regenerate tails. However, he does not explain how regenerating a longer than normal syphon is more of an adaptation to mutilation than not growing a new tail.</i><br /><br /><i>Kammerer always said that failure to repeat his studies resulted from not following his descriptions completely. But a 100% replication of any experiment is never really possible in the life sciences, so that a negative finding cannot be conclusive. </i><b><i>But over and above any need to confirm or disprove, no scientist should feel obligated to investigate a theory that does not meet his interests or give promise of forming the substrate for future investigation.</i></b><i> Theories are not proven or disproven, or voted in or out. They merely gain or lose adherents. This is the way of science. Koestler does not appreciate this and the sterility of Lamarckianism. Finally, </i><b><i>should scientists feel obligated to repeat Kammerer&#39;s experiments aside from their theoretical interpretations? The answer is no. Obviously all phenomena or all possible leads cannot be investigated. The scientist chooses those avenues that promise to bring the greatest advances, those experiments that he believes are most likely to open new vistas.</i></b><i> Kammerer&#39;s experiments were ingenious, and someone may find a renewed interest in them, but not for the reasons that Koestler gives.</i><br /><br /><i>Kammerer espoused a more sensational and scientifically dubious cause and wrote a book translated into English on the subject -- Rejuvenation and the Prolongation of Human Efficiency (1923b). This book is a popular exposition of the then famous but now discredited &#39;Steinach operation.&#39; He explains very clearly how simple vasoligation (now popularly known as vasectomy) causes marked physical rejuvenation of middle-aged men. For women, X-irradiation of the ovaries does the same, but he notes that the techniques are still being developed.</i><br /><br /><i>Kammerer (1919a) was also interested in collecting and analyzing coincidences and published a book on the subject. This phenomenon is akin to parapsychology except that Kammerer deals with naturally occurring situations, whereas in extrasensory perception the experimenter usually arranges the situation. Advocates of ESP talk about unknown psychic processes, whereas Kammerer talks about &#39;a principle in nature which operates independently from the known physical laws of causation.&#39; Koestler seems to be enamoured of this work, and he has made it the subject of another book (Koestler, 1972b) that gives an aura of scientific respectability to spiritualism, clairvoyance, telepathy, psychokinesis, and other invocations of the supernatural. </i><b><i>In the long run, Koestler seems to prefer mystical to rational explanations, thus providing comfort and support to those who wish to avoid valid cause and, with it, legitimate responsibility.</i></b><br /><br /><b><i>Considering the dearth of good writers who can effectively explain science to the public, it is a pity that Koestler is squandering his precious writing talents in ways that will only add confusion to the minds of his lay reader.</i></b><br /><br /><i>ACKNOWLEDGMENTS</i><br /><br /><i>I wish to thank Dr. James W. Atz for his interest, help, and time in the researching and writing of this essay, and Dr. Lee Ehrman for encouraging me to publish it.&quot;</i><br /><br />REFERENCES<br /><br />Bateson, W. (1923a). Dr. Kammerer&#39;s Alytes. Nature (Lond.) 111(2796):738-739.<br /><br />Bateson, W. (1923b). The inheritance of acquired characters in Alytes. Nature (Lond.) 112(2811):391.<br /><br />Cannon, H. G. (1959). Lamarck and Modern Genetics, Charles C Thomas, Springfield, Ill.<br /><br />Cunningham, J. T. (1928). Modern Biology, Kegan Paul, London.<br /><br />Fisher, R. A. (1936). Has Mendel&#39;s work been rediscovered? Ann. Sci. 1:115-137. Reprinted in Stern, C., and Sherwood, E. R. (eds.) (1966). The Origin of Genetics, Freeman, San Francisco.<br /><br />Fox, H. M. (1924). Note on Kammerer&#39;s experiments with Ciona concerning the inheritance of an acquired character. J. Genet. 14:89 91.<br /><br />Freeman, D. (1972). Midwife toad postscript. New Scientist 53(785):509.<br /><br />Kammerer, P. (1913a). Adaptation and inheritance in the light of modern experimental investigation. Ann. Rep. Smithsonian Inst. Wash. 1912:421-442.<br /><br />Kammerer, P. (1913b). Vererbung erzwungener Farbver~inderung. IV. Das Farbkleid des Feuersalamanders (Salamandra maculosa Laurenti) in seiner Abh~ingigkeit yon der Umwelt. Z. Entwicklungsmech. 36:4 193.<br /><br />Kammerer, P. (1919a). Das Gesetz tier Serie, Deutsche Verlagsanstaldt, Stuttgart.<br /><br />Kammerer, P. (1919b). Vererbung erzwungener Farbver~inderungen. I. Die B runstschwiele des Alytes-Manchen aus &quot;Wassereiern.&quot; Arch. Entwicklungsmech. 45:323 370.<br /><br />Kammerer, P. (1923a). Breeding experiments on the inheritance of acquired characters. Nature (Lond.) 111(2793):637-640.<br /><br />Kammerer, P. (1923b). Rejuvenation and the Prolongation of Human Efficiency. Boni and Liveright, New York.<br /><br />Kammerer, P. (1924a). Inheritance of the new. Forum (N. Y.) 71:367-374.<br /><br />Kammerer, P. (1924b). The Inheritance of Acquired Characteristics, Boni and Liveright, New York.<br /><br />Kammerer, P. (1926). Letter to the Moscow Academy of Science. Science 64( 1664):493-494.<br /><br />Koestler, A. (1971). The Case of the Midwife Toad, Random House, New York (paperback 1973, Vantage Books).<br /><br />Koestler, A. (1972a). Koestler replies. New Scientist 53(787):614.<br /><br />Koestler, A. (1972b). The Roots of Coincidence, Random House, New York.<br /><br />Newell, N. D. (1974). Evolution under attack. Nat. Hist. 83(4):32 39.<br /><br />Noble, G. K. (1926). Kammerer&#39;s Alytes. Nature (Lond.) 118(2962):209-210.<br /><br />Noble, G. K. (1931). The Biology of the Amphibia, McGraw-Hill, New York.<br /><br />Przibram, H. (1923). Experiments on Alytes and Ciona. Nature (Lond.) 112(2825):899.<br /><br />Przibram, H. (1926a). Kammerer&#39;s Alytes. Nature (Lond.) 118(2926):210-211.<br /><br />Przibram, H. (1926b). Prof. Paul Kammerer. Nature (Lond.) 118(2972):555.<br /><br />Przibram, H. (1926c). The nuptial pad of Kammerer&#39;s water-bred Alytes. Nature (Lond.) 119(3000):635-636.<br /><br />Werfel, A. M. (1958). And the Bridge is Love, Harcourt Brace, New York.<br /><br />Wettstein, R. (1926). The Vienna Academy of Science and the Kammerer case. Neue Freie Presse, Vienna, Dec. 16.<br /><br />Wright, S. (1966). Mendel&#39;s ratios. In Stern, C., and Sherwood, E. R. (eds.), The Origin of Genetics, Freeman, San Francisco.<br /><br />Zirkle, C. (1949), Death of a Science in Russia, University of Pennsylvania Press, Philadelphia.<br /><br />Zirkle, C. (1959). Evolution, Marxian Biology and the Social Scene, University of Pennsylvania Press, Philadelphia.<br /><br />And now, the 2016 update ...<br /><br /><a href=\"https://vargaslab.files.wordpress.com/2009/03/vargas-et-al-2016.pdf\" class=\"ot-anchor\">https://vargaslab.files.wordpress.com/2009/03/vargas-et-al-2016.pdf</a><br /><br />An Epigenetic Perspective on the Midwife Toad Experiments of Paul Kammerer (1880-1926)<br /><br />Alexander O. Vargas,<br />quirin Krabichler,<br />and Carlos Guerrero-bosagna<br /><br /><i>&quot;Epigenetics: A Modern Framework</i><br /><br /><i>The first step to properly discuss the midwife toad experiments is to review key advances in the field of modern epigenetics, including a phenomenological characterization of recent experiments. One of the first problems that will be encountered by those interested in epigenetics is that the word can currently be used with different meanings, creating some controversy about its proper application. </i><b><i>The word &#39;epigenetics&#39; was originally coined by Conrad Waddington to combine &#39;epi&#39; (&#39;above&#39; or &#39;on top&#39;) with the word &#39;genetics,&#39; while also resembling Aristotle’s developmental concept of &#39;epigenesis.&#39; Waddington used the term for the study of complex interactions among genes and environment in producing the phenotype (Waddington, ’42). Some authors follow this originally broad meaning to refer to any nongenetic factors (environmental or physical) capable of producing developmental variation, regardless of whether they have a stable, inheritable effect. In more recent times, however, the term epigenetics has been increasingly used with a more specific meaning, for the study of changes in gene expression that do not involve a mutation but are, nevertheless, inherited after cell divisions, in the absence of the signal or event that initiated the change (Pearson, 2008; Skinner, 2011). This means changes are stable throughout mitosis, and in multicellular organisms, they may even be inherited through the germ line.</i></b><i> In this article, we will adopt the modern sense of the word and use the term &#39;preepigenetic&#39; to refer to those analyses and accounts that have not considered the recent molecular advances of epigenetics.</i><br /><br /><i>Modern epigenetics is arguably a branch of molecular genetics. However, it uses basic experimental principles that were also popular among the &#39;neo-Lamarckians&#39; of the early 20th century. </i><b><i>Organisms are exposed to an environmental stimulus that leads to an altered phenotype. Then, it is observed whether their progeny will develop the same altered phenotype, without exposure to the same environmental stimulus. If so, the environmental exposure may have induced a heritable modification in the germ line of their parents. The altered phenotype may then continue in further unexposed generations. The more generations it persists, the more stable is the germ line modification. An impressive example of such transgenerational epigenetic inheritance (TEI) is provided by recent experiments on parental olfactory experience (Dias and Ressler, 2014). After being exposed to an odorant, F0 mice were subjected to a stressful sound, learning to dread this odor. They suffered structural modifications of olfactory neurons that express the odorant’s receptor and stressful reactions to the odorant before the onset of the sound. Impressively, the same structural modification of these neurons and the same behavioral response to the odor were found in the subsequent F1 and F2 generations, which were never subjected to such training. Needless to say, the entire experiment smacks of Lamarckism. Perhaps the only reason why the reality of these experiments is no longer questioned is that we are now able to study specific molecular changes that occur in the germ line in response to the environment. For instance, in that experiment, hypomethylation of the odorant receptor gene occurs in the F0 and F1 male germ lines, confirming the role of epigenetic alterations in the inheritance of this induced trait.</i></b><br /><br /><i>Several molecular mechanisms are known to be involved in TEI, including various types of noncoding RNAs, histone modifications, DNA methylation and hydroxy-methylation, and combinations of these mechanisms. Methylation and demethylation of DNA at CpG sites is one of the better-understood mechanisms in modern epigenetics. Early advances on this topic were achieved through the study of genomic imprinting (Reik et al., ’87), a phenomenon underlying congenital diseases such as Prader–Willi and Angelman syndromes. Deleted regions in chromosome 15 can cause these syndromes, but affected people were found with intact sequences (Nicholls et al., ’89). DNA methylation in these sequences was uncovered as a mechanism capable of silencing gene expression. Methylation of a sequence can downregulate its transcription, and a methylated version of a gene can become functionally equivalent to an inactive, recessive allele (Nicholls and Knepper, 2001). The relevant sequences for Prader–Willi syndrome are inactivated through methylation, which normally occurs only in the maternal germ line. The allele inherited from the father will provide for normal development. However, sometimes a zygote will end up with both copies of chromosome 15 inherited from the mother (uniparental disomy): with both copies inactive, this leads to abnormal development and Prader–Willi syndrome. Similarly, inheriting two copies of chromosome 15 from the father results in Angelman syndrome. In this case, another region of chromosome 15 is methylated only in the germ line of the father.</i><br /><br /><i>Genomic imprinting or the selective inactivation of a gene in the germ line of only one parent (mother or father) is found in several epigenetic mechanisms (Surani, 2001). In a hybrid cross, it produces a phenomenon known as &#39;parent-of-origin effects&#39;: whichever the phenotype of the unsilenced parent will be dominant in the offspring. Parent-of-origin effects have been known from the very beginnings of genetics, and even prior, to livestock breeders. The cross of a donkey with a mare will produce a mule, whereas the cross of a jenny (female donkey) with a horse will produce a hinny -- a radically different phenotype. We now understand how these striking differences between mules and hinnies depend on the sex of the parent. Genes of different species are being differentially silenced in their paternally or maternally inherited alleles.</i><br /><br /><i>DNA methylation is not only involved in genomic imprinting, but also in general developmental processes of gene regulation and cell differentiation. While all cells possess essentially the same genome as the totipotential zygote, they differentiate into specialized types (neurons, muscle, etc.) with different genes being selectively silenced. One of these silencing mechanisms is DNA methylation. Gametes are specialized cells and therefore, in order to allow for a totipotential zygote, upon fertilization most of their specific DNA methylation marks are erased or &#39;formatted.&#39; Imprinted genes are an exception (Surani, 2001). DNA methylation survives this formatting process, thus directly affecting the somatic phenotype. Imprinted genes eventually do become erased in primordial germ cells (recruited from somatic cells in the early embryo), which allows them to become remethylated in gametes according to a male or female pattern (Jirtle and Skinner, 2007). Genomic imprinting is an important component of normal development, as exemplified by its disruption in Prader–Willi and Angelman syndromes.</i><br /><br /><i>Importantly, modern epigenetics has shown that environmental changes can alter DNA methylation in the germ line in a way that is stable enough to survive both reformatting episodes (zygote and primordial germ cells), thus being perpetuated for several generations (Guerrero-Bosagna and Skinner, 2012). Initial experimental reports of TEI were made in rodents as a consequence of gestational exposure to endocrine disruptors such as diethylstilbestrol (DES) and vinclozolin. DES is an estrogenic compound prescribed to women from the 1930s to 1970s to prevent miscarriages and other reproductive complications. Later observations showed that DES administration was associated with reproductive defects in the offspring of these treated women (Giusti et al., ’95). Experiments revealed that administration of DES to pregnant rats during early postimplantation development generated reproductive abnormalities of male offspring in the F1 and F2 generations (Newbold et al., 2000). These results led to the suggestion that epigenetic alterations could be involved in the emergence of these reproductive abnormalities and their transmission to future generations. Nonetheless, epigenetic changes in the germ line were only confirmed after experiments involving a gestational exposure to vinclozolin (Anway et al., 2005). Vinclozolin is a fungicide with antiandrogenic effects broadly employed in crops around the world (Wong et al., ’95). Initially, increased apoptosis in spermatogenic cells was observed in the direct (F1) offspring of pregnant rats treated with intraperitoneal injections of the compound (Uzumcu et al., 2004). Follow-up experiments revealed that this increased apoptosis in spermatogenic cells persisted for the next three generations in the absence of any further exposure (Anway et al., 2005; Anway et al., 2006a, 2006b). Changes in DNA methylation of the germ line were also detected three generations after initial maternal exposure (Anway et al., 2005; Guerrero-Bosagna et al., 2010; Manikkam et al., 2012a).</i><br /><br /><i>Importantly, these experiments show </i><b><i>parent-of-origin effects</i></b><i>. </i><b><i>The transmission of the phenotype is sometimes associated with the male germ line, while at other times with the female germ line.</i></b><i> For example, transgenerational effects induced by vinclozolin were shown to be transmitted through the male germ line only (Anway et al., 2005; Anway et al., 2006b), while transgenerational effects related to a developmental exposure to the pesticide methoxychlor were primarily transmitted through the female germ line (Manikkam et al., 2014). Interestingly, germ line transgenerational vinclozolin effects have also been described in imprinted genes (Stouder and Paoloni-Giacobino, 2010), and TEI combined with parent-of-origin effects has also been reported in plants (Haun and Springer, 2008). In addition to vinclozolin and methoxychlor, transgenerational germ line epimutations have been reported to derive from exposures to plasticizers (a mixture of bisphenol A and phthalates), pesticides (a mixture of permethrin and DEET), dioxin (the main component of agent orange), jet fuel 8 (Manikkam et al., 2012b), dichlorodiphenyltrichloroethane (DDT) (Skinner et al., 2013), and dichlorodiphenyldichloroethylene (DDE, the degradation product of DDT, Song et al., 2014). Furthermore, induced transgenerational phenotypes continue to appear in a third generation (grandchildren of exposed parents) or further. In all cases, DNA methylation and its stabilization may occur in the male or female germ line, depending on the environmental signal acting as a stimulus, and the timing of exposure during the development. The numerous case studies of TEI stem from a very diverse sample of distantly related organisms, which suggests the ubiquity and prevalence of these mechanisms (Jablonka and Raz, 2009). </i><b><i>Quite independently of the specific case of Paul Kammerer, this set of scientific advances calls for a reexamination of the experimental work of the &#39;neo-Lamarckians&#39; in the early 20th century. In itself, it suggests that the lack of evidence was not a main reason for their historical &#39;defeat&#39; in academia.</i></b><br /><br /><i>Preepigenetic Accounts On The Midwife Toad Experiments</i><br /><br /><i>Having reviewed the advances of epigenetics, many of which are very recent, we now have the necessary perspective to discuss the midwife toad experiments of Paul Kammerer, and can appreciate the difference with their preepigenetic discussion.</i><br /><br /><i>The essential facts on the biology and natural history of normal midwife toads are well known (Stebbins and Cohen, ’95), setting an undisputed background for discussing Kammerer’s claims. The midwife toad is highly terrestrial for an amphibian, even copulating and fertilizing its eggs on land. Unlike the more aquatic lifestyle of its ancestors (San Mauro et al., 2004), the midwife toad does not lay strings of fertilized eggs in the water. Rather, after copulation, the male wraps the eggs around his legs and carries them on land, which means they are exposed to air during their embryonic development. Finally, during a visit of the male into water, the tadpoles hatch and continue with conventional amphibian development, although the larvae emerge at a more advanced stage than related water-loving species.</i><br /><br /><i>Kammerer, who was a very talented herpetologist, kept and bred midwife toads in terrariums and found that he could, by manipulating the terrarium conditions, induce somatic and behavioral changes some of which were inheritable. The original papers by Kammerer describing the midwife toad experiments are found in several articles written in old-fashioned German (Kammerer, ’06, ’09, ’10, ’11, ’13, ’19). For the larger English-speaking readership, only partial descriptions of Kammerer’s experiments have been available, mostly through second-hand accounts. Inevitably, authors have tended to discuss only those aspects that they considered relevant, according to the knowledge and conceptual tools of their time.</i><br /><br /><i>Until recently, Koestler’s (’71) book provided the most widely known account of Kammerer’s experiments. Koestler described how midwife toads were kept in an artificially overheated room, where they had the choice to remain on land, or visit a basin of water. Under these conditions, the toads lost their terrestrial preference and spent most of their time in the water, even copulating and fertilizing their eggs in it. Eggs were laid directly in the water and were no longer carried by the male. Only a few (3–5%) of these ‘water’ eggs survived. These eggs developed into adult &#39;water-breeding&#39; toads that preferred copulating in water and no longer took care of the eggs, even under normal, nonheated conditions. Kammerer continued to breed these &#39;water-breeding&#39; toads up to F6 generation. </i><b><i>By F3 generation, Kammerer noticed that &#39;water&#39; males were developing nuptial pads during the mating season (upon reexamination, Kammerer found that rudimentary nuptial pads were already present at F2 generation).</i></b><br /><br /><i>Koestler (’71, p. 43) passingly mentions in his book that Kammerer &#39;also made intricate cross-breeding and ovarytransplant experiments, attempting to show that the hereditary changes followed Mendel’s Laws, and to achieve a reconciliation of Lamarckism with Mendelism&#39;. Another account specifically pointed out that &#39;Kammerer back-crossed his midwife toads and salamanders to unmodified individuals and produced hybrids more or less in Mendelian ratios. By Kammerer’s reasoning, this indicated that the acquisition of characteristics caused Mendelian genes to form on the chromosomes&#39; (Gliboff, 2006). These hybrid crosses strike immediately as being crucial to assess the plausibility of Kammerer’s experiments. However, the crucial paper with Mendelian crosses of midwife toads (Kammerer ’11) had appeared in the same rare journal that Gregor Mendel had chosen for his discoveries, Verhandlungen des naturforschenden Vereines in Brünn, which ceased to be published in 1919. Thus, no further details about them are found in preepigenetic accounts on Kammerer’s experiments.</i><br /><br /><i>Epigenetics Enters The Kammerer &#39;crime Scene&#39;</i><br /><br /><i>Intrigued by the possibility that Kammerer may have stumbled upon epigenetic inheritance, one of us (Vargas, 2009) had analyzed a summary account of the midwife toad experiments that appears in Kammerer’s book, &#39;The Inheritance of Acquired Characteristics.&#39; This book was published in 1924 and was aimed at a broad audience, having been translated to English (Kammerer, ’24). The book contains several aspects of the experiments that were not discussed elsewhere, including a more detailed description of the Mendelian crosses between treated and untreated toads. Here, Kammerer made an interesting observation:</i><br /><br /><i>&#39;To be sure, a certain complication arises, inasmuch as the dominant characteristic (i.e.) the one preponderant in all children, and 3/4 of grandchildren, follows the father and, for this reason, a change of dominance from the normal to the changed form is to be observed, depending on whether a normal specimen or a changed specimen plays the part of the father&#39; (Kammerer, ’24, p. 99).</i><br /><br /><b><i>This &#39;complication&#39; (see Figs. 1 and 2) is a clear description of parent-of-origin effects, a detail that bears specific resemblance to currently known experiments like those described above</i></b><i>: Many combine TEI with parent-of-origin effects. The book also provided other details left undiscussed by Koestler, such as the fact that the eggs of &#39;water-breeding&#39; toads were of small size. Also, the coat of eggs spawned in water would swell into a thick gelatinous coat, impeding males from carrying the eggs. In normal land-mating conditions, the coat of the eggs becomes sticky upon contact with the air, helping the male toads to attach them to their legs. Water eggs would also hatch earlier as smaller sized tadpoles, which nevertheless grew faster, becoming larger as adults than normal &#39;land&#39; toads. Yet another crucial experiment was described: If Kammerer scraped the eggs off the legs of normal land-breeding males, and then grew them in water, the resulting toads had normal, land-breeding behavior. </i><b><i>As Kammerer pointed out, this excluded the possibility that the changed breeding habits were merely the result of somatic phenotypic plasticity upon embryonic exposure to water. Rather, they required altered breeding conditions of parental, adult toads, conceivably modifying their germ cells.</i></b><br /><br /><b><i>The fact that parent-of-origin effects were not mentioned in previous discussions is a good example of how modern knowledge changes the focus on previously overlooked details.</i></b><i> Indeed, the possibility of an epigenetic mechanism for Kammerer’s experiments has stimulated a reexamination of Kammerer’s original papers in German (Gliboff, 2010; Svaardal, 2010), highlighting important new aspects. The parent-of-origin effects have now been confirmed with numerical detail of observed phenotypic frequencies (Gliboff, 2010). A very important fact that has finally made its way into the English literature is that most significant changes in the eggs did not occur over successive generations, but in successive breeding seasons of a single F0 parental generation that was kept in the heated room at 25–30°C (Svardal, 2010). Egg survival of the &#39;water eggs&#39; was low in the first breeding season (5–8%), but it increased with successive seasons, until it was equal to the 90% survival of normal eggs. The eggs also became increasingly numerous, smaller (with less yolk), and with thicker gelatinous coat.Water-breeding behavior (mating in water and lack of brood care) stabilized with successive breeding seasons spent in the heated room (Svardal, 2010), such that upon restoring normal temperatures (18°C), the toads would continue water breeding for a greater number of breeding seasons. Importantly, after some breeding seasons spent in the room, they would produce offspring with inherited water breeding, who presented this behavior under normal conditions. Inheritance would also stabilize increasingly, such that offspring from the later breeding seasons continued to display water-breeding behavior for a greater number of seasons at normal temperatures (18°C). Offspring from breeding season 5 and onwards displayed lifelong &#39;water&#39; reproductive behavior, and further inherited it equally stabilized to the next generation of toads. These descriptions point to increased behavioral stabilization and increasingly stable germ line modification, upon repeated breeding seasons spent under changed conditions, consistent with a largely epigenetic mechanism (Svardal, 2010).</i><br /><br /><i>A relevant detail missing from Kammerer’s (’24) book was the conditions in which the eggs resulting from hybrid crosses were raised. It was previously proposed this could have been determined by the male’s instincts, with eggs being exposed to air when the male was a &#39;land&#39; toad, and water when the male was a &#39;water&#39; toad (Vargas, 2009). Since then, it has been clarified that the male followed the female into her preferred habitat, where copulation occurred (Gliboff, 2010; Svardal, 2010). In the case in which the male was a water-breeding toad and the female was a land-breeding toad, copulation occurred on land. Since water males did not carry the eggs (having lost their eggcaring instincts), the eggs remained abandoned on land, where they grew exposed to dry conditions. Thus, the land instincts of the female, rather than the male, determined the conditions in which these eggs (the F1 generation) would develop (Fig. 1). In the case where the male was a land-breeding toad and the female a water-breeding toad, copulation and egg laying occurred in water. However, the land-breeding male always hurried to pull the eggs ashore. The eggs, therefore, would grow exposed to dry conditions, as a result of the male’s preferences (Fig. 2). It has been argued that the male may have succeeded to bring the eggs onto land only in one replicate, but could have failed to do so in others (Gliboff, 2010). The other recent account provides no comment regarding the exposure of these eggs (Svardal, 2010). Clarifying this point is relevant, because the suggestion that these eggs were exposed to air (Vargas, 2009) has been presented by recent comments as a fundamental misunderstanding of Kammerer’s experiments. To assess this and other recent claims, one of us (Q. Krabichler) has investigated Kammerer’s original papers and translated the entire section of his 1911 paper about the crucial Mendelian hybrid crosses, which we provide as a supplementary document to this review (Supporting Information File 1). We are therefore able to contribute to the discussion with more relevant aspects of the experiments, as discussed in Kammerer’s original articles.</i><br /><br /><i>New Information On Kammerer’s Midwife Toad Experiments</i><br /><br /><i>In Kammerer’s (’11) paper, a single replicate was fully described for each type of hybrid cross (water male with land female, land male with water female), while other replicates were only described in summary form since, as Kammerer explains, &#39;it won’t be necessary anymore to report on them in every detail&#39; (Kammerer, ’11, p. 104; see Supporting Information File 1). For the cross in which the male was a &#39;land&#39; toad, the summary of other replicates mentions some variations, but makes no mention of any failed attempts to bring the eggs on land. Nothing like this is mentioned elsewhere in the article, either, despite the fact that Kammerer would often comment on any abnormality, especially regarding breeding behavior. Thus, a straightforward understanding of Kammerer’s article is that these toads succeeded in bringing the eggs out of the water, and the F1 generation was exposed to air during embryonic development, as previously suggested (Vargas, 2009).</i><br /><br /><i>As mentioned above, in the hybrid cross with a &#39;land&#39; male, all F1 toads developed land-breeding habits. Kammerer crossed these F1 toads to obtain an F2 generation of eggs. This resulted in three-fourths of land-breeding toads and one-fourth of toads that had water-breeding instincts, despite all of them having spent their embryonic development exposed to dry conditions. This is consistent with another experiment by Kammerer. Upon taking the eggs from water-breeding toads and raising them on land, this resulted in toads that still presented water-breeding behavior (Kammerer, ’09, pp. 512–513; Kammerer, ‘10, pp. 18–19). As mentioned above, in a converse experiment, Kammerer took eggs from land-breeding toads and raised them in water, obtaining land-breeding behavior. Both experiments discard that breeding behavior could result from mere somatic plasticity during embryonic development. It is worth noting that, unlike breeding behavior, adult body size did show a direct response of somatic plasticity (Svardal, 2010). The eggs of water-breeding toads raised in dry conditions grew into normal-sized, waterbreeding toads, while the eggs of land-breeding toads raised in water resulted in large, land-breeding toads. From these large land-breeding toads, Kammerer bred yet another generation, this time under dry conditions, to check if the increased body size would be inherited. The result was a reversal to normal body sizes (Kammerer dismissed a slight trend to the upper range of normal size variation as &#39;too unclear&#39; Kammerer ’09, p. 501).</i><br /><br /><b><i>It has been argued recently that, through time, Kammerer provided partial and changing accounts of these experiments.</i></b><i> Allegedly, the &#39;controlling&#39; experiment mentioned above (where eggs of land toads were raised in water) was presented in early papers as a successful transformation by water by focusing on traits like body size. Kammerer would have then switched to presenting this experiment in his 1924 book as the inability of water exposure to change the inherited, land-breeding habits (Gliboff, 2010). </i><b><i>The implication is that Kammerer would withhold crucial results, casting doubts over his scientific ethos. However, we found these claims to be unfounded. Kammerer had already reported the persistence of land-breeding habits in 1906, 1909, and 1913.</i></b><i> In ’06, Kammerer had reported growing eggs of normal land toads on soil under humid conditions (daily bathing them in water), from which he obtained normal-sized toads with unaltered breeding behavior (Kammerer, ’09, p. 500 [referring to &quot;Experiment number 3&#39; from Kammerer, ‘06, p. 60 ff]). Moreover, in 1909 he reported similar results for eggs that were either kept on water-saturated soil or completely immersed in water (in the latter case, either taken from brood-caring toads and transferred into water or accidentally lost by brood-caring toads during a bath), obtaining toads with completely normal brood care behavior (Kammerer, ’09, p. 490 ff; p. 501 [referring to &#39;Experiments number 2 and 4&#39; from Kammerer, ’06, p. 58 ff and pp. 69 ff]). These &#39;controlling experiments&#39; (in Kammerer’s own words &#39;Kontrollversuche&#39;), which lead to unchanged breeding behavior, are once again mentioned directly in at least one other paper before 1924 (Kammerer, ’13, p. 161). In summary, </i><b><i>Kammerer always made it clear that a single embryonic exposure was insufficient to modify reproductive behavior, which required adult toads to spend repeated breeding seasons in the heated room.</i></b><br /><br /><b><i>It has also been argued that, according to Kammerer’s own views, temperature was the sole factor capable of inducing water-breeding and other heritable traits, whereas high or low humidity would determine only somatic effects (Svardal, 2010). In fact, Kammerer developed a more complex opinion, with a key role for behavioral plasticity. No factor by itself, either heat or humidity, could bring about the observed results.</i></b><i> Kammerer had set up another experiment with a heated room (25–30°C) where the water basin was removed either entirely or during the breeding season (Kammerer, ’09, p. 461 f.). In these conditions, the egg strings would dry too quickly and become rigid, but the toads would still attempt wrapping them around their legs, even wounding themselves. When Kammerer provided the toads with a wet patch of ground, this impeded the egg coat from drying, and toads could continue with successful landbreeding behavior, despite the high temperatures. These experiments prove that high temperature by itself was insufficient to make the toads abandon their land-breeding habits. The water basin was required to make a successful transition to water breeding. Additionally, Kammerer demonstrated that the same results of his main experiment (i.e., induction of water-breeding behavior) could be obtained if toads spent only breeding seasons under heated conditions, spending the rest of the year under normal, unheated conditions: the only effect was a delay reaching maturity (Kammerer, ’19, p. 329). This experiment demonstrated that neither water nor heat needed to be continuously present. Kammerer also reported yet another compelling experiment. Using toads that had already spent several breeding seasons in the heated room, he initiated a separate breeding line that was always kept under normal, nonheated conditions (Kammerer, ’09, p. 511 ff; Kammerer, ’19, p. 328). Inheritance of water breeding persisted unweakened in the next generations; this in itself is an impressive result because of the stabilization of inheritance. Furthermore, in this line, too, nuptial pads continued to become readily apparent from F3 generation onward, much like the line of toads that were kept constantly under heated conditions. This experiment demonstrated that nuptial pads would make their late appearance despite F1 and F2 generations having been spent in unheated conditions. Only the parental generation was exposed to high temperatures. Importantly, Kammerer reported that nuptial pads were inherited in true fashion, rather than being a merely somatic response. Thus, it is not straightforward to assume that heat alone was responsible for heritable effects. It may be necessary, but not sufficient. In Kammerer’s own words: &#39;Based on these (...) results, however, it can be concluded that in all breeding lines with ‘water eggs,‘ temperature may very possibly only have been the trigger, and not the factor which specifically determined the resulting changes. These in fact should secondarily result from the learning success: the morphological consequences and their inheritance only occur, once heat induction has changed the breeding behavior. The morphological phenomena are probably a result of the eggs being incubated in water instead of air; but the whole course of events -- initiated by the first step of relocating the reproductive instincts into water and depleting them of brood care behavior -- subsequently occurs due to the regime of humidity and density factors, and not anymore due to the temperature&#39; (Kammerer, ’19, p. 329).</i><br /><br /><i>Taken together, our reexamination of Kammerer’s original papers has revealed important new aspects, which other recent accounts had overlooked, despite providing many accurate details. Of course, we are aware that our own review is not the ultimate, decretory representation of Kammerer’s work, either. Instead, our effort is toward encouraging continued translation and research of Kammerer’s writings. No second-hand account, including this review, can be considered a substitute for hundreds of pages of original work.</i><br /><br /><i>Judging Authenticity From Kammerer’s Descriptions: The Importance Of Parent-of-origin Effects</i><br /><br /><i>The data that have entered current discussion, combined with a modern knowledge of epigenetics and inheritance, offer a renewed opportunity to judge the feasibility and authenticity of Kammerer’s results. An important part of doing this analysis properly is that, at least initially, we must take Kammerer’s descriptions at face value. This has been argued to be naïve (Gliboff, 2010), but if scientific analysis alone is to uncover any facts that are inconsistent or implausible, we cannot doubt any data from the outset based only on a historical mistrust of Kammerer.</i><br /><br /><i>In the light of modern scientific knowledge, we can confidently state that Kammerer’s experiments do not contain any phenomenon that cannot be explained by currently known experiments and epigenetic mechanisms. This in itself provides a strong alternative to the traditional belief in fraud. On the other hand, the inheritance of acquired characteristics was a clearly desirable result for any neo-Lamarckian in the early 20th century. As such, the fact that Kammerer reported success in demonstrating this aspect would not specifically suggest authenticity. More dependable evidence, however, is provided by the presence of parent-of-origin effects, because in no way this was an expected or desirable outcome. It did nothing to strengthen any of the arguments of neo-Lamarckians in the early 20th century, who had no theoretical reasons to link these phenomena. </i><b><i>Only in the late 20th century has it become clear how common mechanisms such as DNA methylation underlie parent-of-origin effects, genomic imprinting, and TEI. Experiments combining TEI and parent-of-origin effects have only been published in recent years. The presence of parent-of-origin effects is therefore the centerpiece argument for the authenticity of Kammerer’s midwife toad experiments (Vargas, 2009). Failure to properly acknowledge and discuss this point (as in Svardal, 2010) is a glaring omission, especially if we are to discuss the possibility of an epigenetic explanation.</i></b><br /><br /><i>True, But Irrelevant? Preepigenetic Judgments Of Kammerer’s Experiments</i><br /><br /><i>While the parent-of-origin effects now provide the first argument from the experiments themselves to support their authenticity, Koestler’s historical account of the events surrounding the fraud accusations had already managed to convince many of Kammerer’s potential innocence. However, </i><b><i>a popular preepigenetic opinion was that even if the experiments were real, they were nothing truly out of the ordinary, even in Kammerer’s own time. In the late 20th century, this view was represented in a brief commentary about Koestler’s book penned by Gould (’72), the great paleontologist and science historian. Gould attributed the experimental results to a combination of somatic plasticity, selection, and conventional genetics.</i></b><i> According to Gould:</i><br /><br /><i>&#39;He took hundreds of eggs from females and tried to raise them in an unnatural aqueous environment. Only a few percent survived (pp. 155–156). He then repeated this procedure over several generations. In other words, in each generation he imposed a powerful selection for whatever genetic factors allow an egg to develop successfully in water. His final population differed markedly from natural ones in its progressive accumulation of genes conferring success in aqueous habitats. Is it then surprising that the nuptial pads -- an aqueous adaptation -- gained expression where they had before remained latent?&#39;</i><br /><br /><b><i>In this view, Kammerer never truly deserved consideration as a groundbreaking scientist. It also allowed for comfort in the thought that, despite the sad outcome for Kammerer, true genius was not overlooked, nor was the progress of science slowed down. However, the reexamination of Kammerer’s descriptions has now revealed that the selective process described by Gould never took place.</i></b><i> Egg mortality decreased through environmental modification, within successive breeding seasons of a single generation (Svardal, 2010). Gould’s short commentary also shows no consideration of the controlling experiments, hybrid crosses, or parent-of-origin effects. </i><b><i>This suggests that, beyond relying on Koestler’s account, Gould did not investigate any of Kammerer’s own writings. In a similar vein, Baur, a contemporary of Kammerer, did not accuse him of fraud, but repeatedly argued the midwife toad experiments could be explained by mere somatic plasticity (e.g., Baur, ’11), even after Kammerer’s controlling experiments and hybrid crosses had been published. Kammerer duly denounced these inexplicable omissions (Kammerer, ’13, ’19, ’24).</i></b><br /><br /><b><i>More recently, it has been argued that Kammerer’s experiments were not extraordinary to his contemporaries, and that the fraud accusations were mostly a reaction to Kammerer’s hype and sloppiness (Gliboff, 2010). For instance, it has been pointed out that Kammerer’s contemporaries already knew about parent-of-origin effects, and thus could not have been impressed by the experiments (Gliboff, 2010). This is clearly a misunderstanding. Certainly, the parent-of-origin effects were not in themselves the most novel or controversial aspect and were mentioned as little more than a curiosity by Kammerer himself. They are more interesting to us nowadays‘ scientists, with the hindsight benefit of modern epigenetics, as a hint of authenticity in Kammerer’s experiments. At the time, what was most impressive was that a modified phenotype was being inherited according to Mendelian rules. Kammerer interpreted this as environmentally induced &#39;de novo&#39; formation of new alleles. It is disingenuous to argue that such a possibility would not have been considered radical. However, in modern perspective, Kammerer can hardly be blamed, since an epigenetically silenced gene can become functionally equivalent to a recessive allele, and without molecular tools, they cannot be told apart. Without knowledge of molecular epigenetics, the experiments were indeed hard to make sense of, which may have led to the fraud accusations (Vargas, 2009).</i></b><br /><br /><b><i>More realistically, Kammerer’s contemporaries offered a wide range of reactions, including acclamation, dismissal, and flatout denial.</i></b><i> The challenging nature of Kammerer’s results did not go unnoticed to the influential William Bateson, creator of the word &#39;genetics&#39; and one of the main rediscoverers of Mendel’s laws. Bateson had a significant role in the fraud controversy, since he publicly doubted the reality of the nuptial pads in the famous journal Nature (Bateson, ’19, ’23), thus encouraging fraud accusations. Koestler incorrectly assumed that Bateson was a staunch neo-Darwinist, persecuting Kammerer for this reason (Koestler, ’71). In fact, Bateson (and other early Mendelians) openly questioned Darwinian gradualism, as well as the importance of natural selection (Bateson, 1894; Gould, 2002). Bateson had entirely different reasons to doubt the authenticity of Kammerer’s experiments, namely, the nature of the experiments themselves. This can be appreciated in a revealing commentary in Nature (Bateson, ’23) where he touches on Kammerer’s (’11) paper of Mendelian crosses:</i><br /><br /><i>&#39;The discoveries claimed by Dr. Kammerer are many and extensive. To geneticists, that regarding heredity and segregation in Alytes ([Kammerer] ’11), which I called in question at the Linnean meeting is the most astounding. But what I then heard and saw strengthens me in the opinion expressed in 1913, that until this alleged observations of Brunftschwielen [Nuptial pads] in Alytes have been clearly demonstrated and confirmed, we are absolved from basing broad conclusions on his testimony.&#39;</i><br /><br /><b><i>Note that Bateson speaks of Kammerer’s article on hybrid crosses of midwife toads as &#39;astounding&#39; to geneticists, and leading to &#39;broad conclusions.&#39; This suggests many geneticists indeed had problems with accepting the groundbreaking consequences, causing them to doubt the facts as reported (Vargas, 2009; Wagner, 2009). Importantly, during most of the 20th century, opinions often took a similar vein, alleging mechanistic impossibility to reinforce the fraud accusations. Take this commentary from 1975: &#39;[...] no biochemical mechanism (indeed, no system of any kind) has been forthcoming to explain how acquired characteristics change the genetic constitution [...].&#39; Even direct testing by repeating the experiments was discouraged, since &#39;no scientist should feel obligated to investigate a theory that does not meet his interests or give promise of forming the substrate for future investigation&#39; (Aronson, ’75). Given the advances of modern epigenetics, it is evident that such opinions have now been rendered utterly obsolete. However, we must keep in mind that this important shift has come about only recently.</i></b><br /><br /><i>A Reappraisal Of Kammerer’s Potential Contribution</i><br /><br /><i>In stark contrast with preepigenetic accounts of fraud or irrelevance, the midwife toad experiments, if authentic, would call for a reappraisal of Kammerer’s unfulfilled contribution to early studies of inheritance. Kammerer’s discoveries were indeed unique for his time. The midwife toad experiments combined environmental modification, Mendelian inheritance, and parent-of-origin effects, without any necessity of employing selection. Each phenomenon in isolation had certainly been reported, but experiments combining all three were a novelty, and in recent times, similar experiments have been considered important breakthroughs. It is also worth noting that, for a long time, phenomena like parent-of-origin effects were treated as exceptions to the norm rather than important components of inheritance, under the label of &#39;non-Mendelian genetics&#39; (West-Eberhard, 2003). During the mid-20th century, experiments that evoked the &#39;inheritance of acquired traits,&#39; such as many performed by Conrad Waddington, were considered within the boundaries of conventional genetics, especially because they require selection, which was viewed as playing the crucial role. No special mechanisms of inheritance were thought to be involved. </i><b><i>Truly important implications have only been acknowledged recently, upon the uncovering of specific molecular mechanisms that respond to the environment, in experiments similar to those of the midwife toad, which do not require selection.</i></b><i> Perhaps Kammerer was able to make these discoveries because of his particular intellectual blend. He did not place much importance on selection (Kammerer, ’24) and combined the experimental principles of Lamarckism and Mendelism, two largely opposed schools at the time. These practices anticipated those of modern epigenetics. The effects of environmental modification are nowadays assessed in hybrid crosses through deviation from expected Mendelian frequencies. These may be good reasons to argue that Kammerer deserves consideration as an early forerunner to this field.</i><br /><br /><i>Prospects For New Experiments In The Midwife Toad</i><br /><br /><i>Despite compelling signs of authenticity, only new experiments in the midwife toad can provide conclusive evidence for the authenticity of Kammerer’s reports. Beyond the important consequences for the history of biology, these experiments would also hold great potential for scientific advances. </i><b><i>Currently, the better-studied experiments in TEI deal with pathological phenotypes in laboratory rodents under largely artificial circumstances, such as exposure to pharmacological doses of environmental toxicants. It has long been assumed that the same epigenetic mechanisms should play a role in evolution, inducing and inheriting phenotypes upon naturally occurring environmental changes.</i></b><i> In this regard, the midwife toad may provide an especially clear-cut opportunity to study TEI of adaptive traits and its evolutionary implications.</i><br /><br /><i>While Kammerer’s contemporaries had reported being unable to breed the midwife toad in captivity (Koestler, ’71), during the 1980s, the Mallorcan midwife toad Alytes muletensis (a close relative with the same reproductive habits) was recovered from an almost extinct population in the wild, bred in captivity with great success, and reintroduced to its natural habitat, becoming a classic success story in conservation biology (Buley and Villavicencio, 2000). The Mallorcan midwife toad is small sized and has a fast reproductive cycle, and thus seems ideally suited to become a laboratory model species. The probabilities of success look promising. Evidence for epigenetic mechanisms in amphibians has been available for some time, such as different phenotypes of frogs cloned from different somatic cell types, due to different epigenetic modifications of the donor nuclei (King and Briggs, ‘56), which is consistent with the role of DNA methylation in amphibian development (for instance, see Yakushiji et al., 2007). Parent-of-origin effects in hybrid crosses of different toad species (Elinson, ’77) have now led to evidence of genomic imprinting in anurans (Michalak, 2014, 2015), against the commonplace notion that imprinting is only present in mammals and angiosperm plants.</i><br /><br /><i>Importantly, the impressive body of work left behind by Ryuichi Matsuda (1920–1986) amply documented responses to environmental change in the developmental timing of amphibians (Hall et al., 2004), similar to what Kammerer reported in toads and salamanders. These responses are mediated by hormones and are often inherited transgenerationally, which is in agreement with the fact that endocrine disruptors initiate TEI in a variety of organisms (reviewed in Guerrero-Bosagna, 2015).</i><br /><br /><i>In the light of modern knowledge, we offer a hypothetical model illustrating how epigenetic mechanisms could explain Kammerer’s results for breeding behavior, his main trait of analysis. We provide this model as a supplementary file to this review (Supporting Information File 2). Of course, with absolutely no molecular data, any model of specific mechanisms remains highly speculative. </i><b><i>On the other hand, producing hypotheses with only partial information is a crucial part of the scientific process: confirmation or correction leads to advances.</i></b><i> It is also important to clarify that the fact that we can propose a hypothetical model is far from producing actual proof of authenticity. Conversely, if upon rechecking Kammerer’s extensive descriptions we find some aspect of the model is wrong, this does not mean that the experiments are &#39;inexplicable&#39;, especially if the model is easily corrected. The truly crucial point is to replicate the experiments. If this is achieved, and the underlying mechanisms are unraveled, it is of little importance whether early hypothetical models were correct or not. At this point, the value of producing a hypothetical model is not to argue for or against authenticity, but to stimulate further research, not only of Kammerer’s extensive writings, but in experimental approaches, by providing a set of testable assumptions. Modern molecular tools would allow for quick assessment of epigenetic mechanisms, without the need of replicating the longer, transgenerational experiments of Kammerer. Within a single generation, the epigenomes of the germ cells of control versus treated individuals can be compared to assess for any difference in epigenetic markings. Also, the parent-of-origin effects described by Kammerer suggest that, asides from environmental conditions, paternal or maternal inheritance of alleles should be crucial for the epigenetic status of genes involved. We hope our hypothetical model is encouraging, since we believe our assumptions provide a convincing interpretation for several of Kammerer’s key experiments.</i><br /><br /><i>Conclusion: Moving Away From Cultural Warfare And Closer To Science</i><br /><br /><i>Until this day, many biologists display knee-jerk reactions to any association with Lamarck or the &#39;inheritance of acquired traits.&#39; In contrast, during the early 20th century, many important researchers often referred to Lamarck in a positive light, having been labelled collectively as &#39;neo-Lamarckians.&#39; Of course, many have been quick to point out that most of these scientists, including Kammerer himself, presented important differences with Lamarck (Gliboff, 2006; Svardal, 2010; Burkhardt, 2013). This is hardly surprising, considering that Lamarck belonged to a past generation, when science altogether was a more metaphysical enterprise. And yet, their sympathy for Lamarck contrasts with the attitudes that ensued thereafter, where any effort to create distance from Lamarck was greatly rewarded. </i><b><i>We may justly wonder how our scientific culture became so indoctrinated against anything reminiscing the man that paved the way for Darwin’s descent with modification, and coined the very term &#39;biology.&#39;</i></b><br /><br /><b><i>During the mid-20th century, evolutionary science underwent what has been described as a &#39;dogmatic hardening,&#39; focusing excessively on natural selection (Gould, 2002). This period probably marked the peak of anti-Lamarckism. Certainly, several complex causes explain this turn, and it seems unlikely that Paul Kammerer alone could have changed the outcome. Even so, the incident may have played a more significant role than has been admitted so far. Kammerer was, after all, the most visible Lamarckian of the time. His research was often highlighted in the New York Times, who even described him in 1923 as being &#39;hailed as a second Darwin&#39; (Koestler, ’71). The case of fraud, whether true or not, may have done much to forward the rejection of anything Lamarckian. If Kammerer’s experiments had been accepted as authentic, we can rightfully wonder whether important facts of epigenetics could have made an earlier entrance to mainstream science. It has long been argued that social and cultural factors can condition the progress of science. An example is the slowness with which scientists came to agree upon the basic facts of syphilis disease (Fleck, ’35). The scientific progress of modern epigenetics may ultimately disclose the truth on the case of the midwife toad. However, the embarrassing fact remains that mainstream science spent more than half a century doubting the reality of any special mechanisms by which inheritance could be directly modified by the environment.</i></b><br /><br /><b><i>Even today, we may still need closure regarding Kammerer. One could believe the incident happened so long ago that no one would any longer care, but Kammerer continues to draw unusual criticism. For instance, his retouching and colorization of photographs has been taken out of context to portray Kammerer as &#39;the father of photographic image manipulation&#39; (Weissman, 2010). In fairness, this practice was common at the time, when high-quality photographs were hard to achieve. It was properly disclosed by Kammerer and, much like scientific drawings (which continue to be used today), everybody understood that some faith had to be placed in their correspondence with actual specimens. The fact that even some of Kammerer’s contemporaries demanded better, unretouched photographs reveals the unusual suspicion and criticism placed on Kammerer, rarely afforded to other scientists of the time (Gliboff, 2006). Characterizations of Kammerer’s documentation as &#39;sloppy&#39; (Svardal, 2010) also strike us as unfair for a time in which basic modern tools (such as statistics or photography) were still in the making. The descriptions of modified organisms, which have been argued to be &#39;vague&#39; (Gliboff, 2006), are in our opinion satisfactory, providing explicit details and drawings, with special reference to any variation in experimental animals (morphological, behavioral, and larval). Despite claims to the contrary, the design and execution of the experiments were far from being faulty or ambiguous. They show a keen following of logical implications and great resourcefulness in addressing many variables.</i></b><br /><br /><b><i>Our impression is that Kammerer was a highly intelligent and very dedicated scientist and that most criticism has been facile and disproportionate. Indeed, one of the clearest achievements in Koestler’s anecdote-rich book was to document many unfair attacks flung against Kammerer. Scholars would therefore do well in taking this negative bias into account. In retrospect, the Kammerer episode may be attributed in no small degree to the unfortunate custom of attacking the individual, rather than discussing facts and ideas. Throughout the years, and even today, Kammerer has been called out for having strange beliefs, eugenics, scientifically wrong views, even womanizing, and &#39;amateur musicianship&#39; (Weissman, 2010). We find that Kammerer’s compositions are beautiful (readers may listen to &#39;Paul Kammerer Songs&#39; in the &#39;Darwin Nach Noten&#39; CD released in 2009), but his quality as a composer is quite beyond the point. We do not intend to prove that Kammerer was an infallible or exemplary human being: we only wish to establish whether his experiments could have been authentic. It would be easy to assemble a long list of eminent researchers from the 20th century with questionable beliefs and behaviors, but their scientific contributions cannot be placed in question on this basis. Editors of scientific journals often demand that authors maintain an impersonal discussion of facts and ideas, but even today, articles on Kammerer can take the lowbrow form of cultural warfare, focused on discrediting perceived &#39;enemies&#39; and their motivations, rather than discussing the experiments and their feasibility.</i></b><br /><br /><i>In summary, a near century of bad publicity and anecdotes have been unable to close the case surrounding Kammerer’s midwife toad experiments. Compared to most previous debate, modern epigenetics offers a refreshingly scientific approach. </i><b><i>Historians interested in discussing the authenticity of the experiments may have to move out of their comfort zone and become updated on the continued advances of epigenetics. In any case, an end to the controversy can only come from renewed experimentation on the midwife toad. It is safe to say that Kammerer’s fate now lies fully in the lap of epigenetics. Only researchers in this field can perform these conclusive experiments and decide whether Kammerer deserves a reappraisal.</i></b><br /><br /><i>Acknowledgments</i><br /><br /><i>Special thanks to Jorge Mpodozis, who teaches the undergrad course of evolution at the University of Chile, and taught A.O.V. and C.G.-B. about &#39;Lamarckian experiments,&#39; Waddington, and other topics that are often left out. We thank Joâo Francisco Botelho for comments on an early version of this manuscript. The authors greatly appreciate funding support by Fondecyt, Government of Chile by the grant number 1150906 to A.O.V. and the European Research Council advanced grant ‘GeneWell’ no. 322206.</i><br /><br />LITERATURE CITED<br /><br />Anway MD, Cupp AS, UzumcuM, Skinner MK. 2005. Epigenetic transgenerational actions of endocrine disruptors and male fertility. Science 308:1466–1469.<br /><br />Anway MD, Leathers C, Skinner MK. 2006a. Endocrine disruptor vinclozolin induced epigenetic transgenerational adult-onset disease. Endocrinology 147:5515–5523.<br /><br />Anway MD, Memon MA, Uzumcu M, Skinner MK. 2006b. Transgenerational effect of the endocrine disruptor vinclozolin on male spermatogenesis. J Androl 27:868–879.<br /><br />Aronson LR. 1975. The case of the case of the midwife toad. Behavior Gen 5:115–125.<br /><br />Bateson W. 1894. Materials for the study of variation. London: Macmillan.<br /><br />Bateson W. 1919. Dr. Kammerer’s testimony to the inheritance of acquired characters. Nature 103:344–345.<br /><br />Bateson W. 1923. Dr. Kammerer’s Alytes. Nature 111:738–739.<br /><br />Baur E. 1911. Einführung in Die Experimentelle Vererbungslehre. Berlin: Gebrüder Borntraeger. Available online at: <a href=\"http://www.biodiversitylibrary.org/bibliography/101802\" class=\"ot-anchor\">http://www.biodiversitylibrary.org/bibliography/101802</a>. (Access date: June 30, 2016)<br /><br />Birney E, Smith GD, Greally JM. 2016. Epigenome-wide association studies and the interpretation of disease omics. PLoS Genet 12:e1006105.<br /><br />Buley KR, Villavicencio CG. 2000. The Durrell Wildlife Conservation Trust and the Mallorcan Midwife Toad, Alytes muletensis-into the 21st century. Herpetol Bull 72:17–20.<br /><br />Burkhardt RW Jr. 2013. Lamarck, evolution, and the inheritance of acquired characters. Genetics 194:793–805.<br /><br />Dias BG, Ressler KJ. 2014. Parental olfactory experience influences behavior and neural structure in subsequent generations. Nat Neurosci 17:89–96.<br /><br />Elinson RP. 1977. Macrocephaly and microcephaly in hybrids between the bullfrog Rana catesbeiana and the mink frog Rana septentrionalis (Amphi-bia, Anure, Ranidae). J Herpetol 11:94–96.<br /><br />Fleck L. 1935. Genesis and development of a scientific fact. Chicago, IL: University of Chicago Press (1979, first publication in English).<br /><br />Giusti RM, Iwamoto K, Hatch EE. 1995. Diethylstilbestrol revisited: a review of the long-term health effects. Ann Intern Med 122:778–788.<br /><br />Gliboff S. 2006. The case of Paul Kammerer: evolution and experimentation in the early 20th century. J Hist Biol 39:525–563.<br /><br />Gliboff S. 2010. Did Paul Kammerer discover epigenetic inheritance? No and why not. J Exp Zool B Mol Evol 314:616–624.<br /><br />Gould SJ. 1972. Zealous advocates. Science 176:623–625.<br /><br />Gould SJ. 2002. The structure of evolutionary theory. Cambridge: Harvard University Press.<br /><br />Guerrero-Bosagna C. 2015. Transgenerational epigenetic inheritance: past exposures, future diseases. In: Rosenfeld C, editor. The epigenome and developmental origins of health and disease. Amsterdam: Elsevier Publishing Group. p 425–437.<br /><br />Guerrero-Bosagna C, Skinner MK. 2012. Environmentally induced epigenetic transgenerational inheritance of phenotype and disease. Mol Cell Endocrinol 354:3-8.<br /><br />Guerrero-Bosagna C, Settles M, Lucker B, Skinner MK. 2010. Epigenetic transgenerational actions of vinclozolin on promoter regions of the sperm epigenome. PloS one 5:e13100.<br /><br />Hall BK, Pearson RD, Müller GB, editors. 2004. Environment, development, and evolution: toward a synthesis. Cambridge: MIT Press.<br /><br />Haun WJ, Springer NM. 2008. Maternal and paternal alleles exhibit differential histone methylation and acetylation at maize imprinted genes. Plant J 56:903–912.<br /><br />Jablonka E, Raz G. 2009. Transgenerational epigenetic inheritance: prevalence, mechanisms, and implications for the study of heredity and evolution. Q Rev Biol 84:131–176.<br /><br />Jirtle RL, Skinner MK. 2007. Environmental epigenomics and disease susceptibility. Nat Rev Genet 8:253–262.<br /><br />Kändler R. 1924. Die sexuelle Ausgestaltung der Vorderextremität der anuren Amphibien. Jena Z Naturwiss 60:175–240.<br /><br />Kammerer P. 1906. Experimentelle Veränderung der Fortpflanzungstätigkeit bei Geburtshelferkröte (Alytes obstetricans) und Laubfrosch (Hyla arborea). Arch Entwickl Mech Org 22:48–140.<br /><br />Kammerer P. 1909. Vererbung erzwungener Fortpflanzungsanpassungen. III. Mitteilung: Die Nachkommen der nicht brutpflegenden Alytes obstetricans. Arch Entwickl Mech Org 28:447–545.<br /><br />Kammerer P. 1910. Beweise für die Vererbung erworbener Eigenschaften durch planmässige Züchtung. Vortrag, gehalten am 24. Februar 1910 in der Deutschen Gesellschaft für Züchtungskunde zu Berlin. Verlag der Deutschen Gesellschaft für Züchtungskunde, Berlin.<br /><br />Kammerer P. 1911. Mendelsche Regeln und Vererbung erworbener Eigenschaften. Verhandlungen des naturforschenden Vereines in Brünn 49:72–110.<br /><br />Kammerer P. 1913. Vererbung erzwungener Farbveränderungen. IV. Mitteilung: Das Farbkleid des Feuersalamanders (Salamandra maculosa Laurenti) in seiner Abhängigkeit von der Umwelt. Arch Entwickl Mech Org 36:4–193.<br /><br />Kammerer P. 1919. Vererbung erzwungener Formveränderungen. I. Mitteilung: Die Brunftschwiele des Alytes-Männchen aus “Wassereiern”. (Zugleich: Vererbung erzwungener Fortpflanzungsanpassungen, V.Mitteilung). Arch Entwickl Mech Org 45:323–370.<br /><br />Kammerer P. 1924. The inheritance of acquired characteristics. New York: Boni and Liveright.<br /><br />King TJ, Briggs R. 1956. Serial transplantation of embryonic nuclei. Cold Spr Harb Symp Quant Biol 21:271–290.<br /><br />Koestler A. 1971. The case of the midwife toad. London: Hutchinson. Manikkam M, Tracey R, Guerrero-Bosagna C, Skinner MK. 2012a. Dioxin (TCDD) induces epigenetic transgenerational inheritance of adult onset disease and sperm epimutations. PLoS One 7:e46249.<br /><br />Manikkam M, Guerrero-Bosagna C, Tracey R, Haque MM, Skinner MK. 2012b. Transgenerational actions of environmental compounds on reproductive disease and epigenetic biomarkers of ancestral exposures. PLoS One 7:e31901.<br /><br />Manikkam M, Haque MM, Guerrero-Bosagna C, Nilsson EE, Skinner MK. 2014. Pesticide methoxychlor promotes the epigenetic transgenerational inheritance of adult-onset disease through the female germline. PLoS One 9:e102091.<br /><br />Michalak P. 2014. Evidence for maternal imprinting of 45S ribosomal RNA genes in Xenopus hybrids. Dev Genes Evol 224:125–128.<br /><br />Michalak K, Maciak S, Kim YB, et al. 2015. Nucleolar dominance and maternal control of 45S rDNA expression. Proc R Soc B 282:20152201.<br /><br />Nicholls RD, Knepper JL. 2001. Genome organization, function, and imprinting in Prader-Willi and Angelman syndromes. Ann Rev GenomHum Genet 2:153–175.<br /><br />Nicholls RD, Knoll JH, Butler MG, Karam S, Lalande M. 1989. Genetic imprinting suggested by maternal heterodisomy in non-deletion Prader-Willi syndrome. Nature 342:281–285.<br /><br />Noble G. 1926. Kammerer’s Alytes. Nature 118:209–210.<br /><br />Newbold RR, Hanson RB, Jefferson WN, Bullock BC, Haseman J, McLachlan JA. 2000. Proliferative lesions and reproductive tract tumors in male descendants of mice exposed developmentally to diethylstilbestrol. Carcinogenesis 21:1355–1363.<br /><br />Pearson H. 2008. Disputed definitions: epigenetics. Nature 455:1024.<br /><br />Reik W, Collick A, Norris ML, Barton SC, Surani MA. 1987. Genomic imprinting determines methylation of parental alleles in transgenic mice. Nature 328:248–251.<br /><br />San Mauro D, García-París M, Zardoya R. 2004. Phylogenetic relationships of discoglossid frogs (Amphibia: Anura: Discoglossidae) based on complete mitochondrial genomes and nuclear genes. Gene 343:357–366.<br /><br />Skinner MK. 2011. Environmental epigenetic transgenerational inheritance and somatic epigenetic mitotic stability. Epigenetics 6:838–842.<br /><br />Skinner MK, Manikkam M, Tracey R, Guerrero-Bosagna C, Haque MM, Nilsson E. 2013. Ancestral dichlorodiphenyltrichloroethane (DDT) exposure promotes epigenetic transgenerational inheritance of obesity. BMC Med 11:228.<br /><br />Song Y, Wu N, Wang S et al. 2014. Transgenerational impaired male fertility with an Igf2 epigenetic defect in the rat are induced by the endocrine disruptor p,p’-DDE. Hum Reprod 29:2512–2521.<br /><br />Stouder C, Paoloni-Giacobino A. 2010. Transgenerational effects of the endocrine disruptor vinclozolin on the methylation pattern of imprinted genes in the mouse sperm. Reprod 139:373–379.<br /><br />Stebbins RC, Cohen NW. 1995. A natural history of amphibians. Chichester, West Sussex: Princeton University Press.<br /><br />Surani MA. 2001. Reprogramming of genome function through epigenetic inheritance. Nature 414:122–128.<br /><br />Svardal H. 2010. Can epigenetics solve the case of the midwife toad? — a comment on Vargas. J Exp Zool B Mol Dev Evol 314:625-628. Taschwer K. 2016. Der Fall Paul Kammerer. München: Carl Hanser Verlag.<br /><br />Uzumcu M, Suzuki H, Skinner MK. 2004. Effect of the anti-androgenic endocrine disruptor vinclozolin on embryonic testis cord formation and postnatal testis development and function. Reprod Toxicol 18:765–774.<br /><br />Vargas AO. 2009. Did Paul Kammerer discover epigenetic inheritance? A modern look at the controversial midwife toad experiments. J Exp Zool B Mol Dev Evol 312:667–678.<br /><br />Waddington CH. 1942. The epigenotype. Endeavour 1:18–20.<br /><br />Wagner GP. 2009. Paul Kammerer’s midwife toads: about the reliability of experiments and our ability to make sense of them. J Exp Zool B Mol Dev Evol 312:665–666.<br /><br />Weissmann G. 2010. The midwife toad and Alma Mahler: epigenetics or a matter of deception? FASEB J 24:2591–2595.<br /><br />West-Eberhard MJ. 2003. Developmental plasticity and evolution. Oxford: Oxford University Press.<br /><br />Wong C, Kelce WR, Sar M, Wilson EM. 1995. Androgen receptor antagonist versus agonist activities of the fungicide vinclozolin relative to hydroxyflutamide. J Biol Chem 270:19998–20003.<br /><br />Yakushiji N, Suzuki M, Satoh A, et al. 2007. Correlation between Shh expression and DNA methylation status of the limb-specific Shh enhancer region during limb regeneration in amphibians. Dev Biol 312:171–182.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"The Controversy Graph","summary":"Critics of Modern Science Complain that the Hyper-focus on Specialization has Distracted Us from Synthesizing Together All of those Bits and Pieces / Since the Cutting Edge of Science is Populated by Uncertainty and Controversy, a Scientific Social Network Should Connect what's Happening at this Very Moment - Your Feed - with the Pre-existing Graph of Concepts, Propositions, Models and Worldviews","image":"https://lh3.googleusercontent.com/-Qp-_arnBQeE/WEBNHsXq4RI/AAAAAAAAJgE/JB_7pYlDA0YekGoi4xvv5faVfLiP6FO9wCJoC/w7142-h9999/the-controversy-graph-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/RmaS6RnyXZ7","publishDate":"2016-12-01T16:17:39.870Z","updateDate":"2017-01-03T01:40:06.044Z","text":"<b>The Controversy Graph: Critics of Modern Science Complain that the Hyper-focus on Specialization has Distracted Us from Synthesizing Together All of those Bits and Pieces / Since the Cutting Edge of Science is Populated by Uncertainty and Controversy, a Scientific Social Network Should Connect what&#39;s Happening at this Very Moment - Your Feed - with the Pre-existing Graph of Concepts, Propositions, Models and Worldviews</b><br /><br /><a href=\"http://www.theatlantic.com/technology/archive/2016/12/the-search-for-lost-knowledge/506879/\" class=\"ot-anchor\">http://www.theatlantic.com/technology/archive/2016/12/the-search-for-lost-knowledge/506879/</a><br /><br /><b><i>&quot;Scholars have long wrestled with &#39;undiscovered public knowledge,&#39; a problem that occurs when researchers arrive at conclusions independently from one another, creating fragments of understanding that are &#39;logically related but never retrieved, brought together, [or] interpreted,&#39; as Don Swanson wrote in an influential 1986 essay introducing the concept. &#39;That is,&#39; he wrote, &#39;not only do we seek what we do not understand, we often do not even know at what level an understanding might be achieved.&#39; In other words, on top of everything we don’t know, there’s everything we don’t know that we already know.</i></b><br /><br /><i>Solving this problem, Swanson argued, would require efforts &#39;no less profound than trying to formalize human language, creativity, or inventiveness.&#39; Thirty years after he published his essay, we no longer have to rely on human contrivances alone. Now, with the ubiquity of the internet and the rise of machine learning, a new kind of solution is beginning to take shape. The infrastructure of the web, built to link one resource to the next, was the beginning. </i><b><i>The next wave of information systems promises to more deeply establish links between people, ideas, and artifacts that have, so far, remained out of reach -- by drawing connections between information and objects that have come unmoored from context and history.&quot;</i></b><br /><br /><b>The Controversy-First Approach Starts with the Controversy Card</b><br /><br />When we are grappling with some new idea -- or some idea that we are new to -- we need a broad survey of the reasons to believe.  Accordingly, the selection of the first stories we read on a subject should seek to provide a basic overview. Because people shouldn&#39;t be expected to commit at this stage, it should also be short.  The focus should be at the level of concepts, a review of the history, or the source of the controversy -- whatever mix of those elements serves to pique interest.  No different than telling a joke, the sequence matters.<br /><br />Modeled after a baseball card, the controversy card approach strives to do just that.  Although as information architects, we can never accurately know every context that a person is coming from, each controversy or critique of modern science has its main points.  Readers want to know those upfront -- which is the reason why the summary appears prominently -- and oftentimes just as large as the graphic.  Since people may be quickly scanning many such cards, this text is carefully compressed to its smallest possible unit -- but not at the expense of dropping out crucial information.<br /><br />Since controversies tend to require more than 100 words to explain, they are oftentimes followed by text.<br /><br />When we hear an hour-long talk on some subject, there are two competing approaches to questions -- interject or save them for the end.<br /><br />My hope with the controversy card approach has been to redesign our interactions with scientific controversies by inverting the learning process from the in-the-weeds specialist textbook approach which will predictably lose most readers, to a broad, inclusive generalist approach which gets straight to the point.  The question with each controversy is: How quickly can I get this person to understand the debate?<br /><br />But, the idea has never been to stop there.  The card itself does not satisfy all of our needs.  If we are, for example, filling out a large taxonomy for some obscure topic, it&#39;s intended as a conceptual anchor point, from which the detail of the controversy map fills in.<br /><br />Traditionally, software developers have force-fitted their data into relational tables: this key associates with this value, and a bunch of these key-value pairs belong in this here table.  Although most problems can be shoe-horned into that single approach, the graph database is the more appropriate solution when the relationships between the key-value pairs are just as important as the relationship of the key to the value.<br /><br /><i>Graph Databases</i>, by Ian Robinson, Jim Webber, and Emil Eifrem<br /><br /><b><i>&quot;Graph databases address one of the great macroscopic business trends of today: leveraging complex and dynamic relationships in highly connected data to generate insight and competitive advantage.</i></b><i> Whether we want to understand relationships between customers, elements in a telephone or data center network, entertainment producers and consumers, or genes and proteins, the ability to understand and analyze vast graphs of highly connected data will be key in determining which companies outperform their competitors over the coming decade.</i><br /><br /><i>For data of any significant size or value, graph databases are the best way to represent and query connected data. </i><b><i>Connected data is data whose interpretation and value requires us first to understand the ways in which its constituent elements are related.</i></b><i> More often than not, to generate this understanding, we need to name and qualify the connections between things ...</i><br /><br /><i>This increased focus on graph databases is driven by twin forces: by the massive commercial success of companies such as Facebook, Google, and Twitter, all of whom have centered their business models around their own proprietary graph technologies; and by the introduction of general-purpose graph databases into the technology landscape.&quot;</i><br /><br />When people visit a social network, their interest will tend to root in something that is happening <b>today</b>; I&#39;m talking about the ubiquitous <b>social network feed</b>.  Feeds work.  They serve a simple purpose of characterizing the now.  They&#39;re inherently temporal.<br /><br />On the other hand, in the world of scientific controversies, there exists this pre-existing corpus of material which documents the investigations by others which preceded our own interest; we need as much of that research as possible done for us before we even arrive to a topic -- because this part of the process can take many man-years to compile.<br /><br /><b>A serious problem arises when that pre-existing research becomes completely disconnected from the temporal: Conversations will appear to mostly start and end in the same places.  After observing laypeople interact with controversies for years now on various social media, it&#39;s apparent that most of these conversations never break through the superficial layer that science journalists leave them at.  In a sense, it&#39;s as though we are all being kept in a gestative, baby-talk state on scientific controversies.</b><br /><br />Understanding why that is requires learning the history of science journalism.  In short, science journalism largely abandoned teaching context in the early 1900&#39;s, when it shifted from teaching science as a way to think about the world to the lower bar of simply getting people excited about the prospects of progress.  Although it&#39;s not necessary for this discussion, for those who want to review that history, see ...<br /><br /><b>The Men of Science</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Zzr2XYML8vU\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Zzr2XYML8vU</a><br /><br /><b>The Science Journalist</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/4JJvZu18D9L\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/4JJvZu18D9L</a><br /><br />The source of the problem is plainly the lack of connection between the feed and the pre-existing body of knowledge.  <b>Not only does there need to be a connection between them, but we need to be able to invoke, discuss and refine specific elements of that body of knowledge without dramatically altering our daily process of sorting through the now (going through the feed).</b><br /><br />In short, we need a controversy graph that is deeply connected to our feed.  And to address our scientific need to react to something specific, the root of that connection should grow from annotations.<br /><br /><b>The Annotation</b><br /><br />The subject of annotations is a tricky one in that they probably lack a general-purpose solution.  Since annotations should directly address what the user might do with them in a particular context, they&#39;re going to look and feel different for each space they appear in.  This is probably why annotations have had such a messy history, to date.  I review some of that history here ...<br /><br /><b>The Annotation of Scientific Papers</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/JTW1FYZbDYy\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/JTW1FYZbDYy</a><br /><br />What I&#39;ll be presenting is simply one <b>possible</b> solution.  Realize that there are many ways to do this.<br /><br />What should generally happen -- the process of reacting to something -- is that people will associate a resource with a selection. Those annotations then act much like ant trails which others can subsequently follow into the network graph. From those annotation roots, lines extend outwards: Some are tagged with rebuttals. Others are traced by hypotheses, questions, claims or even speculations.  So long as we keep track of the types of nodes we are creating, the structure will naturally bring order to each controversy for which it fills out.<br /><br />For the text associated with a controversy card, every spot where the cursor might be placed within the text can correspond to a number of annotations which the users can rank.  Thus, the simple act of placing the cursor somewhere in the text will fill out related annotations created by others.<br /><br />Selecting a section of text will do much the same thing -- except that it will of course produce more results.<br /><br />Although the image shown depicts annotations as all controversy cards, there&#39;s no rule which dictates it; <b>part of the value of the graph database is that it exhibits no requirement that the nodes all be of the same type.</b>  This is why I show in the graphic [See element 5] a rich assortment of media types:<br /><br />Wikis<br />Articles and Papers<br />Argument Maps<br />Annotations<br />Concept Maps<br />Infographics<br /><br />Since we are operating at a level of discourse somewhat above the ordinary, our needs are for media types that are more complex than the options for text, images and videos which Google Plus provides today.<br /><br />Since the media type which is appropriate should be determined by the information&#39;s needs, we need access to a spectrum of media types to service this complexity.<br /><br /><b>The Graph Database</b><br /><br />Careful inspection of the rightmost window reveals that our entry point into the controversy graph is the icon within the selected annotation.  <b>This is why each of the controversy cards comes with an icon -- so that a node-like compressed representation of the topic is available.</b><br /><br />Each of the lines of the graph end in other nodes which themselves act as data repositories.  At this point, the nodes need not involve introductory survey materials.  We are now &quot;in the weeds&quot;.<br /><br />The content beneath the graph can help us to understand our future state, if we follow to the selected node.  In the case of the root, it might simply be the text associated with the selected annotation.  But, what I&#39;ve shown here is a helper map of the structure of science, with the type of the current node highlighted: The claim that outsider mavericks matter in science is a proposition.  Such a map could prove to be an extraordinary teaching tool for laypeople who are for the first time diving beneath the superficial.<br /><br />Mapping out claims involves covering all of the argumentative territory, and over time filling in speculations and questions of interest.<br /><br />Each different activity is rated on its own small custom set of values.<br /><br /><b>Eventually, as territories of the map fill in, higher-level activities become possible ... The evolving map will eventually reveal possible experiments which were not formerly apparent when our view of the topic was disorganized.</b><br /><br />New questions will follow. Those in turn will then fill out, and lead into directions which we cannot yet imagine.<br /><br />Eventually, we will come to understand that to the extent that our controversy maps fill out, the order that is brought to these topics will over time help our minds to think more clearly about these issues.  Yes, there is always the ever-present risk of groupthink -- but this is why each of the numerous types of nodes -- concepts, propositions, models and worldviews being just a handful of the total set -- will come with its own short set of values for rating the content ... <b>because a good question should intelligently diverge, whereas a good model should inspire people to converge.</b><br /><br /><b>With the right philosophy of pressing ahead with new questions even when we think we&#39;ve already found the answer, this newly-organized crowd will continue to surprise us with its wisdom.</b><br /><br />The only way to get there is to internalize and process the critiques of modern science until they manifest as design features.  And this is exactly what I&#39;ve tried to do with the controversy graph.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"2 Sociologists Tell the Story of Cold Fusion","summary":"It is Oftentimes Said that the Cold Fusion Scientists Claimed Too Much, Based on Too Little, and in Front of Too Many People / But, Pons and Fleischmann Appear to have been no More Greedy or Publicity-seeking than any Prudent Scientists Would be who Think they have on their Hands a Major Discovery with a Massive Commercial Payoff / Collins and Pinch's Version of the Story Helps Us to Understand How it all Happened - And Why it's Still Possible that there's Something to this","image":"https://lh6.googleusercontent.com/-W4avHAmrg6Q/WD5jTzOy7qI/AAAAAAAAJeE/VZ7tF25lqMEdfu0zOAuVefhxoU7xWCDygCL0B/2-sociologists-tell-the-story-of-cold-fusion-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/ZoCpiYxRGhW","publishDate":"2016-11-30T05:28:49.962Z","updateDate":"2017-01-03T01:41:08.214Z","text":"<b>2 Sociologists Tell the Story of Cold Fusion: It is Oftentimes Said that the Cold Fusion Scientists Claimed Too Much, Based on Too Little, and in Front of Too Many People / But, Pons and Fleischmann Appear to have been no More Greedy or Publicity-seeking than any Prudent Scientists Would be who Think they have on their Hands a Major Discovery with a Massive Commercial Payoff / Collins and Pinch&#39;s Version of the Story Helps Us to Understand How it all Happened - And Why it&#39;s Still Possible that there&#39;s Something to this</b><br /><br /><i>The Golem: What You Should Know About Science</i><br />by Harry Collins and Trevor Pinch<br /><br />Chapter 3: The sun in a test tube: the story of cold fusion<br /><br /><i>&quot;When two chemists working at the University of Utah announced to the world&#39;s press on 23 March 1989 that they had discovered fusion, the controlled power of the hydrogen bomb, in a test tube, they launched the equivalent of a scientific gold rush. And the gold was to be found everywhere -- at least in any well-equipped laboratory. The two scientists were Martin Fleischmann and Stanley Pons.</i><br /><br /><i>The apparatus was simple enough (see figure 3.1): a beaker of heavy water (like ordinary water but with the hydrogen atoms replaced by &#39;heavy hydrogen&#39;, otherwise known as deuterium); a palladium &#39;electrode&#39; known as the cathode, and a platinum electrode, known as the anode. A small amount of the &#39;salt&#39;, lithium-deuteroxide, was added to the heavy water to serve as a conductor. Though these substances are not in everyday use, and are rather expensive, they are quite familiar to any modern scientist; there is nothing exotic about the apparatus. Put a low voltage across this &#39;cell&#39; for a period of up to several hundred hours, and out should come the gold: fusion power. The heavy hydrogen atoms should fuse together into helium, releasing energy; this is the way the sun is powered. The telltale signs of fusion were heat and nuclear byproducts such as neutrons -- sub-atomic particles -- and traces of the super-heavy hydrogen atom, tritium.</i><br /><br /><i>Pons and Fleischmann added an intriguing tease to the account of their success. They warned that the experiment was only to be attempted on a small scale. An earlier cell had mysteriously exploded vaporising the palladium and producing a large hole in the concrete floor of the laboratory. Luckily it had happened during the night and no-one was hurt.</i><br /><br /><i>The experiment seemed straightforward and there were plenty of scientists willing to try it. Many did. It was wonderful to have a simple laboratory experiment on fusion to try after the decades of embarrassing attempts to control hot fusion. This effort required multi-billion dollar machines whose every success seemed to be capped with an unanticipated failure. &#39;Cold fusion&#39; seemed to provide, as Martin Fleischmann said during the course of that famous Utah press conference, &#39;another route&#39; -- the route of little science.</i><br /><br /><i>Scientists the world over immediately started scrambling for information about the experiment. Details were hard to come by. Faxes, electronic mail networks, newspapers and television all played a role. Some scientists did not wait for details. That same night enterprising students at MIT started the first attempted replications based on a video of a television news programme on which the apparatus had briefly been shown. </i><b><i>Such experiments had little chance of success because the exact conditions employed by Pons and Fleischmann were not yet known.</i></b><i> ... cold fusion experiments were to suffer from their apparent simplicity -- at least in the early days before scientists recognised just how complicated a palladium-deuterium electrolytic cell could be. Within a week a photocopied manuscript showing the technical details of the experiment became available. Now replication started with a vengeance. Scarce supplies of palladium were bought up and pieces of equipment were scavenged from everywhere. Many stayed up all night nursing their electrolytic cells. Science had seen nothing like it; neither had the world&#39;s press which ran continuous news items and updates of progress. It was &#39;science by press conference&#39; as scientists queued up to announce their latest findings and predictions to the media.</i><br /><br /><i>And for a while it looked as if cold fusion was real. Amazingly, in the week following the first announcement it became clear that there was not just one Utah cold fusion group, but two. The second independent group was located at nearby Brigham Young University and they too had been getting positive results for the previous three years. This group, headed by physicist Steven Jones, had not found excess heat, but had detected neutrons from a cold fusion cell (although at a much lower level than claimed by Pons and Fleischmann). Both groups had submitted their results to the prestigious scientific journal Nature.</i><br /><br /><i>Texas A&amp;M University soon announced to the waiting media that they too were seeing excess heat from a cold fusion cell, and then came an announcement from Georgia Tech that they were seeing neutrons. Positive results were reported from Hungary and elsewhere in Eastern Europe. Rumours of positive results came in from all over the scientific world. The Japanese were supposed to be launching their own massive detection programme.</i><br /><br /><i>Patents had been filed on behalf of Pons and Fleischmann by the University of Utah. </i><b><i>Indeed part of the reason for the press release before the results were published in a scientific paper (a breach of scientific etiquette which was to be held against the scientists) was the University of Utah&#39;s concern to ensure priority over the nearby group at Brigham Young University.</i></b><i> Utah seemed set to become the Gold Rush State with the State Legislature meeting and voting $5 million towards the cold fusion effort. Congress was approached for a possible further $25 million. Even President Bush was being kept appraised of developments.</i><br /><br /><i>But then doubts started to surface. It transpired that Georgia Tech had made a mistake; their neutron detector turned out to be heat sensitive. The Texas A&amp;M excess heat measurements were explained away by an improperly earthed temperature sensitive device. Groups at MIT and national laboratories such as Lawrence Livermore and Oak Ridge were not yet seeing anything. Pons and Fleischmann&#39;s paper was mysteriously withdrawn from Nature. Congress decided to put the $25 million on hold.</i><br /><br /><i>At the American Physical Society meeting that May in Baltimore, with the ever-present media circus in attendance, criticism reached a crescendo. An MIT group claimed that Pons and Fleischmann had incorrectly interpreted their evidence for neutrons; a prestigious California Institute of Technology (Cal Tech) group reported detailed replication attempts, all negative, and cast doubt upon the correctness of the Utah measurements of excess heat; and finally a Cal Tech theorist pronounced that cold fusion was extremely improbable theoretically and accused Pons and Fleischmann of delusion and incompetence. The University of Utah pair were not at the meeting to defend themselves, but Steven Jones from the other Utah cold fusion group was there. Unfortunately for Pons and Fleischmann, even Jones distanced himself from their work, claiming that he too had doubts about the measurements of excess heat.</i><br /><br /><i>For most of the gathered community of physicists, already sceptical about whether chemists could overturn cherished assumptions about fusion physics, enough was enough. </i><b><i>Gold became fool&#39;s gold, or that at least is how the story goes. As we shall see, like most of the episodes examined in this book, there is more to be said; much more.</i></b><br /><br /><i>The little science route to fusion</i><br /><br /><i>One can chart the rise and decline of cold fusion from the price of palladium. On 23 March 1989, just before the announcement of the discovery, the price was $145.60 an ounce. By May 1989 at the height of the cold fusion frenzy the price had risen to $170.00 an ounce. Prices plummeted following the Baltimore APS meeting. As of today (October 1992) the price has fallen back to $95.00 an ounce.</i><br /><br /><b><i>It is palladium, or rather one property of palladium, which provided the impetus to the search for cold fusion. It is known that palladium has a surprising ability to absorb vast quantities of hydrogen. If a piece of palladium is &#39;charged&#39; with as much hydrogen as it can absorb, then the pressure inside the crystal lattice dramatically increases. Perhaps at such high pressures the normal barrier of positive charge (known as the Coulomb barrier) preventing nuclei coming together to fuse could be overcome. It was a long-shot but scientists before Pons and Fleischmann had actually tried to produce fusion between hydrogen nuclei this way.</i></b><br /><br /><i>In the 1920s, soon after the discovery of the atomic structure of matter, two German chemists working at the University of Berlin attempted to produce fusion of hydrogen using palladium. Fritz Paneth and Kurt Peters were not interested in fusion as a source of energy but in the product, helium, which was used in airships. New ways to make helium were urgently sought by German industry because the USA, the main commercial supplier, refused to sell helium to Germany after the First World War. Paneth and Peters, knowing of palladium&#39;s affinity for hydrogen, set up an experiment in which they passed hydrogen over red-hot palladium. They claimed to detect the presence of small amounts of helium. Unfortunately they later discovered that the probable source of the helium was gas already absorbed in the glass walls of their apparatus. However, their work was taken up another scientist and inventor: John T andberg, a Swede who worked at the Electrolux Corporation Laboratory in Stockholm.</i><br /><br /><i>Tandberg had remarkably similar ideas to those of Pons and Fleischmann 60 years later, or so it seems with hindsight. In 1927 he had applied for a patent for a device to manufacture helium by the electrolysis of water with a palladium cathode. In his device hydrogen produced at the cathode entered the palladium lattice and there, with the huge pressures induced by absorption, underwent fusion to produce helium. That at least was the claim. The only substantial difference between Tandberg&#39;s device and the later set-up of Pons and Fleischmann was the use of light water as the electrolyte. Tandberg&#39;s patent was rejected because the description was said to be too sketchy. However, after the discovery of deuterium (in the 1930s) Tandberg pursued the work further by attempting to create fusion in a wire of palladium which had been saturated with deuterium by electrolysis. It seems he met with little success, at least in regard to the production of helium.</i><br /><br /><i>Pons and Fleischmann were unaware of the earlier work when they started their experiments in 1984. </i><b><i>Martin Fleischmann is one of Britain&#39;s most distinguished electrochemists.</i></b><i> Stanley Pons (an American), visited the University of Southampton to study for his PhD in 1975 and that is where the two met. Fleischmann, who was Faraday Professor of Electrochemistry at Southampton, had a reputation for being the sort of scientist who liked to carry out high-risk science, pursuing bold and innovative ideas and approaches. Indeed he had built his career with such work and some of the risks had paid off. Fleischmann had made a number of important discoveries, as recognised by his election as a Fellow of the Royal Society in 1986.</i><br /><br /><i>The reason for Fleischmann being in Utah in 1989 had to do with British Prime Minister, Margaret Thatcher. The Thatcher cutbacks of funding to British universities in 1983 meant that Fleischmann was forced to take early retirement from the University of Southampton (where he retained an un-paid position). He became a freelance researcher teaming up with Pons who was by now a productive scientist in his own right, and chair of the University of Utah Chemistry Department. Pons was aged 46 and Fleischmann 62 when the discovery was announced. </i><b><i>Pons, too, had a reputation for successful high-risk science. Pons and Fleischmann were well aware that cold fusion was a long-shot. They initially funded experiments with $100,000 of their own money, expecting to see at most tiny traces of tritium and perhaps some neutrons. The levels of excess heat they detected were a complete surprise.</i></b><br /><br /><i>Jones&#39; involvement</i><br /><br /><i>The announcement of 23 March 1989 cannot be understood without reference to the work of the other Utah group, led by Steven Jones at Brigham Young University. While the scientific community were unfamiliar with Pons and Fleischmann&#39;s work on cold fusion, they had been following Jones&#39; progress for several years. </i><b><i>In 1982 Jones and his colleagues had undertaken a major experimental effort looking for fusion triggered by sub-atomic particles produced at the Los Alamos particle accelerator. They had found far more evidence of such fusions than theory would have led them to expect, but not enough to make a source of viable energy. Like hot fusion research, particle-induced fusion was a frustrating step a way from the excess energy output needed for commercial exploitation.</i></b><br /><br /><b><i>Jones had moved on to consider how very high pressures might encourage hydrogen isotopes to fuse. The key breakthrough in his thinking came in 1985 when Brigham Young geophysicist Paul Palmer drew his attention to the anomaly of excess heavy helium (helium-three) found near volcanoes. Palmer and Jones thought this could be explained by deuterium contained in ordinary water undergoing geologically induced cold fusion inside the earth.</i></b><br /><br /><i>The Brigham Young group pursued the idea, attempting to reproduce the geological processes in the laboratory. They were searching for a metal, traces of which in rock might serve as a catalyst for fusion. They built an electrolytic cell essentially similar to that of Tandberg and tried various materials for the electrodes. Soon they too decided that palladium, with its ability to absorb hydrogen, was the most likely candidate. The group built a low-level neutron detector to measure any fusion which was occurring. In 1986 they started to observe neutrons at a rate just above background levels. By 1988, using an improved detector, they felt confident that they had found definite evidence of neutron production.</i><br /><br /><i>Jones had carried out this research unaware of the similar efforts being carried out at the nearby University of Utah. He first heard of Pons and Fleischmann&#39;s experiments in September 1988 when he was sent their research proposal to referee by the Department of Energy (Pons and Fleischmann had at last decided that their work merited funding from public sources).</i><br /><br /><b><i>It was unfortunate for both groups that such similar work was being pursued in such close proximity. In view of the obvious commercial payoff which might result from cold fusion and the need for patent protection it meant that a certain amount of rivalry and suspicion arose between the two groups. Exactly what they agreed regarding the joint publication of their results is still disputed.</i></b><br /><br /><i>It seems that in early 1989 Pons and Fleischmann were hoping that Jones would hold off from publishing for a period (up to eighteen months), giving time for them to refine their measurements. </i><b><i>Pons and Fleischmann were, it appears, confident that they were seeing excess heat, but had no firm evidence for its nuclear origins. Some crude measurements indicated that neutrons were coming out, but more exact measurements were desirable. Fleischmann even tried to arrange to have a cold fusion cell flown out to the Harwell Laboratory in England where he was a consultant and where very sensitive neutron detectors were available. Unfortunately the cell was declared a radiation risk and could not be taken across international boundaries. In the event Pons and Fleischmann claimed they could indirectly detect neutrons by observing interactions in a water shield surrounding the cell. It was these hastily carried out measurements which were later to be challenged by the MIT group; they turned out to be the Achilles heel in the Utah body.</i></b><br /><br /><i>Pons and Fleischmann were under pressure from Jones&#39; impending announcement. Although Jones cancelled one seminar in March, he planned to announce his results at the meeting of the American Ph ysical Society on 1 May. Pons and Fleischmann, in order not to lose their priority claim, reached an agreement with Jones to dispatch separate papers from both groups to Nature on 24 March.</i><br /><br /><b><i>In March, however, communication between the two groups broke down. Although Jones was going to speak in May his abstract was made public beforehand. Pons and Fleischmann, it seems, took this as licence to go public themselves. Also, the University of Utah group were worried that Jones might be stealing their ideas on excess heat, having had access to their work via the Department of Energy research proposal. A further complicating factor was a request in March to Pons from the editor of the Journal of Electroanalytical Chemistry for a paper on his latest work. Pons quickly wrote up an account of the cold fusion experiments which he submitted to the journal. It was this paper (published in April 1989), which would eventually be widely circulated, and provided the first technical details of the experiments.</i></b><br /><br /><i>Under growing pressure from the Utah administration, Pons and Fleischmann decided to go ahead with a press conference on 23 March, the day before the planned joint submission to Nature. A leak from Fleischmann to a British journalist meant that the first report of the discovery appeared in the British Financial Times on the morning of 23 March. Thus the world&#39;s press were primed to descend on Utah. At the press conference no mention was made of the other Utah group.</i><br /><br /><i>Jones, who was by now infuriated by both the press conference and the revelation that a paper had already been submitted, considered the agreement to have been broken and immediately dispatched his own paper to Nature. </i><b><i>Nothing could be more symbolic of the miscommunication which had arisen between the two Utah groups than the lone figure of Marvin Hawkins (a graduate student who worked with Pons and Fleischmann), waiting at the Federal Express office at Salt Lake City Airport at the appointed hour on 24 March for someone from Jones&#39; group to appear. No-one arrived and the Pons and Fleischmann paper was mailed alone.</i></b><br /><br /><i>The controversy</i><br /><br /><i>It was Pons and Fleischmann&#39;s results that gave rise to the cold fusion controversy. The levels of neutrons detected by Jones were of lower orders of magnitude and he has never claimed to observe excess heat. Jones&#39; results also did not pose the same theoretical challenge. Furthermore, Jones, unlike Pons and Fleischmann, made a point of playing down the commercial application angle.</i><br /><br /><i>The detrimental effect on the credibility of scientists&#39; findings when they get caught up in a scientific controversy not of their own choosing is revealed by the reception of Jones&#39; results. </i><b><i>Given his previously established reputation in the field, the minimal theoretical consequences his results posed, and the modest manner in which he presented them, if it had not been for Pons and Fleischmann, Steve Jones would probably have quietly established an interesting fact about the natural world: fusion of small amounts of deuterium in palladium metal.</i></b><br /><br /><i>Despite his attempts to distance himself from the other Utah group, Jones has inevitably been subject to the same suspicions. The reality of his neutron measurements has come into question, and there is no consensus as to whether he has observed fusion.</i><br /><br /><b><i>Pons and Fleischmann, unlike Jones, had no established reputation in the field of fusion research; they were chemists not physicists. Moreover they were claiming something which to most physicists was theoretically impossible. Not only did it seem extremely unlikely that fusion could be occurring, but also, if all the heat excess was caused by fusion, then the levels of neutrons produced should have been more than enough to have killed Pons and Fleischmann and anyone else who happened to be in the close proximity of one of their cells. In short, fusion could not occur and, if it did, then they should be dead. This is what one might refer to as a &#39;knockdown&#39; argument!</i></b><br /><br /><i>There is little doubt that when fusion researchers heard the news on 25 March they were sceptical. The reaction was:</i><br /><br /><b><i>&#39;Suppose you were designing jet airplanes and then you suddenly heard on CBS news that somebody had invented an antigravity machine.&#39;</i></b><i> (Quoted in Mallove, 1991, p. 41).</i><br /><br /><i>Another remarked at the time:</i><br /><br /><b><i>&#39;I&#39;m willing to be open-minded, but it&#39;s really inconceivable that there&#39;s anything there.&#39;</i></b><i> (Ibid., p. 41).</i><br /><br /><b><i>Part of the scepticism came from fusion researchers being only too familiar with grandiose claims for breakthroughs which shortly afterwards turn out to be incorrect. There had been many such episodes in the history of the field and thus fusion scientists were wary of extravagant claims.</i></b><i> For them, solving the world&#39;s energy problems via a breakthrough in fusion had about as much chance of being true as the perennial claims to have superseded Einstein&#39;s theory of relativity.</i><br /><br /><b><i>Though fusion researchers, well-used to spectacular claims, and with their own billion-dollar research programs to protect, were incredulous, other scientists were more willing to take the work seriously. Pons and Fleischmann have fared better with their colleagues in chemistry where, after all, they were acknowledged experts. Early on Pons presented his findings to a meeting of the American Chemical Society where he was given a rapturous reception.</i></b><i> For most, the prejudices of the scientific community probably mattered less than the fact that the experiment seemed easy to perform. If there was anything to it, most scientists reckoned, then all should soon become clear. Pons and Fleischmann had two sorts of evidence to back up their claims: excess heat and nuclear products. These had to be tested.</i><br /><br /><i>Excess heat</i><br /><br /><i>Testing for excess heat was in essence no more than a high-school physics problem. A careful account of the power input and output of the cell was kept, including all the known chemical reactions that are capable of transforming chemical energy into heat. This accounting needs to be carried out over a period of time because at any one moment the books might not balance as energy might get stored in the cell (turning it into a heat bank, as one might say). It is a fairly straightforward procedure to establish the power output by measuring the temperature rise, the cell first having been calibrated using a heater of known power. </i><b><i>In practice the experiment took some time to perform because the palladium electrodes had to be fully charged with deuterium (for 8 mm diameter electrodes this could take several months).</i></b><br /><br /><b><i>The heat excess varied between cells. Some cells showed no heat excess at all. The power sometimes came in surges; in one case four times as much power was recorded coming out as went in. However, more routinely the heat excess was between 10% and 25%.</i></b><br /><br /><i>Despite the capricious nature of the phenomenon, Pons and Fleischmann were confident that the heat excess could not be explained by any known chemical process or reaction.</i><br /><br /><i>Nuclear products</i><br /><br /><b><i>The most direct proof of fusion would be the production of neutrons correlated with the excess heat. The first neutron measurements attempted by Pons and Fleischmann were relatively crude.</i></b><i> The output from a cell was compared with the background as measured at a distance of 50 metres from the cell. A signal three times the background was claimed to be recorded for this one cell. This was a suggestive result, but as neither the energy of the neutrons was known, nor whether the background was the same close to the cell as at 50 metres distance, it was far from conclusive. </i><b><i>A more satisfactory procedure was to measure the gamma-ray bursts produced by neutrons captured by protons in the water bath surrounding the cell. These measurements were made over a two-day period by Bob Hoffman, a Utah radiologist. The numbers of neutrons detected, if any, were billions less than would be expected if all the heat was produced by the deuterium fusion reaction.</i></b><br /><br /><b><i>Another piece of evidence for fusion having taken place would be the presence of its products, such as tritium. Pons and Fleischmann found traces of tritium in the palladium cathode of one cell. The difficulty with this finding -- a problem which has beset all the claims -- is that tritium is a known contaminant of heavy water.</i></b><br /><br /><i>Replication</i><br /><br /><i>As mentioned already, after the announcement on 23 March attempts to repeat the experiments followed fast and furious. Although they got much media attention, these early results (both positive and negative) counted for little. </i><b><i>The embarrassment caused by the premature announcements from Georgia Tech and Texas A&amp;M cautioned those scientists who were seriously trying to repeat the experiment that they faced a long struggle. Many were taken in by the seeming ease of the experiment only to discover that a palladium electrolytic cell was a deal more complicated than expected.</i></b><br /><br /><b><i>Part of the difficulty facing scientists trying to repeat the experiment was that Pons and Fleischmann&#39;s account of what they had done was insufficiently detailed.</i></b><i> There was discussion of the exact size of the electrodes to be used, the current densities at which to operate the cells, whether the lithium salt was crucial or could be substituted by another salt, whether the cathode was &#39;poisoned&#39; and with what, and for how long the experiment should run. None of these were clear. </i><b><i>Following on from their initial announcement, Pons and Fleischmann were inundated with requests for information. In the frenetic atmosphere at Utah it is no wonder that scientists did not always find it easy to get the crucial information.</i></b><br /><br /><i>Some have accused Pons and Fleischmann of deliberate secrecy in order to secure patent rights or (later on, when many became disillusioned) to hide their own incompetence. However, given the commercial importance of the discovery, securing the patent rights is no small matter; it is routine in areas of biotechnology. </i><b><i>Also it seems that Pons and Fleischmann were initially hesitant because of their own uncertainties and their fears about the dangers of the experiment. They were also worried about creating a cheap source of tritium, since tritium is one of the crucial ingredients of a hydrogen bomb.</i></b><br /><br /><b><i>The elusive details of the experiments were soon spreading through an informal network of electronic mail and telephone contacts. Indeed, electronic mail may have been important in this controversy in producing the rapid consensus against cold fusion which developed after the Baltimore American Physical Society meeting. For instance, Douglas Morrison, a CERN (European Organisation for Nuclear Research) physicist and early enthusiast of cold fusion, set up an electronic newsletter which seems to have been widely read. Morrison soon became sceptical of the claims and referred scientists to Irving Langmuir&#39;s notorious talk on &#39;pathological science&#39;, where a number of cases of controversial phenomena (including N-Rays and ESP) in science were dismissed as a product of mass delusion. (Langmuir&#39;s talk was reproduced in Physics Today in October 1989.) Cold fusion was, according to Morrison, the most recent case of pathological science.</i></b><br /><br /><b><i>What became clear early on was that, while most groups saw nothing, a few had positive results. The classic problem of replication during a scientific controversy was surfacing. Negative results could be explained away by the believers as being due to differences in the replicating experiment. To those who failed to find anything, however, this was simply confirmation that there was nothing to be found. Fleischmann and Pons&#39;s own attitude, as expressed in their testimony to Congress in April 1989, was that they were not surprised by the negative results as many cells were being set up with incorrect parameters and dimensions.</i></b><br /><br /><b><i>Of the early positive replications reported, one of the most important came from Robert Huggins, a materials scientist at Stanford University. Huggins had run two cells, one with ordinary water and one with heavy water, and found that only the heavy water cell produced excess heat. This answered a long-running criticism of Pons and Fleischmann for not setting up a &#39;control&#39; cell with ordinary water. Huggins has consistently found positive results over the years.</i></b><br /><br /><i>Another criticism of the Pons and Fleischmann work was that the cells they used were open cells from which the gases produced in electrolysis (deuterium and oxygen) could escape. The worry here was whether the energy balance was affected by the possible chemical recombination of deuterium and oxygen to form heavy water, thereby adding heat to the system. </i><b><i>This objection was finally overcome when John Appleby of Texas A&amp;M (not the same Texas A&amp;M group which prematurely announced positive results) performed closely controlled calorimetry experiments using closed cells. Heat excesses were again found.</i></b><br /><br /><i>Of the negative results, one of the most influential came from a Cal Tech group headed by chemist Nathan Lewis and phvsicist Charlie Barnes. The Cal Tech team had tried a variety of combinations of conditions and had found nothing. As mentioned already, Lewis reported the negative results to the Baltimore American Physical Society meeting to dramatic effect. </i><b><i>His results had extra impact since he implied that Pons and Fleischmann were guilty of an elementary oversight. They had neglected to stir the electrolyte thus allowing hot spots to develop and produce spurious temperature readings.</i></b><br /><br /><b><i>However, it seems that Lewis&#39; charges were misplaced. Pons and Fleischmann claimed that there was no need to stir the electrolyte because the deuterium bubbles produced by the reaction did the job sufficiently well.</i></b><i> In order to demonstrate the error Lewis had tried to make an exact copy of Pons and Fleischmann&#39;s cell. He had taken as his source a photograph of a cell in the Los Angeles Times. It turns out that this cell was only used by Pons and Fleischmann for demonstration purposes and was of much larger dimensions than the cells used in actual experimental runs. </i><b><i>Pons and Fleischmann were later able to demonstrate with a simple experiment placing a few drops of dye in the electrolyte that the bubbles acted as an adequate stirring mechanism.</i></b><br /><br /><i>As in other controversies what was taken by most people to be a &#39;knockdown&#39; negative result turns out, on closer examination, to be itself subject to the same kinds of ambiguities as the results it claims to demolish. If Lewis&#39; measurements had been unpacked in the same kind of detail reserved for Pons and Fleischmann they might not have seemed as compelling as they did at the time. In the atmosphere of the Baltimore meeting, where physicists were baying for the blood of the two chemists, and where a whole series of pieces of negative evidence was presented (see below), Lewis was able to deliver the knock-out blow.</i><br /><br /><i>The classic problem of replication has surfaced with another set of highly influential negative findings, those reported by Harwell. As a result of Fleischmann&#39;s contact with Harwell, David Williams, an ex-graduate student of Fleischmann&#39;s, actually started his experiments before the March announcement. </i><b><i>The results obtained to all intents and purposes killed off cold fusion in Britain. Again on the face of it the experiments look impressive with a number of cells being checked for excess heat and neutrons over lengthy periods.</i></b><br /><br /><b><i>The results, however, are not compelling for proponents of cold fusion such as Eugene Mallove, who claim that almost half the cells were run at currents below the threshold for cell activity. Other criticisms have been made of the Harwell methods of heat estimation. Despite the differing interpretations of the Harwell experiment, for many scientists it was the best word on cold fusion.</i></b><br /><br /><b><i>As well as attempts to replicate the phenomenon by setting up electrolytic cells, new experiments have searched for cold fusion by other methods. One such is cooling and reheating the palladium so that it becomes supersaturated with deuterium. Bursts of neutrons have been detected in such experiments.</i></b><br /><br /><b><i>The difficulty which the proponents face in getting positive results accepted is well illustrated by the fate of the tritium measurements. It will be recalled that Pons and Fleischmann had themselves found traces of tritium. More evidence came from other experimenters, including a group in India with a long history of making tritium measurements, a group at Los Alamos and a third group at Texas A&amp;M University. However, since tritium is a known contaminant of heavy water there is a ready-made &#39;normal&#39; explanation for all such results. It has proved to be impossible to satisfy the critics that no contamination has occurred, because they can always think of ways in which tritium might get into the cell</i></b><i>.</i><br /><br /><i>It has even been suggested that the affair has involved fraud. In 1990, an article in the journal Science puts forward fraud as a factor in the Texas A&amp;M tritium measurements. The impasse between proponents and critics, an impasse made worse by each side accusing the other of &#39;unscientific&#39; behaviour, is typical of scientific controversies. The critics cite a preponderance of negative results as grounds to dismiss the controversial phenomenon and any residual positive results are explained away as incompetence, delusion or even fraud. The proponents, on the other hand, account for the negative results as having arisen from the failure to reproduce exactly the same conditions as used to obtain positive results. Experiments alone do not seem capable of settling the issue.</i><br /><br /><i>Cold fusion: a theoretical impossibility?</i><br /><br /><b><i>Most of the debate has been fought out against a background in which cold fusion has been held to be impossible on theoretical grounds. Although Pons and Fleischmann, like Tandberg beforehand, hoped that the extreme pressures inside the palladium lattice would help enhance fusion of deuterium, there was little theoretical justification that this would be the case.</i></b><br /><br /><i>One of the responses of nuclear physicists to the cold fusion claims has been a detailed re-examination of the theoretical possibilities. Steve Koonin and Mike Nauenberg have devoted considerable time and energy to this problem. </i><b><i>Although in reworking the calculations they discovered errors which increased the rate of deuterium-deuterium fusion by a factor of over 10 billion compared with earlier calculations, the main thrust of his work has been to show why deuterium fusion in palladium in the amount needed to produce excess heat is extremely unlikely. Koonin and Nauenberg have pointed out that the increased pressure inside palladium was not enough to bring about fusion. Indeed, in a palladium lattice the deuterium nuclei would actually be further apart than in ordinary heavy water. Their calculations for the likelihood of deuterium-deuterium fusion showed that the rate would be extremely slow. In a telling comparison Koonin described it this way: &#39;A mass of cold deuterium the size of the Sun would undergo one fusion per year.&#39;</i></b><br /><br /><b><i>Thus Koonin, in reviewing all the theoretical possibilities at the May meeting of the American Physical Society, was able to make theoretical justifications seem preposterous. As Koonin told a New York Times reporter: &#39;It is all very well to theorize about how cold fusion in a palladium cathode might take place ... one could also theorize about how pigs would behave if they had wings. But pigs don&#39;t have wings!&#39;</i></b><i> (quoted in Mallove, 1991, p. 143).</i><br /><br /><i>In a context where the experimental evidence was fast vanishing it was little wonder that most physicists were happy to go along with the accepted wisdom.</i><br /><br /><i>There is no doubt that Koonin represents the standard view. As is typical for a scientific controversy where experiment seems to go against prevailing theory, however, there is more to say than this. </i><b><i>Indeed throughout the cold fusion episode a number of suggestions have been made as to how fusion might occur on the necessary scale and furthermore how it might occur without neutrons being produced.</i></b><i> Some of the more serious suggestions have come from physics Nobel Laureate Julian Schwinger and the MIT laser physicist who helped invent the X-ray laser, Peter Hagelstein. One idea has been to think of ways whereby a rare neutronless fusion reaction could be the source of the excess heat, with energy being transferred into the palladium lattice. Hagelstein, drawing upon ideas in laser physics, has also proposed &#39;coherent fusion&#39;, whereby chains of fusion reactions occur in a kind of domino effect.</i><br /><br /><b><i>With the experimental results under a cloud, most theorists see little reason to entertain such exotic ideas. One is reminded of the solar-neutrino case</i></b><i> (see chapter 7), where many speculative theories were produced to explain the discrepancy between standard theory and the experimental results. Even though the discrepancy there was only a matter of a factor of 3 none of the alternative theories gained widespread acceptance. It seems unlikely that, in a case where the discrepancy is a factor of 57 orders of magnitude (10 with 56 0s after it) and where the experimental results have much less credibility, conventional theory is going to be over-thrown. There is no doubt that Hagelstein himself takes these alternative theories very seriously -- he has even applied for patents for devices based upon his theories. The risks in pursuing such theories beyond the mere &#39;what if&#39; stage (i.e. treating such theories as serious candidates rather than mere speculations) are well illustrated by the Hagelstein case: </i><b><i>there have been persistent rumours that his tenure at MIT was in jeopardy after he started to produce theoretical explanations of cold fusion.</i></b><br /><br /><i>Credibility</i><br /><br /><i>The struggle between proponents and critics in a scientific controversy is always a struggle for credibility. When scientists make claims which are literally &#39;incredible&#39;, as in the cold fusion case, they face an uphill struggle. </i><b><i>The problem Pons and Fleischmann had to overcome was that they had credibility as electrochemists but not as nuclear physicists. And it was nuclear physics where their work was likely to have its main impact.</i></b><br /><br /><b><i>Any claim to observe fusion (especially made in such an immodest and public manner), was bound to tread upon the toes of the nuclear physicists and fusion physicists who had already laid claim to the area. A vast amount of money, expertise, and equipment had already been invested in hot fusion programs and it would be naive to think that this did not affect in some way the reception accorded Pons and Fleischmann.</i></b><br /><br /><i>This is not to say that the fusion physicists simply rejected the claims out of hand </i><b><i>(although a few did)</i></b><i>, or that it was a merely a matter of wanting to maintain billion-dollar investments </i><b><i>(although with the Department of Energy threatening to transfer hot fusion funding to cold fusion research there was a direct threat to their interests)</i></b><i>, or that this was a matter of the blind prejudice of physicists over chemists (although some individuals may have been so prejudiced); </i><b><i>it was simply that no scientists could hope to challenge such a powerfully established group without having his or her own credibility put on the line. As might be expected, the challenge to Pons and Fleischmann has been most acute in the area where the physicists feel most at home, the area of the neutron measurements.</i></b><br /><br /><i>Neutron measurements</i><br /><br /><i>For many physicists it was the neutron measurements which provided the best evidence for fusion. Yet paradoxically these measurements formed the weakest link in Pons and Fleischmann&#39;s claims. As we have seen, </i><b><i>the measurements were carried out belatedly and under pressure from others. Worse, neither Pons nor Fleischmann had any special expertise in such measurements.</i></b><br /><br /><b><i>It was at Harwell, at a seminar given by Fleischmann shortly after the March announcement, that the first inkling of difficulties was to arise. Fleischmann presented the evidence of neutrons and showed a graph of the gamma-ray peak obtained by Hoffman from the water shield. To physicists in the audience who were familiar with such spectra, the peak looked to be at the wrong energy. The peak was at 2.5 MeV whereas the expected peak for gamma-rays produced by neutrons from deuterium should have been at 2.2 MeV. It looked as if something had gone awry with the calibration of the gamma-ray detector, but it was impossible to tell for certain because Fleischmann did not have the raw data with him and had not made the measurements himself. In any event by the time the graph appeared in the ]ournal of Electroanalytical Chemistry the peak was given at the correct value of 2 .2 MeV.</i></b><br /><br /><b><i>Whether the two versions arose from &#39;fudging&#39; or genuine errors and doubt over what had been measured is unclear. Frank Close, in his much publicised sceptical book about the cold fusion controversy, Too Hot To Handle, suggests that the graph was deliberately doctored -- a charge taken up by science journalist William Broad in an article in the New York Times, of 17 March 1991. Such accusations should, however, be treated with caution. Close, in particular, falls into the trap of exposing all the gory detail of the proponents&#39; experiments, leaving the critics&#39; experiments to appear as clear-cut and decisive. Such a one-sided narrative merely serves to reaffirm the critics&#39; victory.</i></b><br /><br /><b><i>The neutron measurements soon came under further scrutiny. Richard Petrasso, of the MIT Plasma Fusion Center, also noticed that the shape of the gamma-ray peak looked wrong. The difficulty in taking this observation further was that Pons and Fleischmann had not yet released their background gamma-ray spectrum. What the MIT scientists did was to pull off something of a scientific scoop. They obtained a video of a news programme which showed the inside of Pons and Fleischmann&#39;s laboratory including a VDU display of their gamma-ray spectrum. Petrasso concluded that the claimed peak could not exist at 2.2 MeV and that furthermore it was impossible to see such a slim peak with the particular instrument used. The absence of a Compton edge also eliminated the peak as a viable candidate for neutron capture. The conclusion of the MIT group was that the peak was &#39;probably an instrumental artefact with no relation to gamma-ray interactions&#39;.</i></b><br /><br /><i>Preliminary reports of the work were given by Petrasso at the Baltimore meeting to maximum rhetorical effect. </i><b><i>In tandem with the Cal Tech negative results they were to have the decisive impact</i></b><i> on the course of the controversy which we have already charted.</i><br /><br /><b><i>The criticism of the neutron measurements was eventually published in Nature along with a reply from Pons and Fleischmann. Although they made much of MIT&#39;s resort to a news video as a source of scientific evidence (pointing out that what had been referred to by Petrasso as a &#39;curious structure&#39; was none other than an electronic cursor and denying that the video had shown a real measurement), the Utah pair were now placed on the defensive. They published their full spectrum showing no peak at 2.2 MeV but claiming evidence for a new peak at 2.496 MeV. Although they could not explain this peak in terms of a known deuterium fusion process they maintained the peak was produced by radiation from the cell. In an ingenious move they tried to turn Petrasso&#39;s argument around by saying that if indeed their instrument was not capable of detecting such peaks then the lack of a peak at 2.2 MeV should not in itself be taken as evidence against fusion. MIT replied in turn claiming that the peak at 2.496 MeV was actually at 2.8 MeV.</i></b><br /><br /><b><i>Many scientists have taken this episode as showing that the main argument in favour of fusion had collapsed. However, another interpretation is possible. This is that the best evidence for cold fusion always came from the excess heat measurements -- the experimenters&#39; own strength. The hastily performed nuclear measurements had always been puzzling because too few neutrons were observed. In trying to &#39;come clean&#39; on the difficulties of interpreting their nuclear measurements Pons and Fleischmann were attempting to draw attention back to the main thrust of their argument -- the excess heat measurements. Indeed, when Pons and Fleischmann finally published their full results in July 1990, the paper was almost entirely about calorimetry -- no nuclear measurements were reported.</i></b><br /><br /><b><i>The trouble was that for many physicists the nuclear data were what had got them excited in the first place and the weakness of the neutron evidence left the excess heat measurements as mere anomalies, possibly of chemical origin. Furthermore, the problems over the nuclear measurements could easily be taken to demonstrate Pons and Fleischmann&#39;s incompetence as experimenters. Despite the Utah pair being widely acknowledged as experts in electrochemistry, this kind of &#39;guilt by association&#39; seems to have paid off for the critics and has helped discredit the experiment as a whole.</i></b><br /><br /><i>Conclusion</i><br /><br /><i>In the cold fusion controversy the stakes were very high and the normally hidden working of science has been exposed. </i><b><i>The cold fusion episode is often taken to show that there is something wrong with modern science. It is said that scientists claimed too much, based on too little, and in front of too many people. Press review is said to have replaced peer review. False hopes of a new age of limitless energy were raised, only to be dashed.</i></b><br /><br /><b><i>Such an interpretation is unfortunate. Pons and Fleischmann appear to have been no more greedy or publicity seeking than any prudent scientists would be who think they have on their hands a major discovery with a massive commercial payoff. The securing of patents and the fanfare of press conferences are inescapable parts of modern science, where institutional recognition and funding are ever more important. There is no turning the clock back to some mythical Golden Age when scientists were all true gentlemen (they never were anyway, as history of science has taught us in recent years). In cold fusion we find science as normal. It is our image of science which needs changing, not the way science is conducted.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"The Low-Energy Nuclear Reaction","summary":"Some Call it Cold Fusion / Regardless of the Theoretical Explanation, Some Say there's by Now no Doubt that Nuclear Reactions can be Triggered Using Chemical Energy / Although these Claimed Reactions are Not Yet Well Understood, a Candidate Hypothesis is that the Electron in Hydrogen Could Transition from its Normal Ground Energy State to Previously Unknown Lower and More Stable States, Liberating Energy in the Process / \"Telling Physicists that they’ve Got [Hydrogen's Ground State] Wrong is like Telling Mothers Across America that they’ve Misunderstood Apple Pie\"","image":"https://lh3.googleusercontent.com/-oyIx01gt2Kk/WD5AC43tyuI/AAAAAAAAJdw/hYYNnjwKB0IG8-rtniD0ansf97dQCSDSACJoC/w1800-h2520/the-low-energy-nuclear-reaction-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/ZZXJMQyGqB1","publishDate":"2016-11-30T02:58:12.014Z","updateDate":"2017-01-03T01:40:52.612Z","text":"<b>The Low-Energy Nuclear Reaction: Some Call it Cold Fusion / Regardless of the Theoretical Explanation, Some Say there&#39;s by Now no Doubt that Nuclear Reactions can be Triggered Using Chemical Energy / Although these Claimed Reactions are Not Yet Well Understood, a Candidate Hypothesis is that the Electron in Hydrogen Could Transition from its Normal Ground Energy State to Previously Unknown Lower and More Stable States, Liberating Energy in the Process / &quot;Telling Physicists that they’ve Got [Hydrogen&#39;s Ground State] Wrong is like Telling Mothers Across America that they’ve Misunderstood Apple Pie&quot;</b><br /><br /><a href=\"https://www.scientificamerican.com/article/cold-fusion-lives-experiments-create-energy-when-none-should-exist1/\" class=\"ot-anchor\">https://www.scientificamerican.com/article/cold-fusion-lives-experiments-create-energy-when-none-should-exist1/</a><br /><br />Cold Fusion Lives: Experiments Create Energy When None Should Exist<br /><br />The field, now called low-energy nuclear reactions, may have legit results — or be stubborn junk science<br /><br />By Stephen K. Ritter, Chemical &amp; Engineering News on <b>November 28, 2016</b><br /><br /><i>&quot;Howard J. Wilk is a long-term unemployed synthetic organic chemist living in Philadelphia. Like many pharmaceutical researchers, he has suffered through the drug industry’s R&amp;D downsizing in recent years and now is underemployed in a nonscience job. With extra time on his hands, Wilk has been tracking the progress of a New Jersey-based company called Brilliant Light Power (BLP).</i><br /><br /><i>The company is one of several that are developing processes that collectively fall into the category of new energy technologies. This movement is largely a reincarnation of cold fusion, the short-lived, quickly dismissed phenomenon from the late 1980s of achieving nuclear fusion in a simple benchtop electrolysis device.</i><br /><br /><b><i>In 1991, BLP’s founder, Randell L. Mills, announced at a press conference in Lancaster, Pa., that he had devised a theory in which the electron in hydrogen could transition from its normal ground energy state to previously unknown lower and more stable states, liberating copious amount of energy in the process. Mills named this curious new type of shrunken hydrogen the hydrino, and he has been at work ever since to develop a commercial device to harness its power and make it available to the world.</i></b><br /><br /><i>Wilk has studied Mills’s theory, read Mills’s papers and patents, and carried out his own calculations on the hydrino. Wilk has gone so far as to attend a demonstration at BLP’s facility in Cranbury, N.J., where he discussed the hydrino with Mills. </i><b><i>After all that, Wilk says he still can’t tell if Mills is a titanic genius, is self-delusional, or is something in between.</i></b><br /><br /><i>This story line is a common refrain for the researchers and companies involved. It all got started in 1989, when electrochemists Martin Fleischmann and Stanley Pons made the stunning announcement at a press conference at the University of Utah that they had tamed the power of nuclear fusion in an electrolysis cell.</i><br /><br /><i>When the researchers applied a current to the cell, they thought deuterium atoms from heavy water that had penetrated into the palladium cathode were fusing to form helium atoms. The excess energy from the process dissipated as heat. Fleischmann and Pons said this process could not be caused by any known chemical reaction, and the nuclear reaction term &#39;cold fusion&#39; was attached to it.</i><br /><br /><i>After months of investigating Fleischmann and Pons’s puzzling observations, however, the scientific community came to a consensus that the effect was inconsistent or nonexistent and that the scientists had made experimental errors. The research was summarily condemned, and cold fusion became a synonym for junk science.</i><br /><br /><i>Cold fusion and making hydrinos both hold the holy-grail promise of generating endless amounts of cheap, pollution-free energy. Scientists were frustrated by cold fusion. They wanted to believe it, but their collective wisdom told them it was all wrong. </i><b><i>Part of the problem was they had no generally accepted theory to guide them and explain the proposed phenomenon</i></b><i> -- as physicists like to say, no experiment should be believed until it has been confirmed by theory.</i><br /><br /><i>Mills has his own theory, but many scientists don’t believe it and think the hydrino improbable. </i><b><i>The research community has stopped short of the public dismissal it gave cold fusion and has tended to just ignore Mills and his work.</i></b><i> Mills has reciprocated by trying to stay out from under the shadow of cold fusion.</i><br /><br /><b><i>In the meantime, the field of cold fusion was rebranded as low-energy nuclear reactions, or LENR, and survives. Some scientists continue to try to explain the Fleischmann-Pons effect. Still others have dismissed the notion of fusion but are investigating other possible processes that can explain the anomalous excess heat effects.</i></b><i> Like Mills, they’ve been lured in by the potential commercial opportunities. Their primary interest is in generating energy for industrial, household, and transportation needs.</i><br /><br /><i>The handful of companies that have emerged in the attempt to get these new energy technologies to market have a business model the same as any technology start-up: Identify a new technology, attempt to patent the idea, raise investor interest and secure funding, build prototypes and have demonstration events, and announce timelines for when working devices might be available for sale. </i><b><i>In this new energy world, however, expired promises are the norm: None have made it to the last step of delivering a working device as advertised.</i></b><br /><br /><i>A NEW THEORY</i><br /><br /><i>Mills grew up on a Pennsylvania farm, earned an undergraduate degree in chemistry from Franklin &amp; Marshall College and a Harvard University medical degree, and studied electrical engineering at Massachusetts Institute of Technology. While a student, he began developing what he calls &#39;The Grand Unified Theory of Classical Physics,&#39; which he says provides </i><b><i>a new model of atoms and molecules that shifts away from quantum theory and is based on classical physics.</i></b><br /><br /><b><i>It’s commonly accepted that hydrogen’s solo electron is whizzing around its nucleus in its most energetically favorable, ground-state atomic orbital -- you simply can’t bring hydrogen’s electron closer to its nucleus. But Mills says you can.</i></b><br /><br /><i>Erik Baard, a journalist who has written stories about Mills, once noted how shocking it is to say the model of hydrogen is up for debate: </i><b><i>&#39;Telling physicists that they’ve got that wrong is like telling mothers across America that they’ve misunderstood apple pie.&#39;</i></b><br /><br /><i>One of those physicists is Andreas Rathke, a former research fellow at the European Space Agency, who is described on the agency’s website as having &#39;debunked a high number of crackpots.&#39; In 2005, Rathke analyzed Mills’s theory and published a paper in which he concluded it was flawed </i><b><i>and incompatible with everything physicists knew</i></b><i> (New J. Phys. 2005, DOI: 10.1088/1367-2630/7/1/127).</i><br /><br /><i>Currently a researcher at Airbus Defence &amp; Space, Rathke says he hasn’t followed the Mills story since about 2007 because there was no unambiguous sign of excess energy in reported experiments. </i><b><i>&#39;And I doubt there have been any experiments published at a later time that pass scientific scrutiny,&#39;</i></b><i> Rathke tells C&amp;EN.</i><br /><br /><i>&#39;I think there is general agreement that the theory Dr. Mills has put forward as the basis for his claims is inconsistent and not capable of making experimental predictions,&#39; Rathke continues. </i><b><i>&#39;Now, one could ask the question, ‘Could he have been lucky and stumbled upon some energy source that experimentally just works by following a wrong theoretical approach?’&#39;</i></b><br /><br /><b><i>In the 1990s, a few researchers, including a team from the National Aeronautics &amp; Space Administration’s Lewis Research Center, did report independently replicating the Mills approach and generating excess heat. The NASA team wrote in a report that the results &#39;fall far short of being compelling&#39; and did not mention anything about hydrinos.</i></b><br /><br /><i>The researchers offered possible electrochemical processes that might explain the heat, including irregularities in the electrochemical cell, possible unknown exothermic chemical reactions, or the recombination of split-apart hydrogen and oxygen atoms of water. </i><b><i>These are the same arguments made by scientific critics of the Fleischmann-Pons experiments. However, the NASA team did say that researchers should leave the door open, just in case Mills really was on to something.</i></b><br /><br /><i>Mills is a mile-a-minute talker who can go on forever spilling out technical details. Besides predicting the hydrino, Mills says his theory can perfectly predict the location of every electron in a molecule using his bespoke Millsian molecular modeling software, even in molecules as complex as DNA. With standard quantum theory, scientists struggle to predict the exact behavior of anything much more complex than a hydrogen atom. Mills further says his theory also explains why the universe is expanding at an accelerating rate, something cosmologists have yet to fully wrap their arms around.</i><br /><br /><i>Mills also says hydrinos are created from burning hydrogen in stars such as our sun and are evident in the spectral lines of starlight. Hydrogen is recognized as the most abundant element in our universe, but Mills goes further to claim that hydrinos are the missing dark matter in the universe. Those proposals come as a bit of a surprise to astrophysicists: &#39;I have never heard of a hydrino,&#39; says the University of Chicago’s Edward W. (Rocky) Kolb, an expert on the dark universe.</i><br /><br /><i>Mills has reported isolating hydrinos and characterizing them using standard spectroscopic methods such as infrared, Raman, and nuclear magnetic resonance. In addition, he says hydrinos can react in the way hydrogen might to form new types of compounds &#39;with amazing properties.&#39; These include conductive materials that Mills says would revolutionize electronic devices and batteries.</i><br /><br /><b><i>Even though popular opinion is against him, Mills’s ideas seem less far-fetched when compared with other unusual components of the universe. For example, a muonium is a known, short-lived exotic entity made of an antimuon particle (a positive, electronlike particle) and an electron. Chemically, muonium behaves like a hydrogen isotope, but it’s nine times as light as hydrogen.</i></b><br /><br /><i>THE HYDRINO SUNCELL</i><br /><br /><i>No matter where hydrinos fit in on the scale of believability, Mills told C&amp;EN a decade ago that BLP had moved past the scientific verification stage and was interested only in discussing commercial applications. Over the years, BLP has collected more than $110 million from investors to see what it can do.</i><br /><br /><i>BLP’s approach to creating hydrinos has taken on different manifestations over time. In an early prototype, Mills and his R&amp;D team used tungsten or nickel electrodes with a lithium or potassium electrolyte solution. An applied electric current splits the water into hydrogen and oxygen, and under the right conditions, lithium or potassium then acts as a catalyst to absorb energy and collapse hydrogen’s electron orbit. The energy released in going from the ground atomic state to a lower energy state comes off as a brilliant emission of light in a high-temperature plasma. The associated heat is then captured to create steam to power an electric generator.</i><br /><br /><i>BLP is currently testing a device called the SunCell in which hydrogen (from splitting water) and an oxide catalyst are introduced into a spherical carbon reactor along with dual streams of molten silver. An electric current applied to the silver ignites a hydrino-forming plasma reaction. Energy from the reaction is then trapped by the carbon, which acts as a &#39;blackbody radiator.&#39; When the carbon heats up to thousands of degrees, it reemits the energy as visible light that is captured by photovoltaic cells, which convert the light to electricity.</i><br /><br /><i>When it comes to commercial development, Mills at times comes off looking paranoid and at other times like a shrewd businessman. Mills has trademarked &#39;Hydrino.&#39; And because his issued patents claim the hydrino as an invention, BLP asserts that it owns all intellectual property rights involving hydrino research. BLP therefore forbids outside experimentalists from doing even the most basic hydrino research, which could confirm or deny hydrinos, without first signing an IP agreement. &#39;We welcome research partners; we want to get others involved,&#39; Mills says. &#39;But we do need to protect our technology.&#39;</i><br /><br /><i>Mills instead has commissioned validators who say they can corroborate that BLP’s inventions work. One of the validators is Bucknell University electrical engineering professor Peter M. Jansson, who is paid for his evaluations of BLP technology through his consulting company, Integrated Systems. Jansson says that being compensated for his time &#39;does not in any way cloud my judgment as an independent investigator of scientific discoveries.&#39; He adds that he debunks most &#39;new discoveries&#39; he checks out.</i><br /><br /><b><i>&#39;BLP scientists are doing real science, and to date, I have found no errors in their scientific methods or approaches,&#39; Jansson says. &#39;Over the years, I have witnessed many BLP devices clearly capable of creating excess energy at meaningful levels. I think it may take some period of time for the scientific community to absorb, digest, and accept the possibility of lower energy states of hydrogen. I think Dr. Mills has made a compelling case.&#39; Jansson adds that commercial viability remains a challenge for BLP, but the way forward is being held up by business issues, not scientific ones.</i></b><br /><br /><i>Meanwhile, BLP has hosted several demonstrations of its latest prototypes for investors since 2014, posting videos on its website after the fact. </i><b><i>But these events do not provide clear evidence one way or the other as to whether the SunCell is legitimate.</i></b><br /><br /><i>In July, after one recent demonstration, the company announced that the anticipated cost of operating the SunCell is so low -- about 1 to 10% of that for any other existing form of power -- that the company &#39;intends to provide autonomous individual power for essentially all stationary and motive applications untethered to the grid or any fuels infrastructure.&#39; In other words, the company plans to build and then lease SunCells or other devices to customers and charge a per diem usage fee, allowing people to go off the power grid and stop buying gasoline or diesel while paying just a fraction of what those things now cost.</i><br /><br /><i>&#39;This is the end of the age of fire, the internal combustion engine, and centralized power and fuels,&#39; Mills says. &#39;Our technology is going to make all other energy technology obsolete. Our concerns about climate change are going to be eliminated.&#39; He adds that it looks like BLP could be in production, at first with megawatt stationary units, and generating revenue by the end of 2017.</i><br /><br /><i>WHAT’S IN A NAME?</i><br /><br /><i>Despite the uncertainty surrounding Mills and BLP, their story is just one part of the ongoing new energy saga. After the dust settled on the original Fleischmann-Pons announcement, the two researchers began figuring out what was right and what was wrong. They were joined by dozens of other collaborators and independent researchers.</i><br /><br /><i>Many of these scientists and engineers, often using money out of their own pocket, have been less concerned about commercial opportunities but rather have focused on basic science: electrochemistry, metallurgy, calorimetry, mass spectrometry, and nuclear diagnostics. They continue to rack up experiments showing excess heat gain, defined as the ratio of energy put out by a system to the energy required to operate it. In some cases, nuclear anomalies such as producing neutrons, α-particles (helium nuclei), isotope shifts of atoms, and transmutation of one element to another have been reported.</i><br /><br /><i>But in the end, most of these researchers are just looking for an explanation and would be happy if even a modest amount of heat generated turns out to be useful in some way.</i><br /><br /><b><i>&#39;LENR is real experimentally, and not understood theoretically,&#39; says David J. Nagel, an electrical and computer engineering professor at George Washington University and a former research manager at the Naval Research Laboratory. &#39;There are results that you just can’t explain away. Whether it’s cold fusion, low-energy nuclear reactions, or something else -- the names are all over the place -- we still don’t know. But there’s no doubt that you can trigger nuclear reactions using chemical energy.&#39;</i></b><br /><br /><b><i>Nagel prefers to call the LENR phenomenon &#39;lattice-enabled nuclear reactions&#39; because whatever is happening takes place within the crystal lattice of an electrode.</i></b><i> The original branch of the field focuses on infusing deuterium into a palladium electrode by turning on the power, Nagel explains. </i><b><i>Researchers have reported such electrochemical systems that can output more than 25 times as much energy as they draw.</i></b><br /><br /><i>The other main branch of the field uses a nickel-hydrogen setup, which can produce greater than </i><b><i>400 times</i></b><i> as much energy as it uses. Nagel likes to compare these LENR technologies to that of the International Thermonuclear Experimental Reactor, a multination high-temperature fusion experiment based on well-understood physics -- merging deuterium and tritium -- being carried out in southern France. At a cost exceeding $20 billion, this 20-year project has set a goal of generating 10 times as much energy as it consumes.</i><br /><br /><i>Nagel says the LENR field continues to grow internationally, and the biggest hurdles remain inconsistent results and lack of funding. For example, </i><b><i>some researchers report that a certain threshold must be reached for a reaction to start.</i></b><i> The reaction may require a minimum amount of deuterium or hydrogen to get going, </i><b><i>or the electrode materials may need to be prepared with a specific crystallographic orientation and surface morphology to trigger the process. The latter is a common issue with heterogeneous catalysts used in petroleum refining and petrochemical production.</i></b><br /><br /><i>Nagel acknowledges that the business side of LENR has had problems too: </i><b><i>Prototypes being developed have been &#39;relatively crude,&#39;</i></b><i> he says, and there has yet to be an LENR-based company to offer a working product or make any money.</i><br /><br /><i>ROSSI’S E-CAT</i><br /><br /><i>One of the notable examples of attempts to commercialize LENR comes from engineer Andrea Rossi of Leonardo Corp., based in Miami. In 2011, Rossi and his colleagues announced at a press conference in Bologna, Italy, that they had built a tabletop reactor, called the Energy Catalyzer, or E-Cat, that produces excess energy via a nickel-catalyzed process. To substantiate his discovery, Rossi has held E-Cat demonstrations for potential investors and members of the media and commissioned independent validation tests.</i><br /><br /><i>Rossi posits that his E-Cat features a self-sustaining process in which electrical power input initiates fusion of hydrogen and lithium from a powdery mixture of nickel, lithium, and lithium aluminum hydride to form a beryllium isotope. The short-lived beryllium decays into two α-particles with the excess energy given off as heat; some of the nickel is reported to turn into copper. Rossi says no waste is created in the process, and no radiation is detected outside the apparatus.</i><br /><br /><i>Rossi’s announcement initially gave many scientists the same queasy feeling as did cold fusion. One reason many people are having trouble believing Rossi is his checkered past. In Italy, he was convicted of white-collar criminal charges related to his earlier business ventures. Rossi says those convictions are behind him and he no longer wants to talk about them. He also once had a contract to make heat-generating devices for the U.S. Army. But the delivered devices did not work according to specifications.</i><br /><br /><i>In 2012, Rossi announced completion of a 1-MW system that could be used to heat or power large buildings. Rossi also anticipated that, by 2013, he’d have a factory annually producing 1 million 10-kW household units about the size of a laptop computer. But neither the factory nor the household units have materialized.</i><br /><br /><i>In 2014, Rossi licensed his technology to a company called Industrial Heat, which was formed by private equity firm Cherokee, a company that focuses on buying real estate and has a goal of cleaning up old industrial sites for redevelopment. In 2015, Cherokee Chief Executive Officer Tom Darden, who trained as an environmental scientist and a lawyer, described Industrial Heat as &#39;a funding source for LENR inventors.&#39;</i><br /><br /><i>Darden said Cherokee started Industrial Heat because the investment firm believed that LENR technology was worth pursuing. &#39;We were willing to be wrong. We were willing to invest time and resources to see if this might be an area of useful research in our quest to eliminate pollution,&#39; he said.</i><br /><br /><i>In the meantime, Industrial Heat and Leonardo have had a falling out, and both are now suing each other in court over violations of their agreement. Rossi would have received a total of $100 million if a yearlong test of his 1-MW system was successful. Rossi says he completed the test, but Industrial Heat disagrees and has expressed concerns that the device doesn’t work.</i><br /><br /><i>George Washington’s Nagel says that Rossi’s E-Cat brought a groundswell of hope to the LENR field. Nagel told C&amp;EN in 2012 that he didn’t think Rossi was a fraud, &#39;but I do not like some of his approaches to testing.&#39; Nagel thought Rossi should have been more thorough and transparent. Yet, at the time, Nagel also said he thought LENR devices would be offered for sale by 2013.</i><br /><br /><i>Rossi continues his research and has announced development of other prototypes. But he gives away few details about what he is doing. Rossi tells C&amp;EN that the industrial 1-MW plants are in construction already and he has obtained the &#39;necessary certifications&#39; for selling the systems. </i><b><i>The household devices are still waiting for safety certification</i></b><i>, he notes.</i><br /><br /><i>Nagel says now that the excitement from Rossi’s initial announcement has died down, the LENR status quo has returned. The likely availability of commercial LENR generators is now at least a few years away, Nagel says. Even if a device clears the hurdles of reproducibility and usefulness, he adds, its developers face an uphill battle of regulatory approval and customer acceptance.</i><br /><br /><i>But Nagel remains optimistic. </i><b><i>&#39;LENR might be commercialized well ahead of its understanding, as were X-rays,&#39;</i></b><i> he says. For that reason, Nagel has just outfitted a lab at George Washington to start a new line of nickel-hydrogen experiments.</i><br /><br /><i>SCIENTIFIC LEGACIES</i><br /><br /><b><i>Many of the researchers who continue to work on LENR are accomplished scientists and are now retired. It hasn’t been easy for them because, for years, their papers have been returned unreviewed from mainstream journals and their abstracts for talks at scientific conferences have tended to go unaccepted. They are becoming more anxious about the status of the field because they are running out of time -- whether to secure their legacy in scientific history if LENR proves correct or just to have peace of mind in knowing their instincts haven’t failed them.</i></b><br /><br /><b><i>&#39;It was unfortunate that cold fusion was initially publicized in 1989 as a new fusion energy source instead of simply as a new scientific curiosity,&#39; says electrochemist Melvin H. Miles. &#39;Perhaps research could then have proceeded normally with more careful and accurate studies of the many variables involved.&#39;</i></b><br /><br /><i>A retired researcher at the Naval Air Warfare Center in China Lake, Calif., Miles at times collaborated with Fleisch­mann, who died in 2012. Miles says he thinks Fleisch­mann and Pons were right all along. Yet, even today he doesn’t know how a commercial energy source could be constructed for the palladium-deuterium system, despite his many experiments that have produced significant excess heat correlated with helium production.</i><br /><br /><i>&#39;Why would anyone have continued research or scientific interest after 27 years on any topic that was reported to be a mistake?&#39; Miles asks. </i><b><i>&#39;I am convinced that cold fusion will eventually be recognized as another important discovery that was very slow to gain acceptance, and a new theoretical framework will emerge to explain the experimental results.&#39;</i></b><br /><br /><i>Nuclear physicist Ludwik Kowalski, an emeritus professor at Montclair State University, agrees cold fusion got off to the wrong start. &#39;I am old enough to remember the effect the initial announcement had on the scientific community, and on the general public,&#39; Kowalski says. At times, he collaborated with LENR researchers, &#39;but my three attempts to validate the sensational claims yielded only negative results.&#39;</i><br /><br /><b><i>Kowalski thinks the social stigma against the research created as part of the initial fallout developed into a bigger problem, one that is unbecoming to the scientific method.</i></b><i> Whether or not the claims of LENR researchers are valid, Kowalski believes a clear yes or no answer is still worth seeking. </i><b><i>But it will not be found as long as cold fusion researchers &#39;are treated as cranks and pseudoscientists,&#39; Kowalski says. &#39;No progress is possible, and no one benefits from not publishing results of honest investigations and not independently testing them in other laboratories.&#39;</i></b><br /><br /><i>TIME WILL TELL</i><br /><br /><i>Even if Kowalski gets a yes to his question and LENR researcher claims are validated, the path to commercialization is fraught with challenges. Not all start-up companies, even ones with sound technology, are successful for reasons that are not scientific in nature: capitalization, cash flow, cost, manufacturing, insurance, and competitive energy pricing, to name a few.</i><br /><br /><i>For example, consider Sun Catalytix. The company spun off from MIT is one example of a start-up built on strong science that fell victim to commercial pressures before it hit its stride. The company was created to commercialize an artificial photosynthesis process developed by chemist Daniel G. Nocera, now at Harvard, to economically and efficiently convert water into hydrogen fuel with sunlight and inexpensive catalysts.</i><br /><br /><i>Nocera envisioned that hydrogen generated in this way could power a simple fuel cell to provide energy to homes and villages in poor regions of the world without access to a power grid, making modern conveniences available and improving quality of life. But the process needed significantly more capital and more time to develop than the company initially thought. After four years, Sun Catalytix abandoned its commercialization effort, turned to making flow batteries, and then was bought in 2014 by Lockheed Martin. Sun Catalytix no longer exists.</i><br /><br /><i>It’s unclear whether the companies pursuing LENR and related technologies have stuttered primarily because of similar business hurdles. For example, Wilk, the organic chemist who has been following Mills’s progress, is becoming a little bit obsessed trying to sort out if BLP’s commercialization efforts are based on something real or make-believe. He simply wants to know, does the hydrino exist?</i><br /><br /><i>In 2014, Wilk asked Mills if he had ever isolated hydrinos, and although Mills had previously written in research papers and patents that he had, Mills replied that he hadn’t and that it would be &#39;a really, really huge task.&#39; But Wilk doesn’t see it that way. If the process generates liters of hydrino gas as he has calculated, it should be obvious. &#39;Show us the hydrino!&#39; Wilk pleads.</i><br /><br /><i>Wilk says Mills’s world, and by extension the world of others involved in LENR, reminds him of one of Zeno’s paradoxes, which suggests that motion is an illusion. </i><b><i>&#39;Every year they make up half the remaining distance to commercialization, but will they ever get there?&#39;</i></b><i> Wilk can think of four possible explanations for BLP: Mills’s science is actually right, it’s a complete fraud, it’s just simply bad science, or it’s what Chemistry Nobel Laureate Irving Langmuir called pathological science.</i><br /><br /><b><i>Langmuir coined the term more than 50 years ago to describe a psychological process in which scientists unconsciously veer away from the scientific method and become so engrossed in what they are doing they develop an inability to be objective and see what is real and not real. Pathological science is &#39;the science of things that aren’t so,&#39; Langmuir said. In some cases, it is embodied in areas of research like cold fusion/LENR that simply will not go away, even when given up on as false by a majority of scientists.</i></b><br /><br /><b><i>&#39;I hope they’re right,&#39; Wilk says about Mills and BLP. &#39;I really do. I’m not out to debunk them, just to get at the truth.&#39; For the sake of the argument—&#39;if pigs could fly,&#39; as Wilk puts it, he says he’ll accept their data, their theory, and other predictions that can be derived from them. But he has never been a true believer. &#39;I think if hydrinos existed, they would have been detected by others in laboratories or in nature years ago and would be used by now.&#39;</i></b><br /><br /><b><i>All the discussions about cold fusion and LENR end that way: They always come back to the fact that no one has a commercial device on the market yet, and none of the prototypes seem workable on a commercial scale in the near future. Time will be the ultimate arbiter.</i></b><br /><br /><i>This article is reproduced with permission from Chemical &amp; Engineering News (© American Chemical Society). The article was first published on November 7, 2016.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"3 Types of Collective Intelligence Problems","summary":"Cognition Problems are Problems that have or Will have Definitive Solutions / Coordination Problems Involve Situations where Each Person's Actions Affect and Depend on what Everyone Else Will Do / Cooperation Problems are Similar, but Require that People Adopt a Broader Definition of Self-interest - Problems like Keeping the Sidewalk Free of Snow, Paying Taxes, and Curbing Pollution / What Distinguishes Coordination from Cooperation Problems is that the Latter Requires Trust / That Need for Trust Does Not Scale Well, and is in Conflict in Large Collaborative Systems with the Need for a Diversity of Perspectives","image":"https://lh3.googleusercontent.com/-rdNHogJB2jU/WD0S6AseDxI/AAAAAAAAJck/TsK_MMjRSk4aYm0CvcA0mOpoCQdA67deQCJoC/w1800-h2520/the-3-types-of-collective-intelligence-problems-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/9rJE8wFKm3G","publishDate":"2016-11-29T05:33:40.965Z","updateDate":"2017-01-03T01:42:10.663Z","text":"<b>3 Types of Collective Intelligence Problems: Cognition Problems are Problems that have or Will have Definitive Solutions / Coordination Problems Involve Situations where Each Person&#39;s Actions Affect and Depend on what Everyone Else Will Do / Cooperation Problems are Similar, but Require that People Adopt a Broader Definition of Self-interest - Problems like Keeping the Sidewalk Free of Snow, Paying Taxes, and Curbing Pollution / What Distinguishes Coordination from Cooperation Problems is that the Latter Requires Trust / That Need for Trust Does Not Scale Well, and is in Conflict in Large Collaborative Systems with the Need for a Diversity of Perspectives</b><br /><br /><i>The Wisdom of Crowds: Why the Many Are Smarter than the Few and How Collective Wisdom Shapes Business, Economics, Societies and Nations</i><br />by James Surowiecki<br /><br /><i>&quot;[C]ollective intelligence can be brought to bear on a wide variety of problems, </i><b><i>and complexity is no bar</i></b><i>. In this book, I concentrate on three kinds of problems. The first are what I&#39;ll call </i><b><i>cognition problems</i></b><i>. </i><b><i>These are problems that have or will have definitive solutions. For example, &#39;Who will win the Super Bowl this year?&#39; and &#39;How many copies of this new ink-jet printer will we sell in the next three months?&#39; are cognition problems. So, too, is &#39;How likely is it that this drug will be approved by the FDA?&#39; Questions to which there may not be a single right answer, but to which some answers are certainly better than others -- such as, &#39;What would be the best place to build this new public swimming pool?&#39; -- are cognition problems, too.</i></b><br /><br /><i>The second kind of problem is what&#39;s usually called a </i><b><i>coordination problem</i></b><i>. </i><b><i>Coordination problems require members of a group (market, subway riders, college students looking for a party) to figure out how to coordinate their behavior with each other, knowing that everyone else is trying to do the same.</i></b><i> How do buyers and sellers find each other and trade at a fair price? How do companies organize their operations? How can you drive safely in heavy traffic? These are all problems of coordination.</i><br /><br /><i>The final kind of problem is a </i><b><i>cooperation problem</i></b><i>. As their name suggests, </i><b><i>cooperation problems involve the challenge of getting self-interested, distrustful people to work together, even when narrow self-interest would seem to dictate that no individual should take part.</i></b><i> Paying taxes, dealing with pollution, and agreeing on definitions of what counts as reasonable pay are all examples of cooperation problems.&quot;</i><br /><br /><b>The Cognition Problem:</b><br /><br />... like having a group estimate the weight of an oxe or count the number of jelly beans in a jar ...<br /><br /><i>&quot;There are two lessons to draw from these experiments. First, in most of them the members of the group were not talking to each other or working on a problem together. </i><b><i>They were making individual guesses, which were aggregated and then averaged</i></b><i> ... it is likely to produce excellent results ... Second, the group&#39;s guess will not be better than that of every single person in the group each time. In many (perhaps most) cases, there will be a few people who do better than the group. This is, in some sense, a good thing, since especially in situations where there is an incentive for doing well (like, say, the stock market) it gives people reason to keep participating. </i><b><i>But there is no evidence in these studies that certain people consistently outperform the group. In other words, if you run ten different jelly-bean-counting experiments, it&#39;s likely that each time one or two students will out-perform the group. But they will not be the same students each time. Over the ten experiments, the group&#39;s performance will almost certainly be the best possible. The simplest way to get reliably good answers is just to ask the group each time.&quot;</i></b><br /><br /><b>The Coordination Problem:</b><br /><br /><i>&quot;What defines a coordination problem is that to solve it, a person has to think not only about what he believes the right answer is but also about what other people think the right answer is. </i><b><i>And that&#39;s because what each person does affects and depends on what everyone else will do, and vice versa.</i></b><br /><br /><i>One obvious way of coordinating people&#39;s actions is via authority or coercion. An army goose-stepping in a parade is, after all, very well-coordinated. So, too, are the movements of workers on an old-fashioned assembly line. But in a liberal society, authority (which includes laws or formal rules) has only limited reach over the dealings of private citizens, and that seems to be how most Americans like it. As a result many coordination problems require bottom-up, not top-down, solutions. And at the heart of all of them is the same question: </i><b><i>How can people voluntarily -- that is, without anyone telling them what to do -- make their actions fit together in an efficient and orderly way?</i></b><br /><br /><i>It&#39;s a question without an easy answer, though this does not mean that no answer exists. What is true is that </i><b><i>coordination problems are less amenable to clear, definitive solutions</i></b><i> than are many of the problems we&#39;ve already considered. </i><b><i>Answers, when they can be found, are often good rather than optimal. And those answers also often involve institutions, norms, and history, factors that both shape a crowd&#39;s behavior and are also shaped by it. When it comes to coordination problems, independent decision making (that is, decision making which doesn&#39;t take the opinions of others into account) is pointless -- since what I&#39;m willing to do depends on what I think you&#39;re going to do, and vice versa. As a result, there&#39;s no guarantee that groups will come up with smart solutions. What&#39;s striking, though, is just how often they do.&quot;</i></b><br /><br />[...]<br /><br /><i>&quot;Bees are remarkably efficient at finding food. According to Thomas Seeley, author of The Wisdom of the Hive, a typical bee colony can search six or more kilometers from the hive, and if there is a flower patch within two kilometers of the hive, the bees have a better-than-half chance of finding it. How do the bees do this? They don&#39;t sit around and have a collective discussion about where foragers should go. Instead, the hive sends out a host of scout bees to search the surrounding area. When a scout bee has found a nectar source that seems strong, he comes back and does a waggle dance, the intensity of which is shaped, in some way, by the excellence of the nectar supply at the site. The waggle dance attracts other forager bees, which follow the first forager, while foragers who have found less-good sites attract fewer followers and, in some cases, eventually abandon their sites entirely. The result is that bee foragers end up distributing themselves across different nectar sources in an almost perfect fashion, meaning that they get as much food as possible relative to the time and energy they put into searching. It is a collectively brilliant solution to the colony&#39;s food problem.</i><br /><br /><b><i>What&#39;s important, though, is the way the colony gets to that collectively intelligent solution. It does not get there by first rationally considering all the alternatives and then determining an ideal foraging pattern. It can&#39;t do this, because it doesn&#39;t have any idea what the possible alternatives -- that is, where the different flower patches -- are. So instead, it sends out scouts in many different directions and trusts that at least one of them will find the best patch, return, and do a good dance so that the hive will know where the food source is.</i></b><br /><br /><i>This is, it&#39;s important to see, different from the kind of problem solving that we looked at earlier. In the case of the ox-weighing experiment, or the location of the Scorpion, or the betting markets, or the IEM, the group&#39;s job was to decide among already defined choices or to solve a well-defined problem. </i><b><i>In those cases, different members of the group could bring different pieces of information to bear on a problem, but the set of possible solutions was already, in a sense, determined.</i></b><i> (Bush or Kerry would become president; the Ravens or the Patriots would win the Super Bowl.) In the case of problems like finding the most nectar-rich flower patches, though, the task is more complicated. </i><b><i>It becomes a twofold process. First, uncover the possible alternatives. Then decide among them.</i></b><br /><br /><b><i>In the first stage of this process, the list of possible solutions is so long that the smart thing to do is to send out as many scout bees as possible.</i></b><i> You can think of Ransom Olds and Henry Ford and the countless would-be automakers who tried and failed, then, as foragers. They discovered (in this case, by inventing) the sources of nectar -- the gasoline-powered car, mass production, the moving assembly line -- and then asked the crowd to render its verdict. You might even see Olds&#39;s publicity stunts as a kind of equivalent to the waggle dance.</i><br /><br /><b><i>One key to this approach is a system that encourages, and funds, speculative ideas even though they have only slim possibilities of success. Even more important, though, is diversity -- not in a sociological sense, but rather in a conceptual and cognitive sense. You want diversity among the entrepreneurs who are coming up with the ideas, so you end up with meaningful differences among those ideas rather than minor variations on the same concept. But you also want diversity among the people who have the money, too. If one virtue of a decentralized economy is that it diffuses decision-making power (at least on a small scale) throughout the system, that virtue becomes meaningless if all the people with power are alike (or if, as we&#39;ll see in the next chapter, they become alike through imitation). The more similar they are, the more similar the ideas they appreciate will be, and so the set of new products and concepts the rest of us see will be smaller than possible. By contrast, if they are diverse, the chances that at least someone will take a gamble on a radical or unlikely idea obviously increases.&quot;</i></b><br /><br /><b>The Cooperation Problem:</b><br /><br /><b><i>&quot;Cooperation problems often look something like coordination problems, because in both cases a good solution requires people to take into account what everyone else is doing. But if the mechanism is right, coordination problems can be solved even if each individual is single-mindedly pursuing his self-interest -- in fact, in the case of price, that&#39;s what coordination seems to require. To solve cooperation problems -- which include things like keeping the sidewalk free of snow, paying taxes, and curbing pollution -- the members of a group or a society need to do more. They need to adopt a broader definition of self-interest than the myopic one that maximizing profits in the short term demands. And they need to be able to trust those around them, because in the absence of trust the pursuit of myopic self-interest is the only strategy that makes sense.</i></b><i> How does this happen? And does it make a difference when it does?</i><br /><br />[...]<br /><br /><i>One of the fundamental problems with trust is that it usually flourishes only where there are what sociologists call &#39;thick relationships&#39; -- relationships of family or clan or neighborhood. </i><b><i>But these kinds of relationships are impossible to maintain with many people at once and they are incompatible with the kind of scope and variety of contacts that a healthy modern economy (or a healthy modern society) needs to thrive.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"The Bug in Science's Machine","summary":"A Survey of 270 Academics Reveals that the Information Engine at the Heart of Science is Broken / When Your Car's Engine Breaks Down, You Pull Off to the Side of the Road and Fix It / Why does the Academic Community Assume that they Can Steer the Machine to Truth when the Flows of Scientific Information are Governed by \"Perverse Incentives\"? / This is Madness","image":"https://lh3.googleusercontent.com/-f0pPZop58Mg/WDz0IeRG61I/AAAAAAAAJbY/-k4jrN1-mgQ6tg90kvEBR8uSwFgRIgXnQCJoC/w1800-h2520/the-bug-in-sciences-machine-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Ksh4DvQGr3b","publishDate":"2016-11-29T03:26:08.059Z","updateDate":"2017-01-03T01:41:40.466Z","text":"<b>The Bug in Science&#39;s Machine: A Survey of 270 Academics Reveals that the Information Engine at the Heart of Science is Broken / When Your Car&#39;s Engine Breaks Down, You Pull Off to the Side of the Road and Fix It / Why does the Academic Community Assume that they Can Steer the Machine to Truth when the Flows of Scientific Information are Governed by &quot;Perverse Incentives&quot;? / This is Madness</b><br /><br />If you drive, you&#39;ve probably had car trouble.<br /><br />The car is a complex machine.  To my knowledge, man has never constructed a car that does not eventually break.<br /><br />Drivers are daily reminded by roadside signs that if your car breaks, you should pull to the side of the road.  What happens next is a sequence -- and fixing your car is the first step.<br /><br />What happens when the information engine at the center of science fails?<br /><br />Science, too, is a complicated machine.<br /><br />Does the information machine at the center of science ever <b>completely</b> break down?<br /><br />How would we know?<br /><br />What I mean is the worst-case-scenario question of all possible questions:<br /><br /><b><i>What if some scientific domains are so wrong that they&#39;re asking the wrong questions?</i></b><br /><br />Modern science points to a growing list of claims as &quot;settled science&quot;.<br /><br /><b>What if some of those &quot;settled&quot; questions are not actually settled?</b><br /><br /><b>What if the academic community daily ridicules the &quot;right&quot; answers?</b><br /><br /><b>What if the truth is actually some crackpot &quot;needle&quot; buried in the pile-of-pseudoscience &quot;haystack&quot;?</b><br /><br /><b>What if the apparent paradoxes of science are mostly information problems?</b><br /><br /><b>What if since we are not even asking the right questions, we have completely lost our way in some domains? ... Spending billions of dollars trying to prove a universe that we imagine we see?</b><br /><br />What would happen?<br /><br />Perhaps we should seriously ask the question ...<br /><br /><a href=\"http://www.vox.com/2016/7/14/12016710/science-challeges-research-funding-peer-review-process\" class=\"ot-anchor\">http://www.vox.com/2016/7/14/12016710/science-challeges-research-funding-peer-review-process</a><br /><br />by Julia Belluz, Brad Plumer, and Brian Resnick on <b>September 7, 2016</b><br /><br /><i>&quot;&#39;Science, I had come to learn, is as political, competitive, and fierce a career as you can find, full of the temptation to find easy paths.&#39; — Paul Kalanithi, neurosurgeon and writer (1977–2015)</i><br /><br /><i>Science is in big trouble. Or so we’re told.</i><br /><br /><i>In the past several years, many scientists have become afflicted with a serious case of doubt -- doubt in the very institution of science.</i><br /><br /><i>As reporters covering medicine, psychology, climate change, and other areas of research, we wanted to understand this epidemic of doubt. So we sent scientists a survey asking this simple question: If you could change one thing about how science works today, what would it be and why?</i><br /><br /><i>We heard back from 270 scientists all over the world, including graduate students, senior professors, laboratory heads, and Fields Medalists. </i><b><i>They told us that, in a variety of ways, their careers are being hijacked by perverse incentives. The result is bad science.</i></b><br /><br /><i>The scientific process, in its ideal form, is elegant: Ask a question, set up an objective test, and get an answer. Repeat. Science is rarely practiced to that ideal. But Copernicus believed in that ideal. So did the rocket scientists behind the moon landing.</i><br /><br /><i>But nowadays, our respondents told us, </i><b><i>the process is riddled with conflict. Scientists say they’re forced to prioritize self-preservation over pursuing the best questions and uncovering meaningful truths.</i></b><br /><br /><b><i>&#39;I feel torn between asking questions that I know will lead to statistical significance and asking questions that matter,&#39;</i></b><i> says Kathryn Bradshaw, a 27-year-old graduate student of counseling at the University of North Dakota.</i><br /><br /><b><i>Today, scientists&#39; success often isn&#39;t measured by the quality of their questions or the rigor of their methods. It&#39;s instead measured by how much grant money they win, the number of studies they publish, and how they spin their findings to appeal to the public.&quot;</i></b><br /><br />Pull the car over!<br /><br />It&#39;s <b>broken</b>.<br /><br />Everybody should stop pretending it&#39;s not broken.<br /><br />We need to fix it.  Get the process right; then <b>good science will follow.</b><br /><br />But, to be clear: don&#39;t just <b>assume</b> that good science will happen with a broken information engine at the core of the scientific enterprise.<br /><br />That is <b>madness</b>.<br /><br />Our interaction with information is crucial to our our decision-making abilities.  Nobody should take it for granted.<br /><br />(People have lost entire elections over this.)<br /><br />There has to be a process at the core of the scientific enterprise which people look at and say with confidence:<br /><br /><b><i>&quot;This system will lead us to the answers.&quot;</i></b><br /><br />In the world of code, developers occasionally take a break from the daily grind of pushing bits -- the &quot;code freeze&quot;.<br /><br />The academic community needs its own version -- a sort of &quot;publication freeze&quot;.<br /><br />Scientists.  Please listen ...<br /><br />If there&#39;s a bug in the machine, you need to stop everything and fix it at once ...<br /><br />... Because there is no more important problem that needs solving than <b>that</b>.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The Wisdom of Crowds","summary":"Under the Right Circumstances, Groups can be Remarkably Intelligent, and are Often Smarter than the Smartest People in Them / However, of the Characteristics which Make them Accurate, the Academic Community has Overlooked the Importance of Diversity of Opinion / The Problem is that Groups that are Too Much Alike Find it Harder to Keep Learning, Because Each Member is Bringing Less and Less New Information to the Table / Homogeneous Groups are Great at Doing what they Do Well, but they Become Progressively Less Able to Investigate Alternatives","image":"https://lh3.googleusercontent.com/-v1R2IfYuGFU/WDt-D7yidFI/AAAAAAAAJZw/yZsTI4QQzgs1fjs7tZSic-Uo_Hi86J_OwCJoC/w3600-h5040/the-wisdom-of-crowds-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/1eZJwiPZMuN","publishDate":"2016-11-28T00:45:50.920Z","updateDate":"2017-01-03T01:42:36.005Z","text":"<b>The Wisdom of Crowds: Under the Right Circumstances, Groups can be Remarkably Intelligent, and are Often Smarter than the Smartest People in Them / However, of the Characteristics which Make them Accurate, the Academic Community has Overlooked the Importance of Diversity of Opinion / The Problem is that Groups that are Too Much Alike Find it Harder to Keep Learning, Because Each Member is Bringing Less and Less New Information to the Table / Homogeneous Groups are Great at Doing what they Do Well, but they Become Progressively Less Able to Investigate Alternatives</b><br /><br />It&#39;s not belaboring the point to be explicit about this, because what is remarkable is that these generally-stated problems appear to line up pretty well with critiques of modern science.<br /><br /><i>The Wisdom of Crowds: Why the Many Are Smarter than the Few and How Collective Wisdom Shapes Business, Economics, Societies and Nations</i><br />by James Surowiecki<br /><br /><i>&quot;[U]nder the right circumstances, groups are remarkably intelligent, and are often smarter than the smartest people in them. Groups do not need to be dominated by exceptionally intelligent people in order to be smart. Even if most of the people within a group are not especially well-informed or rational, it can still reach a collectively wise decision. This is a good thing, since human beings are not perfectly designed decision makers. Instead, we are what the economist Herbert Simon called &#39;boundedly rational.&#39; We generally have less information than we&#39;d like. We have limited foresight into the future. Most of us lack the ability -- and the desire -- to make sophisticated cost-benefit calculations. Instead of insisting on finding the best possible decision, we will often accept one that seems good enough. And we often let emotion affect our judgment. Yet despite all these limitations, when our imperfect judgments are aggregated in the right way, our collective intelligence is often excellent.&quot;</i><br /><br />[...]<br /><br /><i>&quot;One of the striking things about the wisdom of crowds is that even though its effects are all around us, it&#39;s easy to miss, and, even when it&#39;s seen, it can be hard to accept.&quot;</i><br /><br />[...]<br /><br /><i>&quot;Charles Mackay would have scoffed at the idea that a crowd of people could know anything at all. Mackay was the Scottish journalist who, in 1841, published Extraordinary Popular Delusions and the Madness of Crowds, an endlessly entertaining chronicle of mass manias and collective follies, to which the title of my book pays homage. For Mackay, crowds were never wise. They were never even reasonable. Collective judgments were doomed to be extreme. &#39;Men, it has been well said, think in herds,&#39; he wrote. &#39;It will be seen that they go mad in herds, while they only recover their senses slowly, and one by one.&#39; Mackays take on collective madness is not an unusual one. In the popular imagination, groups tend to make people either dumb or crazy, or both. The speculator Bernard Baruch, for instance, famously said: &#39;Anyone taken as an individual is tolerably sensible and reasonable —- as a member of a crowd, he at once becomes a blockhead.&#39; Henry David Thoreau lamented: &#39;The mass never comes up to the standard of its best member, but on the contrary degrades itself to a level with the lowest.&#39; Friedrich Nietzsche wrote, &#39;Madness is the exception in individuals but the rule in groups,&#39; while the English historian Thomas Carlyle put it succinctly: &#39;I do not believe in the collective wisdom of individual ignorance.&#39;</i><br /><br /><i>Perhaps the most severe critic of the stupidity of groups was the French writer Gustave Le Bon, who in 1895 published the polemical classic The Crowd: A Study of the Popular Mind. Le Bon was appalled by the rise of democracy in the West in the nineteenth century, and dismayed by the idea that ordinary people had come to wield political and cultural power. But his disdain for groups went deeper than that. A crowd, Le Bon argued, was more than just the sum of its members. Instead, it was a kind of independent organism. It had an identity and a will of its own, and it often acted in ways that no one within the crowd intended. When the crowd did act, Le Bon argued, it invariably acted foolishly. A crowd might be brave or cowardly or cruel, but it could never be smart. As he wrote, &#39;In crowds it is stupidity and not mother wit that is accumulated.&#39; Crowds &#39;can never accomplish acts demand-ing a high degree of intelligence,&#39; and they are &#39;always intellectually inferior to the isolated individual.&#39; Strikingly, for Le Bon, the idea of &#39;the crowd&#39; included not just obvious examples of collective wildness, like lynch mobs or rioters. It also included just about any kind of group that could make decisions.&quot;</i><br /><br />[...]<br /><br /><i>&quot;Groups work well under certain circumstances, and less well under others. Groups generally need rules to maintain order and coherence, and when they&#39;re missing or malfunctioning, the result is trouble. Groups benefit from members talking to and learning from each other, but too much communication, paradoxically, can actually make the group as a whole less intelligent. While big groups are often good for solving certain kinds of problems, big groups can also be unmanageable and inefficient. Conversely, small groups have the virtue of being easy to run, but they risk having too little diversity of thought and too much consensus.&quot;</i><br /><br />[...]<br /><br /><i>&quot;Diversity and independence are important because the best collective decisions are the product of disagreement and contest, not consensus or compromise. An intelligent group, especially when confronted with cognition problems, does not ask its members to modify their positions in order to let the group reach a decision everyone can be happy with. Instead, it figures out how to use mechanisms -- like market prices, or intelligent voting systems -- to aggregate and produce collective judgments that represent not what any one person in the group thinks but rather, in some sense, what they all think. Paradoxically, the best way for a group to be smart is for each person in it to think and act as independently as possible.&quot;</i><br /><br />[...]<br /><br /><i>&quot;In May 1968, the U.S. submarine Scorpion disappeared on its way back to Newport News after a tour of duty in the North Atlantic. Although the navy knew the sub&#39;s last reported location, it had no idea what had happened to the Scorpion, and only the vaguest sense of how far it might have traveled after it had last made radio contact. As a result, the area where the navy began searching for the Scorpion was a circle twenty miles wide and many thousands of feet deep. You could not imagine a more hopeless task. The only possible solution, one might have thought, was to track down three or four top experts on submarines and ocean currents, ask them where they thought the Scorpion was, and search there. But, as Sherry Sontag and Christopher Drew recount in their book Blind Man&#39;s Bluff, a naval officer named John Craven had a different plan.</i><br /><br /><i>First, Craven concocted a series of scenarios -- alternative explanations for what might have happened to the Scorpion. Then he assembled a team of men with a wide range of knowledge, including mathematicians, submarine specialists, and salvage men. Instead of asking them to consult with each other to come up with an answer, he asked each of them to offer his best guess about how likely each of the scenarios was. To keep things interesting, the guesses were in the form of wagers, with bottles of Chivas Regal as prizes. And so Craven&#39;s men bet on why the submarine ran into trouble, on its speed as it headed to the ocean bottom, on the steepness of its descent, and so forth.</i><br /><br /><i>Needless to say, no one of these pieces of information could tell Craven where the Scorpion was. But Craven believed that if he put all the answers together, building a composite picture of how the Scorpion died, he&#39;d end up with a pretty good idea of where it was. And that&#39;s exactly what he did. He took all the guesses, and used a formula called Bayes&#39;s theorem to estimate the Scorpions final location. (Bayes&#39;s theorem is a way of calculating how new in-formation about an event changes your preexisting expectations of how likely the event was.) When he was done, Craven had what was, roughly speaking, the group&#39;s collective estimate of where the submarine was.</i><br /><br /><i>The location that Craven came up with was not a spot that any individual member of the group had picked. In other words, not one of the members of the group had a picture in his head that matched the one Craven had constructed using the information gathered from all of them. The final estimate was a genuinely collective judgment that the group as a whole had made, as opposed to representing the individual judgment of the smartest people in it. It was also a genuinely brilliant judgment. Five months after the Scorpion disappeared, a navy ship found it. It was 220 yards from where Craven&#39;s group had said it would be.</i><br /><br /><i>What&#39;s astonishing about this story is that the evidence that the group was relying on in this case amounted to almost nothing. It was really just tiny scraps of data. No one knew why the submarine sank, no one had any idea how fast it was traveling or how steeply it fell to the ocean floor. And yet even though no one in the group knew any of these things, the group as a whole knew them all.&quot;</i><br /><br />[...]<br /><br /><b><i>&quot;[There are] four conditions that characterize wise crowds: diversity of opinion (each person should have some private information, even if it&#39;s just an eccentric interpretation of the known facts), independence (people&#39;s opinions are not determined by the opinions of those around them), decentralization (people are able to specialize and draw on local knowledge), and aggregation (some mechanism exists for turning private judgments into a collective decision). If a group satisfies those conditions, its judgment is likely to be accurate. Why? At heart, the answer rests on a mathematical truism. If you ask a large enough group of diverse, independent people to make a prediction or estimate a probability, and then average those estimates, the errors each of them makes in coming up with an answer will cancel themselves out. Each person&#39;s guess, you might say, has two components: information and error. Subtract the error, and you&#39;re left with the information.&quot;</i></b><br /><br />[...]<br /><br /><i>&quot;Truly successful decision making, of course, demands more than just a picture of the world as it is. </i><b><i>It demands in addition a picture of the world as it will (or at least as it may) be. Any decision-making mechanism therefore has to be good under conditions of uncertainty. And what&#39;s more uncertain than the future?</i></b><i> Group intelligence may be good at telling how many jelly beans are in a jar or remembering the year Nirvana released Nevermind. But how does it perform under conditions of true uncertainty, when the right answer is seemingly unknowable -- because it hasn&#39;t happened yet?&quot;</i><br /><br />[...]<br /><br /><b><i>&quot;What makes a system successful is its ability to recognize losers and kill them quickly. Or, rather, what makes a system successful is its ability to generate lots of losers and then to recognize them as such and kill them off. Sometimes the messiest approach is the wisest.&quot;</i></b><br /><br />[...]<br /><br /><i>&quot;Generating a diverse set of possible solutions isn&#39;t enough. </i><b><i>The crowd also has to be able to distinguish the good solutions from the bad.</i></b><i> We&#39;ve already seen that groups seem to do a good job of making such distinctions. But does diversity matter to the group? In other words, </i><b><i>once you&#39;ve come up with a diverse set of possible solutions, does having a diverse group of decision makers make a difference?</i></b><br /><br /><b><i>It does, in two ways. Diversity helps because it actually adds perspectives that would otherwise be absent and because it takes away, or at least weakens, some of the destructive characteristics of group decision making. Fostering diversity is actually more important in small groups and in formal organizations than in the kinds of larger collectives -- like markets or electorates -- that we&#39;ve already talked about for a simple reason: the sheer size of most markets, coupled with the fact that anyone with money can enter them (you don&#39;t need to be admitted or hired), means that a certain level of diversity is almost guaranteed. Markets, for instance, are usually prima facie diverse because they&#39;re made up of people with different attitudes toward risk, different time horizons, different investing styles, and different information. On teams or in organizations, by contrast, cognitive diversity needs to be actively selected, and it&#39;s important to do so because in small groups it&#39;s easy for a few biased individuals to exert undue influence and skew the group&#39;s collective decision.</i></b><br /><br /><i>Scott Page is a political scientist at the University of Michigan who has done a series of intriguing experiments using computer-simulated problem-solving agents to demonstrate the positive effects of diversity. For instance, Page set up a series of groups of ten or twenty agents, with each agent endowed with a different set of skills, and had them solve a relatively sophisticated problem. </i><b><i>Individually, some of the agents were very good at solving the problem while others were less effective. But what Page found was that a group made up of some smart agents and some not-so-smart agents almost always did better than a group made up just of smart agents. You could do as well or better by selecting a group randomly and letting it solve the problem as by spending a lot of time trying to find the smart agents and then putting them alone on the problem.</i></b><br /><br /><b><i>The point of Page&#39;s experiment is that diversity is, on its own, valuable, so that the simple fact of making a group diverse makes it better at problem solving. That doesn&#39;t mean that intelligence is irrelevant -- none of the agents in the experiment were ignorant, and all the successful groups had some high-performing agents in them. But it does mean that, on the group level, intelligence alone is not enough, because intelligence alone cannot guarantee you different perspectives on a problem. In fact, Page speculates, grouping only smart people together doesn&#39;t work that well because the smart people (whatever that means) tend to resemble each other in what they can do. If you think about intelligence as a kind of toolbox of skills, the list of skills that are the &#39;best&#39; is relatively small, so that people who have them tend to be alike. This is normally a good thing, but it means that as a whole the group knows less than it otherwise might. Adding in a few people who know less, but have different skills, actually improves the group&#39;s performance.</i></b><br /><br /><b><i>This seems like an eccentric conclusion, and it is. It just happens to be true. The legendary organizational theorist James G. March, in fact, put it like this: &#39;The development of knowledge may depend on maintaining an influx of the naive and the ignorant, and ... competitive victory does not reliably go to the properly educated.&#39; The reason, March suggested, is that groups that are too much alike find it harder to keep learning, because each member is bringing less and less new information to the table. Homogeneous groups are great at doing what they do well, but they become progressively less able to investigate alternatives. Or, as March has famously argued, they spend too much time exploiting and not enough time exploring. Bringing new members into the organization, even if they&#39;re less experienced and less capable, actually makes the group smarter simply because what little the new members do know is not redundant with what everyone else knows. As March wrote, &#39;[The] effect does not come from the superior knowledge of the average new recruit. Recruits are, on average, less knowledgeable than the individuals they replace. The gains come from their diversity.&#39;&quot;</i></b><br /><br />[...]<br /><br /><i>&quot;(Collective decisions are only wise, remember, when they incorporate lots of different information.)&quot;</i><br /><br />[...]<br /><br /><i>&quot;Organizations, on the other hand, clearly can and should have people offer their judgments simultaneously, rather than one after the other. On a deeper level, the success of the Hung and Plott experiment -- which effectively forced the people in the group to make themselves independent -- underscores the value and the difficulty of autonomy. One key to successful group decisions is getting people to pay much less attention to what everyone else is saying.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"Groupthink vs Conformity vs Social Proof vs Herding","summary":"Homogeneous Groups Are Susceptible to Groupthink Because they Become More Dependent on the Group, More Insulated from Outside Opinions, and Therefore More Convinced that the Group's Judgment Must Be Right / Groupthink Works Not So Much through Censorship as by Making Dissent Seem Somehow Improbable / Conformity, by Contrast, is a Fear of Standing Out / Herding is Similar to Conformity; it's the Sense that it is Better for Reputation to Fail Conventionally than to Succeed Unconventionally / Social Proof, on the Other Hand, is the Tendency to Assume that if Lots of People are Doing Something or Believe Something, there Must be a Good Reason Why","image":"https://lh3.googleusercontent.com/-8IjQX3qs9x0/WDttCALY_tI/AAAAAAAAJY8/mYajLWVE9CUTbi_Xp1QuIsH1BMiUPl_0wCJoC/w1800-h2520/groupthink-vs-conformity-vs-social-proof-vs-herding-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/EUNW1R34Wei","publishDate":"2016-11-27T23:32:58.814Z","updateDate":"2017-01-03T01:42:54.617Z","text":"<b>Groupthink vs Conformity vs Social Proof vs Herding: Homogeneous Groups Are Susceptible to Groupthink Because they Become More Dependent on the Group, More Insulated from Outside Opinions, and Therefore More Convinced that the Group&#39;s Judgment Must Be Right / Groupthink Works Not So Much through Censorship as by Making Dissent Seem Somehow Improbable / Conformity, by Contrast, is a Fear of Standing Out / Herding is Similar to Conformity; it&#39;s the Sense that it is Better for Reputation to Fail Conventionally than to Succeed Unconventionally / Social Proof, on the Other Hand, is the Tendency to Assume that if Lots of People are Doing Something or Believe Something, there Must be a Good Reason Why</b><br /><br /><i>The Wisdom of Crowds: Why the Many Are Smarter than the Few and How Collective Wisdom Shapes Business, Economics, Societies and Nations</i><br />by James Surowiecki<br /><br /><b>Groupthink:</b><br /><br /><b><i>&quot;In part because individual judgment is not accurate enough or consistent enough, cognitive diversity is essential to good decision making. The positive case for diversity ... is that it expands a group&#39;s set of possible solutions and allows the group to conceptualize problems in novel ways. The negative case for diversity is that diversity makes it easier for a group to make decisions based on facts, rather than on influence, authority, or group allegiance. Homogeneous groups, particularly small ones, are often victims of what the psychologist Irving Janis called &#39;groupthink.&#39;</i></b><i> After a detailed study of a series of American foreign-policy fiascoes, including the Bay of Pigs invasion and the failure to anticipate Pearl Harbor, Janis argued that </i><b><i>when decision makers are too much alike -- in worldview and mindset -- they easily fall prey to groupthink. Homogeneous groups become cohesive more easily than diverse groups, and as they become more cohesive they also become more dependent on the group, more insulated from outside opinions, and therefore more convinced that the group&#39;s judgment on important issues must be right. These kinds of groups, Janis suggested, share an illusion of invulnerability, a willingness to rationalize away possible counterarguments to the group&#39;s position, and a conviction that dissent is not useful.</i></b><br /><br /><i>In the case of the Bay of Pigs invasion, for instance, the Kennedy administration planned and carried out its strategy without ever really talking to anyone who was skeptical of the prospects of success. The people who planned the operation were the same ones who were asked to judge whether it would be successful or not. The few people who voiced caution were quickly silenced. And, most remarkably, neither the intelligence branch of the CIA nor the Cuban desk of the State Department was consulted about the plan. The result was a bizarre neglect of some of the most elemental facts about Cuba in 1961, including the popularity of Fidel Castro, the strength of the Cuban army, and even the size of the island itself. (The invasion was predicated on the idea that 1,200 men could take over all of Cuba.) The administration even convinced itself that the world would believe the United States had nothing to do with the invasion, though American involvement was an open secret in Guatemala (where the Cuban exiles were being trained).</i><br /><br /><b><i>The important thing about groupthink is that it works not so much by censoring dissent as by making dissent seem somehow improbable.</i></b><i> As the historian Arthur Schlesinger Jr. put it, &#39;Our meetings took place in a curious atmosphere of assumed consensus.&#39; </i><b><i>Even if at first no consensus exists -- only the appearance of one -- the group&#39;s sense of cohesiveness works to turn the appearance into reality, and in doing so helps dissolve whatever doubts members of the group might have. This process obviously works all the more powerfully in situations where the group&#39;s members already share a common mindset. Because information that might represent a challenge to the conventional wisdom is either excluded or rationalized as obviously mistaken, people come away from discussions with their beliefs reinforced, convinced more than ever that they&#39;re right. Deliberation in a groupthink setting has the disturbing effect not of opening people&#39;s minds but of closing them.</i></b><i> In that sense, Janis&#39;s work suggests that the odds of a homogeneous group of people reaching a good decision are slim at best.&quot;</i><br /><br /><b>Conformity:</b><br /><br /><i>&quot;One obvious cost of homogeneity is also that it fosters the palpable pressures toward conformity that groups often bring to bear on their members. This seems similar to the problem of groupthink, but it&#39;s actually distinct. </i><b><i>When the pressure to conform is at work, a person changes his opinion not because he actually believes something different but because it&#39;s easier to change his opinion than to challenge the group.</i></b><i> The classic and still definitive illustration of the power of conformity is Solomon Asch&#39;s experiment in which he asked groups of people to judge which of three lines was the same size as a line on a white card. Asch assembled groups of seven to nine people, one of them the subject and the rest (unbeknownst to the subject) confederates of the experimenter. He then put the subject at the end of the row of people, and asked each person to give his choice out loud. There were twelve cards in the experiment, and with the first two cards, everyone in the group identified the same lines. Beginning with the third card, though, Asch had his confederates begin to pick lines that were clearly not the same size as the line on the white card. The subject, in other words, sat there as everyone else in the room announced that the truth was something that he could plainly see was not true. Not surprisingly, this occasioned some bewilderment. The unwitting subjects changed the position of their heads to look at the lines from a different angle. They stood up to scrutinize the lines more closely. And they joked nervously about whether they were seeing things.</i><br /><br /><i>Most important, a significant number of the subjects simply went along with the group, saying that lines that were clearly shorter or longer than the line on the card were actually the same size. Most subjects said what they really thought most of the time, but 70 percent of the subjects changed their real opinion at least once, and a third of the subjects went along with the group at least half the time. When Asch talked to the subjects afterward, most of them stressed their desire to go along with the crowd. </i><b><i>It wasn&#39;t that they really believed the lines were the same size. They were only willing to say they were in order not to stand out.</i></b><br /><br /><i>Asch went on, though, to show something just as important: </i><b><i>while people are willing to conform even against their own better judgment, it does not take much to get them to stop.</i></b><i> In one variant on his experiment, for instance, Asch planted a confederate who, instead of going along with the group, picked the lines that matched the line on the card, effectively giving the unwitting subject an ally. And that was enough to make a huge difference. Having even one other person in the group who felt as they did made the subjects happy to announce their thoughts, and the rate of conformity plummeted.</i><br /><br /><b><i>Ultimately, diversity contributes not just by adding different perspectives to the group but also by making it easier for individuals to say what they really think. As we&#39;ll see in the next chapter, independence of opinion is both a crucial ingredient in collectively wise decisions and one of the hardest things to keep intact. Because diversity helps preserve that independence, it&#39;s hard to have a collectively wise group without it.&quot;</i></b><br /><br /><b>Herding:</b><br /><br /><i>&quot;[W]hen all of one&#39;s peers are following the exact same strategy, it&#39;s difficult to follow a different one, especially when the new strategy is more risky and failure will be public and inescapable ... Under those conditions, sticking with the crowd and failing small, rather than trying to innovate and run the risk of failing big, makes not just emotional but also professional sense.  This is the phenomenon that&#39;s sometimes called herding.  Just as water buffalo will herd together in the face of a lion, football coaches, money managers, and corporate executives often find the safety of numbers alluring -- as the old slogan &#39;No one ever got fired for buying IBM&#39; suggests.&quot;</i><br /><br />[...]<br /><br /><i>&quot;Herders may think they want to be right, and perhaps they do.  But for the most part, they&#39;re following the herd because that&#39;s where it&#39;s safest.  They&#39;re assuming that John Maynard Keynes was right when he wrote, in The General Theory of Employment, Interest, and Money, &#39;Worldly wisdom teaches that it is better for reputation to fail conventionally than to succeed unconventionally.&#39;  And yet there is the fact that the crowd is right much of the time, which means that paying attention to what others do should make you smarter, not dumber.  Information isn&#39;t in the hands of one person.  It&#39;s dispersed across many people.  So relying on only your private information to make a decision guarantees that it will be less informed than it could be.  Can you safely rely on the information of others?  Does learning make for better decisions?</i><br /><br /><i>The answer depends on how we learn.&quot;</i><br /><br /><b>Social Proof:</b><br /><br /><i>&quot;In 1968, the social psychologists Stanley Milgram, Leonard Bickman, and Lawrence Berkowitz decided to cause a little trouble. First, they put a single person on a street corner and had him look up at an empty sky for sixty seconds. A tiny fraction of the passing pedestrians stopped to see what the guy was looking at, but most just walked past. Next time around, the psychologists put five skyward-looking men on the corner. This time, four times as many people stopped to gaze at the empty sky. When the psychologists put fifteen men on the corner, 45 percent of all passersby stopped, and increasing the cohort of observers yet again made more than 80 percent of pedestrians tilt their heads and look up.</i><br /><br /><b><i>This study appears, at first glance, to be another demonstration of people&#39;s willingness to conform. But in fact it illustrated something different, namely the idea of &#39;social proof,&#39; which is the tendency to assume that if lots of people are doing something or believe something, there must be a good reason why. This is different from conformity: people are not looking up at the sky because of peer pressure or a fear of being reprimanded. They&#39;re looking up at the sky because they assume -- quite reasonably -- that lots of people wouldn&#39;t be gazing upward if there weren&#39;t something to see. That&#39;s why the crowd becomes more influential as it becomes bigger: every additional person is proof that something important is happening. And the governing assumption seems to be that when things are uncertain, the best thing to do is just to follow along. This is actually not an unreasonable assumption. After all, if the group usually knows best (as I&#39;ve argued it often does), then following the group is a sensible strategy. The catch is that if too many people adopt that strategy, it stops being sensible and the group stops being smart.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"The Debate Over the Science Futures Market","summary":"Many of the Arguments Against a Science Futures Market are Also Arguments Against Peer Review / Some Concerns Could be Avoided by Simply Restricting the Types of Bets Accepted / The System Would Act to Discourage Insincere Debators / It Could Increase Interest in the Cutting Edge of Science / It Might Help the Public to Identify Underperforming Scientific Consensus, to Knock Down the Ideological Barriers Between Disciplines, to Reward Skepticism of Scientific Fads and Pseudoscientific Claims, and to Correct for Biases in Science Journalism","image":"https://lh3.googleusercontent.com/-mF9rycBWdDA/WDteGDtaE7I/AAAAAAAAJYE/vRz3y_W068MVi0HxGnITZZyf5BN7WlGIgCJoC/w3600-h5040/the-debate-over-the-science-futures-market-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/7UACtWwASmz","publishDate":"2016-11-27T22:30:03.068Z","updateDate":"2017-01-03T01:43:15.416Z","text":"<b>The Debate Over the Science Futures Market: Many of the Arguments Against a Science Futures Market are Also Arguments Against Peer Review / Some Concerns Could be Avoided by Simply Restricting the Types of Bets Accepted / The System Would Act to Discourage Insincere Debators / It Could Increase Interest in the Cutting Edge of Science / It Might Help the Public to Identify Underperforming Scientific Consensus, to Knock Down the Ideological Barriers Between Disciplines, to Reward Skepticism of Scientific Fads and Pseudoscientific Claims, and to Correct for Biases in Science Journalism</b><br /><br /><i>Social Epistemology</i>, 1995, Vol. 9, No. 1, 3 - 33<br /><br />Symposium on the possibility of science funding as an &#39;idea futures&#39;<br />market<br /><br />Could gambling save science?<br />Encouraging an honest consensus<br /><br />Robin Hanson<br /><br />[...]<br /><br /><i>&quot;6. Objections</i><br /><br /><i>The main difference between &#39;blue sky&#39; fantasies and serious but radical suggestions is in how well they handle the details. </i><b><i>If you are like most readers, you will by now have thought of one or more problems with or objections to idea futures. If so, you are encouraged to scan this section and go directly to the issues of concern to you.</i></b><i> (Most of these issues have been raised by at least three independent commentators in previous discussions.)&quot;</i><br /><br />The List of Objections:<br /><br />6.1. Isn&#39;t gambling illegal?<br />6.2 Isn&#39;t betting a useless zero-sum game?<br />6.3 Does anybody ever bet this way?<br />6.4 What about compulsive gambling?<br />6.5 Is there enough interest in science questions?<br />6.6 Will these markets be too thin?<br />6.7 Doesn&#39;t betting only work for clear-cut questions like horse races?<br />6.8 How often do beliefs really converge?<br />6.9 What if beliefs never converge?<br />6.10 What do convergent beliefs have to do with truth?<br />6.11 What about badly-worded claims?<br />6.12 Can&#39;t wrong ideas still be useful?<br />6.13 What if the fine print differs from the summary?<br />6.14 What aboout sucker bets?<br />6.15 Don&#39;t science questions resolve too slowly?<br />6.16 Why should I trust the judges?<br />6.17 Won&#39;t judging cost too much?<br />6.18 Won&#39;t wealthy people have too much influence?<br />6.19 Won&#39;t the market be dominated by fools?<br />6.20 Won&#39;t advertising manipulate opinion?<br />6.21 Aren&#39;t markets full of cheats and thieves?<br />6.22 What about insider trading?<br />6.23 What about &#39;moral hazard&#39;?<br />6.24 What about incentives to start false rumors?<br />6.25 What about incentives to keep information secret?<br />6.26 Won&#39;t an apparent consensus create a crowd mentality?<br />6.27 Will the new incentives slow or stop convergence?<br />6.28 Won&#39;t different claim wordings, judges, and base assets confuse the consensus?<br />6.29 Won&#39;t the consensus reflect risk preferences as well as beliefs?<br />6.30 Won&#39;t betting challenges discourage creativity?<br />6.31 What&#39;s the point of a &#39;consensus&#39; that people disagree with?<br />6.32 Isn&#39;t it better for people to argue out their own disputes?<br />6.33 Won&#39;t this have the same problems as patents?<br />6.34 Wouldn&#39;t anonymous trading screw up reputation statistics?<br />6.35 If this is so great, why hasn&#39;t it happened already?<br />6.36 Can nations funding research in this way appropriate the benefits?<br />6.37 Shouldn&#39;t we appeal to higher motives than greed?<br /><br /><b><i>&quot;6.1. Isn&#39;t gambling illegal?</i></b><br /><br /><i>Yes, betting markets on science questions appear to be legal only in Great Britain, where they are highly regulated. Even Nevada, which allows sports betting, prohibits general betting to avoid scandals that might &#39;taint&#39; the gambling industry. </i><b><i>This is a shame because most of the arguments against betting, discussed below, do not apply well to science betting.</i></b><i> Scattered markets are allowed that give rather good consensus estimates on horse races and football teams, yet not on important science and technology questions! In the long term perhaps legislators can be persuaded to allow science bets because of their extra benefits and reduced problems. Science betting certainly seems to be easier to justify than the currently popular regressive taxation through state lotteries.</i><br /><br /><b><i>6.2 Isn&#39;t betting a useless zero-sum game?</i></b><br /><br /><i>A standard argument for making betting illegal is to keep people from wasting their energies in unproductive activities. The only obvious value in betting on dice throws is entertainment, but laws to prohibit this usually also prohibit much more. </i><b><i>Life insurance, joint stock companies (Brenner and Brenner, 1990), and commodity futures markets (Rose, 1986) were all prohibited by anti-gambling laws until advocates managed to obtain exemptions.</i></b><br /><br /><i>Participating in a zero-sum game does not make betting useless. Betting markets allow traders to reduce risk, and create informative prices. In liquid markets most of the trading, liquidity, and price rationalization comes from speculators, for whom the market is basically a betting game. Buying any particular stock in the stock market, for example, is basically a bet in a zero-sum game when compared to investing in the standard &#39;market&#39; combination of all assets in the same tax and risk category. (While, if the prices are irrational, such bets may help the economy as a whole, this &#39;externality&#39; also benefits people not betting on that question.)</i><br /><br /><i>In fact, a standard way to analyze financial portfolios is to break them into contingent assets, each of which has value in only one possible world (Sharpe, 1985). A &#39;complete&#39; market, where one can bet on anything, is best, allowing investors to minimize risk and maximize expected return (Laffont, 1989).</i><br /><br /><i>Science bets would not only allow corporations to more easily insure against technological risk, but also they would create prices embodying the sort of valuable information that governments now fund research to obtain. When the betting stakes are invested in stocks, the money is hopefully being put into productive use by those companies. Therefore, ignoring transaction costs and judging fees, the average rate of return of contingent assets split from stocks would be the same as the return on those stocks.</i><br /><br /><b><i>6.3 Does anybody ever bet this way?</i></b><br /><br /><i>Liquid markets in contingent assets are a somewhat different betting mechanism from the usual bookies or parimutuels. However, they are not untried. </i><b><i>Such markets are widely used to teach MBA students about how markets work (Forsythe et al., 1991), and are usually done on elections. Financial traders sometimes use them to bet on sports. I have developed a board game where players use such a market to bet on a murder mystery as it unfolds. Most ordinary people learn the mechanism very quickly.</i></b><br /><br /><b><i>6.4 What about compulsive gambling?</i></b><br /><br /><i>About 2% of the population seems unable to resist the temptation to risk more than they can afford to lose (American Psychiatric Association, 1987) in casinos, racetracks, and high-risk financial markets. Lost in the thrill of &#39;action&#39; and the hope that all of their financial worries will soon be over, they often regret their excess later, and resort to desperate measures, like theft, to pay debts.</i><br /><br /><i>Compulsive gambling is encouraged by advertising and by easy access to games with a quick and possibly large payoff. British law reduces this problem by requiring casino players to apply 48 hours in advance, by allowing them to sign up on lists of people to be excluded from all casinos, and by forbidding youth and on-site alcohol, entertainment, and credit (Kelley, 1988). Margin limits in financial markets serve some similar functions.</i><br /><br /><i>Governments may impose similar rules to discourage compulsive gambling in idea futures, although it is important that any advertising restrictions do not prevent the wide dissemination of current consensus odds on important issues. More importantly, unless options or investments on margin are offered, </i><b><i>science questions are generally too long-term to be a problem</i></b><i>, offering no more &#39;action&#39; than longterm stock investments. </i><b><i>Traders who regret their purchase a few days later can sell and get most of their money back. Given that many other options markets exist, it is not clear that allowing science options would increase opportunities for compulsive risky investing.</i></b><br /><br /><b><i>6.5 Is there enough interest in science questions?</i></b><br /><br /><i>A recent science fiction novel (Brunner, 1975) imagined widespread betting on science and technology questions, supplanting horse racing in popularity. </i><b><i>It is possible that having a direct, if small, influence and personal stake in science would heighten the public&#39;s interest.</i></b><i> At present, though, fewer people probably follow science than football.</i><br /><br /><b><i>It is not necessary to interest everyone, however, just enough people to pay for the modest overheads involved. Few people have interest and opinions about the future price of corn, yet corn futures markets thrive. A great many people are now involved in scientific research, many more read scientific journals, and even more follow science in the popular media. Many of these people have strong opinions on various science controversies and feel that they have insufficient opportunity to express them. Idea futures would thrive if it tapped only a small fraction of current interest and effort.</i></b><br /><br /><i>Having a fraction of science funding channeled through betting markets would certainly accomplish this. </i><b><i>So might basic attitude changes toward seeing markets as a legitimate place to &#39;take a stand&#39; on important issues, trading scores as indicators of who is right more often, and the market price as a valid consensus measure. Idea futures does not need large sums of money to be successful; even when there is only $100 bet on a question, the market still offers the social benefit of a visible consensus and incentives for honesty.</i></b><br /><br /><b><i>6.6 Will these markets be too thin?</i></b><br /><br /><i>In a market with low &#39;liquidity&#39;, there are so few traders that you have to wait a while to find someone willing to trade with you. Automated market-makers (Hakansson et al., 1985), always ready to trade at prices determined by their current inventory, can increase liquidity and maintain a small &#39;spread&#39; between their buying and selling price offers. They can be very cheap if the basic transaction costs are low, which they could be if thousands of markets shared the same computerized market place.</i><br /><br /><b><i>The market might, however, remain &#39;thin&#39; in the sense that prices could change quickly against a trader in response to each small amount traded, so they would have to wait to get a &#39;reasonable&#39; price. A lack of expected market thickness can be a self-fulfilling prophecy, since traders prefer thick markets (Economides and Siow, 1988). This is a standard explanation for the limited number of futures and options markets currently available.</i></b><br /><br /><i>Funding channeled through market-makers would of course thicken the markets, as would consistency arbitrage and conditional offers that connect questions. Improving computer technology, with lower transaction costs and automated trading strategies, should make thinner markets more tolerable. Besides, two people making a bet is a very thin market, but it happens all the time. Just one person posting an offer to bet on a new subject could be an important contribution to our social consensus.</i><br /><br /><b><i>Thin markets are known for being good places to find overlooked bargains, and are less prone to speculative bubbles (a single rational person can squash one). A thin idea futures market may actually seem better to some people, as the cost to change the current market consensus would be less. However, thicker markets are better in general.</i></b><br /><br /><b><i>6.7 Doesn&#39;t betting only work for clear-cut questions like horse races?</i></b><br /><br /><b><i>Most organized betting focuses on questions which, like sporting events, will become very clearly resolved in a fixed time. This minimizes disputed verdicts and judging costs, and it makes sense for risk and entertainment-seeking bettors to focus on such subjects. This does not imply that, given a specific subject area, betting markets are not a reasonable alternative to other consensus, reputation, and incentive mechanisms. Any incentive mechanism must pick some arbiter of quality, and subjects that are difficult for bets are also difficult for other approaches.</i></b><i> For example, peer review, which uses averages of anonymous expert reviews as a quality measure, is widely believed to work better in the &#39;hard sciences&#39; than elsewhere.</i><br /><br /><i>Eventually most scientific controversies seem to get resolved enough to settle a bet. This resolvability is in fact central to popular notions of what defines science. Scientific claims are often defined as claims of &#39;fact&#39;, which future evidence could possibly disprove (Popper, 1985), or at least alter our degree of confidence in. Science is widely believed to be &#39;progressive&#39; so that as evidence accumulates and relevant studies continue opinions gradually converge. Beautiful theories killed by ugly facts are left behind. As Bacon said, &#39;Truth is the daughter, not of authority, but of time&#39;.</i><br /><br /><i>Actually, most people believe that opinions on most questions of fact usually converge with time, evidence, and sincere study. We hope that history will prove us right. We debate and discuss, essentially saying &#39;I&#39;ll bet if we talked it out, you&#39;d see I&#39;m right&#39;. We take the advice of experts, indicating that we think we would come to believe what the experts believe, if only we were to study what the experts have studied.</i><br /><br /><i>Even if we aren&#39;t sure whether opinions will converge, we think that there is a good chance that they would converge if only a knowledgeable and detached enough group would spend enough effort to study and debate the question. If that group is diverse and independent enough, we believe that we would probably agree with them. If so, we should accept their verdict to settle a bet.</i><br /><br /><b><i>6.8 How often do beliefs really converge?</i></b><br /><br /><i>Just because people believe that their opinions converge, it doesn&#39;t mean that they do. After all, there are strong social reasons to want to believe in convergence. </i><b><i>Even if most questions that are settled today were once controversial, this doesn&#39;t mean that most old controversies are now settled. Perhaps yesterday&#39;s question referred to concepts that are not even considered to make sense today. Historial studies, examining random scientific questions and claims of several centuries ago, should be done to shed light on these doubts.</i></b><br /><br /><i>There are reasons to be optimistic. Standard decision theory, though it does not adequately account for the computational costs of deducing the implications of theories and evidence, is instructive and indicates that rational agents should come to agree (Seidenfeld, 1990). Consider an ideal decision theory agent who has a degree of belief in some particular claim A and continues to observe new evidence. Asymptotically, either all new evidence will be irrelevant and have no bearing on A, or the agent will become certain about whether A is true or false. Now imagine that the claim A specifies a detailed possible world, i.e. says that the real world is one particular world out of the many possible worlds. If two ideal agents start out with wildly different beliefs, but neither of them is completely certain about A, and if they both observe the same not asymptotically-irrelevant evidence, then they will asymptotically come to agree about A.</i><br /><br /><i>Studies indicate that people also have strong tendencies to conform and agree when exposed to each others&#39; opinions (Linstone and Turoff, 1975) and arguments (Myers, 1983). In fact, the rate at which they come to agree often seems faster than can be rationally justified by decision theory. Randomly-selected legal juries usually come to a unanimous verdict on complex legal questions.&quot;</i><br /><br />(Note: I believe there is a much simpler answer to this question: The agencies responsible for wording the bets are responsible for taking care to only select those bets which avoid these problems.  Rather than accepting vague bets like &quot;Vindication of theory X over theory Y&quot;, they would favor specific, quantitative bets related to specific missions or observations.)<br /><br /><b><i>&quot;6.9 What if beliefs never converge?</i></b><br /><br /><i>Even if beliefs usually converged, idea futures might be unworkable if they dealt badly enough with situations where beliefs don&#39;t converge. One approach is to have mutually exclusive claim sets include a &#39;this question too vague to judge&#39; claim, which the judges could choose if it seemed clear that no amount of study or time would ever allow a choice between the rest. Most people could then bet on the question conditional on it being resolved. This solution fails, however, if sincere beliefs never converge and yet it does not become clear whether or not beliefs will converge. A deadline by which a question must be resolved could deal with this, but has other disadvantages.</i><br /><br /><i>If investors can reasonably estimate the chances that a question will be unresolvable in this manner, then the problem is manageable. High-risk questions will only be traded if there is enough disagreement (Jaffray, 1989) or subsidies to justify it, and for low-risk questions the problem can be ignored. In addition, it seems, resolvability can be estimated. Questions about religion and morals are more difficult, as are certain long-standing riddles like the nature of consciousness. On the other hand, a question about a physical property of a substance, like a bond angle in some new molecule, seems to be quite resolvable. </i><b><i>As a rule, one should prefer questions closer to direct observations</i></b><i>. General claims for which relevant evidence will always be available should be better than claims such as what someone had for breakfast ten years ago.</i><br /><br /><b><i>6.10 What do convergent beliefs have to do with truth?</i></b><br /><br /><i>The philosopher Peirce claimed that &#39;The opinion which is fated to be ultimately agreed to by all who investigate, is what we mean by the truth&#39; (Thayer, 1980). However, the question of whether the convergence opinion that we might all come to with unlimited evidence, study, and debate is the way the world &#39;really&#39; is, is beyond the scope of the paper. Even if it isn&#39;t &#39;truth&#39;, we are all interested in it, and it is hard to think of a better truth-estimate on which to base academic incentives.&quot;</i><br /><br />(Again, I feel that different markets would handle such situations differently ... I know how <b>I</b> would deal with this: heavily restrict the types of bets accepted.)<br /><br /><b><i>&quot;6.11 What about badly-worded claims?</i></b><br /><br /><i>Even if an issue becomes settled, a poorly-worded claim on that issue may be unresolvable. To avoid this, we need techniques for avoiding ambiguity and incentives for players to use them.</i><br /><br /><i>Wording a claim so that it is both relevant to some important issue and minimally ambiguous is a skill that is routinely learned in many professions. Lawyers and philosophers obtain clarity through standardized words and language, and experimental scientists are adept at finding connections between abstract theories and specific observations. </i><b><i>Claims should avoid slippery concepts and phrasing that allows many interpretations. Verbose annotations can also help by discussing motivations, examples, intended word meanings, judging criteria, etc.</i></b><br /><br /><i>If copyright laws are interpreted as applying to claim wordings, then claim authors may be able to charge an extra royalty fee for each &#39;join&#39;. Claim authors would then compete with each other for royalties for investors, who would prefer authors with reputations for writing clear and interesting claims. Added incentives come if authors bet against their claim being judged to be too vague.</i><br /><br /><i>To avoid excessive costs in forming a claim, a question could hold a &#39;clarification lottery&#39;. After a certain time, or when the market capital reached a certain amount, judges could be funded in the usual manner to replace a hastily-worded claim with a more considered one.</i><br /><br /><i>Even when one can not really word a good claim to bet on directly, markets offer other ways to bet on a subject. For example, if one believed that when physicists disagree with chemists, the chemists are usually right, one could invest in a &#39;basket&#39; or mutual fund that bets on the side of chemists in as many controversies as possible.</i><br /><br /><b><i>6.12 Can&#39;t wrong ideas still be useful?</i></b><br /><br /><i>Absolutely. If you think an idea is probably wrong, but is probably more like the right answer than anything else around, then bet on that. If you just think that work on the idea is likely to inspire something interesting, then bet on that. These questions will be harder to judge though.</i><br /><br /><b><i>6.13 What if the fine print differs from the summary?</i></b><br /><br /><i>Verbose claims would probably be described by short summary sentences or phrases in price lists, offers, etc. As with contracts and political ballot initiatives, there are problems when a deceptive title differs from the fine print. In extreme cases people might sue for misrepresentation, but usually we can only encourage the buyer to beware.</i><br /><br /><b><i>6.14 What aboout sucker bets?</i></b><br /><br /><i>If a stranger offers to bet you on an oddball subject, there is a good chance that he is trying to trick you with a deceptive claim. Even if it looks as though you couldn&#39;t lose, you are well-advised to decline; </i><b><i>the fact that he is making an offer gives you information.</i></b><br /><br /><i>In markets on pre-existing controversies where many traders have already examined the claims this is less likely, though still possible. In general, traders should look claims over carefully and not bet unless they honestly think that they know better than the other traders.</i><br /><br /><b><i>6.15 Don&#39;t science questions resolve too slowly?</i></b><br /><br /><i>The fundamental questions that get people interested in science, such as whether the universe is infinite, can take decades or even centuries to resolve. However, this does not prevent markets in such questions. Almost any newspaper will show that people regularly buy bonds scheduled to mature in forty years. Fifty-year-olds who buy such bonds are not counting on living to be ninety; they know that they can sell the bonds in the market at any time.</i><br /><br /><i>At present, usually you cannot get a PhD on whether the universe is infinite; you would focus instead on a smaller question that is hopefully relevant for the bigger ones. </i><b><i>Idea futures investors will similarly prefer shorter-term questions. A question that takes ten years to resolve (say starting at 50/50 and ending more than 90% certain 90% of the time) should have the same sort of daily price fluctuations (around 1.5%) as stocks do</i></b><i>, and so support a similar mix of short-term speculators, and long-term fundamentals-oriented investors.</i><br /><br /><b><i>For longer-term questions investing in fundamentals is less attractive.</i></b><i> Less information comes out per unit time in a long-term market, so there is less money to be made for a given market thickness. If you must hold out for decades until other investors come to their senses, the extra rate of return above the market average that you get for your information may be very small, and so you may prefer to quit now if you have better opportunities elsewhere. To make things worse, this creates an opportunity for strategic behavior. Someone might move the price in some direction and try to hold it there in the hope that other traders will not be willing to hold out for as long and therefore will quit at a loss.</i><br /><br /><i>Finally, </i><b><i>you may not trust the underlying financial institutions to remain stable over a century or more.</i></b><i> Few people would probably bet that &#39;Nuclear war will destroy most of civilization&#39;, even though many people would like to for insurance reasons. Even if the banks don&#39;t go bankrupt, uncertainties about the relative longterm value of different base assets that the betting stakes could be invested in may completely swamp any added return from winning a bet. This problem could be minimized if the &#39;market asset&#39; (Sharpe, 1985), a maximally diversified world mutual fund, became the standard base asset.</i><br /><br /><b><i>Even with all these problems, there will probably be rather thick and well-subsidized markets on a few very basic science questions [like climate science], as funding agencies and amateurs seeking to influence important issues would focus on them.</i></b><i> Such questions could be connected, through a network of conditional offers, to related shorter-term questions that research could more directly resolve, allowing researchers of shorterterm questions to obtain some of the subsidies on the basic questions.</i><br /><br /><i>In financial markets, the conventional wisdom is that longer-term price movements are less rational, as there is less incentive to correct irrational deviations. However, there is still some incentive, and so idea futures may still offer an improvement over the existing situation.</i><br /><br /><b><i>6.16 Why should I trust the judges?</i></b><br /><br /><i>Even when sincere opinions would converge, investors may worry about judges being biased by bribes or various shared interests and associations. Fortunately, </i><b><i>investors get to pick the assets that they buy, and therefore the judges that they bet on. So they can prefer long-lived judging organizations with reputations for fairness and avoiding scandals, and which use various available means to discourage foul play.</i></b><br /><br /><i>Incentives for traders to settle out of court and avoid judging altogether certainly help to avoid judging foul play. So do clear-cut claims and judging criteria that leave little room for judging discretion. If we wait so long that the right verdict becomes &#39;obvious&#39; it would also be hard for judges to cheat. Also more trustworthy are juries of people who have never had a stake in the question, randomly selected from a large population, deliberating openly and offering to consider any relevant evidence.</i><br /><br /><i>The question of whether some proposed evidence is relevant for some deliberation could even have its own betting market; juries could offer to consider any evidence for which the market odds of relevance were above some threshold.</i><br /><br /><i>Incentives to detect foul play could come from both the ability to sue cheating judges, and possibly from large bonds that judges might post payable to anyone who uncovers such corruption. Also, any persistent difference in the market odds on the same claim with different judges would constitute consensus about judging bias, flagging those judges for closer scrutiny. Judge rating agencies might form. Finally, &#39;appeals&#39; markets can give judges a direct incentive to be careful and honest, since judges must then bet that their verdict will be upheld on appeal.</i><br /><br /><b><i>6.17 Won&#39;t judging cost too much?</i></b><br /><br /><i>Through audit lotteries, one can keep the percentage taken by judges below any given threshold, and still afford to pay for very detailed judging, </i><b><i>even going so far as to choose many jurors from widely different cultures and train them in one or more specialities before having them adjudicate on some specific issue!</i></b><i> This approach is mainly limited by risk aversion, which limits the attractiveness of large wins. Most people will not want to bet so much on any one question that the amount they might win would be much more than their total wealth. A one in a billion chance of winning a billion dollars is not worth as much to most people as a one in a thousand chance of winning a thousand dollars. If the amount one would need to bet to avoid this effect is too small, it is not worth the bother and people will bet nothing on the question.</i><br /><br /><b><i>6.18 Won&#39;t wealthy people have too much influence?</i></b><br /><br /><i>Markets are not opinion polls where the rich get more votes; to use market influence one must risk losing it. As in existing financial markets, rich investors who are not specialists in some particular area will prefer to get investment advice from someone who is a specialist, or avoid investing in that area entirely. This is similar to the way that powerful people defer to academic specialists now. Rich people who carelessly throw their weight around will lose their riches.</i><br /><br /><i>Even so, the wealthier social classes will have more influence, as they do now in most areas of life, including academia. If this is a problem that you are willing to invoke the force of government to solve (I am reluctant to do so), then the natural solution is general wealth redistribution. This is much more cost-effective than crudely trying to keep the rich out of any particular walk of life.</i><br /><br /><i>If you worry that markets would create large inequalities in academia, don&#39;t. Influence in academia, as measured for example by number of papers published (Price, 1963), is far more concentrated than in most walks of life. </i><b><i>It seems unlikely that markets would make things worse, and could well make things much better, as people would not need degrees or the blessing of the academic élites to play as equals.</i></b><br /><br /><b><i>6.19 Won&#39;t the market be dominated by fools?</i></b><br /><br /><i>Again, markets are not opinion polls. </i><b><i>Anyone can invest in any open market, but they only choose to invest where they think they have special insight or insurance needs.</i></b><i> Even if they are mistaken about their special insights into, say, the gold market, they are fairly quickly taught otherwise. Most people who play commodity markets, for example, lose their stake and quit within a year. Such markets are dominated by the minority who have managed to play and not go broke. If you believe otherwise, and know of some market where the prices are obviously wrong, I challenge you to &#39;put your money where your mouth is&#39; and take some of that free money you believe is there for the taking. It&#39;s easy to bad-mouth the stupid public before you have tried to beat them.</i><br /><br /><b><i>6.20 Won&#39;t advertising manipulate opinion?</i></b><br /><br /><i>Advertising, in the sense of campaigns to persuade through evidence and arguments, exists now in academia and would certainly persist. Advertising, in the sense of clever jingles and sex appeal to grab the subconscious of the impulse buyer, should not be a problem. </i><b><i>People do not try to affect the price of corn futures with clever jingles</i></b><i>; it would be like trying to sell cars by offering free balloons to Consumer Reports technicians. The clever investors who dominate markets are smarter than that.</i><br /><br /><b><i>6.21 Aren&#39;t markets full of cheats and thieves?</i></b><br /><br /><i>Yet, but this does not usually distort the incentives or the consensus price much. Most cheating is not &#39;manipulating the price&#39;, which is rather hard to do in a liquid market, but conflicts of interest where people who supposedly represent others instead act in their own interest, giving poor advice to clients and using information gained from clients.</i><br /><br /><i>Insider trading is mentioned below. However, brokers and investment advisors are the worst case. In markets you win whenever you can get others to do what you just did, or when you predict what they will do and do it first. Brokers and investment advisors often tell you to buy whatever they would like to sell, and charge you large commissions for the &#39;advice&#39;. Brokers often trade for themselves just before they execute trades for you; stop orders and margin calls are especially lucrative.</i><br /><br /><b><i>To avoid being cheated, be careful who you trust. Avoid brokers who trade for themselves, and advisors who do not take the same risk as they advise for you. As bets, idea futures markets cannot be cornered or monopolized. No matter how many bets have been made, other people are always free to bet more.</i></b><br /><br /><b><i>6.22 What about insider trading?</i></b><br /><br /><i>When an employee of a company makes money by trading on inside information that they have about that company, or by telling someone else so that they can trade, that employee is considered to be going against the interest of the other stockholders who own the company. Employment contracts and laws can forbid this conflict of interest, although price movements just before major announcements show that a substantial amount of such trading happens anyway.</i><br /><br /><b><i>Fortunately nature has no insiders or employees. The only similar problem in idea futures is when a research laboratory is trying to keep a result temporarily secret before trading on it, and an employee sneaks out and trades first. This could be dealt with exactly as stock insider trading is now, through private trading records accessible to criminal investigators.</i></b><br /><br /><b><i>6.23 What about &#39;moral hazard&#39;?</i></b><br /><br /><b><i>One of the advantages of a market is that it offers incentives to anyone to come and contribute their knowledge about the world. A disadvantage is that, since changing the world can give one special knowledge about it, people may have an incentive to cause harm. If we allow anyone to bet on your lifespan, then someone may decide to kill you just to win a bet. This murder may be much harder to solve than most since, with anonymous trading, almost anyone might be a potential suspect. (Though criminal investigators may be able to learn who really made what &#39;anonymous&#39; trades.) For this reason, there are usually restrictions on who may buy how much life insurance on you.</i></b><br /><br /><b><i>Moral hazard should be less of a problem for basic questions about nature that people cannot change, although it would conceivably be a problem for short-term trading and options that bet on when information will come out. We wouldn&#39;t want someone to blow up the latest accelerator to prevent results from coming out, or to kill some patients to slant a medical study.</i></b><br /><br /><i>Yet we shouldn&#39;t prevent open markets if the chance of foul play seems small. Anyone is allowed to trade stock, even though there is a possibility that someone will sell short the stock of the makers of a pain reliever, and then poison some packages to depress their sales. Only for the rare claim where the risk of harm seemed particularly high might one justify a prior restraint limiting who could have how much stake on the different sides of a question.</i><br /><br /><b><i>6.24 What about incentives to start false rumors?</i></b><br /><br /><i>A &#39;rumor&#39; is just information, perhaps false, passed informally through a social network. Maliciously false rumors occur whenever people both have an interest in what other socially connected people think about a question, and when there is inadequate feedback for learning what rumors were false, so that people can discount unreliable sources.</i><br /><br /><i>In current academia, there is often enough feedback to discourage false rumors about what results are about to be published. Word of mouth that discredits a junior researcher, however, can trash his or her reputation without others ever really finding out if the rumors were right.</i><br /><br /><b><i>Markets both encourage and discourage false rumours. Markets give more people an interest in fooling other people, but also improve the feedback about what rumors were right. The market price offers an alternative to informal information channels.</i></b><i> Again, don&#39;t believe everything you hear; trust advisors with a good track record who take the same risk as they advise you to take.</i><br /><br /><b><i>6.25 What about incentives to keep information secret?</i></b><br /><br /><b><i>If you acquired a piece of information where it was clear which side of what questions the information favored, then your best strategy would be to buy on those favored sides, reveal and publicize your information (perhaps after selling it to other traders), wait for the price to rise, and then sell at a profit. If, however, the implications of the information were not clear, you might be tempted to sit tight and wait for further revelations, even though you risk other people stumbling on to your insight in the meantime. It is similar with incentives to publish. Unless you can connect your insight to currently popular issues, and package enough of them together to make a paper, you cannot get published and so you may keep the idea to yourself.</i></b><br /><br /><i>One approach might be to formulate a question more closely related to your information, and then try to convince some funding agency that your question is interesting, even if its implications are not clear. Alternatively, you could subsidize your question in the hope that this would encourage others to figure out its implications and create conditional offers connecting it to other questions. Either approach might induce enough market thickness to make your information pay off.</i><br /><br /><b><i>6.26 Won&#39;t an apparent consensus create a crowd mentality?</i></b><br /><br /><i>People might think that they agreed more than they actually did, defer to a consensus that had little thought behind it, and so create the social analogues of anchoring and overconfidence (Kahneman and Tversky, 1982). </i><b><i>Would creativity be suppressed?</i></b><br /><br /><b><i>Markets with less thought behind them should give themselves away by being thinner. If not, and some of us catch wind of this trend, we could make money by correcting for it. The market odds at horse races actually tend to be underconfident, being biased toward long-shots. Markets encourage people to be contrarian; the only reason to trade, to not own the same mix of investments as everyone else, is because you think that the consensus is wrong, or for insurance needs.</i></b><br /><br /><b><i>6.27 Will the new incentives slow or stop convergence?</i></b><br /><br /><i>This is the opposite of the above problem. People with a stake on a certain side may become mentally biased toward that side, resisting the rational implications of mounting evidence. </i><b><i>This is, of course, not a new phenomenon in academia, and so it&#39;s hard to see why the problem would be worse. Except for issues closely connected to basic &#39;ideologies&#39; about which almost everyone has an opinion, we can hope to find impartial jurors not overwhelmingly biased by either side.</i></b><br /><br /><b><i>6.28 Won&#39;t different claim wordings, judges, and base assets confuse the consensus?</i></b><br /><br /><i>Unless the performance of a base asset correlates with a claim, the claim&#39;s market odds should be independent of base, and arbitrageurs can easily enforce this. If the prices on the same claim judged by different judges were persistently different, this would constitute consensus about judging bias, a situation that judges would want to avoid. If different claim wordings on an issue have very different prices, this represents consensus that there are really several different issues to be distinguished. For each distiguishable issue, traders seeking liquidity will probably congregate around one or a handful of base asset/wording/judge combinations, thereby avoiding a combinatorial explosion.</i><br /><br /><b><i>6.29 Won&#39;t the consensus reflect risk preferences as well as beliefs?</i></b><br /><br /><i>Yes, the amount one should bet depends on one&#39;s beliefs, attitude toward risk, and the stake one already has in a question (Kadane and Winkler, 1988). Risk-avoiders bet less than risk-takers, and bet less on the side that they already have a stake in. Price distortions from this should be minor, unless almost everyone has substantial non-betting stakes on the same side, or if beliefs correlate significantly with such stakes, and if the stakes held approach each person&#39;s total wealth. One exception is that few people would bet that &#39;Technology will soon make us all too rich to care about money&#39;, even if they believed it.</i><br /><br /><i>It might seem that questions with extremely lop-sided odds would also be a problem. Too few people might bet that &#39;energy is conserved&#39; (EC) if they very confidently expect to win very little. By splitting EC assets along other questions, people could jointly support EC, debate other questions, and get a higher average return. In general, traders should keep splitting until liquidity or risk considerations dominate.</i><br /><br /><i>Some people have worried that opinionated yet extremely risk-averse people, unwilling to bet on anything, would be unfairly labeled &#39;insincere&#39; debaters. It is hard for me to imagine that they could not afford to risk even $10 a year so that we could develop a reputation score for them. If it is the risk of a low reputation score that scares them, perhaps they should not act in such an opinionated way.</i><br /><br /><b><i>6.30 Won&#39;t betting challenges discourage creativity?</i></b><br /><br /><i>If people were expected to bet on every idea that comes out of their mouth, they would be more reluctant to think up wild ideas, most of which are going to be bad. Hopefully we can maintain a distinction between saying &#39;Here is an interesting idea to think about&#39; and &#39;This is the way it is, why won&#39;t you agree?&#39;, only expecting people to put up or shut up in the second case.&quot;</i><br /><br />(It&#39;s hard to imagine how an ideas market would harm creativity more so than peer review already does ...)<br /><br /><b><i>&quot;6.31 What&#39;s the point of a &#39;consensus&#39; that people disagree with?</i></b><br /><br /><i>Regardless of the name used, people often want to pool their differing individual estimates on some issue into a composite estimate. This is more clearly needed in the &#39;public choice&#39; problem, where citizen estimates must be combined into government policy. We also have a more general need for social institutions where experts combine their estimates on some subject into composite estimates, estimates that non-experts can use to make individual choices. Several such institutions may compete for attention, but the need remains.</i><br /><br /><i>Most work on consensus measures (Genest and Zidek, 1986; Grofman and Owen, 1986; Synthese, 1985) focuses on various explicit functions for combining individual beliefs, and some simple variations of these (Mann, 1990) are now used as academic consensus mechanisms. Compared to these, betting markets not only offer superior incentives (Eisenberg and Gale, 1959) for people to bother to make their beliefs explicit and honest, but also betting markets have the following unique claim to the word &#39;consensus&#39;.</i><br /><br /><i>It is in the personal interest of an ideal decision theory agent to make all external actions as if they agreed with the market consensus (Kadane and Winkler, 1988; Nau and McCardle, 1992), without any coercion. Agents should buy contingent assets up to the point where their marginal rates of substitution are the same, i.e. where they all agree on the relative value of getting one more dollar for sure versus even more dollars in some contingency. An external observer, who can offer agents trades or choices but cannot tell how much each agent has already bet, cannot tell that the agents internally disagree.</i><br /><br /><i>Insurance-based proposals (Fairley et al., 1984) are similar in spirit to the betting markets proposed here, as is the following proposal for dealing with the public choice problem (Mueller, 1979). If a government threatens to make a change, sells insurance on the change either way, and then makes the choice that is cheapest for them, they produce an efficient &#39;Pareto optimal&#39; result.</i><br /><br /><b><i>6.32 Isn&#39;t it better for people to argue out their own disputes?</i></b><br /><br /><i>Yes, which is why we want incentives, such as audit lotteries, for parties to settle out of court and avoid judging. </i><b><i>Idea futures is only intended to discourage insincere debaters.</i></b><br /><br /><i>Another way to avoid judging is to hold &#39;argue lotteries&#39;, which are like audit lotteries except that judges are not invoked. The idea is to focus attention on a smaller number of markets where more is at stake. This should induce more discussion and examination of such questions, perhaps resulting in more related questions being formed. Hopefully, opinions would naturally converge, and people would leave the market. Judges are really only there to discourage self-deception and strategic bargaining, so that the market odds eventually reflect the &#39;obvious&#39;.</i><br /><br /><b><i>6.33 Won&#39;t this have the same problems as patents?</i></b><br /><br /><i>No (Hirshleifer, 1971). With patents it must be decided who owns an idea, and so a centralized legal system must make a great many subtle decisions with insufficient evidence and expertise. History must be examined to decide who contributed how much to the idea. Some sharp legal boundaries that determine what it is to use the idea must be defined. </i><b><i>With present patent law, it must be decided if an idea is true, if it is &#39;original&#39;, if it is &#39;obvious&#39;, if it is a &#39;process&#39;, if it was revealed properly, etc. Bets are much more flexible; it need only be decided if an idea is right, and who is to judge that question can be chosen by each person. Government intervention and international agreement are not needed.</i></b><br /><br /><b><i>6.34 Wouldn&#39;t anonymous trading screw up reputation statistics?</i></b><br /><br /><i>Perhaps people could make private trades to move prices out of line, and then make public trades on the other side to bring them back, so that those trades do better than average. This is somewhat like giving someone a wad of money by dropping it in the park and having them wander by an hour later to pick it up. If the park is crowded enough, someone else will have found it by then. In the market, anyone else could make money by stopping the price from moving out of line. The problem is more serious, however, if everyone accepts that only one trader has any information about a question, and so no one else wants to bet there. If identifiable, such markets should be excluded from reputation scores.</i><br /><br /><b><i>6.35 If this is so great, why hasn&#39;t it happened already?</i></b><br /><br /><i>If it was in people&#39;s interest, wouldn&#39;t there be such markets by now? </i><b><i>Well, if we always assumed this we might never do anything new,</i></b><i> but it&#39;s important to ask this question. The fact that science bets have been legal only in Britain, and then only in the last three decades is only part of an explanation.</i><br /><br /><b><i>English bookmakers perceive little demand for science bets, and so take them mainly to induce popular articles mentioning the going odds on unusual subjects</i></b><i> (Sharpe, 1990). This publicity brings in new clients, who may then be switched to the &#39;real&#39; betting on sports. Because of this, bookmakers prefer small bets on subjects &#39;in good taste&#39; that anyone can understand, such as UFOs, Yetis, and Moon landings. </i><b><i>They avoid subjects that seem too esoteric for the general public, like the recent &#39;cold fusion&#39; claims, and subjects that won&#39;t very clearly resolve themselves, as a judging industry has not yet evolved.</i></b><br /><br /><b><i>Bookmakers traditionally prefer to set prices and stick to them, rather than setting up markets in order to play market-maker with the fluctuating prices. This means that they are usually unwilling to offer bets on claims where they do not know how to estimate the odds, and few bookmakers are educated in advanced science. As a result, they mainly take safe bets, siding with the scientific establishment against &#39;crazy&#39; outside theories, which doesn&#39;t help the image problem that betting has in many quarters.</i></b><br /><br /><b><i>English bookmakers do not seem to have seriously tried to sell image-conscious academics on science bets, through arguments like those in this paper. Nor, to my knowledge, has the possibility for betting markets as a funding mechanism been pointed out. Questions of interest to academics are now avoided and no visible consensus that would influence them is formed; one cannot even subscribe to a publication listing the going prices on science questions. It should be possible to improve on this.</i></b><br /><br /><b><i>6.36 Can nations funding research in this way appropriate the benefits?</i></b><br /><br /><i>A popular argument for government-funded research says that, left to themselves, people won&#39;t produce sufficient basic knowledge. If a knowledge producer publishes its results, then &#39;free riders&#39; can use this knowledge without paying the producer (Pavitt, 1991). Patents on basic knowledge are considered to be too fuzzy to enforce, and trade secrets are said to fail because of difficulties in keeping basic information secret, and in figuring out who would find basic knowledge useful.</i><br /><br /><i>Of course if nations do subsidize research, they can fall victim to international free riders, i.e. countries that mainly apply research carried out elsewhere. Some discount this by saying that most research knowledge is never published, but tacit and embodied in the skills of the researchers. Thus subsidies largely benefit researchers and companies located near enough to easily collaborate with and hire such researchers. Of course such a locality of benefits would suggest that research is best funded at a local level, or even privately within a large university campus.</i><br /><br /><i>A need for local appropriation of benefit argues against indirect funding methods, like prizes or idea futures, which cannot as easily control where the research that they induce is performed. However, such mechanisms might be well suited for science-for-its-own-sake philanthropy and for international funding of research, as such indirect methods can better avoid favoritism toward any particular nation.</i><br /><br /><b><i>6.37 Shouldn&#39;t we appeal to higher motives than greed?</i></b><br /><br /><i>The very formulation of the patron&#39;s problem, how to best promote scientific progress given a fixed pile of money, forces one to deal with money. Money is what the patron has to offer. So the patron can only influence people that care to some degree about money, or that care about something else controlled by people who care about money.</i><br /><br /><i>7. Strategy</i><br /><br /><i>It is a lot easier to sketch a grand Utopian vision than it is to figure out how to get there from here. An ideal development strategy would show how to grow incrementally, with each self-supporting step leading naturally to the next one. Most Utopian visions fail because they, instead, require too many things to change all at once.</i><br /><br /><b><i>One advantage of idea futures is that, if not legally prohibited or socially shunned, it can co-exist with existing academic institutions and incrementally attract investors, patrons, and controversies. Papers would still be published and élite committees would still convene. Professors would gradually make more side bets, and begin to challenge each other to bets. Journalists would gradually rely more on the market odds for news stories, and funding agencies would gradually try larger levels of subsidies. Idea futures could rise or fall on its own merits, as people studied how well its predictions compared to other consensus measures, and how the rate of progress in a field depends on the fraction of funding channeled through the markets.</i></b><br /><br /><i>Unfortunately, there seem to be some obstacles to overcome before gradual growth is possible. </i><b><i>Economies of scale in forming reputable judging organizations or building secure computerized marketplaces may mean that certain levels of participation may be required before idea futures can &#39;take off.</i></b><i> However, the major hurdle seems to be attitudes toward the very idea, attitudes reflected in the world-wide legal prohibitions. There are several possible strategies here.</i><br /><br /><i>One approach is more discussions, like those in this paper, of the need for alternative academic institutions, and of betting markets as a particular alternative. Perhaps we need simple word tricks, like insurance and stock bets have used, to disassociate idea futures from ordinary betting, though the concept of bets is very useful in explaining how it works.</i><br /><br /><i>Also helpful is further research on markets in conditional assets, such as recent attempts to show them to be superior to opinion polls at predicting elections (Forsythe et al. 1991). Laboratory experiments (Smith, 1986) comparing betting markets to some mockup of existing peer review institutions would be very interesting, although not of course decisive.</i><br /><br /><b><i>A different approach, which I am also pursuing, would be to create an electronic mail-based reputation game, where people play for &#39;bragging rights&#39; instead of money.</i></b><i> An internet web game is now available at </i><i><a href=\"http://skyler.arc.ab.ca/~morgan/\" class=\"ot-anchor\">http://skyler.arc.ab.ca/~morgan/</a></i><i> IF/lF.shtml. This would avoid legal problems and the discomfort that academics have in dealing explicitly with money, and would allow many people from around the world to participate in a less-threatening partial test of markets as an academic consensus mechanism. However, avoiding money makes the incentives suspect, and precludes many of the advantages, like insurance, that idea futures offer. In particular, it makes it hard to pay judges enough to do a careful job. If enough people played, the scores would mean something to observers, and so people might have an incentive to play and play well. However, building a game up to this status would be hard, probably requiring some &#39;big name&#39; players to attract others.</i><br /><br /><i>If the basic idea became plausible enough to enough private patrons in Britain (because that&#39;s where it&#39;s legal) or government patrons anywhere, idea futures could be tried seriously. The initial field would preferably be one where bets are easier to settle, like number theory, though such subjects tend to be ones where existing institutions also work better, and so perceive less of a need for change. A socially important question with minimal opportunities for conflict of interest would also be nice. Attractive initial candidate fields include number theory, meteorology, remote sensing, and particle properties.</i><br /><br /><b><i>Idea futures will have &#39;made it&#39; when it becomes known as a good place to find out the latest thinking on certain issues, reliably predicting what will later become consensus in other social contexts.</i></b><br /><br /><i>8. Advantages</i><br /><br /><i>If its potential problems can be overcome, and a development path charted and followed, idea futures offers many advantages, most of which have already been mentioned.</i><br /><br /><i>There would be a clear incentive to be careful, honest, and expert when making public statements. </i><b><i>Opinion holders could be rewarded for being right, rather than just for being liked by academic insiders. Those who invest wisely would accumulate capital and gain influence, which they could reinvest in discretionary research or in influencing future consensuses.</i></b><br /><br /><b><i>Funding agencies would only need to pick important questions, not who would be good to research them, what methods ought to be used, or when the research should be done. Diverse approaches could be tried to research a question, without arbitrary penalties against crossing disciplinary boundaries, ignoring fashion and insiders, integrating pre-existing knowledge, violating methodological ideologies, or using insights too small or inarticulate to make a publishable unit. Any approach by which one could reliably make the consensus odds better informed could be financially rewarded.</i></b><br /><br /><b><i>Anyone, not just articulate PhDs, could contribute directly to the world&#39;s corpus of knowledge. Easily published science odds and amateur betting should increase popular interest in science. Even if the &#39;great unwashed&#39; turn out to be poor contributors, they would subsidize professional efforts on questions of popular interest, and perhaps increase the general savings rate. I suspect that they will do better than most élites expect.</i></b><br /><br /><b><i>Clear market odds would ease science reporting. A visible scientific consensus would be available to guide public policy, a consensus that would be self-consistent across a wide range of issues and harder for media campaigns to distort. Compared to competing consensus mechanisms, idea futures should be relatively simple, cheap, decentralized, egalitarian, responsive to new information, and at least as informative. This consensus should correct for many current biases, such as overconfidence.</i></b><br /><br /><b><i>The mere threat of betting challenges could improve incentives in discussions and debates. If the market consensus carried social weight, it could serve as a co-ordination point for thousands of independent conversations. A rejected visionary would have a new way to get publicity for his ideas, and a reward for being right against the establishment; true cranks would subsidize leveler heads. As debates became settled, they would leave a trail of agreed-upon statements that could be used to counter bogus claims made by those ignorant of solid expert consensus.</i></b><br /><br /><i>Businesses could make insurance hedges against technological risk, as in the cold fusion case. While such insurance may be legal now, the introduction of speculators would increase market thickness to a point where it might be practical.</i><br /><br /><b><i>Reputation scores offer a new way to evaluate people&#39;s ability to separate the wheat from the chaff in ideas and arguments, and these scores should depend less on whether one has curried favor from the right people.</i></b><br /><br /><b><i>Idea futures is well-grounded in our best theories of decision and incentives. Once legal and accepted, idea futures could grow incrementally, and perhaps dramatically increase our rate of scientific progress per funding spent.</i></b><br /><br /><i>9. Conclusions</i><br /><br /><i>Markets in contingent assets, more commonly known as &#39;bets&#39;, offer a needed alternative to existing academic institutions. Betting markets cannot solve all current problems, or replace all current institutions. However, if this paper has been successful, the potential of such markets should be clear, and most of the obvious problems with such markets should have been addressed in sufficient detail that we can say that the idea still seems plausible on a closer examination. If so, more serious intellectual discussion is justified, and perhaps some small-scale experiments. We could do much worse than having intellectual institutions as open, flexible, diverse, and egalitarian as the stock market, with incentives as wellgrounded and with estimates on important issues as unbiased and predictive.</i><br /><br /><i>Notes</i><br /><br /><i>1. Teaching reform is beyond the scope of this paper. However, I see no reason why ray research reform proposal should make teaching worse.</i><br /><br /><i>2. Early peer reviewers often personally observed experiments and tried to reproduce anlayses.</i><br /><br /><i>3. This definition of progress is more objective than citation counts (Cole and Cole, 1987), and hopefully avoids debates about whether more knowledge is good, or whether there is really an ultimate truth.</i><br /><br /><i>4. This name is suggested by the way that an auditor might randomly select expense reports for more careful scrutiny.</i><br /><br /><i>5. Investors can insure against the added risk that audit lotteries impose by putting money into a pot to be gambled in the same lottery, but on the other side.&quot;</i><br /><br />References<br /><br />AMERICAN PSYCHIATRIC ASSOCIATION (1987), Diagnostic and Statistical Manual of Mental Disorders, 3rd edn, American Psychiatric Association, Washington, DC.<br /><br />BERNAL, J. (1989), &#39;After twenty-five years&#39;, Science and Public Policy, 16(3): 143-151.<br /><br />BRENNER, R. and BRENNER, G. (1990) Gambling and Speculation, Cambridge University Press, New York.<br /><br />BRUNNER, J. (1975), The Shockwave Rider, Harper &amp; Row, New York.<br /><br />BYRNE, G. (1989), &#39;A modest proposal&#39;, Science, 244: 290.<br /><br />CHUBIN, D., HACKETT, E., Peerless Science, SUNY Press, NY. (1990).<br /><br />CIALDINI, R. (1988), Influence, Sdence and Practice, Scott, Foresman and Co., Boston.<br /><br />CICCHETTI, D. (1991), &#39;The reliability of peer review for manuscript and grant submissions: a cross-disciplinary investigation&#39;, Behavioral and Brain Sciences, 14(1): 119-135.<br /><br />COLE, S. and COLE, J. (1987), &#39;Testing the Ortega Hypothesis: milestone or millstone?&#39;, Scientometrics, 12(5-6): 345-353.<br /><br />DEBUS, A. (1970), Science and Education in the Seventeenth Century, MacDonald, London.<br /><br />DREXLER, K. E. (1986), Engines of Creation, Doubleday, New York.<br /><br />DOWIE, J. &quot;Gambling in Education and Education in Gambling&quot;, Proc. Fifth Intl. Conf. on Gambling and Risk Taking, ed. William Eadington, pp. 23-29. (1982)<br /><br />ECONOMIDES, N. and Slow, A. (1988), &#39;The division of markets is limited by the extent of liquidity (spatial competition with externalities)&#39;, American Economic Review, 78 March, 108-121<br /><br />EISENBERG, E. and GALE, D. (1959), &#39;Consensus of subjective probabilities. The pari-mutuel method&#39;, Annals of Mathematics and Statistics, 30: 165-168.<br /><br />EVANS, D. (1991), &#39;Atom priests&#39;, San Jose Mercury News, 28 October, p. 7B.<br /><br />FAIRLEY, W., MEYER, M. and CHERNICK, P. (1984), &#39;Insurance market assessment of technological risk&#39;, Proceedings of the Society for Risk Analysis International Workshop on Uncertainty in Risk Assessment, Risk Management, and Decision Making, 30 September, pp. 89-102.<br /><br />FEYERABEND, P. (1975), &#39;How to defend society against science&#39;, Radical Philosophy, 2 (Summer): 4-8.<br /><br />FORSYTHE, R., NELSON, F., NEUMANN, G. and WRIGHT, J. (1991), &#39;The explanation and prediction of Presidential elections: a market alternative to polls&#39;, in T. PALFREY (ed.), Laboratory Research in Political Economy, University of Michigan Press, Ann Arbor, pp. 69-112.<br /><br />FULLER, S. Social Epistemology Indiana Univ. Press, Bloomington (1988).<br /><br />GALILEO (1967), Dialogue Concerning the Two Chief World Systems, translated by Stillman Drake, 2nd edn, University of California Press, Berkeley.<br /><br />GARWIN, R. (1989), &#39;Consensus on cold fusion still elusive&#39;, Nature, 338, 20 April.<br /><br />GEIS, G. (1974), One Eyed Justice, Drake, New York.<br /><br />GENEST, C. and ZIDEK, J. (1986), &#39;Combining probability distributions: a critique and annotated bibliography&#39;, Statistical Science, 1(1): 114-148.<br /><br />GHISELIN, M. T. (1989), Intellectual Compromise: The Bottom Line, Paragon House, New York.<br /><br />GILMORE, J. B. (1991), &#39;On forecasting validity and finessing reliability&#39;, Behavioral and Brain Sciences, 14(1): 148-149.<br /><br />GROFMAN, B. and OWEN, G. (eds), (1986), Information pooling and group decision making, Proceedings of the Second University of California Irvine Conference on Political Economy, JAI Press, London.<br /><br />HAKANSSON, N., BEJA, A. and KALE, J. (1985), &#39;On the feasibility of automated market making by a programmed specialist&#39;, Journal of Finance, March.<br /><br />HALL, S. (1989), &#39;Professor Thorne&#39;s time machine&#39;, California, October, 68-77.<br /><br />HANSON, R. (1988), &#39;Toward Hypertext publishing, issues and choices in database design&#39;, ACM SIGIR Forum, 22(12).<br /><br />HANSON, R. &quot;Could Gambling Save Science? Encouraging an Honest Consensus&quot; Proc. Eighth Intl. Conf on Risk and Gambling, July, London. (1990)<br /><br />HEILBRON, J . (1982), Elements of Early Modern Physics, University of California Press, Berkeley.<br /><br />HENRION, M. and FISCHHOFF, B. (1986), &#39;Assessing uncertainty in physical constants&#39;, American Journal of Physics, 54(9): 791-798.<br /><br />HIRSHLEIFER, J. (1971), &#39;The private and social value of information and the reward to inventive activity&#39;, American Economics Review, 61(4): 561-574.<br /><br />HOFSTEE, W. (1984), &#39;Methodological decision rules as research policies: a betting reconstruction of empirical research&#39;, Acta Psychological, 56: 93-109.<br /><br />HOFSTEE, W. and NEVELS, K. (1981), &#39;Do not take the betting model literally&#39;, Kwantitatieve Methoden, 3: 70-72.<br /><br />JACOBS, J . (1991), &#39;Fremont, film capital&#39;, San Jose Mercury News West Magazine, 1 September, p. 3.<br /><br />JAFFRAY, J . (1989), &#39;Coherent bets under partially resolving uncertainty and belief functions&#39;, Theory and Decision, 26: 99-105.<br /><br />KADANE, J. and WINKLER, R. (1988), &#39;Separating probability elicitation from utilities&#39;, Journal of the American Statistical Association, 83: 402, 357-363.<br /><br />KAHNEMAN, D. and TVERSKY, A. (eds) (1982), Judgment under Uncertainty: Heuristics and Biases, Cambridge University Press, New York.<br /><br />KANTROWITZ, A. (1977), &#39;The Science Court Experiment: criticisms and responses&#39;, Bulletin of the Atomic Scientists, April, 44-50.<br /><br />KELLEY, J. (1988), &#39;Compulsive gambling in Britain&#39;, Journal of Gambling Behavior, 4(4): 291.<br /><br />LAFFONT, J . J . (1989), The Economics of Uncertainty and Information, MIT Press, Cambridge, Mass.<br /><br />LEAMER, E. (1986), &#39;Bid-ask spreads for subjective probabilities&#39;, in P. GOEL and A. ZELLNER (eds), Bayesian Inference and Decision Techniques, Elsevier, North Holland, pp. 217-232.<br /><br />LEDERBERG, J. (1989), &#39;Does scientific progress come from projects or people?&#39; Current Contents, 48, 27 November, 4-12.<br /><br />LEWIS, H. W. (1989), &#39;Fusion isn&#39;t just around the corner&#39;, San Jose Mercury News, 7 April, Opinion page.<br /><br />LINSTONE, L. and TUROFF, M. (eds) (1975), The Delphi Method, Addison-Wesley, London.<br /><br />MALKIEL, B. (1989), &#39;Is the Stock Market efficient?&#39; Science, 243, 10 March, 1313-1318.<br /><br />MANN, C. (1990), &#39;Meta-analysis in the breech&#39;, Science, 249, 3 August, 476-480.<br /><br />MARSHALL, E. (1990), &#39;Science beyond the pale&#39;, Science, 249, 6 July, 14-16.<br /><br />MAYER, M. Markets, Norton &amp; Co., NY. (1988)<br /><br />MUELLER, D, (1979), Public Choice, Cambridge University Press, Cambridge.<br /><br />MYERS, D. (1983), Social Psychology, 2nd edn, McGraw-Hill, New York.<br /><br />NAU, R. and MCCARDLE, K. (1991), &#39;Arbitrage, rationality, and equilibrium&#39;, Theory and Decision, 31, 199-240. New Scientist (1990), &#39;Feedback column&#39;, 14 July, see also 10 February and 28 July.<br /><br />PAVITT, K. (1991), &#39;What makes basic research economically useful?&#39; Research Policy, 20: 109-119.<br /><br />PEARL, J. (1988), Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann, San Mateo.<br /><br />POLINSKY, M. (1983), An Introduction to Law and Economics, Little Brown &amp; Co., Boston.<br /><br />POOL, R. (1989), &#39;Bulls outpace bears-for now!&#39;, Science, 244, 28 April, 421.<br /><br />POPPER, K. (1985), Popper Selections, edited by D. MILLER, Princeton University Press, Princeton.<br /><br />POWER, W. &quot;Hottest Commodity In Wall Street Pits? Georgetown Hoyas&quot;, Wall Street Journal, March 24, p. A1. (1989)<br /><br />PRICE, D. J. DE SOLLA (1963), Little Science, Big Science, and Beyond, Columbia University Press, New York.<br /><br />REDNER, H. (1987), &#39;Pathologies of science&#39;, Social Epistemology, 1(3): 215-247.<br /><br />RESCHER, N. Scientific Progress, Camelot Press, London (1978)<br /><br />ROSE, I. N. (1986), Gambling and the Law, Gambling Times, Hollywood.<br /><br />ROY, R. (1985), &#39;Funding science: the Real defects of peer review and an alternative to it&#39;, Science, Technology and Human Values, 10(3): 73-81.<br /><br />SEIDENFELD, T. (1990), &#39;Two perspectives on consensus for (Bayesian) inference and decisions&#39;, in H. Kyburg, R. Loui and G. Carlson, (eds), Knowledge Representation and Defeasible Reasoning, Kluwer, Boston, pp. 267-286.<br /><br />SHAPIN, S. and SCHAFFER, S. (1985), Leviathan and the Air-Pump, Princeton University Press, Princeton.<br /><br />SHARPE, G. (1990), Telephone conversations, William Hill Organization Ltd, 19 Valentine Place, London SE1 8QH, July.<br /><br />SHARPE, W. (1985), Investments, 3rd edn, Prentice Hall, New Jersey.<br /><br />SIMON, J. (1991), Here&#39;s An Offer You Ought to Refuse, 25 February, open letter available from author, 110 Primrose St., Chevy Chase, MD 20815.<br /><br />SMITH, P. (1990), Killing the Spirit, Viking/Penguin, New York.<br /><br />SMITH, V. (1986), &#39;Experimental methods in the political economy of exchange&#39;, Science, 234 (10 October): 167-173.<br /><br />SOBEY, P. and MADDESS, T. (1991), &#39;You can bet on it&#39;, New Scientist, August.<br /><br />STOLL, H. (1989), &#39;Inferring the components of the bid-ask spread: theory and empirical tests&#39;, Journal of Finance, 44(1): 115-134.<br /><br />SYKES, C. (1988), ProfScam, St. Martin&#39;s Press, New York.<br />Synthese, (1985), &#39;Consensus&#39; issue, 62(1).<br /><br />THAYER, H. (1980), &#39;Peirce on truth&#39;, in P. Caws (ed.), Two Centuries of Philosophy in America, Towman &amp; Littlefield, Totowa.<br /><br />TIERNEY, J. (1991), &#39;A bet on planet Earth&#39;, Reader&#39;s Digest, March, 61-64.<br /><br />TRUMPBOUR, J. (1989), How Harvard Rules, South End Press, Boston.<br /><br />TSAO, J. (1989), &#39;Consumer preferences and funding priorities in scientific research&#39;, Science and Public Policy, 16(5): 294-298.<br /><br />TULLOCK, G. (1966), The Organization of Inquiry, Duke University Press, London.<br /><br />TURNER, S. &quot;Forms of Patronage&quot;, Theories of Science in Society, ed. S. Cozzens, T. Gieryn, Indiana U. Press, pp. 185-21 (1990).<br /><br />VERNE, J. (1872), Around the World in Eighty Days, Watermill Press, New Jersey.<br /><br />WADE, N. (1980), &#39;Why government should not fund science&#39;, Science, 210(3): 33.<br /><br />WHEELER, S., &quot;Betting on Futures in Financial Markets: A Variation on Old Institutions&quot;, Proc. Eighth Intl. Conf. on Risk and Gambling, July, London. (1990) Wall Street Journal, (1989), &#39;Fusion fuss is turning scientists into gamblers&#39;, 18 April.<br /><br />WHITE, F. (1946), Famous Utopias of the Renaissance, Hendricks House, Putney, Vermont.<br /><br />ZECKHAUSER, R. and Viscusi, W. (1990), &#39;Risk within reason&#39;, Science, 248: 559-564.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Science Futures Market","summary":"Although there Exist Multiple Conceptions of What it Might Look Like, they're Generally a Form of Decision Market where People can Wager Bets on Well-Crafted, Specific, Time-Limited Scientific Claims / A Social Network Dedicated to Scientific Controversies Could - in Theory at Least - Identify the True Experts for Each Specialization with Such a System / There's by Now Significant History, Philosophy and Discussion of this Crazy Idea / Could it Work as an Actual Financial Market?","image":"https://lh3.googleusercontent.com/-jbjJfTTYNTM/WDtUrmo-1CI/AAAAAAAAJXg/PVJyzLljkJ8PaKjIKKoFHGPbtG5mdqpjwCJoC/w3600-h5040/the-science-futures-market-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/im3GayFgEGr","publishDate":"2016-11-27T21:49:47.449Z","updateDate":"2017-01-03T01:43:32.619Z","text":"<b>The Science Futures Market: Although there Exist Multiple Conceptions of What it Might Look Like, they&#39;re Generally a Form of Decision Market where People can Wager Bets on Well-Crafted, Specific, Time-Limited Scientific Claims / A Social Network Dedicated to Scientific Controversies Could - in Theory at Least - Identify the True Experts for Each Specialization with Such a System / There&#39;s by Now Significant History, Philosophy and Discussion of this Crazy Idea / Could it Work as an Actual Financial Market?</b><br /><br /><i>The Wisdom of Crowds: Why the Many Are Smarter than the Few and How Collective Wisdom Shapes Business, Economics, Societies and Nations</i><br />by James Surowiecki<br /><br /><i>&quot;In the fall of 2003, meanwhile, MIT&#39;s Technology Review set up a site called Innovation Futures, where people could wager on future technological developments. And Robin Hanson, an economics professor at George Mason University who was one of the first to write about the possibility of using decision markets in myriad contexts, has suggested that such markets could be used to guide scientific research and even as a tool to help governments adopt better policies.</i><br /><br /><i>Some of these markets will undoubtedly end up being of little use, either because they&#39;ll fail to attract enough participants to make intelligent forecasts or because they&#39;ll be trying to predict the unpredictable. But given the right conditions and the right problems, a decision market&#39;s fundamental characteristics -- diversity, independence, and decentralization -- are guaranteed to make for good group decisions. And because such markets represent a relatively simple and quick means of transforming many diverse opinions into a single collective judgment, they have the chance to improve dramatically the way organizations make decisions and think about the future.</i><br /><br /><i>In that sense, the most mystifying thing about decision markets is how little interest corporate America has shown in them. Corporate strategy is all about collecting information from many different sources, evaluating the probabilities of potential outcomes, and making decisions in the face of an uncertain future. These are tasks for which decision markets are tailor-made. Yet companies have remained, for the most part, indifferent to this source of potentially excellent information, and have been surprisingly unwilling to improve their decision making by tapping into the collective wisdom of their employees. We&#39;ll look more closely at people&#39;s discomfort with the idea of the wisdom of crowds, but the problem is simple enough: just because collective intelligence is real doesn&#39;t mean that it will be put to good use.</i><br /><br /><i>A decision market is an elegant and well-designed method for capturing the collective wisdom. But the truth is that the specific method that one uses probably doesn&#39;t matter very much. In this chapter, we&#39;ve looked at a host of different ways of tapping into what a group knows: stock prices, votes, point spreads, parimutuel odds, computer algorithms, and futures contracts. Some of these methods seem to work better than others, but in the end there&#39;s nothing about a futures market that makes it inherently smarter than, say, Google or a parimutuel pool. These are all attempts to tap into the wisdom of the crowd, and that&#39;s the reason they work. The real key, it turns out, is not so much perfecting a particular method, but satisfying the conditions -- diversity, independence, and decentralization -- that a group needs to be smart. As we&#39;ll see in the chapters that follow, that&#39;s the hardest, but also perhaps the most interesting, part of the story.&quot;</i><br /><br />[...]<br /><br /><i>&quot;The most important of these, arguably, was the FutureMAP program, an abortive plan to set up decision markets -- much like those of the IEM -- that would have, in theory, allowed analysts from different agencies and bureaucracies to buy and sell futures contracts based on their expectations of what might happen in the Middle East and elsewhere. FutureMAP, which got its funding from the Defense Advanced Research Projects Agency (DARPA), had two elements. The first was a set of internal markets, which would have been quite small (perhaps limited to twenty or thirty people), and open only to intelligence analysts and perhaps a small number of outside experts. These markets might actually have tried to predict the probability of specific events (like, presumably, terrorist attacks), since the traders in them would have been able to rely on, among other things, classified information and hard intelligence data in reaching their conclusions. The hope was that an internal market would help circumvent the internal politics and bureaucratic wrangling that have indisputably had a negative effect on American intelligence gathering, in no small part by shaping the kinds of conclusions analysts feel comfortable reaching. </i><b><i>In theory, at least, an internal market would have placed a premium not on keeping one&#39;s boss or one&#39;s agency happy (or on satisfying the White House) but rather on offering the most accurate forecast.</i></b><i> And since it would have been open to people from different agencies, it might have offered the kind of collective judgment that the intelligence community has found difficult to make in the past decade.&quot;</i><br /><br />[...]<br /><br /><i>&quot;The idea of the wisdom of crowds also takes decentralization as a given and a good, since it implies that if you set a crowd of self-interested, independent people to work in a decentralized way on the same problem, instead of trying to direct their efforts from the top down, their collective solution is likely to be better than any other solution you could come up with ...</i><br /><br /><i>What do we mean by &#39;decentralization,&#39; anyway? It&#39;s a capacious term, and in the past few years it&#39;s been tossed around more freely than ever. Flocks of birds, free-market economies, cities, peer-to-peer computer networks: these are all considered examples of decentralization. Yet so, too, in other contexts, are the American public-school system and the modern corporation. These systems are dramatically different from each other, but they do have this in common: </i><b><i>in each, power does not fully reside in one central location, and many of the important decisions are made by individuals based on their own local and specific knowledge rather than by an omniscient or farseeing planner.</i></b><br /><br /><i>In terms of decision making and problem solving, there are a couple of things about decentralization that really matter. </i><b><i>It fosters, and in turn is fed by, specialization -- of labor, interest, attention, or what have you. Specialization, as we&#39;ve known since Adam Smith, tends to make people more productive and efficient. And it increases the scope and the diversity of the opinions and information in the system (even if each individual person&#39;s interests become more narrow).</i></b><br /><br /><i>Decentralization is also crucial to what the economist Friedrich Hayek described as tacit knowledge. Tacit knowledge is knowledge that can&#39;t be easily summarized or conveyed to others, because it is specific to a particular place or job or experience, but it is nonetheless tremendously valuable. (In fact, figuring out how to take advantage of individuals&#39; tacit knowledge is a central challenge for any group or organization.) Connected with this is the assumption that is at the heart of decentralization, namely that the closer a person is to a problem, the more likely he or she is to have a good solution to it. This practice dates back to ancient Athens, where decisions about local festivals were left up to the demes, as opposed to the Athenian assembly, and regional magistrates handled most nonserious crimes. It can also be seen in Exodus, where Moses&#39; father-in-law counseled him to judge only in &#39;great matter[s]&#39; and to leave all other decisions to local rulers.</i><br /><br /><b><i>Decentralization&#39;s great strength is that it encourages independence and specialization on the one hand while still allowing people to coordinate their activities and solve difficult problems on the other. Decentralization&#39;s great weakness is that there&#39;s no guarantee that valuable information which is uncovered in one part of the system will find its way through the rest of the system. Sometimes valuable information never gets disseminated, making it less useful than it otherwise would be. What you&#39;d like is a way for individuals to specialize and to acquire local knowledge -- which increases the total amount of information available in the system -- while also being able to aggregate that local knowledge and private information into a collective whole</i></b><i>, much as Google relies on the local knowledge of millions of Web-page operators to make Google searches ever-smarter and ever-quicker. </i><b><i>To accomplish this, any &#39;crowd&#39; -- whether it be a market, a corporation, or an intelligence agency -- needs to find the right balance between the two imperatives: making individual knowledge globally and collectively useful (as we know it can be), while still allowing it to remain resolutely specific and local.&quot;</i></b><br /><br />[...]<br /><br /><b><i>&quot;[A] decentralized system can only produce genuinely intelligent results if there&#39;s a means of aggregating the information of everyone in the system. Without such a means, there&#39;s no reason to think that decentralization will produce a smart result.&quot;</i></b><br /><br />[...]<br /><br /><b><i>&quot;If a group of autonomous individuals tries to solve a problem without any means of putting their judgments together, then the best solution they can hope for is the solution that the smartest person in the group produces, and there&#39;s no guarantee they&#39;ll get that.</i></b><i> If that same group, though, has a means of aggregating all those different opinions, the group&#39;s collective solution may well be smarter than even the smartest person&#39;s solution. </i><b><i>Aggregation -- which could be seen as a curious form of centralization -- is therefore paradoxically important to the success of decentralization.</i></b><i> If this seems dubious, it may be because when we hear centralization we think &#39;central planners,&#39; as in the old Soviet Union, and imagine a small group of men -- or perhaps just a single man -- deciding how many shoes will be made today. But in fact there&#39;s no reason to confuse the two. It&#39;s possible, and desirable, to have collective decisions made by decentralized agents.&quot;</i><br /><br />[...]<br /><br /><i>&quot;It&#39;s hard to make real decentralization work, and hard to keep it going, and easy for decentralization to become disorganization.&quot;</i><br /><br /><a href=\"http://www.theatlantic.com/science/archive/2015/11/gambling-on-the-reliability-on-science-literally/414834/\" class=\"ot-anchor\">http://www.theatlantic.com/science/archive/2015/11/gambling-on-the-reliability-on-science-literally/414834/</a><br /><br />Can a Futures Market Save Science?<br />A creative solution for psychology’s replication problem<br /><br />ED YONG - NOV 9, 2015<br /><br /><b><i>&quot;You read a scientific paper, look at the results, and ask yourself: Are these real? Do they reflect something genuine about the world, or are they statistical flukes?</i></b><i> This ability to critically analyze publications girds all of science. It is the essence of the peer-review process. And, apparently, it&#39;s harder than it looks.</i><br /><br /><i>Consider psychology. The field has been recently embarrassed by failed attempts to repeat the results of classic textbook experiments, and a mounting realization that many papers are the result of commonly accepted statistical shenanigans rather than careful attempts to test hypotheses. Tellingly, as I covered in August, a coordinated attempt to repeat 100 published experiments, led by Brian Nosek from the University of Virginia, managed to reproduce the results of just a third of them.</i><br /><br /><i>Which raises the question: Exactly how good are psychologists at working out whether their own studies are reliable?</i><br /><br /><b><i>Pretty good, actually, according to Anna Dreber from the Stockholm School of Economics, provided you pool their wisdom—and get them to gamble. Dreber created a stock market for scientific publications, where psychologists could buy or sell &#39;stocks&#39; in 44 published studies based on how reproducible they deemed the findings. And these markets predicted the outcomes of actual replication attempts pretty well, and certainly far better than any of the traders did on their own.</i></b><br /><br /><i>Dreber&#39;s experiment was born in a bar. Over drinks with her husband Johan Almenberg and roommate Thomas Pfeiffer, she was talking about an attention-grabbing psychological study that she thought was &#39;cute, but unlikely to be true.&#39; When she wondered how good her instincts were, Pfeiffer brought up another paper by economist Robin Hanson at George Mason University. Titled Could Gambling Save Science?, </i><b><i>it suggested that researchers could get a more honest consensus on scientific controversies by betting on their outcomes, in the way that traders bet on the future prices of goods.</i></b><br /><br /><i>&#39;It blew us all away,&#39; says Dreber. In 2012, she and her colleagues contacted Nosek, who agreed to add prediction markets to his big Reproducibility Project.</i><br /><br /><b><i>Here&#39;s how it worked. Each of the 92 participants received $100 for buying or selling stocks on 41 studies that were in the process of being replicated. At the start of the trading window, each stock cost $0.50. If the study replicated successfully, they would get $1. If it didn&#39;t, they&#39;d get nothing. As time went by, the market prices for the studies rose and fell depending on how much the traders bought or sold.</i></b><br /><br /><b><i>The participants tried to maximize their profits by betting on studies they thought would pan out, and they could see the collective decisions of their peers in real time. The final price of the stocks, at the end of two-week experiment, reflected the probability that each study would be successfully replicated, as determined by the collective actions of the traders. If it was $0.83, that meant the market predicted an 83 percent chance of replication success. If that final price was over $0.50, Dreber&#39;s team considered it to be a prediction of success; if it was under, it was a prediction of failure.</i></b><br /><br /><b><i>In the end, the markets correctly predicted the outcomes of 71 percent of the replications</i></b><i> -- a statistically significant, if not mind-blowing score. Then again, based on the final prices, the team only expected the markets to be right just 69 percent of the time -- which they roughly were. (Remember that those prices are probabilities of success, so they naturally contain uncertainties about their own predictions.)</i><br /><br /><b><i>&#39;There is some wisdom of crowds; people have some intuition about which results are true and which are not,&#39; says Dreber. &#39;Which makes me wonder: What&#39;s going on with peer review? If people know which results are really not likely to be real, why are they allowing them to be published?&#39;</i></b><br /><br /><i>Well, says Nosek, </i><b><i>market participants only care about whether the study will replicate, while reviewers are also looking at experimental design, importance, interest, and other factors. Also, reviewers, by their nature, work alone, and Dreber&#39;s traders performed poorly when working solo. When Dreber actually asked them to predict the replication odds for each study, they were right just 58 percent of the time—no better than chance. Collectively, they became more effective because they could see what their peers were thinking.</i></b><br /><br /><i>&#39;This shows that there is information known in advance of conducting a replication that anticipates replication success,&#39; says Nosek. What kind of information? &#39;I&#39;m not sure I had a clear strategy,&#39; says Marcus Munafo from Bristol University, who was one of the better-performing traders and who has also used prediction markets to evaluate science. He paid attention to statistical power, the journals that the original studies were published in, and which branch of psychology they were part of. &#39;Beyond that, I simply used my gut instinct for whether the original finding felt plausible.&#39;</i><br /><br /><i>That&#39;s the most interesting bit, says Daniele Fanelli from Stanford University, who studies research bias and misconduct. &#39;It opens some fascinating research questions about understanding which factors are consciously or unconsciously most informative to participants,&#39; he says.</i><br /><br /><i>Nosek adds, </i><b><i>&#39;We may be able to use prediction markets to be more efficient in deciding what needs to be replicated, and making estimates of uncertainty about studies for which replication is not likely or possible.&#39;</i></b><br /><br /><i>But Fanelli isn&#39;t convinced, saying that it &#39;seems like a rather laborious process that&#39;s unlikely to be applied across the board.&#39; Hanson has heard similar skepticism before. &#39;We’ve had enough experiments with prediction markets over the years that these findings are not at all surprising,&#39; he says, </i><b><i>but &#39;I expect that most ordinary academic psychologists will require stronger incentives than personal curiosity to participate.&#39;</i></b><br /><br /><b><i>Their success in these markets would have to be tied to tangible benefits, like actual money or the likelihood of securing publications, grants, and jobs.</i></b><i> &#39;Imagine that one or more top journals used prediction-market chances that a paper’s main result would be confirmed as part of deciding whether to publish that paper,&#39; he says. &#39;The authors and their rivals would have incentives to trade in such markets, and others would be enticed to trade when they expect that trades by insiders, or their rivals alone, are likely to produce biased estimates.&#39;</i><br /><br /><i>The prediction markets have uses beyond analyzing the reliability of individual studies. They also provide an interesting look at the scientific process itself. Using the final market prices and a few statistical traits, Dreber&#39;s team could backtrack through each study&#39;s history and show how its hypotheses became strengthened or weakened with every step along the way.</i><br /><br /><i>For example, before any of these experiments were actually done, what were the odds that they were testing hypotheses that would turn out to be true? Just 8.8 percent, it turned out. This reflects the fact that psychologists often look for phenomena that would be new and surprising.</i><br /><br /><i>More worryingly, after the experiments were completed, reviewed, and published, the odds that their hypotheses were true improved to just 56 percent. &#39;So, if you read through these journals and ask, ‘Is this true or not?,’ you could flip a coin!&#39; says Dreber. &#39;That&#39;s pretty bad I think. People often say that if you have a p-value that&#39;s less than 0.05, there is a 95 percent probability that the hypothesis is true. That&#39;s not right. You need a high-powered replication.&#39;</i><br /><br /><i>Indeed, the team calculated that if other researchers successfully replicated the studies&#39; results, then their hypotheses stand a 98 percent chance of being true. If the attempts failed, the odds dropped back down to 6 percent. &#39;The failed replications provided about equivalent amount of doubt as the initial demonstration provided belief,&#39; says Nosek. &#39;It&#39;s as if we are back to square one -- an interesting set of mostly implausible ideas awaiting evidence to draw a strong conclusion.&#39;</i><br /><br /><i>Dreber is now repeating her experiments for other fields like experimental economics. </i><b><i>&#39;I don&#39;t want to single out psychology,&#39; she says. &#39;Maybe things are worse in other fields and at least the psychologists seem willing to take this seriously.&#39;&quot;</i></b><br /><br />The following paper is an excellent introduction to this subject, but it&#39;s also quite long.  So, I leave it up to the reader to decide how much they want to know about the idea ...<br /><br /><i>Social Epistemology</i>, 1995, Vol. 9, No. 1, 3 - 33<br /><br />Symposium on the possibility of science funding as an &#39;idea futures&#39;<br />market<br /><br />Could gambling save science?<br />Encouraging an honest consensus<br /><br />Robin Hanson<br /><br /><i>&quot;Abstract</i><br /><br /><b><i>The pace of scientific progress may be hindered by the tendency of our academic institutions to reward being popular rather than being right. A market-based alternative, where scientists can more formally &#39;stake their reputation&#39;, is presented here. It offers clear incentives to be careful and honest while contributing to a visible, self-consistent consensus on controversial (or routine) scientific questions. In addition, it allows patrons to choose questions to be researched without choosing people or methods.</i></b><i> The bulk of this paper is spent in examining potential problems with the proposed approach. After this examination, the idea still seems to be plausible and worthy of further study.</i><br /><br /><i>1. Introduction</i><br /><br /><i>After reviewing the discrepancy between what is wanted from academic institutions and what is obtained from current institutions, a market-based alternative called &#39;idea futures&#39; is suggested. It is described through both a set of specific scenarios and a set of detailed procedures. Over thirty possible problems and objections are examined in detail. Finally, a development strategy is outlined and the possible advantages are summarized.</i><br /><br /><i>2. The problem</i><br /><br /><i>2.1 The Scientific Revolution</i><br /><br /><b><i>Four centuries ago, some Europeans complained that the existing academic institutions were biased against them. Insiders, it was said, were &#39;inflated by letters&#39; and shunned anyone who dared to &#39;speculate on anything out of the common way&#39; (Debus, 1970). Outsiders -- astrologers, chemists, and people such as Bacon and Galileo -- argued that they and their theories should be judged by how well they agreed with observations, and not by how well they agreed with the authorities of the day (Galileo, 1967).</i></b><i> This was the age of Utopias (White, 1946), as these rebels debated possible academic reforms and imagined whole new social institutions, for both academia in particular and society in general.</i><br /><br /><i>Within a century or so, the intellectual descendants of these outsiders became the new insiders in a process now called the &#39;Scientific Revolution&#39;. They introduced a new respect for observations along with new social institutions, such as the Royal Society of London, inspired by those Utopian ideals. Since then science has made impressive progress. Most controversial issues of four centuries ago seem long settled by now, and continued research may well settle most of today&#39;s controversies. Academia can claim some credit for this, and academic institutions have continued to evolve in response to perceived problems, formalizing publication in journals, credit in citations, and evaluation in anonymous peer review.</i><br /><br /><i>2.2 Problems with academia</i><br /><br /><b><i>However, little has really changed; academia is still largly a medieval guild, with a few powerful élites, many slave-like apprentices, and members who hold a monopoly on the research patronage of princes and the teaching of their sons. Outsiders still complain about bias, saying that their evidence is ignored, and many observers (Ghiselin, 1989; Redner, 1987; Sykes, 1988; Trumpbour, 1989; Tullock, 1966) have noted some long-standing problems with the research component of academia. [1]</i></b><br /><br /><b><i>As currently practiced [2] peer review is just another popularity contest, inducing familiar political games; knowledgeable players criticize outsiders, praise insiders, follow the fashions that insiders indicate, and avoid subjects between or outside thé familiar subjects. It can take surprisingly long for outright lying by insiders to be exposed (Redner, 1987). There are too few incentives to correct for cognitive (Kahneman and Tversky, 1982) and social (Myers, 1983) biases, such as wishful thinking, overconfidence, anchoring (Henrion and Fischhoff, 1986), and preferring people with a background similar to one&#39;s own.</i></b><br /><br /><i>Publication quantity is often the major measure of success, encouraging redundant publication of &#39;smallest publishable units&#39; by many co-authors. The need for one&#39;s research to appear to be original gives too little incentive to see if it has already been done elsewhere, as is often the case, and neglects efforts to integrate previous research. </i><b><i>A preoccupation with &#39;genius&#39; and ideological wars over &#39;true&#39; scientific method (Ghiselin, 1989) needlessly detract from just trying to be useful.</i></b><br /><br /><b><i>Perhaps the core problem is that academics are rewarded mainly for telling a good story, rather than for being right.</i></b><i> (By &#39;right&#39; I include not only being literally correct, but also being on the right track, or enabling work on the right track.) </i><b><i>Publications, grants, and tenure are based on what other insiders think today, independent of whether one&#39;s ideas and results are proved to be correct or valuable later. Even for researchers with a good track record, grant proposals must usually describe in some detail exactly what will be discovered and how; true exploratory work is done on the sly. This emphasis on story-telling rewards the eloquent, who know how to persuade by ignoring evidence that goes against their view, and by other standard tricks (Cialdini, 1988).</i></b><br /><br /><i>Admittedly, someone who has published an unusual idea that has proven to be right is thought of more highly, all other things being equal. </i><b><i>However, all other things are usually not equal. Outsiders find it hard to get an unusual idea published</i></b><i>, and being able to say &#39;I told you so&#39; is of little help to academics who have failed to gain tenure. The powerful often get credit for the successes of those under them (Redner, 1987). Only in the most experimental fields, where feedback is direct and frequent, can we expect people who are disliked -- but usually right -- to be rewarded through informal reputations. Perhaps the biggest problem is the distortion evident when a science question becomes relevant for public policy, as in the recent debates over &#39;Star Wars&#39; or the greenhouse effect. The popular media tend to focus on those scientists prone to hyperbole. An honest consensus of relevant experts is often lost from public view, as advocates on each side accuse the other of bias and self-interest. Public policy can suffer dramatically as a result, a consequence that becomes more serious as the pace of technological change quickens. </i><b><i>On the whole, current academic institutions seem to be less than ideal, with incentives that reward being popular, fashionable, and eloquent, instead of being right.</i></b><br /><br /><i>2.3 Incentives matter</i><br /><br /><i>Are these complaints just sour grapes? Those who do well by an existing system tend to believe that problems are minor; but even if the best ideas eventually win, we should worry if the people who advocate those ideas don&#39;t win. </i><b><i>Good intentions and culture can only go so far in countering bad incentives; if you must publish or perish, you will do what it takes to publish (or perish).</i></b><br /><br /><b><i>The social organization of any human effort can have a tremendous effect on its efficiency. Consider that different past societies with different ways of organizing science have had very different rates of scientific progress; compare Europe with China over the last five centuries. Our rate of progress may be less than 2% of what it could be (Bemal, 1989).</i></b><br /><br /><i>Are we wasting precious resources? Imagine what would happen if we used academic peer review to decide what products to manufacture. Proposals for new products would be reviewed anonymously by powerful people who produce similar products. These reviewers would pass judgement without taking any personal risk, and those judged favorably would win regardless of how useful their product turned out to be.</i><br /><br /><i>I much prefer our current business system, with all of its problems, </i><b><i>where investors must take a personal risk when they endorse a product. Institutions like the stock market are comparatively egalitarian and flexible, allowing almost anyone to participate in the ongoing debate about the profit potential of any public business or the relative potential of various industries, management styles, etc. Why can&#39;t we have academic research institutions more like this?</i></b><br /><br /><i>2.4 Academic reforms</i><br /><br /><i>Most efforts to improve academic institutions focus on incremental reform within the existing peer review framework. Should reviewers be anonymous? Should submissions be anonymous? How many people should review each proposal?</i><br /><br /><i>Occasionally someone proposes a more radical reform within the current framework. The surprising lack of agreement among reviewers (Cicchetti, 1991) has lead some (Gilmore, 1991) to suggest that we fund equally or randomly among &#39;qualifled&#39; applicants, and let everything be published. Conversely, the fact that a small fraction of scientists receive most citations (Cole and Cole, 1987) has lead some (Byrne, 1989) to suggest that we simply give $1 million a year, no strings attached, to the top thousand scientists, chosen by an iterated popularity poll. Some have suggested that universities and private laboratories be funded in proportion to their publication count (Roy, 1985) or citation count (Tsao, 1989). Tullock (1966) advocates prizes, once a central method for funding research (Heilbron, 1982). Still others suggest scrapping the whole thing, abolishing tenure (Smith, 1990) or government funding (Feyerabend, 1975; Wade, 1980) in favor of some existing alternative such as private patrons, popular media, patents, or research tax credits.</i><br /><br /><i>Once in a while a whole new social institution is proposed. Science courts (Kantrowitz, 1977) (also called &#39;scientific adversary procedures&#39;) were invented to blunt hyperbole on science controversies by using court-like proceedings to encourage cross-examination and to document areas of agreement. Hypertext publishing (Drexler, 1986; Hanson, 1988) imagines an electronic publishing medium where any critic could directly link a criticism to any published item, and where readers could decide what is worth reading by having software automatically combine the direct evaluations of previous readers whom they respect.</i><br /><br /><i>In this paper I propose a new academic institution, tentatively called &#39;idea futures&#39;, intended to overcome some of the limitations of existing alternatives. It is Utopian in the sense of describing a coherent vision of how things might be rather different, but hopefully practical in the sense of considering what could go wrong and how to start small.</i><br /><br /><i>2.5 What we want</i><br /><br /><i>Before considering specific mechanisms, let us reflect for a moment on what we want from academic incentives. We want to encourage honesty and fair play; the game should be open to anyone to prove him/herself. Patrons who fund research, either private foundations or governments, presumably want research to be directed toward the academic subjects and questions of interest to those funders. (Patrons also include the researchers themselves, to the extent that reduced salaries are understood to be in exchange for some research autonomy.) On controversial questions, we want a clear measure of the current opinion of relevant experts, a measure that political advocates could not easily distort. Those who contribute to such a measure should have clear incentives to be careful and honest.</i><br /><br /><i>Presumably we want as much progress as possible per effort invested, at least in situations where the following notion of &#39;progress&#39; makes sense. Consider a wellposed question, such as &#39;Is the Earth basically spherical?&#39;, with a handful of possible answers (such as &#39;No, it&#39;s flat&#39;). Experience indicates that, with enough study and evidence, one of the answers will eventually stand out as best to almost anyone who considers the question carefully. At least this seems to happen for most questions that have been traditionally labeled &#39;scientific&#39;; questions about the morality of abortion or the nature of God may not fare so well. Where there is such a limiting &#39;right&#39; answer, &#39;progress&#39; can mean the rate at which general scientific opinion converges to that answer. [3]</i><br /><br /><i>Translating these goals to an individual level, </i><b><i>we want our institutions to reward academics for pushing scientific opinion toward the &#39;right&#39; answer, presumably by somehow increasing their reputation, influence, or resources.</i></b><i> Let us imagine an academic who, after some reflection or observation, comes to a tentative conclusion that he/she would like others to consider. If almost everyone already agrees with this conclusion, even without seeing the new supporting evidence or analysis, the academic should receive little credit for just making an &#39;obvious&#39; claim.</i><br /><br /><i>However, credit should be possible if the claim is surprising, i.e. if people who have not yet seen the evidence are not yet willing to agree. If, upon reviewing the evidence, almost everyone now agrees with the surprising claim, then the academic should certainly receive some credit. In fact, peer review can handle this case. But what if there is not uniform agreement? It still seems that the academic should be rewarded, if this surprising claim is eventually born out. Others who supported this claim in the face of disagreement should also gain credit (Lederberg, 1989), since they helped to push the general opinion in the right direction.</i><br /><br /><i>Why shouldn&#39;t knowledgeable academics now win credit by supporting as many claims as possible, or by multiplying controversies? </i><b><i>Clearly they should risk losing credit when they are wrong, so that credit is in some ways conserved. The ratio of possible loss to gain should depend on how unusual one&#39;s position is. Siding with the majority and being right should gain one less than siding with a minority and being right. The total amount gained or lost should depend on how much of their reputation each academic has chosen to stake on this issue, as well as on how interesting the issue is to the ultimate research funders.</i></b><br /><br /><i>In summary, part of what is wanted from academic incentives is a fair game for staking a reputation, so that on questions of interest to funders, the general opinion converges as fast as possible to the &#39;right&#39; answer.&quot;</i><br /><br /><i>3. The proposal</i><br /><br /><i>Surprising as it may seem, such a social institution exists. It is relatively simple, cheap, decentralized, and egalitarian. </i><b><i>It could create a consensus on disputed science questions that would be clear, expert, honest, and self-consistent across a wide range of issues. This consensus should respond quickly to new information, and predict at least as well as any other co-existing consensus mechanism. It is well-grounded in our best theories of decision and incentives.</i></b><br /><br /><i>It is also ancient. We need only to revive and embellish a suggestion made during the Utopian scientific revolution. Chemical physicians, excluded by the standard physicians from teaching in the British schools, repeatedly offered challenges such as the following (c. 1651):</i><br /><br /><i>&#39;Oh ye Schooles ... Let us take out of the hospitals, out of the Camps, or from elsewhere, 200, or 500 poor People, that have Fevers, Pleurisies, etc. Let us divide them into halfes, let us cast lots, that one hälfe of them may fall to my share, and the other to yours; ... we shall see how many Funerals both of us shall have: But let the reward of the contention or wager, be 300 Florens, deposited on both sides: Here your business is decided. (Debus, 1970)&#39;</i><br /><br /><b><i>They proposed to bet on their medical therapies, apparently believing bets to be a useful augmentation of the existing academic incentives!</i></b><i> Bets are a long-established and robust reputation mechanism, </i><b><i>widely seen as a cure for excessive verbal wrangling; you &#39;put your money where your mouth is&#39;</i></b><i>. In science and elsewhere, phrases like &#39;you bet&#39; are standard ways to express confidence. Offers to make token bets are particularly compelling, and </i><b><i>scientists of equal stature often make and publicize such bets, with recent bets on resource depletion, computer chess, black holes (Hall, 1989), solar neutrinos, nuclear weapon yields (Evans 1991), and cold fusion (Garwin 1989, Lewis 1989, Wall Street Journal, 1989).</i></b><br /><br /><i>Nor is gambling foreign to science funding. King Charles II, founding patron of the Royal Society of London, was fond of laying wagers on the outcome of the Society&#39;s experiments (Shapin and Schaffer, 1985). </i><b><i>Until 1830, public lotteries funded Colombia, Harvard, and Yale (Geis, 1974). In 1872 Leland Stanford, founder of Stanford University, hired Eadweard Muybridge to help him to win his bet that a trotting horse has all four legs off the ground at some point; in the process Muybridge invented moving pictures (Jacobs, 1991).</i></b><br /><br /><b><i>Consider the example of Piers Corbyn, a London astrophysicist who has been unable to get academic meteorologists interested in his unusual theory of long-term weather cycles (New Scientist, 1990). Since June 1988 he has been making bets to gain publicity, betting against the bookmakers William Hill, who uses odds posted by the British Meteorological Service. He has been winning. Over the 26 months from April 1989 to May 1991, Corbyn has made at least 9 bets a month (and averaged over 20 bets a month) and has won 80% of these bets, gaining an average rate of return of over 25% per bet. (Depending on what independence you assume between bets in a given month, the chance of this happening randomly is between 1 in 400 and 1 in 1050.) Yet the Service still refuses to take Corbyn seriously, or make even token bets against him. This doesn&#39;t seem quite fair; hasn&#39;t Corbyn earned the right to be considered? William Hill has taken on the bets for the publicity but is tired of losing, and has adjusted their odds accordingly. Why shouldn&#39;t these be the odds used for official British agricultural policy, instead of the Service&#39;s predictions?</i></b><br /><br /><b><i>Consider Julian Simon, a population and natural resource optimist, who found that he could not compete for either popular or academic attention with best-selling doomsayers like Paul Ehrlich. In 1980 Simon challenged Ehrlich to bet on whether the price of five basic metals, corrected for inflation, would rise or fall over the next decade. Ehrlich accepted, and Simon won, as would almost anyone who bet in the same way in the last two centuries. This win brought Simon publicity (Tierney, 1991), but mostly in the form of high-profile editorials saying &#39;Yeah he won this one, but I challenge him to bet on a more meaningful indicator such as ... &#39;. In fact, however, not only won&#39;t Ehrlich bet again, although his predictions remain unchanged, but also none of these editorial writers will actually put their money where their mouths are! In addition, the papers that published these editorials won&#39;t publish letters from Simon accepting their challenges (Simon, 1991). Shouldn&#39;t Simon&#39;s open challenges count as much as best-sellers in setting environmental policy?</i></b><br /><br /><b><i>If the primary way that academics are now rewarded for being right, rather than popular, is an informal process for staking their reputation, which has various biases because of its informality, and if we want a better reputation game, why not literally make bets and formalize the process?</i></b><br /><br /><i>Imagine a betting pool or market on most disputed science questions, with the going odds available to the popular media, and treated socially as the current academic consensus. Imagine that academics are expected to &#39;put up or shut up&#39; and accompany claims with at least token bets, and that statistics are collected on how well people do. </i><b><i>Imagine that funding agencies subsidize pools on questions of interest to them, and that research laboratories pay for much of their research with winnings from previous pools. Imagine that anyone could play, either to take a stand on an important issue, or to insure against technological risk.</i></b><br /><br /><i>This would be an &#39;idea futures&#39; market, which I offer as an alternative to existing academic social institutions. Somewhat like a corn futures market, where one can bet on the future price of corn, </i><b><i>here one bets on the future settlement of a present scientific controversy.</i></b><i> This is admittedly an unusual, although not entirely original, suggestion (Brunner, 1975; Hofstee and Nevéis, 1981; Hofstee, 1984; Learner, 1986; Sobey and Maddess, 1991). But consider what might happen.</i><br /><br /><i>4. Scenarios</i><br /><br /><i>4.1 Continental drift</i><br /><br /><i>In 1915 German meteorologist Alfred Wegener published his theory of continental drift, for which he had collected extensive evidence. However, contemporaries considered his theory to be &#39;impossible&#39;, and Wegener died as an intellectual outcast in 1930 (Marshall, 1990). Yet in the 1960s his theory began to be taken seriously, and is now the established view. Wegener eventually gained fame, but overall academia seems to discourage activity like his. </i><b><i>Some of Wegener&#39;s peers, for example, probably found his thesis plausible, but decided that to say so publicly would be a poor career move.</i></b><br /><br /><b><i>With idea futures, Wegener could have opened a market for people to bet on his theory, perhaps to be judged by some official body of geologists in a century. He could have then offered to bet a token amount at, say, 1 - 4 odds, in effect saying that there was at least a 20% chance that his claim would be vindicated. His opponents would have had to accept this estimate, and its implications about the importance of Wegener&#39;s research, or they would have to bet enough to drive the market odds down to something a little closer to &#39;impossible&#39;. They could not suppress Wegener merely by silence or ridicule.</i></b><br /><br /><b><i>As Wegener increased his stake, buying more bets to move the price back up, his opponents would hopefully think just a little more carefully before betting even more to move the price back down. Others might find it in their interest to support Wegener; anyone who thought that the consensus odds were wrong would expect to make money by betting, and would thereby move the consensus toward what they believe. Everyone would have a clear incentive to be careful and honest.</i></b><br /><br /><b><i>The market would encourage more research related to continental drift, as one could make money by being the first to trade on new relevant information.</i></b><i> Eventually the evidence would more clearly tip in Wegener&#39;s favor, and the price of his bets would rise. Wegener, or his children, could then sell those bets and reap some rewards. While those rewards would not make up for years of neglect, at least he would get something.</i><br /><br /><i>As the controversy became settled, and opinions converged, people would gradually sell and leave the market. Few people, if any, need be left for the final judging, which could usually be avoided (using mechanisms to be described below).</i><br /><br /><i>4.2 Cold fusion</i><br /><br /><i>A more recent controversy began in March 1989, when Pons and Fleishman announced &#39;fusion in a jar&#39; at a dramatic press conference. In the months that followed, media aftershocks of confirmation attempts were tracked by thousands of scientists and others, who argued with each other about the chances of cold fusion being real. </i><b><i>Proposals to bet came up often, even in the public debates.</i></b><i> Critics, uncomfortable with airing scientific disputes in public, complained that Pons and Fleishman broke the rules by going to the popular media instead of through normal peer review channels, unfairly gaining extra attention and funding. Supporters countered that the popular media spread information quickly to other scientists; cold fusion, if right, was too important to wait for normal channels.</i><br /><br /><i>In the journal Science, Robert Pool speculated on the course that a market in cold fusion might have taken (Pool, 1989). </i><b><i>If there had been a betting market, then there would have been a market price that journalists like Pool could publish as news. A table of going prices might appear on the science page in the newspaper, much like the stock page in the business section, conveying current scientific opinion better than the current &#39;balanced&#39; interviews with extremists on all sides. It has been suggested (Zeckhauser and Viscusi, 1990) that the added information in betting market prices might have helped to resolve the debate more quickly.</i></b><br /><br /><i>There need not be a conflict between going through slow proper channels and getting the word out, if a fast market were a proper channel. The effect of staged media events might be reduced as it might not be news if the price didn&#39;t change; </i><b><i>advocates would have to convince not the average listener but those people willing to make bets.</i></b><i> Remaining biases, such as overconfidence (Pool, 1989: 421), would be reduced by technical traders and other trading specialists.</i><br /><br /><i>Cold fusion businesses would have been less risky to start. As it was, a new fusion business had to bet both that cold fusion was real, and that they were the best group to develop and market it in that case. With idea futures they could, by both starting a business and betting against cold fusion (essentially taking out insurance), really only be betting on their ability to develop cold fusion if it were real.</i><br /><br /><b><i>Insights from a great many people whose opinions on the cold fusion controversy were ignored, such as inarticulate people without doctorates, could have been integrated in a decentralized manner.</i></b><i> Popular play would end up by subsidizing professional efforts of questions of popular interest, offering more &#39;direct democracy&#39; in setting research priorities.</i><br /><br /><i>4.3 Neutrino mass</i><br /><br /><i>Betting markets could also function in the absence of overt controversy, as in the following (hypothetical) story.</i><br /><br /><b><i>Once upon a time the Great Science Foundation decided that it would be a &#39;good thing&#39; to know the mass of the electron neutrino. Instead of trying to figure out who would be a good person to work on this, or what a good research strategy would be, they decided simply to subsidize betting markets on the neutrino mass. They spent millions.</i></b><br /><br /><i>Soon the market odds were about 5% that the mass was above 0.1 eV, and Gung Ho Labs became intrigued by the profits to be made. They estimated that for about $300,000 spent on two researchers over 3 years, they could make a high confidence measurement of whether the mass was above 0.1 eV. So they went ahead with the project, and later got their result, which they kept very secret. While the market now estimated the chance of a mass over 0.1 eV at 4%, their experiment said that the chance was at most 0.1%.</i><br /><br /><b><i>They quietly bought bets against a high mass, moving the price down to 2.5% in the process. They then revealed their results to the world, and tried their best to convince people that their experiment was solid. After a few months they mostly succeeded, and when the price had dropped to 0.7% they began to sell the bets that they had made. They made $500,000 from the information that they had created, which more than covered their expenses to get that information.</i></b><br /><br /><i>If Gung Ho Labs had failed to convince the world of their results, they would have faced the difficult choice of quitting at a loss, or holding out for the long-term. A careful internal review would probably be conducted before making such a decision.</i><br /><br /><i>Internally, Gung Ho would be free to use whatever organizational structures it found to be effective; even peer review, tenure, and fixed salaries. The two researchers need not risk their life savings to be paid for their efforts. However, the discipline of the external market should keep these internal institutions from degenerating into mere popularity contests.</i><br /><br /><i>4.4 Killer peanut butter</i><br /><br /><i>In another hypothetical story Munchem Biolabs found compelling evidence that peanut butter was more deadly than most pesticides, a conclusion that Lunch Industries Exclusive (LIE) wanted desperately to suppress. LIEs usual procedure was to fund a few competing studies to come to opposite conclusions, which usually kept the waters muddy enough that legislators and customers would ignore it all. This time they had to deal with an idea futures market on the question, and the public was beginning to take the odds in such markets seriously.</i><br /><br /><b><i>Munchem had moved the market odds of deadly peanut butter up rather high. LIE now had two choices; either they could use overwhelming cash to move the odds back down, or use competing studies, advertising, etc. to persuade others to bet on their side.</i></b><br /><br /><i>If they bet alone, they would know that they were throwing their money away with no obvious limit on future spending. Not only might Munchem find allies, but LIE employees who knew that they were bluffing might be tempted to pick up a little free money with some anonymous bets. If word of LIEs bluff got out, as insider information often does, investors would flock in and wipe out the effect of LIE bets. If LIE tried to throw away other people&#39;s money through a persuasion campaign, they would face a market dominated, as most liquid markets are, by battle-hardened speculators. These investors, not easily persuaded by clever jingles, would quickly hook up with research insiders, who generally know which laboratories tend to find whatever results their customers want.</i><br /><br /><i>In the end, Lunch Industries accepted the market odds, and began research on non-toxic peanut butter.</i><br /><br /><i>5.1 Assets</i><br /><br /><i>Imagine that John bets Mary $5, at even odds, that it will rain next Monday. Since they don&#39;t entirely trust each other, John and Mary put the bet in writing and each give $5 to Frank, a trusted third party. John has essentially paid $5 for an IOU that says &#39;Worth $10 if rain Monday&#39;, since if he wins he gets $5 from Mary and his own $5 back. Mary&#39;s IOU says &#39;Worth $10 if not rain Monday&#39;. On Tuesday one of them can cash in their IOU for $10 from Frank.</i><br /><br /><i>This standard betting scenario can be improved by breaking it into different transactions; first create the IOUs and then sell them. Replace Frank with a stable financial institution, let&#39;s call it a &#39;bank&#39;, which will sell a pair of &#39;$10 if rain&#39;, &#39;$10 if not rain&#39; coupons to anyone for a price of $10. The bank takes no risk, since exactly one of the coupons will be worth $10 in the end. </i><b><i>Since the bank holds the $10 in the meantime, it can afford to offer interest on the $10, and perhaps pay a local meteorologist to be an impartial judge.</i></b><i> Now Mary can first buy a coupon pair from the bank for $10 and then offer to sell her &#39;$10 if rain&#39; coupon to John or anyone for $5, retaining the &#39;$10 if not rain&#39; for herself.</i><br /><br /><i>A central clearinghouse for such offers, which matched compatible offers and ensured that traders made good on their offers, would always hold a best current offer to sell and to buy. If the transaction costs of processing an offer through the clearinghouse were small, as current technology allows, then the &#39;spread&#39; between these offers could be quite small, leaving a going &#39;market price&#39;. A going price of $3.20 for &#39;$10 if rain Monday&#39; would represent a temporary consensus of a 32% chance of rain on Monday.</i><br /><br /><b><i>In general, these markets trade assets of the form &#39;X if A&#39; (often called &#39;contingent assets&#39;), where X is some pre-existing &#39;base&#39; asset and A is one of a set of mutually exclusive claims that some judging organization agrees, eventually, to choose from. The base X can be any stock, bond, currency, commodity, or even another compatible contingent asset. The set of claims constitutes a &#39;question&#39;, and each claim is one possible answer to the question. To enable trading on a question, an agreement is required between several parties -- an author, a judge, and one or more banks, clearinghouses, and randomness checkers.</i></b><br /><br /><i>An author carefully words a set of claims, and a judging organization agrees if necessary, to offer a verdict in favor of one of these claims at some, perhaps indirectly specified, date. </i><b><i>Registries hold records of public, i.e. not anonymous, trades made at clearinghouses. (Clearinghouses may be required to hold additional private records of all trades, available to be subpoenaed by criminal investigators.)</i></b><br /><br /><i>Consider a question with possible answers {A, B, ...}. Any bank authorized in the agreement on that question can &#39;split&#39; any allowed base X (usually anything) into the assets {&#39;X if A&#39;, &#39;X if B&#39;, ...} , or &#39;join&#39; those assets back into X. In the example above, $10 was split into &#39;$10 if rain&#39; and &#39;$10 if not rain [1]. The bank is trusted to report the net effect of these transactions to a central agent, who keeps track of the net &#39;market capital&#39; that has been split along this question.</i><br /><br /><b><i>On the specific date, and a short wait after a public announcement, the judges are given an agreed-upon judging-fee in order to study the question and render their verdict. Verdicts assign a percentage of validity to each of the possible question answers. If the verdict is 98% in favor of A, then banks are authorized to let people exchange their &#39;X if A&#39; assets for 98% of X.</i></b><br /><br /><b><i>The judging-fee is obtained from the banks, who devalue the current assets contingent on that question by some precentage, a percentage that can be no more than a pre-specified max-judging-percentage. This devaluation creates an incentive for traders to &#39;settle out of court&#39; and sell before the judging date.</i></b><br /><br /><i>What if there is too little capital in the market to support the required judging fee? John and Mary&#39;s market only has $10 in it, and with a 10% max-judging-fee, only $1 is available for judging, short of the $5 a meteorologist judge might require. In this case an &#39;audit lottery&#39; [4] can be held (Polinsky, 1983). The current market capital, $10, is gambled with whomever offers the best price, among those approved by the randomness checker. If the gamble is won, every asset contingent on this question increases in value, resulting in enough market capital for judging to proceed, in this case $50. If the gamble is lost, all such assets become worthless and judging is not needed. [5]</i><br /><br /><i>Judges can be given more flexibility to deal better with uncertainties regarding when a question will be judgeable and how much that will cost. For example, the max-judging-percentage could be spent in discrete units, each with a specific percentage-unit and fee-unit. After spending each percentage-unit, the judges would have the choice to postpone judging to a later date and/or raise the next fee-unit. If necessary, an audit lottery would be held before each new unit.</i><br /><br /><i>If desired, judges can also be given a direct financial incentive to be careful and honest. &#39;Appeals&#39; markets can be created on the same question, but judged by an independent group much later and/or with a much higher judging-fee. For a limited period after a verdict is announced, an amount, up to a fixed fraction of the original judging-fee, would be spent in trying to move the price in the appeals market toward the verdict specified. Judges would end up with some contingent assets saying their verdict would be upheld in the appeals market, assets that they could sell immediately, at a loss, if they so chose.</i><br /><br /><b><i>Idea futures markets need no central management. Anyone could author a claim on any subject of interest to them, contract with different judging groups to judge that claim on different dates, and allow different banks to deal in each question. Anyone should be able to open a clearinghouse to sell any asset. All of these groups could compete openly for the attention and respect of investors.</i></b><br /><br /><i>5.2 Investors</i><br /><br /><i>Investors could be as diverse as they are in current markets, each focusing on some speciality while avoiding risk from other areas. For example, if the market odds are &#39;incoherent&#39;, i.e. deviate from the standard axioms of probability, a trader who corrects that deviation can make better than the average rate of return without significant risk. Therefore coherence specialists should keep the market consensus roughly consistent over a wide range of subjects. Similarly, technical trades would keep the pattern of price changes close to the ideal random walk (Malkiel, 1989). The market odds should also quickly reflect information contained in any co-existing consensus measures, such as opinion polls or reports of élite committees, as traders could make easy money if alternative measures were reliably better predictors than the market.</i><br /><br /><b><i>A contingent asset, like &#39;X if F&#39;, that is split again creates conjunctive contingent assets like &#39;X if F and A&#39;. Conjuncts that combine many claims may be popular, since they offer investors the greatest expected return. Conjunctive assets also allow one to bet on the conditional probability of A given F and remain insensitive to the verdict on F. In this way diverse traders, each of whom has only local knowledge, could manage a large network of dependencies such as the currently popular &#39;Bayes net&#39; models</i></b><i> (Pearl, 1988).</i><br /><br /><i>5.3 Social attitudes</i><br /><br /><i>Some new social attitudes toward these new markets are important elements of the envisioned approach. As with current financial markets, the market odds should be treated as the current social consensus on a question by popular media and policymakers. While one may of course disagree with this consensus in conversation, it is not impolite for others to enquire whether one who so disagrees has made investments commensurate with their wealth and the fuss that they are making. People who do so invest should receive the same sort of social credit now granted to &#39;do-gooder&#39; advocates who devote personal resources to changing current opinion on some important issue. Like Phileas Fogg, the hero of Verne&#39;s Around the World in Eighty Days, &#39;a man who rather laid wagers for honor&#39;s sake than for the stake proposed&#39;(Verne, 1872: 258), these investors should not be treated as mere riskloving gamblers.</i><br /><br /><i>Social credit should also go to philanthropists who choose to subsidize a market on some important question. By funding an automatic inventory-based (Stoll, 1989) market-maker, which always offers to buy or sell at prices determined solely by its current inventory, one gives away money only to those who move the market price in the direction of its final verdict.</i><br /><br /><i>Reputation scores could be computed from each person&#39;s public trades, recorded at registries. A trade is considered to be &#39;public&#39; if the trader committed at trading time to a date at which the trade would be publicly revealed, and that date has passed. One simple reputation score would be the ratio of the current market value of assets held to their value when purchased, corrected for a few distortions. People with high reputation scores should be respected for having been right despite opposition, and such scores might even compete with GPAs (i.e. Grade Point Averages) or number of papers published as an evaluation measure.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Information Cascade vs the Independent Thinker","summary":"It's the Spread of an Idea that Happens when Peoples' Decisions are Made in Sequence Rather than All at Once / People May Fall in Line Because they Think that Others are Making Decisions Based on what they Know, when in Fact People are Making Decisions Based on what they Think the Others who Came Before them Knew / Instead of Aggregating All the Information Individuals have, the Cascade Becomes a Sequence of Uninformed Choices / Independent Thinkers Make the Public Information Seem Less Certain / That Encourages Others to Rely on Themselves Rather than Just Copying Others","image":"https://lh3.googleusercontent.com/-jSz1acV5KQk/WDsWS3X2ijI/AAAAAAAAJWs/8jgmVAzl9_At8R26lXDJkbedBn7h7BCTACJoC/w1800-h2520/the-information-cascade-vs-the-independent-thinker-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/DGgoWbwziq8","publishDate":"2016-11-27T17:23:57.584Z","updateDate":"2017-01-03T01:43:53.533Z","text":"<b>The Information Cascade vs the Independent Thinker: It&#39;s the Spread of an Idea that Happens when Peoples&#39; Decisions are Made in Sequence Rather than All at Once / People May Fall in Line Because they Think that Others are Making Decisions Based on what they Know, when in Fact People are Making Decisions Based on what they Think the Others who Came Before them Knew / Instead of Aggregating All the Information Individuals have, the Cascade Becomes a Sequence of Uninformed Choices / Independent Thinkers Make the Public Information Seem Less Certain / That Encourages Others to Rely on Themselves Rather than Just Copying Others</b><br /><br /><i>The Wisdom of Crowds: Why the Many Are Smarter than the Few and How Collective Wisdom Shapes Business, Economics, Societies and Nations</i><br />by James Surowiecki<br /><br /><i>&quot;In the early part of the twentieth century, the American naturalist William Beebe came upon a strange sight in the Guyana jungle. A group of army ants was moving in a huge circle. The circle was 1,200 feet in circumference, and it took each ant two and a half hours to complete the loop. The ants went around and around the circle for two days until most of them dropped dead.</i><br /><br /><b><i>What Beebe saw was what biologists call a &#39;circular mill.&#39; The mill is created when army ants find themselves separated from their colony. Once they&#39;re lost, they obey a simple rule: follow the ant in front of you. The result is the mill, which usually only breaks up when a few ants straggle off by chance and the others follow them away.</i></b><br /><br /><i>As Steven Johnson showed in his illuminating book Emergence, an ant colony normally works remarkably well. No one ant runs the colony. No one issues orders. Each individual ant knows, on its own, almost nothing. Yet the colony successfully finds food, gets all its work done, and reproduces itself. But the simple tools that make ants so successful are also responsible for the demise of the ants who get trapped in the circular mill. Every move an ant makes depends on what its fellow ants do, and an ant cannot act independently, which would help break the march to death.</i><br /><br /><i>So far in this book, I&#39;ve assumed that human beings are not ants. In other words, I&#39;ve assumed that human beings can be independent decision makers. </i><b><i>Independence doesn&#39;t mean isolation, but it does mean relative freedom from the influence of others. If we are independent, our opinions are, in some sense, our own. We will not march to death in a circle just because the ants in front of us are doing so.</i></b><br /><br /><i>This is important because a group of people -- unlike a colony of ants -- is far more likely to come up with a good decision if the people in the group are independent of each other. Independence is always a relative term, but the story of Francis Galton and the [challenge at a county fair to guess the weight of an oxe] illustrates the point. Each fairgoer figured out his estimate of the weight of the ox on his own (with allowances made for kibitzing), relying on what economists call &#39;private information.&#39; (Private information isn&#39;t just concrete data. It can also include interpretation, analysis, or even intuition.) And when you put all those independent estimates together, the combined guess was ... near perfect.</i><br /><br /><b><i>Independence is important to intelligent decision making for two reasons. First, it keeps the mistakes that people make from becoming correlated. Errors in individual judgment won&#39;t wreck the group&#39;s collective judgment as long as those errors aren&#39;t systematically pointing in the same direction. One of the quickest ways to make people&#39;s judgments systematically biased is to make them dependent on each other for information. Second, independent individuals are more likely to have new information rather than the same old data everyone is already familiar with. The smartest groups, then, are made up of people with diverse perspectives who are able to stay independent of each other. Independence doesn&#39;t imply rationality or impartiality, though. You can be biased and irrational, but as long you&#39;re independent, you won&#39;t make the group any dumber.</i></b><br /><br /><i>Now, the assumption of independence is a familiar one. It&#39;s intuitively appealing, since it takes the autonomy of the individual for granted. It&#39;s at the core of Western liberalism. And, in the form of what&#39;s usually called &#39;methodological individualism,&#39; it underpins most of textbook economics. Economists usually take it as a given that people are self-interested. And they assume people arrive at their ideas of self-interest on their own.</i><br /><br /><i>For all this, though, independence is hard to come by. We are autonomous beings, but we are also social beings. </i><b><i>We want to learn from each other, and learning is a social process. The neighborhoods where we live, the schools we attend, and the corporations where we work shape the way we think and feel. As Herbert J. Simon once wrote, &#39;A man does not live for months or years in a particular position in an organization, exposed to some streams of communication, shielded from others, without the most profound effects upon what he knows, believes, attends to, hopes, wishes, emphasizes, fears, and proposes.&#39;</i></b><br /><br /><b><i>Even while recognizing (how could they not?) the social nature of existence, economists tend to emphasize people&#39;s autonomy and to downplay the influence of others on our preferences and judgments. Sociologists and social-network theorists, by contrast, describe people as embedded, in particular social contexts, and see influence as inescapable. Sociologists generally don&#39;t view this as a problem. They suggest it&#39;s simply the way human life is organized. And it may not be a problem for everyday life. But what I want to argue here is that the more influence a group&#39;s members exert on each other, and the more personal contact they have with each other, the less likely it is that the group&#39;s decisions will be wise ones. The more influence we exert on each other, the more likely it is that we will believe the same things and make the same mistakes. That means it&#39;s possible that we could become individually smarter but collectively dumber. The question we have to ask in thinking about collective wisdom, then, is: Can people make collectively intelligent decisions even when they are in constant, if erratic, interaction with each other?&quot;</i></b><br /><br />[...]<br /><br /><i>&quot;Consider the story of plank-road fever, which the economist Daniel B. Klein and the historian John Majewski uncovered a decade ago. In the first half of the nineteenth century, Americans were obsessed with what were then known as &#39;internal improvements&#39; -- canals, railroads, and highways. The country was growing fast and commerce was booming, and Americans wanted to make sure that transportation -- or rather the lack of it -- didn&#39;t get in the way. In 1825, the Erie Canal was completed, linking New York City to Lake Erie via a 363-mile-long channel that cut travel time from the East Coast to the western interior in half and cut shipping costs by 90 percent. Within a few years, the first local rail lines were being laid, even as private companies were busy building private turnpikes all over the eastern part of the country.</i><br /><br /><i>There was a problem, though, that all this feverish building did not solve. Although the canals and railroads would do an excellent job of connecting major towns and cities (and of turning small villages into thriving commercial hubs merely by virtue of going through them), they made it no easier for people who lived outside of those towns -- which is to say, most Americans -- to get their goods to market, or for that matter to get from one small town to the next. There were local public roads, different stretches of which were maintained by individual villages (much as in a city people take care, at least in theory, of the patch of sidewalk in front of their apartment building), but these roads were usually in pretty bad shape. &#39;They had shallow foundations, if any, and were poorly drained,&#39; write Klein and Majewski. &#39;Their surfaces were muddy ruts in wet weather, dusty ruts in dry; travel was slow and extremely wearing on vehicles and on the animals that drew them.&#39;</i><br /><br /><i>An engineer named George Geddes, though, believed he had uncovered a solution to this problem: the plank road. The plank road -- which, as its name suggests, consisted of wooden planks laid over two lines of timber -- had been introduced in Canada in the early 1840s, and after seeing evidence of its success there, Geddes was convinced it would work in the United States as well. There was no question that a plank road was superior to a rutted, muddy path. What wasn&#39;t clear was whether a plank road -- which would, in most cases, be privately owned and supported by tolls -- would last long enough to be cost-effective. Geddes believed that a typical road would last eight years, more than long enough to provide a reasonable return on investment, and so, in 1846, he convinced some of his fellow townsmen in Salina, New York, to charter a company to build the state&#39;s first plank road.</i><br /><br /><i>The road was a roaring success, and soon plank-road fever swept through first New York, then through the mid-Atlantic states and the Midwest. Geddes became a kind of spokesman for the industry, even as other promoters played a similar role in states across the country. Within a decade, there were 352 plank-road companies in New York, and more than a thousand in the United States as a whole.</i><br /><br /><i>Unfortunately, the whole business was built on an illusion. Plank roads did not last the eight years Geddes had promised (let alone the twelve years that other enthusiasts had suggested). As Klein and Majewski show, the roads&#39; actual life span was closer to four years, which made them too expensive for companies to maintain. By the late 1850s, it was clear that the plank road was not a transportation panacea. And though a few roads -- including a thirteen-mile stretch along what is now Route 27A in Jamaica, Queens -- remained in operation until the 1880s, by the end of the Civil War almost all of them had been abandoned.</i><br /><br /><i>Plank-road fever was a vivid example of a phenomenon that economists call an </i><b><i>&#39;information cascade.&#39;</i></b><i> The first Salina road was a success, as were those which were built in the years immediately following. People who were looking around for a solution to the problem of local roads had one ready-made at hand. </i><b><i>As more people built plank roads, their legitimacy became more entrenched, and the desire to consider other solutions shrank. It was years before the fundamental weakness of the roads -- they didn&#39;t last long enough -- became obvious, and by that time plank roads were being built all over the country.</i></b><br /><br /><i>Why did this happen? The economists Sushil Bikhchandani, David Hirshleifer, and Ivo Welch, who offered the first real model of an information cascade, suggest that it works like this. Assume you have a large group of people, all of whom have the choice of going to either a new Indian restaurant or a new Thai place. The Indian restaurant is better (in an objective sense) than the Thai place. And each person in the group is going to receive, at some point, a piece of information about which restaurant is better. But the information is imperfect. Sometimes it will be wrong -- that is, it will say the Thai place is better when it&#39;s not -- and will guide a person in the wrong direction. So to supplement their own information, people will look at what others are doing. (The economists assume that everyone knows that everyone else has a piece of good information, too.)</i><br /><br /><b><i>The problem starts when people&#39;s decisions are not made all at once but rather in sequence, so that some people go to one of the two restaurants first and then everyone else follows in order. Remember, the information people have is imperfect. So if the first couple of people happen to get bad information, leading them to believe that the Thai restaurant is great, that&#39;s where they&#39;ll go. At that point, in the cascade model, everyone who follows assumes -- even if they&#39;re getting information telling them to go to the Indian restaurant -- that there&#39;s a good chance, simply because the Thai place is crowded, that it&#39;s better. So everyone ends up making the wrong decision, simply because the initial diners, by chance, got the wrong information.</i></b><br /><br /><i>In this case, a cascade is not the result of mindless trend-following, or conformity, or peer pressure. (&#39;Everyone likes that new Britney Spears song, so I will, too!&#39;) </i><b><i>People fall in line because they believe they&#39;re learning something important from the example of others.</i></b><i> In the case of the plank roads, for instance, it wasn&#39;t simply that George Geddes was a smooth talker, or that townspeople across the country said, &#39;We just have to have a new plank road because the town across the river has one.&#39; Plank-road fever spread because plank roads really seemed to be a better solution. They cut travel time between towns in half. You could ride on them in any kind of weather. And they allowed small farmers to expand the markets for their goods far beyond what had previously been possible. </i><b><i>These were genuine improvements, and as more and more plank roads were built, the fact that those improvements were real and long lasting seemed increasingly plausible. Each new road that was built was in a sense telling people that plank roads worked. And each new road that was built made coming up with an alternative seem increasingly improbable.</i></b><br /><br /><b><i>The fundamental problem with an information cascade is that after a certain point it becomes rational for people to stop paying attention to their own knowledge -- their private information -- and to start looking instead at the actions of others and imitate them. (If everyone has the same likelihood of making the right choice, and everyone before you has made the same choice, then you should do what everyone else has done.) But once each individual stops relying on his own knowledge, the cascade stops becoming informative. Everyone thinks that people are making decisions based on what they know, when in fact people are making decisions based on what they think the people who came before them knew. Instead of aggregating all the information individuals have, the way a market or a voting system does, the cascade becomes a sequence of uninformed choices, so that collectively the group ends up making a bad decision -- spending all that money on plank roads.</i></b><br /><br /><b><i>That original model is far from the only theory of how cascades work, of course. In The Tipping Point, for instance, Malcolm Gladwell offered a very different account, which emphasized the importance of particular kinds of individuals -- what he called mavens, connectors, and salesmen -- in spreading new ideas. In Bikhchandani, Hirshleifer, and Welch&#39;s model of cascades, everyone had as much private information as everyone else. The only thing that made the early adopters of a product more influential was the fact that they were early, and so their actions were the ones that everyone who came after them observed. In Gladwell&#39;s world, some people are far more influential than others, and cascades (he writes of them as epidemics) move via social ties, rather than being a simple matter of anonymous strangers observing each other&#39;s behavior. People are still looking for information, but they believe that the ones who have it are the mavens, connectors, and salesmen (each of whom has a different kind of information).</i></b><br /><br /><i>Do cascades exist? Without a doubt. </i><b><i>They are less ubiquitous than the restaurant-going model suggests, since, as Yale economist Robert Shiller has suggested, people don&#39;t usually make decisions in sequence.</i></b><i> &#39;In most cases,&#39; Shiller writes, &#39;many people independently choose their action based on their own signals, without observing the actions of others.&#39; But there are plenty of occasions when people do closely observe the actions of others before making their own decisions. In those cases, cascades are possible, even likely. That is not always a bad thing. For instance, one of the most important and valuable innovations in American technological history was made possible by the orchestrating of a successful information cascade. The innovation was the humble screw, and in the 1860s a man named William Sellers, who was the most prominent and respected machinist of his era at a time when the machine-tool industry was the rough equivalent of the technology industry in the 1990s, embarked on a campaign to get America to adopt a standardized screw, which happened to be of his own design. When Sellers started his campaign, every American screw had to be hand-made by a machinist. This obviously limited the possibilities for mass production, but it also allowed the machinists to protect their way of life. In economic terms, after all, anything tailor-made has the advantage of locking in customers. If someone bought a lathe from a machinist, that person had to come back to the machinist for screw repairs or replacements. But if screws became interchangeable, customers would need the craftsmen less and would worry about the price more.</i><br /><br /><i>Sellers understood the fear. But he also believed that interchangeable parts and mass production were inevitable, and the screw he designed was meant to be easier, cheaper, and faster to produce than any other. His screws fit the new economy, where a premium was placed on speed, volume, and cost. But because of what was at stake, and because the machinist community was so tight-knit, Sellers understood that connections and influence would shape people&#39;s decisions. </i><b><i>So over the next five years, he targeted influential users, like the Pennsylvania Railroad and the U.S. Navy, and he successfully created an air of momentum behind the screw. Each new customer made Sellers&#39;s eventual triumph seem more likely, which in turn made his eventual triumph more likely. Within a decade the screw was on its way to becoming a national standard.</i></b><i> Without it, assembly-line production would have been difficult at best and impossible at worst. In a sense, Sellers had helped lay the groundwork for modern mass production.</i><br /><br /><b><i>Sellers&#39;s story is of a beneficial cascade.</i></b><i> The screw&#39;s design was, by all accounts, superior to its chief competitor, a British screw. And the adoption of a standard screw was a great leap forward for the U.S. economy. But there is an unnerving idea at the heart of Sellers&#39;s story: if his screw was adopted because he used his influence and authority to start a cascade, we were just lucky that Sellers happened to design a good screw. If the machinists were ultimately following Sellers&#39;s lead, rather than acting on their own sense of which screw was better, it was pure chance that they got the answer right.</i><br /><br /><b><i>In other words, if most decisions to adopt new technologies or social norms are driven by cascades, there is no reason to think that the decisions we make are, on average, good ones. Collective decisions are most likely to be good ones when they&#39;re made by people with diverse opinions reaching independent conclusions, relying primarily on their private information. In cascades, none of these things are true. Effectively speaking, a few influential people -- either because they happened to go first, or because they have particular skills and fill particular holes in people&#39;s social networks—determine the course of the cascade. In a cascade, people&#39;s decisions are not made independently, but are profoundly influenced -- in some cases, even determined -- by those around them.&quot;</i></b><br /><br />[...]<br /><br /><b><i>&quot;In a sense, imitation is a kind of rational response to our own cognitive limits. Each person can&#39;t know everything. With imitation, people can specialize and the benefits of their investment in uncovering information can be spread widely when others mimic them. Imitation also requires little top-down direction. The relevant information percolates quickly through the system, even in the absence of any central authority. And people&#39;s willingness to imitate is not, of course, unconditional.</i></b><i> If I get a couple of tickets because of bad information, I&#39;ll soon make sure I know when I have to move my car ... </i><b><i>In the long run, imitation has to be effective for people to keep doing it.</i></b><br /><br /><i>This wisdom was in the decision to imitate Imo.  As I suggested in the last chapter, groups are better at deciding between possible solutions to a problem than they are at coming up with them.  </i><b><i>Invention may still be an individual enterprise (although, as we&#39;ll see, invention has an inescapably collective dimension), but selecting among inventions is a collective one.  Used well, imitation is a powerful tool for spreading good ideas fast</i></b><i> -- whether they be  in culture, business, sports, or the art of wheat eating.  At its best, you can see it as a way of speeding up the evolutionary process -- the community can become more fit without the usual need for multiple generations of genetic winnowing.  Scientists Robert Boyd and Peter J. Richerson have pioneered the study of the transmission of social norms, trying to understand how groups arrive at collectively beneficial conclusions.  They&#39;ve run a series of computeried simulations looking at the behavior of agents who are trying to discover which of two different behaviors is best suited to the environment they&#39;re living in.  In the simulation, each agent can try out a behavior for himself and see what happens, but he can also observe the behavior of someone else who&#39;s already made a decision about which behavior is best.  </i><b><i>Boyd and Richerson found that under these circumstances, everyone benefits when a sizable percentage of the population imitates.  But this is only true as long as people are willing to stop imitating and learn for themselves when the benefits of doing so become high enough.  In other words, if people just keep following the lead of others regardless of what happens, the well-being of the group suffers.  Intelligent imitation can help the group -- by making it easier for good ideas to spread quickly -- but slavish imitation hurts.</i></b><br /><br /><i>Distinguishing between the two kinds of imitation is, of course, not easy, since few people will admit that they&#39;re mindlessly conforming or herding.  </i><b><i>But it does seem clear that intelligent imitation depends on a couple of things: first, an initially wide array of options and information; and second, the willingness of at least some people to put their own judgment ahead of the group&#39;s, even when it&#39;s not sensible to do so.</i></b><br /><br /><i>Do such people exist?  Actually they&#39;re a lot more common than you&#39;d expect.  One reason is that people are, in general, overconfident.  They overestimate their ability, their level of knowledge, and their decision-making prowess.  And people are more overconfident when facing difficult problems than when facing easy ones.  </i><b><i>This is not good for the overconfident decision makers themselves, since it means that they&#39;re more likely to choose badly.  But it is good for society as a whole, because overconfident people are less likely to get sucked into a negative information cascade, and, in the right circumstances, are even able to break cascades.  Remember that a cascade is kept going by people valuing public information more highly than their private information.  Overconfident people don&#39;t do that.  They tend to ignore public information and go on their gut.  When they do so, they disrupt the signal that everyone else is getting.  They make the public information seem less certain.  And that encourages others to rely on themselves rather than just follow everyone else.&quot;</i></b><br /><br />[...]<br /><br /><i>&quot;But roughly speaking, all of the problems that cascades can cause are the result of the fact that some people make their decisions before others. </i><b><i>If you want to improve an organization&#39;s or an economy&#39;s decision making, one of the best things you can do is make sure, as much as possible, that decisions are made simultaneously (or close to it) rather than one after the other.&quot;</i></b><br /><br />[...]<br /><br /><b><i>&quot;[C]ertain products or problems are more susceptible to cascades than others.</i></b><i> For instance, fashion and style are obviously driven by cascades, which we call fads, because when it comes to fashion, what you like and what everyone else likes are clearly wrapped up with each other. I like to dress a certain way, but it&#39;s hard to imagine that the way I like to dress is disconnected from the kind of impression I want to make, which in turn must have something to do with what other people like. The same might also be said, though less definitively, about cultural products (like TV shows) where part of why we watch the show is to talk about it with our friends, or even restaurants, since no one likes to eat in an empty restaurant. No one buys an iPod because other people have them -- the way they might, in fact, go to a movie because other people are going -- but many technology companies insist that information cascades (of the good kind, they would say) are crucial to their success, as early adopters spread the word of a new product&#39;s quality to those who come after. </i><b><i>The banal but key point I&#39;m trying to make is that the more important the decision, the less likely a cascade is to take hold. And that&#39;s obviously a good thing, since it means that the more important the decision, the more likely it is that the group&#39;s collective verdict will be right.</i></b><br /><br /><i>What makes information cascades interesting is that they are a form of aggregating information, just like a voting system or a market. And the truth is that they don&#39;t do a terrible job of aggregation.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Misconceptions of Expertise","summary":"Expertise is Spectacularly Narrow and Experts Commonly Exhibit a Dismal Performance Record / Their Judgments Frequently Lack Internal Consistency and Consistency Across Individual Experts / They Routinely Overestimate the Likelihood that they're Right / And Even if Superior Beings do Exist, there is No Easy Way to Distinguish them / Trying to Find Smart People Will Not Lead You Astray, but Trying to Find the Smartest Person - to Trust - Will","image":"https://lh3.googleusercontent.com/-ZMZ39s-h_Pg/WDsPR3D4ulI/AAAAAAAAJVw/VqPV8aAq9-IQKzcQRw1wVYSvu1404zjjwCJoC/w3600-h5040/the-misconceptions-of-expertise-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/3BT3iKQDtFW","publishDate":"2016-11-27T16:52:55.166Z","updateDate":"2017-01-03T01:44:23.082Z","text":"<b>The Misconceptions of Expertise: Expertise is Spectacularly Narrow and Experts Commonly Exhibit a Dismal Performance Record / Their Judgments Frequently Lack Internal Consistency and Consistency Across Individual Experts / They Routinely Overestimate the Likelihood that they&#39;re Right / And Even if Superior Beings do Exist, there is No Easy Way to Distinguish them / Trying to Find Smart People Will Not Lead You Astray, but Trying to Find the Smartest Person - to Trust - Will</b><br /><br /><i>The Wisdom of Crowds: Why the Many Are Smarter than the Few and How Collective Wisdom Shapes Business, Economics, Societies and Nations</i><br />by James Surowiecki<br /><br /><i>&quot;The fact that cognitive diversity matters does not mean that if you assemble a group of diverse but thoroughly uninformed people, their collective wisdom will be smarter than an expert&#39;s. </i><b><i>But if you can assemble a diverse group of people who possess varying degrees of knowledge and insight, you&#39;re better off entrusting it with major decisions rather than leaving them in the hands of one or two people, no matter how smart those people are. If this is difficult to believe ... it&#39;s because it runs counter to our basic intuitions about intelligence and business. Suggesting that the organization with the smartest people may not be the best organization is heretical, particularly in a business world caught up in a ceaseless &#39;war for talent&#39; and governed by the assumption that a few superstars can make the difference between an excellent and a mediocre company. Heretical or not, it&#39;s the truth: the value of expertise is, in many contexts, overrated.&quot;</i></b><br /><br />[...]<br /><br /><b><i>&quot;A chess expert knows about chess, and that&#39;s it. We intuitively assume that intelligence is fungible, and that people who are excellent at one intellectual pursuit would be excellent at another. But this is not the case with experts. Instead, the fundamental truth about expertise is that it is ... &#39;spectacularly narrow.&#39;</i></b><br /><br /><i>More important, </i><b><i>there&#39;s no real evidence that one can become expert in something as broad as &#39;decision making&#39; or &#39;policy&#39; or &#39;strategy.&#39;</i></b><i> Auto repair, piloting, skiing, perhaps even management: these are skills that yield to application, hard work, and native talent. But forecasting an uncertain future and deciding the best course of action in the face of that future are much less likely to do so. </i><b><i>And much of what we&#39;ve seen so far suggests that a large group of diverse individuals will come up with better and more robust forecasts and make more intelligent decisions than even the most skilled &#39;decision maker.&#39;</i></b><br /><br /><i>We&#39;re all familiar with the absurd predictions that business titans have made: Harry Warner of Warner Bros, pronouncing in 1927, &#39;Who the hell wants to hear actors talk?,&#39; or Thomas Watson of IBM declaring in 1943, &#39;I think there is a world market for maybe five computers.&#39; These can be written off as amusing anomalies, since over the course of a century, some smart people are bound to say some dumb things. </i><b><i>What can&#39;t be written off, though, is the dismal performance record of most experts.</i></b><br /><br /><i>Between 1984 and 1999, for instance, almost 90 percent of mutual-fund managers underperformed the Wilshire 5000 Index, a relatively low bar. The numbers for bond-fund managers are similar: in the most recent five-year period, more than 95 percent of all managed bond funds underperformed the market. After a survey of expert forecasts and analyses in a wide variety of fields, Wharton professor J. Scott Armstrong wrote, &#39;I could find no studies that showed an important advantage for expertise.&#39; Experts, in some cases, were a little better at forecasting than laypeople (although a number of studies have concluded that nonpsychologists, for instance, are actually better at predicting people&#39;s behavior than psychologists are), but above a low level, Armstrong concluded, &#39;expertise and accuracy are unrelated.&#39; James Shanteau is one of the country&#39;s leading thinkers on the nature of expertise, and has spent a great deal of time coming up with a method for estimating just how expert someone is. Yet even he suggests that &#39;experts&#39; decisions are seriously flawed.&#39;</i><br /><br /><b><i>Shanteau recounts a series of studies that have found experts&#39; judgments to be neither consistent with the judgments of other experts in the field nor internally consistent.</i></b><i> For instance, the between-expert agreement in a host of fields, including stock picking, livestock judging, and clinical psychology, is below 50 percent, meaning that experts are as likely to disagree as to agree. More disconcertingly, one study found that the internal consistency of medical pathologists&#39; judgments was just 0.5, meaning that a pathologist presented with the same evidence would, half the time, offer a different opinion. </i><b><i>Experts are also surprisingly bad at what social scientists call &#39;calibrating&#39; their judgments. If your judgments are well calibrated, then you have a sense of how likely it is that your judgment is correct. But experts are much like normal people: they routinely overestimate the likelihood that they&#39;re right.</i></b><br /><br /><b><i>A survey on the question of overconfidence by economist Terrance Odean found that physicians, nurses, lawyers, engineers, entrepreneurs, and investment bankers all believed that they knew more than they did.</i></b><i> Similarly, a recent study of foreign-exchange traders found that 70 percent of the time, the traders overestimated the accuracy of their exchange-rate predictions. In other words, </i><b><i>it wasn&#39;t just that they were wrong; they also didn&#39;t have any idea how wrong they were. And that seems to be the rule among experts.</i></b><i> The only forecasters whose judgments are routinely well calibrated are expert bridge players and weathermen. It rains on 30 percent of the days when weathermen have predicted a 30 percent chance of rain.</i><br /><br /><i>Armstrong, who studies expertise and forecasting, summarized the case this way: &#39;One would expect experts to have reliable information for predicting change and to be able to utilize the information effectively. However, expertise beyond a minimal level is of little value in forecasting change.&#39; Nor was there evidence that even if most experts were not very good at forecasting, a few titans were excellent. Instead, Armstrong wrote, &#39;claims of accuracy by a single expert would seem to be of no practical value.&#39; This was the origin of Armstrong&#39;s &#39;seer-sucker theory&#39;: &#39;No matter how much evidence exists that seers do not exist, suckers will pay for the existence of seers.&#39;</i><br /><br /><i>Again, this doesn&#39;t mean that well-informed, sophisticated analysts are of no use in making good decisions. (And it certainly doesn&#39;t mean that you want crowds of amateurs trying to collectively perform surgery or fly planes.) It does mean that </i><b><i>however well-informed and sophisticated an expert is, his advice and predictions should be pooled with those of others to get the most out of him. (The larger the group, the more reliable its judgment will be.) And it means that attempting to &#39;chase the expert,&#39; looking for the one man who will have the answers to an organization&#39;s problem, is a waste of time. We know that the group&#39;s decision will consistently be better than most of the people in the group, and that it will be better decision after decision, while the performance of human experts will vary dramatically depending on the problem they&#39;re asked to solve. So it is unlikely that one person, over time, will do better than the group.</i></b><br /><br /><i>Now, it&#39;s possible that a small number of genuine experts -- that is, people who can consistently offer better judgments than those of a diverse, informed group -- do exist. The investor Warren Buffett, who has consistently outperformed the S&amp;P 500 Index since the 1960s, is certainly someone who comes to mind. </i><b><i>The problem is that even if these superior beings, do exist, there is no easy way to identify them. Past performance, as we are often told, is no guarantee of future results. And there are so many would-be experts out there that distinguishing between those who are lucky and those who are genuinely good is often a near-impossible task.</i></b><i> At the very least, it&#39;s a job that requires considerable patience: if you wanted to be sure that a successful money manager was beating the market because of his superior skill, and not because of luck or measurement error, you&#39;d need many years, if not decades, of data. </i><b><i>And if a group is so unintelligent that it will flounder without the right expert, it&#39;s not clear why the group would be intelligent enough to recognize an expert when it found him.</i></b><br /><br /><i>We think that experts will, in some sense, identify themselves, announcing their presence and demonstrating their expertise by their level of confidence. But it doesn&#39;t work that way. Strangely, experts are no more confident in their abilities than average people are, which is to say that they are overconfident like everyone else, but no more so. Similarly, there is very little correlation between experts&#39; self-assessment and their performance. </i><b><i>Knowing and knowing that you know are apparently two very different skills.</i></b><br /><br /><i>If this is the case, then why do we cling so tightly to the idea that the right expert will save us? And why do we ignore the fact that simply averaging a group&#39;s estimates will produce a very good result? Richard Larrick and Jack B. Soli suggest that the answer is that we have bad intuitions about averaging. We assume averaging means dumbing down or compromising. </i><b><i>When people are faced with the choice of picking one expert or picking pieces of advice from a number of experts, they try to pick the best expert rather than simply average across the group. Another reason, surely, is our assumption that true intelligence resides only in individuals, so that finding the right person -- the right consultant, the right CEO -- will make all the difference.</i></b><i> In a sense, the crowd is blind to its own wisdom. Finally, we seek out experts because we get, as the writer Nassim Taleb asserts, &#39;fooled by randomness.&#39; If there are enough people out there making predictions, a few of them are going to compile an impressive record over time. That does not mean that the record was the product of skill, nor does it mean that the record will continue into the future. Again, </i><b><i>trying to find smart people will not lead you astray. Trying to find the smartest person will.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"Innovation's Long Tail","summary":"Despite their Curious Omission in Most Statistics Courses, Many Phenomena Demonstrate Power Law Behavior / Innovation Challenges are a Case in Point / There is no Symmetric, Gaussian Peak for the Average Number of Ideas Entered per Person / And the Range is Enormous","image":"https://lh3.googleusercontent.com/-wdF3dMN8Q-o/WDqH61omtCI/AAAAAAAAJUw/tDrCs6pH944BrbSCrDSLpx2W4-zAXc2MgCJoC/w7142-h9999/innovations-long-tail-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/HrBYGqWXFwY","publishDate":"2016-11-27T07:17:12.237Z","updateDate":"2017-01-03T01:44:58.404Z","text":"<b>Innovation&#39;s Long Tail: Despite their Curious Omission in Most Statistics Courses, Many Phenomena Demonstrate Power Law Behavior / Innovation Challenges are a Case in Point / There is no Symmetric, Gaussian Peak for the Average Number of Ideas Entered per Person / And the Range is Enormous: Many People Put in Just One Idea / Can Crowdsourcing Prove to be a Better Approach than a Closed Network of Academic Experts for a Site Dedicated to Methodically Mapping Out Scientific Controversies?</b><br /><br /><i>The Long Tail: Why the Future of Business Is Selling Less of More</i><br />by Chris Anderson<br /><br />The 98 Percent Rule<br /><br /><i>&quot;This book began with a quiz I got wrong. One of the things I do as the editor of Wired is give speeches about technology trends. Because I started my career in the science world and then learned economics at The Economist, I look for those trends first in hard data. And, fortunately enough, there has never been more data available. The secrets of twenty-first-century economics lie in the servers of the companies that are all around us, from eBay to Wal-Mart. Although it’s not always easy to get the raw numbers, the executives at those companies swim in that data every day and have a great intuitive feel for what’s meaningful and what isn’t. So the trick to trend-spotting is to ask them.</i><br /><br /><i>Which is what I was doing in January 2004, in the offices of Robbie Vann-Adibé, the CEO of Ecast, a &#39;digital jukebox&#39; company. Digital jukeboxes are just like regular jukeboxes -- a big enclosure with speakers and blinking lights, often found in bars -- with the difference that rather than a hundred CDs, they have a broadband connection to the Internet and patrons can choose from thousands of tracks that are downloaded and stored on a local hard drive.</i><br /><br /><b><i>During the course of our conversation, Vann-Adibé asked me to guess what percentage of the 10,000 albums available on the jukeboxes sold at least one track per quarter.</i></b><br /><br /><i>I knew, of course, that Vann-Adibé was asking me a trick question. The normal answer would be 20 percent because of the 80/20 Rule, which experience tells us applies practically everywhere. That is: 20 percent of products account for 80 percent of sales (and usually 100 percent of the profits).</i><br /><br /><i>But Vann-Adibé was in the digital content business, which is different. So I thought I’d go way out on a limb and venture that a whopping 50 percent of those 10,000 albums sold at least one track a quarter.</i><br /><br /><i>Now, on the face of it, that’s absurdly high. Half of the top 10,000 books in a typical book superstore don’t sell once a quarter. Half of the top 10,000 CDs at Wal-Mart don’t sell once a quarter; indeed, Wal-Mart doesn’t even carry half that many CDs. It’s hard to think of any market where such a high fraction of such a large inventory sells. But my sense was that digital was different, so I took a chance on a big number.</i><br /><br /><i>I was, needless to say, way, way off. </i><b><i>The answer was 98 percent.</i></b><br /><br /><i>&#39;It’s amazing, isn’t it?&#39; Vann-Adibé said. </i><b><i>&#39;Everyone gets that wrong.&#39;</i></b><i> Even he had been stunned: As the company added more titles to its collections, far beyond the inventory of most record stores and into the world of niches and subcultures, they continued to sell. And the more the company added, the more they sold. The demand for music beyond the hits seemed to be limitless. </i><b><i>True, the songs didn’t sell in big numbers, but nearly all of them sold something. And because these were just bits in a database that cost nearly nothing to store and deliver, all those onesies and twosies started to add up.</i></b><br /><br /><i>What Vann-Adibé had discovered was that the aggregate market for niche music was huge, and effectively unbounded. He called this the &#39;98 Percent Rule.&#39; As he later put it to me, </i><b><i>&#39;In a world of almost zero packaging cost and instant access to almost all content in this format, consumers exhibit consistent behavior: They look at almost everything.</i></b><i> I believe that this requires major changes by the content producers -- I’m just not sure what changes!&#39;</i><br /><br /><i>I set out to answer that question. I realized that his counterintuitive statistic contained a powerful truth about the new economics of entertainment in the digital age. With unlimited supply, our assumptions about the relative roles of hits and niches were all wrong. Scarcity requires hits -- if there are only a few slots on the shelves or the airwaves, it’s only sensible to fill them with the titles that will sell best. And if that’s all that’s available, that’s all people will buy.</i><br /><br /><i>But what if there are infinite slots? Maybe hits are the wrong way to look at the business. There are, after all, a lot more non-hits than hits, and now both are equally available. What if the non-hits -- from healthy niche product to outright misses -- all together added up to a market as big as, if not bigger than, the hits themselves? The answer to that was clear: It would radically transform some of the largest markets in the world.</i><br /><br /><i>And so I embarked on a research project that was to take me to all the leaders in the emerging digital entertainment industry, from Amazon to iTunes. Everywhere I went the story was the same: Hits are great, but niches are emerging as the big new market. </i><b><i>The 98 Percent Rule turned out to be nearly universal. Apple said that every one of the then 1 million tracks in iTunes had sold at least once (now its inventory is twice that). Netflix reckoned that 95 percent of its 25,000 DVDs (that’s now 90,000) rented at least once a quarter. Amazon didn’t give out an exact number, but independent academic research on its book sales suggested that 98 percent of its top 100,000 books sold at least once a quarter, too. And so it went, from company to company.</i></b><br /><br /><i>Each company was impressed by the demand they were seeing in categories that had been previously dismissed as beneath the economic fringe, from the British television series DVDs that are proving surprisingly popular at Netflix to the back-catalog music that’s big on iTunes. I realized that, </i><b><i>for the first time, I was looking at the true shape of demand in our culture, unfiltered by the economics of scarcity.</i></b><br /><br /><i>That shape is, to be clear, really, really weird. To think that basically everything you put out there finds demand is just odd. The reason it’s odd is that we don’t typically think in terms of one unit per quarter. When we think about traditional retail, we think about what’s going to sell a lot. You’re not much interested in the occasional sale, because in traditional retail a CD that sells only one unit a quarter consumes exactly the same half-inch of shelf space as a CD that sells 1,000 units a quarter. There’s a value to that space -- rent, overhead, staffing costs, etc. -- that has to be paid back by a certain number of inventory turns per month. In other words, the onesies and twosies waste space.</i><br /><br /><i>However, when that space doesn’t cost anything, suddenly you can look at those infrequent sellers again, and they begin to have value. This was the insight that led to Amazon, Netflix, and all the other companies I was talking to. All of them realized that where the economics of traditional retail ran out of steam, the economics of online retail kept going. The onesies and twosies were still only selling in small numbers, but there were so, so many of them that in aggregate they added up to a big business.</i><br /><br /><i>Throughout the first half of 2004 I fleshed out this research in speeches, the thesis advancing with each talk. Originally the speech was called &#39;The 98 Percent Rule.&#39; Then it was &#39;New Rules for the New Entertainment Economy&#39; (not one of my better naming moments).</i><br /><br /><i>But by then I had some hard data, thanks to Rhapsody, which is one of the online music companies. They had given me a month’s worth of customer usage data, and when I graphed it out, I realized that the curve was unlike anything I’d seen before.</i><br /><br /><i>It started like any other demand curve, ranked by popularity. A few hits were downloaded a huge number of times at the head of the curve, and then it fell off steeply with less popular tracks. But the interesting thing was that it never fell to zero. I’d go to the 100,000th track, zoom in, and the downloads per month were still in the thousands. And the curve just kept going: 200,000, 300,000, 400,000 tracks -- no store could ever carry this much music. Yet as far as I looked, there was still demand. Way out at the end of the curve, tracks were being downloaded just four or five times a month, but the curve still wasn’t at zero.</i><br /><br /><b><i>In statistics, curves like that are called &#39;long-tailed distributions,&#39; because the tail of the curve is very long relative to the head.</i></b><i> So all I did was focus on the tail itself, turn it into a proper noun, and &#39;The Long Tail&#39; was born. It started life as slide 20 of one of my &#39;New Rules&#39; presentations. I think it was Reed Hastings, the CEO of Netflix, who convinced me that I was burying my lead. By the summer of 2004 &#39;The Long Tail&#39; was not just the title of my speeches; I was nearly finished with an article of the same name for my own magazine.</i><br /><br /><b><i>When &#39;The Long Tail&#39; was published in Wired in October 2004, it quickly became the most cited article the magazine had ever run.</i></b><i> The three main observations -- (1) the tail of available variety is far longer than we realize; (2) it’s now within reach economically; (3) all those niches, when aggregated, can make up a significant market -- seemed indisputable, especially backed up with heretofore unseen data.&quot;</i><br /><br /><a href=\"http://scaledinnovation.com/innovation/publications/2010-06-longtail.pdf\" class=\"ot-anchor\">http://scaledinnovation.com/innovation/publications/2010-06-longtail.pdf</a><br /><br /><b><i>&quot;Despite their curious omission in most statistics courses, many phenomena demonstrate power law behavior</i></b><i> and have robust theories behind them. We can learn from these examples and develop a credible model to account for the shape of our data, </i><b><i>and then apply that model to suggest future actions which could make collaborative innovation even richer and more reliable.</i></b><br /><br /><b><i>For example, graphs of the size vs. the rank (largest to smallest) of landslides follow power laws [7] as do forest fires [8], human disease epidemics [9], internet website connections [10], and collaboration networks of movie stars [10]. Note the exceptional diversity of these examples: from nature and human behavior, timeless and modern. When such things share a common generating property, that property must be incredibly simple and powerful.</i></b><br /><br /><i>The simple and powerful generating function behind these phenomena is the </i><b><i>Yule process</i></b><i> [11] or simply </i><b><i>&#39;the rich get richer&#39;</i></b><i> [5]. </i><b><i>It’s more likely for rocks on a hillside to roll downhill if the rocks around them are rolling (they bump into each other). It’s more likely for trees to catch fire if the trees around them are already on fire. It’s more likely that you’ll get sick if the people around you are sick, that you’ll get asked to be in a new movie if you’re already well known, or that you’ll visit a website if it’s already highly connected to other websites.</i></b><i> It may help to think in the converse: if the rocks on a hillside are quiet, it’s unlikely that any particular rock will start rolling downhill, if the forest is quiet, it’s unlikely that a tree will just catch fire, and if you’ve just launched your website or have just arrived in Hollywood, it’s not very likely you’ll get a lot of connections tomorrow. Things may begin for nonpower law reasons (lightning strikes the forest, a stranger with a disease visits your city, the famous star breaks her ankle and the ingénue gets the part), but once initiated the Yule process dominates and a power law results. </i><b><i>If events are independent (like tossing a coin), classical Gaussian bell-curve statistics result, but if events have some positive dependence on what already exists, then power laws result.</i></b><i> The mathematics is well developed by Newman [5].&quot;</i><br /><br />[...]<br /><br /><i>&quot;Our investigation into the mathematical structure of innovation challenges begins with a simple question that could easily come from an executive sponsor: </i><b><i>&#39;What’s the average number of ideas entered per person?&#39; A little exploration of this innocent and apparently reasonable question reveals that any answer we could give -- for example by dividing the total number of ideas by total participants -- would be &#39;not even wrong.&#39;</i></b><i> This nice phrase comes from physicists, who say that an incorrect measurement based on a good theory might be wrong, but a measurement based on a bad theory would be &#39;not even wrong&#39;. It’s an insult and a warning to recognize that results based on bad theory should not be calculated or presented, because doing so propagates misunderstanding and implicitly endorses the bad theory.</i><br /><br /><i>Why this is a case of bad theory is illustrated in [Figure 1]. </i><b><i>If we ask &#39;what’s the average height of our employees?&#39; we expect a result like that on the left: a bell-shaped curve, an obvious peak (average), and some reasonable width (variance). From the shortest to the tallest person, height varies by less than a factor of two. The right of [Figure 1], presented in the same way, is from the Pfizer Idea Farm over thousands of people and thousands of ideas. There is no symmetric peak, no obvious average, and the range is enormous: many people put in just one idea and a few put in hundreds each.&quot;</i></b><br /><br />[...]<br /><br /><i>&quot;[V]oluntary large-scale collaboration systems have a consistent statistical signature which has significant consequences that innovation leaders should appreciate. But before getting to the numeric observations it is useful to review the event model, </i><b><i>also referred to as the challenge or diverge-converge model</i></b><i>, which underpins most of our data. The model is </i><b><i>centuries old</i></b><i>, </i><b><i>proven effective</i></b><i>, and well illustrated by the classic Longitude Problem. As recounted by Dava Sobel [1], determining accurate longitude at sea was an unsolved and important problem in the early 18th century, and its consequences became all too apparent in October 1707 when four English warships struck rocks in a thick fog and 2000 men drowned. In response, Parliament’s Longitude Act of 1714 set the terms of a great challenge: the problem was the determination of longitude at sea, to win required a specified accuracy demonstrated on a real voyage, anyone could submit an answer (you didn’t even have to be English, let alone connected to the navy or scientific establishment), Parliament and the Admiralty would fund and develop the solution, there was a formal review committee and process (initially led by Sir Isaac Newton), and the prize was the unimaginable sum of £20,000. John Harrison, an unknown clockmaker with no formal education, solved detail after detail and ultimately won the challenge (and the prize) by inventing what we now recognize as the first marine chronometer.</i><br /><br /><b><i>The longitude story illustrates the elements of a good challenge: state the problem clearly, do not prejudge the solutions, say why it’s important and who will implement the best ideas, expose the challenge to a large diverse audience, let everyone know the review process and criteria, and reward success (with involvement, recognition, or money).&quot;</i></b><br /><br /><a href=\"http://scaledinnovation.com/innovation/publications/2012-06-longitude.pdf\" class=\"ot-anchor\">http://scaledinnovation.com/innovation/publications/2012-06-longitude.pdf</a><br /><br />Open Innovation in the Eighteenth Century<br />The Longitude Problem<br /><br />Robin W. Spencer<br /><br /><i>&quot;Overview: Forget Linux, Netflix, DARPA, and the X Prizes. Deliberate, managed open innovation goes back 300 years. At that time, the determination of longitude at sea was both vitally important and apparently impossible; </i><b><i>so having failed with a team of experts, Parliament sought solutions from a wider field via the Longitude Act of 1714</i></b><i>, which offered a prize of £20,000 for a method of determining longitude to within 30 miles, to be proven on a voyage from Britain to the West Indies. The Act established a Board of Longitude to manage submissions, decide upon the prize, and, if necessary, grant additional money to support an inventor’s progress. The process created for the administration of the Longitude prize remains as relevant today as any modern prize-based example.</i><br /><br /><i>Keywords: Open innovation, Innovation prizes, Longitude prize</i><br /><br /><i>On the night of October 22, 1707, a fleet of Royal Navy warships was homeward bound in wind and rain. Far off course, they struck the rocks of the Scilly Isles off the coast of Cornwall, losing four ships and </i><b><i>2,000 men</i></b><i> in one of history’s greatest naval disasters. </i><b><i>This tragedy was to spark the first great open innovation challenge -- one that still has powerful lessons for innovation in the twenty-first century.</i></b><br /><br /><b><i>The fundamental problem for the hapless commander of the fleet, Admiral Sir Cloudesley Shovell, and for Britain, which depended on mastery of the seas for security and trade, was that it was very difficult to know the longitude of a ship at sea.</i></b><i> Latitude (distance north or south) was easily found by the angle of the North Star or noonday sun over the horizon. Determining one’s longitude -- east-west position relative to an arbitrary zero point -- was far more difficult. The ancient Greeks knew that the spherical earth made a full rotation per day, and therefore finding longitude was equivalent to knowing simultaneously the local time and the time at the reference point. Local time was easy to find by sun sighting; the problem hinged on knowing the time elsewhere. </i><b><i>Accuracy mattered: at the equator, the speed of the earth’s rotation is about 1,040 miles per hour or 17 miles per minute; at the latitude of London, it is 11 miles per minute. Even if the required sun sighting were perfect, a timepiece in error by a minute would produce this much error in position. All of this was well understood at the time of the disaster, and there had been serious attacks on the problem going back to Galileo. In Britain, King Charles II had founded the Royal Observatory in 1675 for the explicit purpose of addressing the longitude problem, but there had been no useful results nearly 40 years later. Many considered the accurate determination of longitude on a moving ship to be impossible.</i></b><br /><br /><i>But the Scilly Isles disaster could not be ignored, and so Parliament took a new approach with the Longitude Act of 1714. Enacted in the name of Queen Anne on behalf of the Admiralty, the Longitude Act offered a prize of £20,000 to whoever could produce a practicable method of determining longitude to within 30 miles, to be proven on a voyage from Britain to the West Indies. The Act established a Board of Longitude to manage submissions, decide upon the prize winner, and, if necessary, grant additional money to support an inventor’s progress.</i><br /><br /><i>The Act attracted the attention of John Harrison, a 21-year-old carpenter and clockmaker from Yorkshire who, with his father, had been making wooden pendulum clocks. </i><b><i>Pursuing the prize, he realized that pendulums were infeasible on a rolling ship so he developed a system of balanced springs as the oscillating heart of his timepieces.</i></b><i> He presented the Board with his brass and steel H1 model in 1736, which might have sufficed to win the prize. However, Harrison was a perfectionist and asked only for funds to continue improving his invention. Two £500 grants, two models, and 23 years later, Harrison produced the remarkable H4, the first true marine chronometer. The H4 resembled a large pocket watch and incorporated the newest technologies and several Harrison inventions: hardened steel springs, an automatic rewinder, diamond pivots, and a bimetallic temperature compensator. </i><b><i>It was more than accurate enough to win the prize.</i></b><i> As Dava Sobel (1995) vividly relates, Harrison suffered years of delay, jealously, and obstruction, but finally, with personal support from the King, he was vindicated and over his lifetime received £23,065 from Parliament. He lived long enough to hear Captain Cook sing his invention’s praises after using it to map the South Pacific.</i><br /><br /><i>The Longitude Act resulted in the invention of the marine chronometer, which solved the problem the Act sought to address. </i><b><i>But the Act itself was also an important invention: the well-defined, goal-oriented, open innovation challenge. Chronometers and sextants may be museum pieces today but the Longitude Act remains highly relevant as the invention of a process. Every aspect of the Act holds lessons for innovation challenges today, establishing best practices that every modern organization should consider:</i></b><br /><br />• <b><i>Clearly define the strategic importance of the initiative.</i></b><i> Even though the Scilly Isles tragedy was fresh in memory, the Act reminds the reader of the pressing need for better navigation to save lives and property, encourage commerce, benefit the Royal Navy, and bring honor to the kingdom. The potential contributor need not ask &#39;why should I bother?&#39;</i><br /><br />• <i>Establish high-level sponsorship. The Queen, Parliament, and the Admiralty are all named in the act as sponsors of the effort. Contributors could have confidence that submissions would be received seriously, and that meritorious solutions would be implemented and broadly applied as quickly as possible. Nor should anyone doubt that the reward, however huge, was credible.</i><br /><br />• <b><i>Define the desired outcome.</i></b><i> The goal is very simply stated: the determination of longitude to within half a degree or 30 miles. In fact, the Act sets different prize amounts for accuracies of 30, 40, and 60 miles.</i><br /><br />• <b><i>Provide for testing and verification.</i></b><i> The Act provides a clear measure for success: the method must achieve the stated accuracy on a voyage from Britain to the West Indies. Everyone would understand this to be a realistic test with no control over weather, duration of the voyage, or even the skill of the person conducting the test. This stipulation excludes astronomical methods which require clear skies and stable, careful observations.</i><br /><br />• <b><i>Establish a review team.</i></b><i> About half of the Act is taken up with establishing a Board of Longitude, naming its members, and spelling out rules and responsibilities. </i><b><i>The Longitude Act even provides for diversity in the board’s membership, striking an effective balance by including technical expertise (the Realm’s best scientists), those with direct responsibility for testing and implementation (plenty of Admirals), and political authority (members of Parliament and aristocrats).</i></b><i> By specifically naming the appointees, the Act makes them responsible to the public for fair decisions and results. Among the members was Sir Isaac Newton, then 71 years old and the president of the Royal Society, who had been consulted beforehand on the state of the art and prospects for success.</i><br /><br />• <b><i>Provide for wide distribution of information about the effort.</i></b><i> Royal proclamations were typically printed in at least a thousand copies and distributed to the sheriffs of counties and towns, who would post them (Brigham 1911). We know that the Act was widely distributed simply by the fact that John Harrison responded to it from the rural hamlet of Barrow-upon-Humber in the north of England, very far from London, Oxford, or Cambridge.</i><br /><br />• <b><i>Offer a meaningful reward.</i></b><i> In government or corporate R&amp;D departments, researchers are salaried while the organization absorbs the risk that goals might not be achieved. </i><b><i>But in a winner-take-all challenge, the risk of failure is borne by the contributors, who will have to devote unpaid time to the quest and absorb production costs for prototypes. Therefore the prize (and any interim grants) must be sufficient to induce talented people to take such risks, because for some it may mean turning away from conventional employment.</i></b><i> Certainly £20,000 sufficed in 1714, when annual wages were £13-£30 (Williamson 1982). </i><b><i>The Act wisely offers lesser prizes for less accurate solutions, and even some money for attempts that don’t make the grade at all. Recent evidence suggests that this an important part of reward structure, to spread risk and promote diverse solutions: a single-winner prize may bias toward a precise but conservative approach, while healthy second and third prizes elicit more radical, less certain attempts</i></b><i> (DiJusto 2006).</i><br /><br />• <b><i>Strive for brevity and simplicity.</i></b><i> Even in the florid language of the time, the Act is only 1200 words long, just two or three single-spaced pages. By comparison, just the legal terms at Innocentive (not including information about any particular challenge) weigh in at 2860 words. [1]</i><br /><br />• <b><i>Open the effort to the widest field possible.</i></b><i> The Act specified only that the reward be offered to &#39;Such Person or Persons as Shall Discover the Longitude at Sea.&#39; There was no restriction in participation by rank, privilege, education, nationality, or gender. Harrison was in fact a self-educated commoner.</i><br /><br />• <b><i>Don’t suggest or exclude particular solutions.</i></b><i> The Longitude Act set forth a simply stated goal with no mention of any specific method or solution that might lead to success -- or any discussion of solutions that should be avoided. It did not matter that the Royal Observatory had failed to deliver an astronomical solution. It did not matter that Newton had implied to the Parliamentary authors of the Act that a timepiece of sufficient accuracy might not be feasible (Brewster 1855). Rather, </i><b><i>the Act left the field of possible solutions entirely open, and potential contributors free to pursue their own divergent ideas.</i></b><br /><br /><i>Stories of challenges tend to be heroic and compelling: Harrison and the Longitude problem, Lindbergh and the solo crossing of the Atlantic (yes, he did it for prize money). But challenges have been minor contributors in the history of innovation; Morgan (2008) struggles to come up with more than a handful of significant advances resulting from challenges. In any comparison of results, the continuously funded R&amp;D laboratory wins hands down -- consider Edison’s Menlo Park, Bell Labs, NASA, and the combined experience of the aerospace, pharmaceutical, and semiconductor industries. </i><b><i>By comparison, external challenges seem to be a tool of last resort, to be employed if all else fails or to fill in the gaps in an organization’s mainstream innovation pipeline.</i></b><br /><br /><i>Still, the challenge approach is worth our attention today. John Seely Brown and John Hagel (2005) note that ever since Alfred Sloan, corporations have been dominated by a &#39;push&#39; approach to innovation: problems are identified and prioritized and resources are assembled and pushed at the problem by management, with an emphasis on control and efficiency. This is fine if the world is predictable, controllable, and stable over the time it takes to do all that analysis and organization. But what Gary Pisano (2006) says about the pharmaceutical-biotech sector applies nearly everywhere now: </i><b><i>businesses can survive and thrive only by mastering complexity, uncertainty, and an ever-increasing speed of change. Seely Brown and Hagel’s alternative to &#39;push&#39; is &#39;pull,&#39; in which uncertainty reigns, participation is open, control is decentralized, and the emphasis is on innovation rather than control or efficiency.</i></b><i> The challenge model is tailor-made to support &#39;pull&#39; innovation in a tight global economic climate and the internet age:</i><br /><br />• <i>Challenges have a simpler and less expensive &#39;supply chain&#39; than in-house R&amp;D. Just as manufacturing has gotten more efficient by supply chain management, </i><b><i>a challenge can be run &#39;just in time&#39; without continuous support for an R&amp;D department.</i></b><br /><br />• <b><i>&#39;Push&#39; puts financial risk on the organization which has to maintain R&amp;D that may or may not produce desired outputs. &#39;Pull&#39; removes that risk by paying out only upon demonstration that goals having been achieved.</i></b><i> The marketplace will put a price to that risk-removal: a proven solution will cost more than a preliminary one. But when cash is tight and uncertainty high, pull becomes more attractive.</i><br /><br />• <i>Pull is more agile and responsive. Push cycles of R&amp;D analysis, goals, and funding are typically annual, while a challenge can be drawn up and launched at any time to address a sudden need or opportunity.</i><br /><br />• <i>Business is truly global. Challenges can be run at a global scale or targeted at an emerging market in a faraway place, thereby tapping into opinions, culture, and knowledge far from the company’s home office. Note that a challenge does not have to be about a difficult technical problem; it can be as simple as asking for people to write an essay about what they think of your new clothing styles in exchange for discount coupons.</i><br /><br />• <i>With the internet, communication has become essentially free, and importantly it is direct-to-contributor and independent of hierarchy.</i><br /><br />• <b><i>The precedents are there: not just for DARPA robot vehicles and X Prizes, but in Proctor and Gamble’s overt reliance on outside knowledge for its innovation pipeline</i></b><i> (Huston and Sakkab 2006).</i><br /><br /><b><i>Notice all the echoes of the Longitude story: important problems, insufficient in-house R&amp;D, previously unknown expertise found by far-reaching yet direct communication, cost deferred until a solution is proven. The only aspect of the Longitude challenge that doesn’t fit today is its time scale. It took forty years to realize that the Royal Observatory wasn’t going to solve the problem, a crisis, and another fifty to achieve the H4 chronometer and begin moving it into production. Keep all of the other lessons (sponsorship, purpose, measurables, review process, sufficient reward), speed things up twenty-fold, and the Longitude Act of 1714 stands as a model for challenge-based open innovation in the twenty-first century.&quot;</i></b><br /><br />References<br /><br />Brown, J. S., and Hagel, J. 2005. From push to pull: The next frontier of innovation. McKinsey Quarterly 3, August.<br /><a href=\"https://www.mckinseyquarterly.com/Strategy/Strategic_Thinking/From_push_to_pull_The_next_frontier_of_innovation_1642?gp=1\" class=\"ot-anchor\">https://www.mckinseyquarterly.com/Strategy/Strategic_Thinking/From_push_to_pull_The_next_frontier_of_innovation_1642?gp=1</a>.<br /><br />DiJusto, P. 2006. Winner takes all: Are tech prizes the best way to the cutting edge? Scientific American 294(2), February: 25.<br /><a href=\"http://www.scientificamerican.com/article.cfm?id=winner-takes-all\" class=\"ot-anchor\">http://www.scientificamerican.com/article.cfm?id=winner-takes-all</a>.<br /><br />Huston, L., Sakkab, N. 2006. Connect and Develop: Inside Procter &amp; Gamble&#39;s New Model for Innovation, Harvard Business Review, 84:3, March 2006, 58-66.<br /><br />Morgan, J. 2008. Inducing innovation through prizes. Innovations 3(4): 105–117.<br /><br />Pisano, G. 2006. Science Business. Cambridge, MA: Harvard Business School Press.<br /><br />Readers interested in John Harrison and the Longitude Act can find a number of interesting resources online and in print:<br /><br />Betts, J. 2006. Time Restored: The Story of the Harrison Timekeepers and R.T. Gould, “The Man who Knew (Almost) Everything.” Cambridge, England: NMM &amp; Oxford. Extracts available as “John Harrison (1693–1776) and Lt. Cdr Rupert T.<br />Gould R.N. (1890–1948)” at<br /><a href=\"http://www.rmg.co.uk/upload/pdf/Gould-Harrisonlongitude-JBetts.pdf\" class=\"ot-anchor\">http://www.rmg.co.uk/upload/pdf/Gould-Harrisonlongitude-JBetts.pdf</a>.<br /><br />Brewster, D. 1855. Chapter XXI. Memoirs of the Life, Writings, and Discoveries of Sir Isaac Newton, vol. 2. Edinborough. Reprinted in Iliffe, R., and Mandelbrote, S., The Newton Project,<br /><a href=\"http://www.newtonproject.sussex.ac.uk/view/texts/normalized/OTHE00082\" class=\"ot-anchor\">http://www.newtonproject.sussex.ac.uk/view/texts/normalized/OTHE00082</a>.<br /><br />Brigham, C. 1911. British Royal Proclamations Relating to America 1603-1783. New York: Burt Franklin, viii.<br /><a href=\"http://books.google.com/books?id=Y1HERbRKEboC&amp;pg=PR8\" class=\"ot-anchor\">http://books.google.com/books?id=Y1HERbRKEboC&amp;pg=PR8</a><br /><br />Harrison, J., Maskelyne, N. 1767. The Principles of Mr. Harrison&#39;s Time-Keeper, with Plates of the Same. London: W. Richardson and S. Clarke.<br /><a href=\"http://www.christies.com/LotFinder/lot_details.aspx?intObjectID=5216963\" class=\"ot-anchor\">http://www.christies.com/LotFinder/lot_details.aspx?intObjectID=5216963</a><br /><br />O’Donnell, J. 2002. John Harrison and the longitude problem. Royal Museums<br />Greenwich Astronomy Fact File. <a href=\"http://www.rmg.co.uk/harrison\" class=\"ot-anchor\">http://www.rmg.co.uk/harrison</a>.<br /><br />Siegel, J. 2009. Law and longitude. Tulane Law Review 84(1): 1–66.<br /><a href=\"http://docs.law.gwu.edu/facweb/jsiegel/publications/Longitude.pdf\" class=\"ot-anchor\">http://docs.law.gwu.edu/facweb/jsiegel/publications/Longitude.pdf</a>. The Longitude Act itself is reprinted on pp 64–66.<br /><br />Sobel, D. 1995. Longitude. London: Fourth Estate.<br /><br />Williamson, J. 1982. Great Britain: Nominal Annual Earnings for Various Occupations in England and Wales. The Marteau Early 18th-Century Currency Converter.<br /><a href=\"http://www.pierre-marteau.com/currency/indices/uk-03.html\" class=\"ot-anchor\">http://www.pierre-marteau.com/currency/indices/uk-03.html</a>.<br /><br />GRAPHIC REFERENCES:<br /><br />[1] <a href=\"http://scaledinnovation.com/innovation/publications/2012-06-longitude.pdf\" class=\"ot-anchor\">http://scaledinnovation.com/innovation/publications/2012-06-longitude.pdf</a><br /><br />[2] <a href=\"http://scaledinnovation.com/innovation/conferences/2010-09-open_pharma.pdf\" class=\"ot-anchor\">http://scaledinnovation.com/innovation/conferences/2010-09-open_pharma.pdf</a><br /><br />[3] <a href=\"http://scaledinnovation.com/innovation/publications/2010-06-longtail.pdf\" class=\"ot-anchor\">http://scaledinnovation.com/innovation/publications/2010-06-longtail.pdf</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Connector","summary":"Being in an Open Instead of a Closed Network is the Best Predictor of Career Success / The More Closed the Network, the More You Repeatedly Hear the Same Ideas - which Reaffirm what You Already Believe / It’s Easier to Get Things Done in Closed Groups Because You’ve Built Up Trust, and You Know All the Shorthand Terms and Unspoken Rules / It’s Comfortable Because the Group Converges on the Same Ways of Seeing the World that Confirm Your Own / But, Connectors have a More Accurate View of the World Because the Errors Cancel Themselves Out when the Information Comes from Diverse Clusters","image":"https://lh3.googleusercontent.com/-PVZR4IMXeyE/WDpYUBFB_II/AAAAAAAAJUE/vpjaqXi9NqkRGEYPwMWv7gnOoM_F8UZNQCJoC/w7142-h9999/the-connector-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/RA8mn7tAJ7R","publishDate":"2016-11-27T03:52:21.270Z","updateDate":"2017-01-03T01:45:19.383Z","text":"<b>The Connector: Being in an Open Instead of a Closed Network is the Best Predictor of Career Success / The More Closed the Network, the More You Repeatedly Hear the Same Ideas - which Reaffirm what You Already Believe / It’s Easier to Get Things Done in Closed Groups Because You’ve Built Up Trust, and You Know All the Shorthand Terms and Unspoken Rules / It’s Comfortable Because the Group Converges on the Same Ways of Seeing the World that Confirm Your Own / But, Connectors have a More Accurate View of the World Because the Errors Cancel Themselves Out when the Information Comes from Diverse Clusters</b><br /><br /><a href=\"https://medium.com/the-mission/the-number-one-predictor-of-career-success-according-to-network-science#.vdzyojcom\" class=\"ot-anchor\">https://medium.com/the-mission/the-number-one-predictor-of-career-success-according-to-network-science#.vdzyojcom</a><br /><br />Michael Simmons<br />Award-Winning Entrepreneur / Bestselling Author / Forbes Columnist / Co-Founder of @iEmpact/ Blogger at <a href=\"http://t.co/T32xDLUBLJ\" class=\"ot-anchor\">http://t.co/T32xDLUBLJ</a><br />Aug 10, 2015<br /><br />The No. 1 Predictor Of Career Success According To Network Science<br /><br /><i>&quot;It has been over three years since Steve Jobs died.</i><br /><br /><i>Since then, books have been written and movies have been made.</i><br /><br /><i>Each has celebrated his legacy and aimed to share the secrets he used to build the largest company in the world; things like attention to detail, attracting world-class talent and holding them to high standards.</i><br /><br /><i>We think we understand what caused his success.</i><br /><br /><i>We don’t.</i><br /><br /><i>We dismiss usable principles of success by labeling them as personality quirks.</i><br /><br /><b><i>What’s often missed is the paradoxical interplay of two of his seemingly opposite qualities; maniacal focus and insatiable curiosity.</i></b><i> These weren’t just two random strengths. They may have been his most important as they helped lead to everything else.</i><br /><br /><i>Jobs’ curiosity fueled his passion and provided him with access to unique insights, skills, values, and world-class people who complemented his own skillset. Job’s focus brought those to bear in the world of personal electronics.</i><br /><br /><i>I don’t just say this as someone who has devoured practically every article, interview, and book featuring him.</i><br /><br /><i>I say this as someone who has interviewed many of the world’s top network scientists on a quest to understand how networks create competitive advantage in business and careers.</i><br /><br /><i>The Simple Variable That Explains What Really Causes Career Success</i><br /><br /><i>In December of 2013, I interviewed one of the world’s top network scientists, Ron Burt. During it, he shared a chart that completely flipped my understanding of success. Here is a simplified version:&quot;</i><br /><br />[See graphic]<br /><br /><i>&quot;The bottom line? </i><b><i>According to multiple, peer-reviewed studies, simply being in an open network instead of a closed one is the best predictor of career success.</i></b><br /><br /><b><i>In the chart, the further to the right you go toward a closed network, the more you repeatedly hear the same ideas, which reaffirm what you already believe. The further left you go toward an open network, the more you’re exposed to new ideas. People to the left are significantly more successful than those to the right.</i></b><br /><br /><i>In fact, </i><b><i>the study shows that half of the predicted difference in career success (i.e., promotion, compensation, industry recognition) is due to this one variable.</i></b><br /><br /><b><i>Do you ever have moments where you hear something so compelling that you need to know more, yet so crazy that you’d have to let go of some of your core beliefs in order to accept the idea?</i></b><br /><br /><i>This was one of those moments for me. Never in all of the books I had read on self-help, career success, business, or Steve Jobs had I come across this idea.</i><br /><br /><i>I wondered, &#39;How is it possible that the structure of one’s network could be such a powerful predictor for career success?&#39;</i><br /><br /><i>How A Closed Network Impacts Your Career</i><br /><br /><i>To understand the power of open networks, it’s important to understand their opposite.</i><br /><br /><i>Most people spend their careers in closed networks; networks of people who already know each other. People often stay in the same industry, the same religion, and the same political party. </i><b><i>In a closed network, it’s easier to get things done because you’ve built up trust, and you know all the shorthand terms and unspoken rules. It’s comfortable because the group converges on the same ways of seeing the world that confirm your own.</i></b><br /><br /><i>To understand why people spend most of their time in closed networks, consider what happens when a group of random strangers is thrown together:&quot;</i><br /><br />[See circle in graphic]<br /><br /><i>&quot;David Rock, the founder of the Neuroleadership Institute, the top organization helping leaders through neuroscience research, explains the process well:</i><br /><br /><i>&#39;We’ve evolved to put people in our ingroup and outgroup. We put most people in our outgroup and a few people in our ingroup. It determines whether we care about others. It determines whether we support or attack them. The process is a byproduct of our evolutionary history where we lived in small groups and strangers we didn’t know well weren’t to be trusted.&#39;</i><br /><br /><i>By understanding this process, we can begin to understand why the world is the way it is. We understand why Democrats and Republicans can’t pass bills with obvious benefits to society. We understand why religions have gone to war over history. It helps us understand why we have bubbles, panics, and fads.</i><br /><br /><i>The Surprising Power And Pain Of Open Networks</i><br /><br /><b><i>People in open networks have unique challenges and opportunities. Because they’re part of multiple groups, they have unique relationships, experiences, and knowledge that other people in their groups don’t.</i></b><br /><br /><b><i>This is challenging in that it can lead to feeling like an outsider as a result of being misunderstood and under-appreciated because few people understand why you think the way you do. It is also challenging, because it requires assimilating different and conflicting perspectives into one worldview.</i></b><br /><br /><i>In one of my all-time favorite movies, The Matrix, the main character, Neo, is exposed to a completely new world. Once he is, he can’t go back. He’s an outsider in the new group, and he’s an outsider in his old life. He’s had an experience that everyone he’s ever met would never understand. This same phenomenon happens when we enter new worlds of people.</i><br /><br /><i>On the other hand, having an open network is a huge opportunity in a few ways:</i><br /><br />- <b><i>More accurate view of the world. It provides them with the ability to pull information from diverse clusters so errors cancel themselves out. Research by Philip Tetlock shows that people with open networks are better forecasters than people with closed networks.</i></b><br /><br />- <b><i>Ability to control the timing of information sharing. While they may not be the first to hear information, they can be the first to introduce information to another cluster. As a result, they can leverage the first move advantage.</i></b><br /><br />- <b><i>Ability to serve as a translator / connector between groups. They can create value by serving as an intermediary and connecting two people or organizations who can help each other who wouldn’t normally run into each other.</i></b><br /><br />- <b><i>More breakthrough ideas.</i></b><i> Brian Uzzi, Professor of Leadership and Organizational Change at the Kellogg School of Management, performed a landmark study where he delved into the tens of millions of academic studies throughout history. He compared their results by the number of citations (links from other research papers) they received and the other papers they referenced. A fascinating pattern emerged. The top performing studies had references that were 90% conventional and 10% atypical (i.e., pulling from other fields). This rule has held constant over time and across fields. People with open networks are more easily able to create atypical combinations.</i><br /><br /><i>The Revisionist Timeline Of Steve Jobs Success</i><br /><br /><i>As a result of pursuing his curiosity in different fields throughout his life, Steve Jobs developed an extremely unique perspective, skillset, and network; one that no one else in the computer industry had. He turned these unique advantages into the largest company in the world by having a razor sharp focus. Within Apple, he cut out people, products, and systems that weren’t world-class.&quot;</i><br /><br /><b><i>Curiosity-Based Experience</i></b><br />--&gt; <b><i>Application</i></b><br /><br /><i>Tinkering with machinery with his father</i><br />--&gt; <i>Understanding craftsmanship and attention to detail</i><br /><br /><i>Dropping out of college and sitting in on a calligraphy class</i><br />--&gt; <i>Appreciation of design (Macintosh&#39;s varied fonts)</i><br /><br /><i>Exploring India and buddhism</i><br />--&gt; <i>Apple&#39;s simple aesthetic</i><br /><br /><i>Living on an Apple orchard</i><br />--&gt; <i>The inspiration for the Mac logo</i><br /><br /><i>Pursuing his hobby electronics in the Home Brew computer club</i><br />--&gt; <i>Creating the first Mac with Steve Wozniack</i><br /><br /><i>Starting NeXT during his wilderness years</i><br />--&gt; <i>Using NeXT&#39;s operating system as a core in the new MAC operating system</i><br /><br /><i>Lifelong passion for music (particularly U2, Beatles, John Lennon)</i><br />--&gt; <i>Launch of iTunes</i><br /><br /><i>&quot;Many are quick to label parts of Steve Jobs’ life as the ‘lost’ or ‘wilderness’ years. However, when we view his life in retrospect, we see that his diversions were critical to his success.</i><br /><br /><i>What is labeled as the magic of Steve Jobs or the quirks of his character become replicable principles we can all follow.</i><br /><br /><i>It is from this vantage point that we can begin to understand the following quote from a Steve Jobs interview for Wired in 1995:</i><br /><br /><b><i>&#39;Creativity is just connecting things. When you ask creative people how they did something, they feel a little guilty because they didn’t really do it, they just saw something.</i></b><br /><br /><b><i>It seemed obvious to them after a while. That’s because they were able to connect experiences they’ve had and synthesize new things. And the reason they were able to do that was that they’ve had more experiences or they have thought more about their experiences than other people.</i></b><br /><br /><b><i>Unfortunately, that’s too rare a commodity. A lot of people in our industry haven’t had very diverse experiences.</i></b><br /><br /><b><i>So they don’t have enough dots to connect, and they end up with very linear solutions without a broad perspective on the problem. The broader one’s understanding of the human experience, the better design we will have.&#39;</i></b><br /><br /><i>Stay Hungry. Stay Foolish.</i><br /><br /><i>Throughout human history, all societies including our own have created myths that share one common element, the hero’s journey.</i><br /><br /><i>Here’s what the journey looks like according to Joseph Campbell, the originator of the term ...</i><br /><br /><b><i>Things are going great. You feel normal and fit in. Then something happens, and you change. You start to feel like an outsider in your own culture.</i></b><i> You hide parts of yourself to fit in, but that doesn’t help. You feel called to leave and fulfill part of yourself, but that has a lot of uncertainty. So, you hesitate at first.</i><br /><br /><i>Finally, you take the plunge. You go through difficult times as you’re learning to navigate the new world. Finally, you overcome the challenges. </i><b><i>Then, you go back to your old culture and have a huge impact because you share the unique insights you’ve learned.</i></b><br /><br /><i>The hero’s journey myth is embedded in everything from our society’s classic movies (i.e., Star Wars) to the heroes we glorify (i.e., Steve Jobs), because it hits on core parts of the human experience.</i><br /><br /><i>The field of network science shows us two things. (1) The hero’s journey is the blueprint for creating career success. (2) We can all be heroes. It just takes a little faith as you follow your heart and curiosity into unknown worlds. As Steve Jobs said, </i><b><i>&#39;You can’t connect the dots looking forward; you can only connect them looking backwards. So you have to trust that the dots will somehow connect in your future.&#39;&quot;</i></b><br /><br />Some may prefer to simply watch the video at this link ...<br /><br /><a href=\"http://news.stanford.edu/2005/06/14/jobs-061505/\" class=\"ot-anchor\">http://news.stanford.edu/2005/06/14/jobs-061505/</a><br /><br />&#39;You&#39;ve got to find what you love,&#39; Jobs says<br /><br />This is a prepared text of the Commencement address delivered by Steve Jobs, CEO of Apple Computer and of Pixar Animation Studios, on June 12, 2005.<br /><br />JUNE 14, 2005<br /><br /><i>&quot;I am honored to be with you today at your commencement from one of the finest universities in the world. I never graduated from college. Truth be told, this is the closest I’ve ever gotten to a college graduation. Today I want to tell you three stories from my life. That’s it. No big deal. Just three stories.</i><br /><br /><i>The first story is about connecting the dots.</i><br /><br /><i>I dropped out of Reed College after the first 6 months, but then stayed around as a drop-in for another 18 months or so before I really quit. So why did I drop out?</i><br /><br /><i>It started before I was born. My biological mother was a young, unwed college graduate student, and she decided to put me up for adoption. She felt very strongly that I should be adopted by college graduates, so everything was all set for me to be adopted at birth by a lawyer and his wife. Except that when I popped out they decided at the last minute that they really wanted a girl. So my parents, who were on a waiting list, got a call in the middle of the night asking: &#39;We have an unexpected baby boy; do you want him?&#39; They said: &#39;Of course.&#39; My biological mother later found out that my mother had never graduated from college and that my father had never graduated from high school. She refused to sign the final adoption papers. She only relented a few months later when my parents promised that I would someday go to college.</i><br /><br /><i>And 17 years later I did go to college. But I naively chose a college that was almost as expensive as Stanford, and all of my working-class parents’ savings were being spent on my college tuition. After six months, I couldn’t see the value in it. I had no idea what I wanted to do with my life and no idea how college was going to help me figure it out. And here I was spending all of the money my parents had saved their entire life. So I decided to drop out and trust that it would all work out OK. It was pretty scary at the time, but looking back it was one of the best decisions I ever made. The minute I dropped out I could stop taking the required classes that didn’t interest me, and begin dropping in on the ones that looked interesting.</i><br /><br /><i>It wasn’t all romantic. I didn’t have a dorm room, so I slept on the floor in friends’ rooms, I returned Coke bottles for the 5¢ deposits to buy food with, and I would walk the 7 miles across town every Sunday night to get one good meal a week at the Hare Krishna temple. I loved it. And much of what I stumbled into by following my curiosity and intuition turned out to be priceless later on. Let me give you one example:</i><br /><br /><i>Reed College at that time offered perhaps the best calligraphy instruction in the country. Throughout the campus every poster, every label on every drawer, was beautifully hand calligraphed. Because I had dropped out and didn’t have to take the normal classes, I decided to take a calligraphy class to learn how to do this. I learned about serif and sans serif typefaces, about varying the amount of space between different letter combinations, about what makes great typography great. It was beautiful, historical, artistically subtle in a way that science can’t capture, and I found it fascinating.</i><br /><br /><b><i>None of this had even a hope of any practical application in my life. But 10 years later, when we were designing the first Macintosh computer, it all came back to me. And we designed it all into the Mac. It was the first computer with beautiful typography. If I had never dropped in on that single course in college, the Mac would have never had multiple typefaces or proportionally spaced fonts. And since Windows just copied the Mac, it’s likely that no personal computer would have them. If I had never dropped out, I would have never dropped in on this calligraphy class, and personal computers might not have the wonderful typography that they do. Of course it was impossible to connect the dots looking forward when I was in college. But it was very, very clear looking backward 10 years later.</i></b><br /><br /><b><i>Again, you can’t connect the dots looking forward; you can only connect them looking backward. So you have to trust that the dots will somehow connect in your future. You have to trust in something -- your gut, destiny, life, karma, whatever. This approach has never let me down, and it has made all the difference in my life.</i></b><br /><br /><i>My second story is about love and loss.</i><br /><br /><i>I was lucky -- I found what I loved to do early in life. Woz and I started Apple in my parents’ garage when I was 20. We worked hard, and in 10 years Apple had grown from just the two of us in a garage into a $2 billion company with over 4,000 employees. We had just released our finest creation -- the Macintosh -- a year earlier, and I had just turned 30. And then I got fired. How can you get fired from a company you started? Well, as Apple grew we hired someone who I thought was very talented to run the company with me, and for the first year or so things went well. But then our visions of the future began to diverge and eventually we had a falling out. When we did, our Board of Directors sided with him. So at 30 I was out. And very publicly out. What had been the focus of my entire adult life was gone, and it was devastating.</i><br /><br /><i>I really didn’t know what to do for a few months. I felt that I had let the previous generation of entrepreneurs down -- that I had dropped the baton as it was being passed to me. I met with David Packard and Bob Noyce and tried to apologize for screwing up so badly. I was a very public failure, and I even thought about running away from the valley. But something slowly began to dawn on me -- </i><b><i>I still loved what I did. The turn of events at Apple had not changed that one bit. I had been rejected, but I was still in love. And so I decided to start over.</i></b><br /><br /><b><i>I didn’t see it then, but it turned out that getting fired from Apple was the best thing that could have ever happened to me. The heaviness of being successful was replaced by the lightness of being a beginner again, less sure about everything. It freed me to enter one of the most creative periods of my life.</i></b><br /><br /><i>During the next five years, I started a company named NeXT, another company named Pixar, and fell in love with an amazing woman who would become my wife. Pixar went on to create the world’s first computer animated feature film, Toy Story, and is now the most successful animation studio in the world. In a remarkable turn of events, Apple bought NeXT, I returned to Apple, and the technology we developed at NeXT is at the heart of Apple’s current renaissance. And Laurene and I have a wonderful family together.</i><br /><br /><i>I’m pretty sure none of this would have happened if I hadn’t been fired from Apple. It was awful tasting medicine, but I guess the patient needed it. Sometimes life hits you in the head with a brick. Don’t lose faith. I’m convinced that the only thing that kept me going was that I loved what I did. You’ve got to find what you love. And that is as true for your work as it is for your lovers. Your work is going to fill a large part of your life, and the only way to be truly satisfied is to do what you believe is great work. And the only way to do great work is to love what you do. If you haven’t found it yet, keep looking. Don’t settle. As with all matters of the heart, you’ll know when you find it. And, like any great relationship, it just gets better and better as the years roll on. So keep looking until you find it. Don’t settle.</i><br /><br /><i>My third story is about death.</i><br /><br /><b><i>When I was 17, I read a quote that went something like: &#39;If you live each day as if it was your last, someday you’ll most certainly be right.&#39; It made an impression on me, and since then, for the past 33 years, I have looked in the mirror every morning and asked myself: &#39;If today were the last day of my life, would I want to do what I am about to do today?&#39; And whenever the answer has been &#39;No&#39; for too many days in a row, I know I need to change something.</i></b><br /><br /><b><i>Remembering that I’ll be dead soon is the most important tool I’ve ever encountered to help me make the big choices in life. Because almost everything -- all external expectations, all pride, all fear of embarrassment or failure -- these things just fall away in the face of death, leaving only what is truly important. Remembering that you are going to die is the best way I know to avoid the trap of thinking you have something to lose. You are already naked. There is no reason not to follow your heart.</i></b><br /><br /><i>About a year ago I was diagnosed with cancer. I had a scan at 7:30 in the morning, and it clearly showed a tumor on my pancreas. I didn’t even know what a pancreas was. The doctors told me this was almost certainly a type of cancer that is incurable, and that I should expect to live no longer than three to six months. My doctor advised me to go home and get my affairs in order, which is doctor’s code for prepare to die. It means to try to tell your kids everything you thought you’d have the next 10 years to tell them in just a few months. It means to make sure everything is buttoned up so that it will be as easy as possible for your family. It means to say your goodbyes.</i><br /><br /><i>I lived with that diagnosis all day. Later that evening I had a biopsy, where they stuck an endoscope down my throat, through my stomach and into my intestines, put a needle into my pancreas and got a few cells from the tumor. I was sedated, but my wife, who was there, told me that when they viewed the cells under a microscope the doctors started crying because it turned out to be a very rare form of pancreatic cancer that is curable with surgery. I had the surgery and I’m fine now.</i><br /><br /><i>This was the closest I’ve been to facing death, and I hope it’s the closest I get for a few more decades. Having lived through it, I can now say this to you with a bit more certainty than when death was a useful but purely intellectual concept:</i><br /><br /><b><i>No one wants to die. Even people who want to go to heaven don’t want to die to get there. And yet death is the destination we all share. No one has ever escaped it. And that is as it should be, because Death is very likely the single best invention of Life. It is Life’s change agent. It clears out the old to make way for the new. Right now the new is you, but someday not too long from now, you will gradually become the old and be cleared away. Sorry to be so dramatic, but it is quite true.</i></b><br /><br /><b><i>Your time is limited, so don’t waste it living someone else’s life. Don’t be trapped by dogma -- which is living with the results of other people’s thinking. Don’t let the noise of others’ opinions drown out your own inner voice. And most important, have the courage to follow your heart and intuition. They somehow already know what you truly want to become. Everything else is secondary.</i></b><br /><br /><i>When I was young, there was an amazing publication called The Whole Earth Catalog, which was one of the bibles of my generation. It was created by a fellow named Stewart Brand not far from here in Menlo Park, and he brought it to life with his poetic touch. This was in the late 1960s, before personal computers and desktop publishing, so it was all made with typewriters, scissors and Polaroid cameras. It was sort of like Google in paperback form, 35 years before Google came along: It was idealistic, and overflowing with neat tools and great notions.</i><br /><br /><i>Stewart and his team put out several issues of The Whole Earth Catalog, and then when it had run its course, they put out a final issue. It was the mid-1970s, and I was your age. On the back cover of their final issue was a photograph of an early morning country road, the kind you might find yourself hitchhiking on if you were so adventurous. Beneath it were the words: </i><b><i>&#39;Stay Hungry. Stay Foolish.&#39; It was their farewell message as they signed off. Stay Hungry. Stay Foolish. And I have always wished that for myself. And now, as you graduate to begin anew, I wish that for you.</i></b><br /><br /><i>Stay Hungry. Stay Foolish.</i><br /><br /><i>Thank you all very much.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"Turning Data Into Insight","summary":"Academic Researchers Could Learn a Lot about How to Think about Scientific Controversies from How Corporate Researchers Study Consumers / Good Consumer Researchers Only Form a Hunch After Taking the Broadest View Possible and Studying Each of the Outliers / They then Create an Abundance of Ideas to Test / The Point of the Research Process is to Identify the Insight, a Universal \"Human Truth\" / Many Corporate Researchers Never Make it that Far","image":"https://lh3.googleusercontent.com/-w5Vy7JPOc3g/WDH3S1o_ZvI/AAAAAAAAJTA/PClcXGhsorIjPiOB6nhdp8s7utIBq99IQCJoC/w3600-h5040/turning-data-into-insight-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/84U3KskkPbY","publishDate":"2016-11-20T19:20:00.313Z","updateDate":"2017-01-03T01:45:45.635Z","text":"<b>Turning Data Into Insight: Academic Researchers Could Learn a Lot about How to Think about Scientific Controversies from How Corporate Researchers Study Consumers / Good Consumer Researchers Only Form a Hunch After Taking the Broadest View Possible and Studying Each of the Outliers / They then Create an Abundance of Ideas to Test / The Point of the Research Process is to Identify the Insight, a Universal &quot;Human Truth&quot; / Many Corporate Researchers Never Make it that Far: They Confuse the Data for Insight</b><br /><br />Academics have yet to formulate a general process which can consistently take them from data to insight when the textbooks become contested.  They&#39;ve been trained to force-fit the existing set of specialist solutions to the observations.  This can lend a false sense of confidence in the consensus, since the conclusions were largely predetermined before the observations were even made.<br /><br />Corporate innovation, by contrast, is a level more complex than this insofar as it operates at the level of competing worldviews.  When there&#39;s a financial expectation that an idea must be correct (the insight must generate profits), researchers interact with their data in fundamentally different ways: Suddenly, other opposing worldviews matter.<br /><br />All of the lessons detailed below will prove crucial to constructing a social network focused on problem-solving scientific controversies.  Where academics tend to go wrong on controversies is that they seek out their physical truth without considering the rich ecosystem of worldviews to interpret the data from: Although we are seeking out fundamental physical truths about how the universe works, streamlining the process requires deep understandings about the social construction of innovative ideas.<br /><br /><a href=\"http://secondcityworks.com/improvisers-guide-finding-insight-five-ways-get-data-insight/\" class=\"ot-anchor\">http://secondcityworks.com/improvisers-guide-finding-insight-five-ways-get-data-insight/</a><br /><br />An Improviser’s Guide To Finding Insight Or, Five Ways To Get From Data To Insight<br /><br />THOUGHTS - Nov 02, 2016<br />By Andy Eninger - Design Lead<br /><br /><i>&quot;There is a moment in the creation of every Second City sketch revue where a scene is in precarious shape: The director and the cast know they have something there, but it’s not there yet. The audience doesn’t get it. Or they straight up don’t like it. </i><b><i>The creators haven’t found the truth yet</i></b><i> -- otherwise known as the ‘Awful Truth,’ that recognition of some audience reality represented in some unexpected way.</i><br /><br /><i>It’s either too obvious (Rush hour sucks!)</i><br /><br /><i>Or true for only part of the audience (Become an actor so you never have to drive during rush hour…because you can’t afford a car.)</i><br /><br /><i>Or too weird (Rush hour is like a party for cars to get together)</i><br /><br /><i>At Second City, we’re obsessed with finding the human truth. We work hard to understand our audience’s fears, pains and frustrations. We don’t stop at merely observing the facts, we search for the why behind the facts to help our audiences understand their world.</i><br /><br /><i>In other words, we look for </i><b><i>insight</i></b><i>.</i><br /><br /><i>In our work with marketers, we’ve heard the word &#39;Insight&#39; used to mean different things, ranging from research data to trends to those big-ticket, sexy strategic insights. People throw around the word Insight like Altoids at a bad breath convention. That performance metric for a streaming pre-roll ad? INSIGHT! That purchase analysis of teen boys? INSIGHT! My youngest child’s name? INSIGHT!</i><br /><br /><b><i>Data is important, but it’s not necessarily insightful on its own. It can be the building block to insight. It can inform strategic decisions. But data is not insight. It requires human beings to elevate it into insight.</i></b><br /><br /><i>In the same way that our shows succeed when they nail the human truth, great creative marketing work stands out when it is based on strategic insights, the kind that can’t be summed up in an Excel sheet. They build upon the data to add 50% more empathy for human behavior, and 100% more &#39;I wish I’d thought of it that way!&#39;</i><br /><br /><i>&#39;But,&#39; you’re thinking, &#39;I don’t have time to come up with a sexy insight, I’m too busy graphing up this research results on my deck and calling it insight because I have to include an insight slide and that’s all I have to work with!&#39;</i><br /><br /><i>Don’t worry; the same techniques that help us discover human insights for satire can help you craft more powerful insights (jokes not included.) The distance from your observable data to great insights is shorter than you think. You just need to look at the facts like an improviser.</i><br /><br /><b><i>Here are five ways to think like an improviser to turn Data into Insight:</i></b><br /><br /><b><i>Listen Longer</i></b><br /><br /><i>The best improvisers are also the best listeners. When an audience member shouts a suggestion, we listen to more than the words. We listen to the emotion in the their voice and the way the rest of the audience reacts. </i><b><i>When we try out a new idea, we watch the audience’s reaction to the scene moment by moment -- and then we test it across multiple audiences: One great reaction doesn’t mean that the scene is a hit.</i></b><br /><br /><b><i>As you pull together your data, don’t jump to conclusions too quickly. Stay open as you gather the facts. Look for disparities, things that don’t jive. Don’t think like a detective, who has a hunch early on and follows it. Think like a couples therapist, who seeks to understand all the factors before coming to a conclusion. Great insights come from deferring the hunch-making until you have a more complete picture. Dig deeper, pull multiple sources, and get creative with your listening. Stay dubious. Pull together a mini-focus group of friends who fit the target; pull interesting articles that unpack trends -- and then go to the source of those articles to gather more information.  If you’re working from a brief, don’t take the data at face value -- cross-reference against other sources.</i></b><br /><br /><b><i>Create Unlikely Connections</i></b><br /><br /><b><i>Once you’ve collected all the data and perspective you can, it’s time to be brave. Now it’s time for gut-feelings and hunches. Start by asking &#39;Why?&#39;</i></b><i> Why do people click on the ad after midnight but only on Tuesdays? Why are women buying more of these men’s underwear -- are they wearing them? If so, why? Why are people in major cities deferring retirement -- are they cash-poor, or just happier in their jobs? Or something else?  And why? For sketch comedians, this line of thinking helps to create the connection between the observed and the deeply understood.</i><br /><br /><i>For example:</i><br /><br /><i>People will post hateful things online they would never say in person</i><br /><br /><i>(Why?) Because they are not face to face with their audience</i><br /><br /><i>(Why?) Because they are too busy to get together</i><br /><br /><i>(Why?) Because they are spending too much time online</i><br /><br /><i>(Why?) Because they crave human connection</i><br /><br /><i>Unlikely Connection: I’m feeling lonely -- I think I’ll troll some Facebook feeds!</i><br /><br /><i>Improvisers might riff backstage, trying out multiple ideas in a quick-fire conversation to find a direction that works. If you don’t have a willing ensemble of improvisers handy, you can replicate this process through visual Clustering.  </i><b><i>Write down a key idea or fact or question at the center of a sheet of paper -- and quickly jot down ideas, thoughts, questions, and presumptions in associated clusters radiating out from the central fact. Write down everything. By making a visual record of your internal thought process, you can begin to notice patterns and connections your conscious brain might miss.</i></b><br /><br /><b><i>Say, Yes And</i></b><br /><br /><i>To create our stage shows, we generate hundreds of scenic ideas, most of which die on the cutting room floor. </i><b><i>To get to the gold, we have to create an overabundance of ideas.</i></b><i> We also explore different iterations of individual ideas, sometimes swapping out major elements to discover the right setting, characters and tone. It takes trial and error to find the right approach to the human truth. </i><b><i>In elevating Data to Insight, be willing to throw a lot of spaghetti at the wall.</i></b><br /><br /><i>Brainstorm laterally -- Make multiple attempts. Follow different directions. If you are working in a team trying to get to the insight, break up into pairs or work on your own in parallel, then come back together and compare your discoveries. </i><b><i>Sometimes the best ideas only stand out when you have other options. Whatever way you do it, don’t stop with just one attempt.</i></b><br /><br /><i>Brainstorm vertically -- For each idea direction, spend time thinking deeply and fleshing it out. Try different options within that insight to fully explore it. </i><b><i>Rather than poking holes, look for possibility -- keep asking &#39;What’s missing? What’s the why behind this why?&#39;</i></b><br /><br /><b><i>Create an abundance of ideas.</i></b><br /><br /><i>&#39;Rush hour is when we choose to go the same direction at the same time across limited resources and blame it on all those other jerks on the road.&#39;</i><br /><br /><i>&#39;When I retire, I think I’ll still take the train during rush hour because I’ll miss the smell of gamey hipsters at 5:30PM.&#39;</i><br /><br /><i>&#39;Rush hour would be fine, if it wasn’t for all the other jerks doing it at the same time as me.&#39;</i><br /><br /><i>&#39;Rush hour is our communal pact to pursue happiness through total misery.&#39;</i><br /><br /><i>&#39;Rush hour is proof that humans are basically pack animals. By which I mean animals that need a 12-pack after driving in this madness.&#39;</i><br /><br /><b><i>Pitch It - Get it on it’s feet</i></b><br /><br /><i>At Second City, we don’t wait for a scene to be perfect to put it on stage. </i><b><i>We get it out there prematurely, testing it in front of the audience, as a way to see if it flies and, if so, to refine it. When you have an insight that you feel good about, try it out. Show it to others, see if it continues to resonate.</i></b><i> Consider whether it’s actionable -- can it drive great work? Is it surprising, but recognizable at the same time?  Is it simple and meaningful? Great storytellers develop their stories over multiple attempts; iterating upon your insight may help you find the simplest, most elegant articulation.</i><br /><br /><b><i>Test it</i></b><br /><br /><i>Take that big, sexy strategic insight and go back to the data -- do you need additional research to pressure-test your insight? Does it jive with the research, or have you been seduced by some pithy catchy phrasing that sounds great but ignores the facts? </i><b><i>The best insights are the ones that include all the data, not just the parts you like. Ask yourself &#39;have I retrofitted an insight to justify one of my cool creative ideas?&#39;</i></b><i>  One of the most painful backstage experiences at Second City is when we have to cut a scene that we love, that makes us laugh, but makes the audience say &#39;Meh.&#39;</i><br /><br /><i>Data will always be important, but insight is what makes the people important. The ones whose behavior is explained as well as the ones doing the explaining. There will always be a place for people who can think strategically to discover great insights. Some of them will sell products, some will sell punchlines, but all of them will be more fun at a party than an Excel spreadsheet.&quot;</i><br /><br />Don&#39;t forget the context: I&#39;m comparing the corporate research approach to understanding consumers to the social construction of solutions to scientific controversies.<br /><br />The belief that we can just ignore the social aspects of theory-making -- and instead just focus on finding the physical truth -- is why academics have to-date failed to streamline the process.  The distinction matters most when scientific knowledge becomes constested, and learning to make the distinction will make us better problem-solvers.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Origin of the Solar Cycle","summary":"\"The Solar Activity Cycle has Fascinated Scientists and Amateurs Alike for Over a Century, but its Mystery Remains, and Even Deepens, as we Collect New Data that Reveals its Full Complexity\" (National Solar Observatory) / Based on Theoretical Studies of how Magnetism is Generated in Stars, it’s Thought that the Fully Convective Interiors of Ultracool Dwarfs Can’t Support Large-scale Magnetic Field Formation / And Yet, Recent Observations Reveal that Ultracool Dwarfs Exhibit Cycling Similar to Our Own Sun / Have We Fundamentally Misunderstood the Origin of Solar Magnetism?","image":"https://lh3.googleusercontent.com/-jI2It5btqh4/WCeof3GfY1I/AAAAAAAAJRs/abE45KynZlgMUdcvUyNIfNymoys3LgAwACJoC/w1800-h2520/the-origin-of-the-solar-cycle-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/A7mxRfyc8yz","publishDate":"2016-11-12T23:41:58.433Z","updateDate":"2017-01-03T01:46:05.701Z","text":"<b>The Origin of the Solar Cycle: &quot;The Solar Activity Cycle has Fascinated Scientists and Amateurs Alike for Over a Century, but its Mystery Remains, and Even Deepens, as we Collect New Data that Reveals its Full Complexity&quot; (National Solar Observatory) / Based on Theoretical Studies of how Magnetism is Generated in Stars, it’s Thought that the Fully Convective Interiors of Ultracool Dwarfs Can’t Support Large-scale Magnetic Field Formation / And Yet, Recent Observations Reveal that Ultracool Dwarfs Exhibit Cycling Similar to Our Own Sun / Have We Fundamentally Misunderstood the Origin of Solar Magnetism?</b><br /><br />The National Solar Observatory quote comes from ...<br /><br /><a href=\"https://books.google.com/books?id=u3EAvRsJoZgC&amp;pg=PA782&amp;lpg=PA782&amp;dq=The+solar+activity+cycle+has+fascinated+scientists+and+amateurs+alike+for+over+a+century,+but+its+mystery+remains,+and+even+deepens,+as+we+collect+new+data+that+reveals+its+full+complexity.&amp;source=bl&amp;ots=KkyANyjoxh&amp;sig=BmJW5NPlW_j8f4lmzkAGQ0m01U8&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiS_I_npaTQAhWLv1QKHS-SCMgQ6AEIGzAA#v=onepage&amp;q=The%20solar%20activity%20cycle%20has%20fascinated%20scientists%20and%20amateurs%20alike%20for%20over%20a%20century%2C%20but%20its%20mystery%20remains%2C%20and%20even%20deepens%2C%20as%20we%20collect%20new%20data%20that%20reveals%20its%20full%20complexity.&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=u3EAvRsJoZgC&amp;pg=PA782&amp;lpg=PA782&amp;dq=The+solar+activity+cycle+has+fascinated+scientists+and+amateurs+alike+for+over+a+century,+but+its+mystery+remains,+and+even+deepens,+as+we+collect+new+data+that+reveals+its+full+complexity.&amp;source=bl&amp;ots=KkyANyjoxh&amp;sig=BmJW5NPlW_j8f4lmzkAGQ0m01U8&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiS_I_npaTQAhWLv1QKHS-SCMgQ6AEIGzAA#v=onepage&amp;q=The%20solar%20activity%20cycle%20has%20fascinated%20scientists%20and%20amateurs%20alike%20for%20over%20a%20century%2C%20but%20its%20mystery%20remains%2C%20and%20even%20deepens%2C%20as%20we%20collect%20new%20data%20that%20reveals%20its%20full%20complexity.&amp;f=false</a><br /><br /><a href=\"http://aasnova.org/2016/11/09/could-ultracool-dwarfs-have-sun-like-activity/\" class=\"ot-anchor\">http://aasnova.org/2016/11/09/could-ultracool-dwarfs-have-sun-like-activity/</a><br /><br />Could Ultracool Dwarfs Have Sun-Like Activity?<br />By Susanna Kohler on 9 November 2016<br /><br /><i>&quot;Solar-like stars exhibit magnetic cycles; our Sun, for instance, displays an 11-year period in its activity, manifesting as cyclic changes in radiation levels, the number of sunspots and flares, and ejection of solar material. Over the span of two activity cycles, the Sun’s magnetic field flips polarity and then returns to its original state.</i><br /><br /><i>But what about the magnetic behavior of objects near the cooler end of the stellar main sequence -- do they exhibit similar activity cycles?</i><br /><br /><i>Effects of a Convecting Interior</i><br /><br /><i>Dwarf stars have made headlines in recent years due to their potential to harbor exoplanets. Because these cooler stars have lower flux levels compared to the Sun, their habitable zones lie much closer to the stars. The magnetic behavior of these stars is therefore important to understand: could ultracool dwarfs exhibit solar-like activity cycles that would affect planets with close orbits?</i><br /><br /><i>There’s a major difference between ultracool dwarfs (stars of spectral type higher than M7 and brown dwarfs) and Sun-like stars: their internal structures. Sun-like stars have a convective envelope that surrounds a radiative core. The interiors of cool, low-mass objects, on the other hand, are fully convective.</i><br /><br /><b><i>Based on theoretical studies of how magnetism is generated in stars, it’s thought that the fully convective interiors of ultracool dwarfs can’t support large-scale magnetic field formation. This should prevent these stars from exhibiting activity cycles like the Sun. But recent radio observations of dwarf stars have led scientist Matthew Route (ITaP Research Computing, Purdue University) to question these models.</i></b><br /><br /><i>A Reversing Field?</i><br /><br /><i>During observations of the brown dwarf star J1047+21 in 2010–2011, radio flares were detected with emission primarily polarized in a single direction. The dwarf’s flares in late 2013, however, all showed polarization in the opposite direction. </i><b><i>Could this be an indication that J1047+21 has a stable, global dipolar field that flipped polarity in between the two sets of observations? If so, this could mean that the star has a magnetic cycle similar to the Sun’s.</i></b><br /><br /><b><i>Inspired by this possibility, Route conducted an investigation of the long-term magnetic behavior of all known radio-flaring ultracool dwarfs, a list of 14 stars. Using polarized radio emission measurements, he found that many of his targets exhibited similar polarity flips, which he argues is evidence that these dwarfs are undergoing magnetic field reversals on roughly decade-long timescales, analogous to those reversals that occur in the Sun.</i></b><br /><br /><b><i>If this is indeed true, then we need to examine our models of how magnetic fields are generated in stars</i></b><i>: the interface between the radiative and convective layers may not be necessary to produce large-scale magnetic fields. Understanding this process is certainly an important step in interpreting the potential habitability of planets around ultracool dwarfs.&quot;</i><br /><br />Citation: Matthew Route 2016 ApJL 830 L27. doi:10.3847/2041-8205/830/2/L27<br /><br /><a href=\"https://youtu.be/5AUA7XS0TvA?t=2173\" class=\"ot-anchor\">https://youtu.be/5AUA7XS0TvA?t=2173</a> (timestamped)<br /><br />Thunderbolts of the Gods<br />Youtube Documentary<br /><br />Don Scott:<br /><br /><i>&quot;The Sun is actually a fairly typical star.  And so, if we want to understand stars and cosmology, we really have to understand the Sun.</i><br /><br /><i>And the standard model of the Sun, so called, that astronomers are very proud of these days, really doesn&#39;t explain very much about the Sun.</i><br /><br /><i>Why is there a corona in the first place?  Everybody knows about the corona -- the beautiful corona that we see during solar eclipses ... Why is it there?  It&#39;s clearly an electrical phenomenon.&quot;</i><br /><br />[...]<br /><br /><b><i>&quot;If you ask any astronomer why are there sunspots?  Why are the umbra dark?  They will blame it on some sort of warped, twisted magnetic fields.  They will say that it has to do with the solar dynamo, all of which lurk unseen like a big genie somewhere beneath the surface.&quot;</i></b><br /><br /><a href=\"https://www.thunderbolts.info/wp/2011/12/08/our-mysterious-and-variable-sun/\" class=\"ot-anchor\">https://www.thunderbolts.info/wp/2011/12/08/our-mysterious-and-variable-sun/</a><br /><br />Our Mysterious and Variable Sun<br />by David Talbott<br />December 8, 2011<br /><br /><i>&quot;Quantification</i><br /><br /><i>It is the general agreement between theory and observation that provides the foundation for quantification. Specialized inquiry can then test the rigor and precision of the qualitative argument with equations and numbers. In a successful test, the quantitative results will correlate well with predictions arising from the underlying theoretical assumptions; they will add logical strength and precision to the prior qualitative argument.</i><br /><br /><i>The concrete relationship of quantification to underlying fact has borne fruits in plasma physics, where qualitative extrapolations from laboratory findings have repeatedly anticipated observations in space and supported practical mathematical modeling.</i><br /><br /><b><i>In the case of the Sun, however, neither a qualitative nor a quantitative argument exists, since the dominant attributes of the Sun, now revealed to us in stunning detail, lie beyond the predictive ability of the theoretical assumptions.</i></b><i> This sweeping failure of predictive ability removes the rationale for the more specialized assumptions, equations, and simulations offered in the name of solar physics today. The only way to overcome this spectacular deficiency would be to demonstrate a logical pathway of quantified analysis leading from the theoretical starting point to the major attributes of the Sun. After decades of trying, the promise of a quantified model was never fulfilled, not even in a limited sense. No direct line of reasoning from the assumed nuclear furnace to even one enigmatic attribute of the Sun can be substantiated. And so the specialized debates go on and on, guided by the dogmatic certainty that an acceptable answer must be available. </i><b><i>After 60 years and billions of dollars spent exploring the Sun, no peer-reviewed article has yet questioned the fusion model.</i></b><br /><br /><i>&#39;Meeting Our Global Energy Needs&#39;</i><br /><br /><b><i>In the absence of successful tests of a hypothesis it is a grave mistake to pretend that issues are settled.</i></b><i> Nevertheless, with the support of popular media, a guess about the &#39;nuclear core&#39; of the Sun led to a leap of faith. Limitless energy should be available to humanity by controlling a fusion process -- &#39;just like the controlled fusion in the center of the sun.&#39;</i><br /><br /><i>The cost of this exuberance may never be accurately calculated. Globally, governments poured billions upon billions of dollars into research, seeking to replicate the imagined events hidden inside the Sun. From the 1950s onward it was an easy sell. But the only fusion the experiments provoked lasted a second or so -- typically much less than a second—and never produced as much energy as was pumped into the experiments. In physics, that’s the definition of an unworkable idea -- and it’s very likely the most expensive failure of theory the world has ever witnessed. [16]</i><br /><br /><i>Contrasting Theory and Observation</i><br /><br /><i>The &#39;settled science&#39; of the Sun sees it as an isolated ball of gas in space, slowly consuming itself through nuclear reactions at its core. </i><b><i>In the electrical alternative, the Sun’s energetic output is largely -- perhaps entirely -- the consequence of external electric fields and the heliospheric movement of charged particles, powered by circuitry along the arms of the Milky Way.</i></b><i> Given the volume of available data, a comparative test of predictive failure and predictive success is long overdue.</i><br /><br /><i>Is it possible that the failures of the standard model are, in fact, the predictions of the electric model? To see that this is so, one must trace the connection between theoretical assumptions and their inescapable implications. Wherever the implications are logical requirements of the model, the absence of the predicted findings will amount to falsification of the model as stated.</i><br /><br /><i>Though the electrical hypothesis remains in its infancy, and the foundational phase of the investigation is far from complete, an issue-by-issue evaluation of the two models cannot be avoided.</i><br /><br /><i>The Constant and Inconstant</i><br /><br /><b><i>Under the assumptions of the fusion model the Sun’s electromagnetic emissions appear enigmatic, with unexplained variations depending on wavelength.</i></b><i> &#39;Solar spectral irradiance variations are known to exhibit a strong wavelength dependence with the amount of variability increasing towards shorter wavelengths.&#39; [17]</i><br /><br /><i>Traditional theory assumes that, over hundreds of thousands of years, heat from a fusion reaction at the Sun’s core travels first through a supposed &#39;radiative zone.&#39;  It then rushes upward through an imagined &#39;convective zone&#39; to create the Sun’s visible surface, the photosphere. Unexplained events then energize the chromosphere and corona from below. </i><b><i>But why would this theorized process produce highly constant visible light but much more variable extreme UV light and X-rays above the photoshere?</i></b><br /><br /><i>&#39;Solar Constant&#39;</i><br /><br /><i>The least variable emissions occur in the infrared, which accounts for more than half of the Sun’s radiative output. Moving up to visible light the Sun’s output varies only slightly more. In the recent solar minimum its visible light dimmed by only 0.1%. [18]</i><br /><br /><b><i>Does the constancy of the Sun’s output in infrared and visible light follow logically from the standard model? The only known analogies for nuclear fusion are at the extremes of inconstancy: on the one hand a hydrogen bomb and on the other the failed laboratory attempts to control the fusion process. A hydrogen bomb underscores the fact that thermonuclear reaction rates are highly unstable and particularly sensitive to core temperature. Even a modest increase in temperatures at the Sun’s core would multiply the likelihood of a runaway reaction a thousandfold and more.</i></b><br /><br /><b><i>The refusal of the Sun to become a &#39;hydrogen bomb&#39; is a good reason to consider the electrical alternative.</i></b><br /><br /><i>Solar Variability</i><br /><br /><i>At higher frequencies the Sun’s constancy disappears. At the wavelengths of extreme ultraviolet light the Sun’s emissions dimmed by 30% during the last solar minimum, a 300% greater dimming than in visible light. And at the frequency of X-ray generation the Sun is vastly more variable, as seen in the X-ray images of a solar cycle below. &#39;The Sun is a variable X-ray star,&#39; states R L F Boyd. &#39;It is fortunate for us that the variability is not reflected in the energy flux in the visible.&#39; [19]</i><br /><br /><b><i>What could be causing a constant Sun at one frequency to become an inconstant Sun at a higher frequency? From the region below the photosphere, up through the photosphere, the chromosphere and the transition region, into the corona, we find an increasing dominance of higher frequencies and greater variability.</i></b><br /><br /><i>Perhaps our own Earth provides a useful analogy. Above the earth -- in the ionosphere and Van Allen radiation belt -- with energy levels much greater than at the surface. We know that the flow of charged particles and associated energetic activity is not generated from within the Earth. It is a direct result of arriving particles from beyond the Earth, specifically, from the Sun. </i><b><i>Is it not reasonable, therefore, to ask if the layers of more variable and energetic activity around the Sun could be due to electrical contributions from its larger environment, the heliosphere, fed by electrical currents along the arms of the Milky Way?&quot;</i></b><br /><br /><a href=\"http://www.holoscience.com/wp/sciences-looming-tipping-point/\" class=\"ot-anchor\">http://www.holoscience.com/wp/sciences-looming-tipping-point/</a><br /><br />The Solar Cycle<br /><br /><i>&quot;The solar cycle is an enigma for the standard solar model. </i><b><i>&#39;The solar activity cycle has fascinated scientists and amateurs alike for over a century, but its mystery remains, and even deepens, as we collect new data that reveals its full complexity.&#39;</i></b><i> [D. M. Rabin et al., Solar Interior &amp; Atmosphere] Attempts to model the solar cycle have relied on a hypothetical dynamo inside the Sun, in other words, the inductive action of fluid motions pervading the solar interior, to explain features that occur outside the Sun. </i><b><i>But nothing remotely resembling consensus currently exists as to the mode of operation of the solar dynamo. And the coup de grace seems to have been delivered by the recently discovered lack of fluid motions beneath the photosphere.</i></b><br /><br /><i>Solar transformer</i><br /><br /><i>In the Electric Universe model, the solar sunspot cycle is simply the result of varying the direct current (DC) input to a plasma discharge focused on the Sun. For a continuous current to flow there must be a circuit. Hannes Alfvén provided the circuit but misidentified the Sun as a generator in a local closed circuit instead of a load in a galactic circuit. Dr. D. Scott provided an electrical engineer’s perspective of the Sun’s magnetic field changes due to a changing input current.The simplicity is obvious. The reversal of the Sun’s dipole field and sunspot order during the solar cycle is a natural result of a transformer action as the main current increases and decreases but never reverses. The solar cycle is to be expected since plasma circuits are notorious for their oscillatory tendency.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Annotation of Scientific Papers","summary":"There Are Reasons to Believe that Physicists Pay Less Attention to Theories that are Crammed with Mathematical Details, Despite their Significant Mathematical Training / \"The Researchers Think the Solution Lies in Clearer Communication of Highly Technical Work, Such as Taking the Time to Describe What the Equations Mean\" / A System Which Annotates Scientific Papers Might be a Viable Business Model / It Could Dominate Science Journalism / It Could Eventually Blur the Lines Between Laypeople and Scientists, and Create a New Era of Generalist Scientific Thinkers","image":"https://lh3.googleusercontent.com/-HPQUWTyZewM/WCeGGc3gmZI/AAAAAAAAJQ8/FxPTDfn-7XUe90cR2Cpi_4d_2eN96KFPQCJoC/w3600-h5040/the-annotation-of-scientific-papers-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/JTW1FYZbDYy","publishDate":"2016-11-12T21:15:58.658Z","updateDate":"2017-01-03T01:46:27.819Z","text":"<b>The Annotation of Scientific Papers: There Are Reasons to Believe that Physicists Pay Less Attention to Theories that are Crammed with Mathematical Details, Despite their Significant Mathematical Training / &quot;The Researchers Think the Solution Lies in Clearer Communication of Highly Technical Work, Such as Taking the Time to Describe What the Equations Mean&quot; / A System Which Annotates Scientific Papers Might be a Viable Business Model / It Could Dominate Science Journalism / It Could Eventually Blur the Lines Between Laypeople and Scientists, and Create a New Era of Generalist Scientific Thinkers</b><br /><br /><a href=\"https://www.sciencedaily.com/releases/2016/11/161111132118.htm\" class=\"ot-anchor\">https://www.sciencedaily.com/releases/2016/11/161111132118.htm</a><br /><br />Even physicists are &#39;afraid&#39; of mathematics<br />November 11, 2016<br />University of Exeter<br /><br /><i>Physicists avoid highly mathematical work despite being trained in advanced mathematics, new research suggests.</i><br /><br /><i>The study, published in the New Journal of Physics, shows that physicists pay less attention to theories that are crammed with mathematical details. This suggests there are real and widespread barriers to communicating mathematical work, and that this is not because of poor training in mathematical skills, or because there is a social stigma about doing well in mathematics.</i><br /><br /><i>Dr Tim Fawcett and Dr Andrew Higginson, from the University of Exeter, found, using statistical analysis of the number of citations to 2000 articles in a leading physics journal, that </i><b><i>articles are less likely to be referenced by other physicists if they have lots of mathematical equations on each page.</i></b><br /><br /><i>Dr Higginson said: </i><b><i>&#39;We have already showed that biologists are put off by equations but we were surprised by these findings, as physicists are generally skilled in mathematics.</i></b><br /><br /><i>&#39;This is an important issue because it shows there could be a disconnection between mathematical theory and experimental work. This presents a potentially enormous barrier to all kinds of scientific progress.&#39;</i><br /><br /><i>The research findings suggest improving the training of science graduates won&#39;t help, because physics students already receive extensive maths training before they graduate. </i><b><i>Instead, the researchers think the solution lies in clearer communication of highly technical work, such as taking the time to describe what the equations mean.</i></b><br /><br /><i>Dr Fawcett said: </i><b><i>&#39;Physicists need to think more carefully about how they present the mathematical details of their work, to explain the theory in a way that their colleagues can quickly understand.</i></b><i> It takes time to scrutinise the details of a technical article -- even for the most distinguished physics professors -- so with many competing demands on their time scientists may be choosing to skip over articles that take too much effort to digest.&#39;</i><br /><br /><i>&#39;Ideally, the impact of scientific work should be determined by its scientific value, rather than by the presentational style,&#39; said Dr Higginson.</i><br /><br /><b><i>&#39;Unfortunately, it seems valuable papers may be ignored if they are not made accessible. As we have said before: all scientists who care about the dialogue between theory and experiment should take this issue seriously, rather than claiming it does not exist.&#39;&quot;</i></b><br /><br />The statistical analysis is free-to-view at <a href=\"http://dx.doi.org/10.5281/zenodo.58792\" class=\"ot-anchor\">dx.doi.org/10.5281/zenodo.58792</a><br /><br />The idea that academic researchers are going to all start taking the time to clarify the meaning of their equations is probably wishful thinking.  But, that&#39;s not to say that there isn&#39;t a solution.<br /><br /><b>What Would We Annotate?</b><br /><br />A platform which crowdsources scientific controversies could also double as a platform for annotating crucial scientific papers with all sorts of content:<br /><br />(1) Automated identification of variables and established formulae, based upon the context of the article; an approach known as topic modeling can be used to identify the context.<br /><br />(2) Crowdsourced annotation of challenges to stated claims.<br /><br />(3) More explicit statements of assumptions, which are oftentimes completely left out of scientific papers.<br /><br />(4) Explicit statement of the reasons to believe any proposition made within the paper.  What is the experimental or observational evidence for this claim or speculation?<br /><br />(5) Inspired questions which perhaps the researchers have omitted or even refuse to ask.<br /><br />(6) Links to the best critiques which challenge the claims of the paper (this is obviously the part I have decided to build out first).  This repository of critique does not yet exist, and since it requires a methodical review of the entire volume of critique, it can be expected to take a couple of years to complete.<br /><br />(7) Readily accessible definitions for scientific jargon; these can be of a variety of formats -- wikis, dictionary definitions or even concept maps.<br /><br />(8) When a subject becomes enormously complex, an argument map can be created which shows how the various papers and claims relate to one another.<br /><br />(9) Historical background and hand-selected non-technical introductions designed to ease a layperson into the topic; the quality of these types of materials can widely vary, so it&#39;s crucial to have somebody hand-curating them.<br /><br /><b>The Fascinating History of Annotations</b><br /><br />The idea for Google&#39;s PageRank is rooted in the idea of academic citation tracking, and it followed from discussions about how to implement a system for annotating web pages, in the early formative years of the Internet.<br /><br /><a href=\"https://books.google.com/books?id=V1u1f8sv3k8C&amp;pg=PA16&amp;lpg=PA16&amp;dq=%22Page+was+also+seeking+a+dissertation+topic.+One+idea+he+presented+to+Winograd%22&amp;source=bl&amp;ots=BStTdw9jfH&amp;sig=Gg0JtOUUMuX-bAoaJsbvs1k6tFw&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjopbLcgqTQAhVrqFQKHaYXA7wQ6AEIGzAA#v=onepage&amp;q=%22Page%20was%20also%20seeking%20a%20dissertation%20topic.%20One%20idea%20he%20presented%20to%20Winograd%22&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=V1u1f8sv3k8C&amp;pg=PA16&amp;lpg=PA16&amp;dq=%22Page+was+also+seeking+a+dissertation+topic.+One+idea+he+presented+to+Winograd%22&amp;source=bl&amp;ots=BStTdw9jfH&amp;sig=Gg0JtOUUMuX-bAoaJsbvs1k6tFw&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjopbLcgqTQAhVrqFQKHaYXA7wQ6AEIGzAA#v=onepage&amp;q=%22Page%20was%20also%20seeking%20a%20dissertation%20topic.%20One%20idea%20he%20presented%20to%20Winograd%22&amp;f=false</a><br /><br /><i>In the Plex - How Google Thinks, Works and Shapes Our Lives</i><br />by Steven Levy<br /><br /><i>&quot;Page was also seeking a dissertation topic. One idea he presented to Winograd, a collaboration with Brin, seemed more promising than the others: creating a system where people could make annotations and comments on websites. </i><b><i>But the more Page thought about annotation, the messier it got. For big sites, there would probably be a lot of people who wanted to mark up a page. How would you figure out who gets to comment or whose comment would be the one you&#39;d see first? For that, he says, &#39;We needed a rating system.&#39;</i></b><br /><br /><i>Having a human being determine the ratings was out of the question. First, it was inherently impractical. Further, humans were unreliable. Only algorithms well drawn, efficiently executed, and based on sound data could deliver unbiased results. So the problem became finding the right data to determine whose comments were more trustworthy, or interesting, than others. Page realized that such data already existed and no one else was really using it. He asked Brin, &#39;Why don&#39;t we use the links on the web to do that?&#39;</i><br /><br /><b><i>Page, a child of academia, understood that web links were like citations in a scholarly article. It was widely recognized that you could identify which papers were really important without reading them -- simply tally up how many other papers cited them in notes and bibliographies. Page believed that this principle could also work with web pages.</i></b><i> But getting the right data would be difficult. Web pages made their outgoing links transparent: built into the code were easily identifiable markers for the destinations you could travel to with a mouse click from that page. But it wasn&#39;t obvious at all what linked to a page. To find that out, you&#39;d have to somehow collect a database of links that connected to some other page. Then you&#39;d go backward.&quot;</i><br /><br /><a href=\"http://genius.com/1105272/Marc-andreessen-why-andreessen-horowitz-is-investing-in-rap-genius/Which-would-obviously-have-had-to-scale-to-enormous-size\" class=\"ot-anchor\">http://genius.com/1105272/Marc-andreessen-why-andreessen-horowitz-is-investing-in-rap-genius/Which-would-obviously-have-had-to-scale-to-enormous-size</a><br /><br />Why Andreessen Horowitz is Investing in Rap Genius<br />by Marc Andreessen (the coauthor of Mosaic, the first widely used web browser)<br /><br /><i>&quot;Only a handful of people know that the big missing feature from the web browser -- is the ability to annotate any page on the Internet with commentary and additional information.</i><br /><br /><b><i>Back in 1993, when Eric Bina and I were first building Mosaic, it seemed obvious to us that users would want to annotate all text on the web -- our idea was that each web page would be a launchpad for insight and debate about its own contents.  So we built a feature called &#39;group annotations&#39; right into the browser -- and it worked great -- all users could comment on any page and discussions quickly ensued.  Unfortunately, our implementation at that time required a server to host all the annotations, and we didn&#39;t have the time to properly build that server, which would obviously have had to scale to enormous size.  And so we dropped the entire feature.</i></b><br /><br /><b><i>I often wonder how the Internet would have turned out differently if users had been able to annotate anything -- to add new layers of knowledge to all knowledge, on and on, ad infinitum.&quot;</i></b><br /><br />B Horowitz adds in annotation:<br /><br /><i>&quot;While it may be surprising to some that Marc didn&#39;t include such an important invention in the browser, because he ran out of time, it shouldn&#39;t be.  Marc may be the most prolific creator of Internet technologies ever.</i><br /><br /><i>It&#39;s well known that Marc invented the browser.  Less well known is that the company Marc founded invented a dizzying number of fundamental Internet technologies including Javascript, SSL, Cookies, and Frames among many others.  Knowing Marc for many years, I can tell you that for every invention that you know about, there are another 20 in a drawer somewhere.  That is one of the many reasons why I call him &#39;Big Tunechi.&#39;&quot;</i><br /><br />Marc explains:<br /><br /><i>&quot;We tried to get the National Science Foundation to fund the next phase of the Mosaic project, which would have included this feature.  Unfortunately they decided the project had no justifiable technical merit!&quot;</i><br /><br />B Horowitz:<br /><br /><i>&quot;This would not only have been a tremendous technical challenge in 1993, but a monstrous economic one as well.  Check out the price of storage in 1993.</i><br /><br /><i>Storing the entire annotated Internet would have been more than a couple of college kids could afford for sure.&quot;</i><br /><br />Marc:<br /><br /><i>&quot;How it works: every time you access a document in Mosaic, the group annotation server [if you&#39;re using one] is queried with the URL of the document you&#39;re viewing; if any group annotations exist for that document, the gruop annotation server returns to Mosaic corresponding hyperlinks which are inlined into the document just like personal annotations.&quot;</i><br /><br />Note that this is just one aspect of what a modern annotation service would look like; they could be added in through a variety of mechanisms, some of them of course automated and layered in real-time, so long as the context is limited to just science.  Once the scope is expanded beyond one single subject, then the details of the implementation become confused between competing sets of needs.<br /><br /><b>It&#39;s important to realize that the idea of a general-purpose annotation service might actually be a malformed idea; we should tailor each annotation solution to each topic, for they must address the specific needs of that topic.</b><br /><br /><b>The Promise of Annotation</b><br /><br /><a href=\"https://www.youtube.com/watch?v=8dUZzXo1ag0\" class=\"ot-anchor\">https://www.youtube.com/watch?v=8dUZzXo1ag0</a><br /><br /><i>&quot;An honest world where ... peers evaluate each other, not on terms established by those in power, but on the merit of their words and deeds ...&quot;</i> (Hypothes.is Foundation)<br /><br /><i>&quot;The (Sometimes Ugly) Reality: &#39;Reasoning is more often used to win arguments than it is used to find and determine truth ... We systematically and effectively strive for arguments that justify our pre-existing beliefs ...&#39;&quot;</i> (Dan Whaley, Hypothes.is Foundation)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Crowdsourcing of Scientific Controversies","summary":"Since Certain Questions in Science Become \"Settled\", Academics Don't Follow All Claims of Controversy / What if a New Observation Was to Settle One of those Debates? / Would the Experts Notice? / Some Corporations Already Effectively Tap Into Long Tail Collaboration / It Seems the Area is Ripe for Disruption because Academia Refuses to Build Such a System","image":"https://lh5.googleusercontent.com/-eqcpbtQ_zg0/WCVGll_JC4I/AAAAAAAAJOc/P4G930r7BV4BbOiDjD5J56mhRjja60OnwCL0B/the-crowdsourcing-of-scientific-controversies-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/9UVEhtvjuAo","publishDate":"2016-11-11T04:20:17.086Z","updateDate":"2017-01-03T01:46:45.334Z","text":"<b>The Crowdsourcing of Scientific Controversies: Since Certain Questions in Science Become &quot;Settled&quot;, Academics Don&#39;t Follow All Claims of Controversy / What if a New Observation Was to Settle One of those Debates? / Would the Experts Notice? / Some Corporations Already Effectively Tap Into Long Tail Collaboration / It Seems the Area is Ripe for Disruption because Academia Refuses to Build Such a System</b><br /><br />The accomplishment by some of coming together, despite a highly partisan age, to construct a detailed map of argumentation on both sides -- in a manner which is respectful of both sides -- will come to be recognized as one of the greatest achievements of our era.<br /><br />Exhibit A:<br /><br />The quote below is a very special type of quote because it is an example of researchers having the courage, in a very public setting, to tell their entire community of peers that a very fundamental mistake has been made on the issue of stellar formation -- a subject for which certain questions are no longer considered worthwhile.<br /><br />The researchers may or may not understand the larger context for their admission -- that it represents a crucial pillar of an opposing cosmology.  We don&#39;t know, because they&#39;re not affiliated with the idea&#39;s proponents.<br /><br />Either way, on a social network which crowdsources scientific controversies, the finding of such an extraordinary quote should be worth immense social capital.  And when something comparable is discovered, there should be a process which incorporates it into the existing knowledge structure.  A board of experts in controversy might help to guide the process.<br /><br />Either way, the following quote is now the benchmark standard of all golden eggs for a scientific social network ...<br /><br /><a href=\"http://www.aanda.org/articles/aa/pdf/2016/06/aa27979-15.pdf\" class=\"ot-anchor\">http://www.aanda.org/articles/aa/pdf/2016/06/aa27979-15.pdf</a><br /><br /><i>&quot;The discovery by Heiles (1997) that the Orion A filament (the largest nearby star-forming structure of this kind) is enveloped in a helical magnetic field greatly clarified the nature of these filaments. </i><b><i>The observations (their Fig. 24) show magnetic field lines changing direction as they cross the filament, first into then out of the plane of the sky. Since these are one-dimensional (1D) projections of an intrinsically 3D field structure, they cannot be uniquely interpreted by themselves. However, given that circular (or more generally, helical) fields, which would be generated primarily by currents moving along the filaments, are the form that is necessary to confine filaments of approximately uniform thickness, the Heiles (1997) observations constituted a &#39;smoking gun&#39;. Moreover, polarization measurements by Matthews &amp; Wilson (2000) provide information in a second dimension that confirms this picture. That is, under the assumption that this polarization arises from dust grains aligned by either paramagnetic inclusions or radiative torques, the field lines pass over the filament perpendicular to its axis. See their Fig. 1. Subsequent observations confirm these results (Poidevin et al. 2010, 2011). See also Pillai et al. (2015) for dust polarization examples in more distant and massive clouds.</i></b><br /><br /><i>[...]</i><br /><br /><i>It is possible to counter that if there were strong toroidal fields, then these would have prevented formation of the filament in the first place by blocking infall. </i><b><i>Our answer: subcritical magnetic fields are there. Heiles (1997) measured their amplitude 20 yr ago and we have now shown they are subcritical by measuring the potential. Such high field strengths are naturally explained by magnetic compression of gas due to currents. That is, the material gets to its current position not by crossing field lines but by compressing them. It may be objected that this process would lead to pinching instabilities. Our answer: yes, pinching instabilities are expected and this is exactly what leads to cluster formation.&quot;</i></b><br /><br />If this doesn&#39;t register as meaningful to you, then learn what electricity looks like when it travels through space ...<br /><br /><b>The Lorentz Force and Marklund Convection: The Alternative to Gravitational Accretion</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/5BkRHtgS2mB\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/5BkRHtgS2mB</a>﻿<br /><br />... and review this historical revision of the textbooks ...<br /><br /><b>The History of the Birkeland Current</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/TFL3iHyjnwz\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/TFL3iHyjnwz</a><br /><br />Anybody who is taking the time to understand new discoveries in the light of existing controversies could discover it.  We should reward the people who take the time to do that.  When we do that in a systematic manner, innovation will rise.  Wherever that happens on the Internet, the best critical thinkers will aggregate, and people will be more open to engaging ideas they don&#39;t already believe.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"Rational Skepticism vs Denialism","summary":"There's a Fine Line Between Rational Skepticism and Denialism / Rational Skepticism is When We Try to Disprove an Idea Using What We Currently Know / Denialism is to Claim that Something is Definitely Not True Based On What We Already Know, and Nothing Will Ever Change That / Rhys Taylor Argues that We Must Consider What People Were Saying, How Long they Were Saying it For, and Whether they Changed their Minds for Good Reasons","image":"https://lh3.googleusercontent.com/-ZiyjsI8qWOo/WCSaw5HuLxI/AAAAAAAAJN8/hrKlLmybJDUM-Gbsb6Jmy-9HheMy4XawQCJoC/w1800-h2520/rational-skepticism-vs-denialism-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/L4FEGSX6BXT","publishDate":"2016-11-10T16:06:00.281Z","updateDate":"2017-01-03T01:47:06.088Z","text":"<b>Rational Skepticism vs Denialism: There&#39;s a Fine Line Between Rational Skepticism and Denialism / Rational Skepticism is When We Try to Disprove an Idea Using What We Currently Know / Denialism is to Claim that Something is Definitely Not True Based On What We Already Know, and Nothing Will Ever Change That / Rhys Taylor Argues that We Must Consider What People Were Saying, How Long they Were Saying it For, and Whether they Changed their Minds for Good Reasons</b><br /><br /><a href=\"http://astrorhysy.blogspot.cz/2016/07/they-said-i-was-maaaaad.html\" class=\"ot-anchor\">http://astrorhysy.blogspot.cz/2016/07/they-said-i-was-maaaaad.html</a><br /><br />Saturday, 16 July 2016<br /><br />They Said I Was Maaaaad!<br />by Rhys Taylor<br /><br /><i>&quot;... As we&#39;ve seen, revolutions triggered entirely by lone geniuses are largely -- if not entirely -- mythical; while there is certainly a germ of truth here, the reality is much more nuanced. Geniuses don&#39;t make all their breakthroughs by themselves, revolutions don&#39;t really go from zero to paradigm shift overnight and their impact isn&#39;t immediately understood. The myth of the lone genius makes for inspiring stories. Work hard, stay the course, and maybe you too can be the next Einstein. You&#39;re not wrong, everyone else is wrong. They&#39;re laughing at you? Well, they did the same to Edison/Columbus/Wegener/etc. too, so that means you&#39;re probably a misunderstood genius as well. They&#39;re all just too proud to admit that they&#39;re wrong. Which is motivational and encouraging, but often it&#39;s also damaging and misleading. Of course, scientific beliefs change with time because they&#39;re evidence-based and provisional. Few people ever suspect that we&#39;ve got science well and truly licked, though, inevitably, there are always some people convinced that some particular aspect is completely done and dusted which later turns out to be fundamentally wrong. The question is one of degree: to what extent does the scientific community act with an attitude of denial rather than skepticism when a new idea comes along? Is there any truth to this at all, or is it a pure fiction invented by pseudoscientists to justify their half-baked quackery?</i><br /><br /><i>This is a somewhat subtle notion which can&#39;t be readily quantified. </i><b><i>First, there&#39;s a fine line between rational skepticism (&#39;using what we know currently, let&#39;s try and disprove this idea&#39;) and denialism (&#39;we already know enough to say that this is definitely not true and nothing will ever change that&#39;).</i></b><i> Even apparent deniers are sometimes just full of bluster and can end up changing their minds if sufficiently strong evidence is presented; true skeptics can sometimes be entirely rational but examine every minor detail to a ludicrous degree before accepting a new idea. Attitude is not something you can put a number on, so we&#39;ll have to look at -- wherever possible -- what it was people actually said.</i><br /><br /><b><i>Second, our brains are not Bayesian nets -- we don&#39;t update our ideas immediately when new evidence is presented. It takes a while for entrenched beliefs to shift.</i></b><i> It&#39;s one thing to yell, &#39;you crazy loon!&#39; at someone and then five minutes later say, &#39;whoops, I was wrong, how about that&#39;, </i><b><i>and quite another to engage in a systematic campaign of denial for a few decades.</i></b><i> The thing is that even staunch denialism does make sense in some circumstances: at one point there was little or no evidence for a round Earth, so saying, &#39;I think it&#39;s round, because the magic pixie told me so&#39; should have been met with denial. Repeating the claim that the magic pixie said so wouldn&#39;t have made it any more likely to be correct. </i><b><i>So it&#39;s very important to consider what people were saying, how long they were saying it for, and whether they changed their minds for good reasons.</i></b><i> All this already makes the notion that all good ideas were once dismissed as crazy to seem like, potentially, a gross over-simplification. All new ideas are rightly considered controversial -- almost by definition -- but are revolutionary ideas really dismissed as heresy? Does the establishment really label all dissenters as cranks and crackpots, or just the genuine loonies?</i><br /><br /><i>&#39;The fact that some geniuses were laughed at does not imply that all who are laughed at are geniuses.  They laughed at Columbus, they laughed at Fulton, they laughed at the Wright brothers.  But they also laughed at Bozo the Clown.&#39;</i> (Carl Sagan)<br /><br /><i>But did they even do that? Did they really laugh at all these popular examples pseudoscientists drag up as examples of mainstream science refusing to listen to reason? </i><b><i>Were they treated as merely controversial -- which is absolutely the proper way of doing science -- or was there a more sinister form of dogmatism at work?&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"The Lorentz Force and Marklund Convection, the Alternative to Gravitational Accretion","summary":"\"Previous Research Often Envisioned Protostars Growing in a Simple Manner, Steadily Accumulating or Accreting Fuel from Surrounding Clouds\" / Yet, Some Appear to Significantly Deviate from these Expectations / Rather than Seeking Out a More Appropriate Conceptual Toolset to Better Explain these Observations, Theorists are Force-Fitting Gravity and Fluids to Do Things for Which Electromagnetism and Plasmas are a Better Fit","image":"https://lh3.googleusercontent.com/-X2q4SIyRPsQ/WCPlnH-6rTI/AAAAAAAAJNI/o2AiGsWGo6kurEg-rACx9pBA4dSC_LRDgCJoC/w7142-h9999/the-lorentz-force-and-marklund-convection-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/5BkRHtgS2mB","publishDate":"2016-11-10T03:13:03.499Z","updateDate":"2017-01-03T01:47:25.883Z","text":"<b>The Lorentz Force and Marklund Convection, the Alternative to Gravitational Accretion: &quot;Previous Research Often Envisioned Protostars Growing in a Simple Manner, Steadily Accumulating or Accreting Fuel from Surrounding Clouds&quot; / Yet, Some Appear to Significantly Deviate from these Expectations / Rather than Seeking Out a More Appropriate Conceptual Toolset to Better Explain these Observations, Theorists are Force-Fitting Gravity and Fluids to Do Things for Which Electromagnetism and Plasmas are a Better Fit</b><br /><br />There is some conceptual overlap between these two concepts of the Lorentz force and Marklund convection: The Lorentz force is the more general-purpose electromagnetic cause, whereas Marklund convection is the more detailed result of that cause which is observed to occur within filamentary plasmas.<br /><br />PART 1: THE ELECTRICAL EXPLANATION FOR STAR FORMATION<br /><br /><a href=\"https://en.wikipedia.org/wiki/Marklund_convection\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Marklund_convection</a><br /><br /><i>&quot;Marklund convection, named after Göran Marklund, is </i><b><i>a convection process that takes place in filamentary currents of plasma. It occurs within a plasma with an associated electric field, that causes convection of ions and electrons inward towards a central twisting filamentary axis.</i></b><i> A temperature gradient within the plasma will also cause chemical separation based on different ionization potentials. [1]</i><br /><br /><i>Mechanism</i><br /><br /><i>In Marklund&#39;s paper, </i><b><i>the plasma convects radially inwards towards the center of a cylindrical flux tube. During this convection, the different chemical constituents of the plasma, each having its specific ionization potential, enters into a progressively cooler region. The plasma constituents will recombine and become neutral, and thus no longer under the influence of the electromagnetic forcing. The ionization potentials will thus determine where the different chemicals will be deposited.</i></b><i> [1]</i><br /><br /><b><i>This provides an efficient means to accumulate matter within a plasma. [2] In a partially ionized plasma, electromagnetic forces act on the non-ionized material indirectly through the viscosity between the ionized and non-ionized material.</i></b><br /><br /><b><i>Hannes Alfvén showed that elements with the lowest ionization potential are brought closest to the axis, and form concentric hollow cylinders whose radii increase with ionization potential. The drift of ionized matter from the surroundings into the rope means that the rope acts as an ion pump, which evacuates surrounding regions, producing areas of extremely low density. [3]&quot;</i></b><br /><br />[1] Marklund, Göran (February 1979). &quot;Plasma convection in force-free magnetic fields as a mechanism for chemical separation in cosmical plasma&quot;. <i>Nature</i>. 277: 370–371. Bibcode:1979Natur.277..370M. doi:10.1038/277370b0.<br /><br />[2] Peratt, Anthony (1992). <i>Physics of the Plasma Universe</i>. Springer-Verlag.<br /><br />[3] Alfvén, Hannes (1981). <i>Cosmic Plasma</i>.<br /><br />This is a tricky subject to cover, because it involves a basic understanding of vector algebra and some concepts from electromagnetism.  What follows is a slightly more detailed explanation which explains why matter is drawn into a conducting plasma filament.  Since the explanation is perhaps not as clear as it could be, I&#39;ve attempted to annotate this one where it seems necessary with additional text and graphics ...<br /><br /><a href=\"https://www.thunderbolts.info/wp/2011/12/06/essential-guide-to-the-eu-chapter-6/\" class=\"ot-anchor\">https://www.thunderbolts.info/wp/2011/12/06/essential-guide-to-the-eu-chapter-6/</a><br /><br />Excerpts from <i>The Essential Guide to the Electric Universe</i>, on Current Pinches --&gt; Field-Aligned Currents --&gt; Condensation of Matter --&gt; Marklund Convection in Plasmas<br />by Bob Johnson, Jim Johnson<br /><br /><i>&quot;6.7  Current Pinches</i><br /><br /><i>Any current I flowing in a conductor or filament will cause a magnetic field B around it. The lines of equal magnetic force will be in the form of rings around the axis of the current. The magnetic force will decrease with radial distance from the axis. [See Figure A]</i><br /><br /><i>From consideration of the Lorentz Force [Figure B], it can be shown that the interaction of the current I with its own magnetic field B will cause a pressure radially inward on the current filament, written as I × B (that is, &#39;I cross B&#39; in vector terminology). This is called a ‘pinch’ or ‘z-pinch’ (when defining the current flow as parallel with the ‘z’ co-ordinate’s direction).&quot;</i><br /><br />This effect can be difficult to visualize, but becomes easier once we look at the interaction of two separate wires:<br /><br /><a href=\"https://en.wikipedia.org/wiki/Lorentz_force\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Lorentz_force</a><br /><br /><i>&quot;One application of [the Lorentz Force] is Ampère&#39;s force law, which describes how two current-carrying wires can attract or repel each other, since each experiences a Lorentz force from the other&#39;s magnetic field.&quot; [Figure C]</i><br /><br />It should be easy to see that we need not have two wires: <b>There will also be a &quot;pressure&quot; applied upon any charged particles within the vicinity of the wire</b>.<br /><br />Continuing with <i>The Essential Guide</i> ...<br /><br /><i>&quot;In a metal conductor, the I × B pressure is resisted by the atomic ion lattice [Figure D]. In a plasma current, the pressure can be balanced by the pressure of the plasma inside the filament. This results in a steady state where the current can flow axially across its own azimuthal or circling magnetic field [Figure E]. The balancing equation is known as the Bennett Pinch equation.</i><br /><br /><i>Lab demonstrations can use the pinch effect to crush aluminum cans by applying a strong magnetic field very quickly [Again, see Figure D]. </i><b><i>The can is crushed before the pressure in the can is able to build up sufficiently to resist the pinch force. Magnetic field forces in lightning can create an inward pinch that will crush a solid copper grounding rod.&quot;</i></b><br /><br />[Figure F: The crushed tubular section of the Hartley Vale lightning rod. (School of Physics, University of Sydney)<br /><br /><a href=\"http://sydney.edu.au/science/physics/about/pinch.shtml\" class=\"ot-anchor\">http://sydney.edu.au/science/physics/about/pinch.shtml</a><br /><br />&quot;The lightning conductor referred to was installed on the chimney stack of the Hartley Vale Kerosene Refinery (near Lithgow, NSW). Following a lightning strike to the conductor, a crushed tubular section was sent to Professor Pollock by Mr G.H. Clark, under whose supervision the lightning conductor has been erected. The tubular section, near the top of the lightning conductor, joined a short solid rod attached to a ball with one large spike at the top and four smaller radiating ones to the main solid conductor to ground. The crushed conductor (see figure 3) remains in the School of Physics at the University of Sydney where it is used regularly to exemplify the pinch effect in undergraduate lectures.&quot;]<br /><br />Continuing with <i>The Essential Guide</i> ...<br /><br /><i>&quot;6.8  Field-Aligned Currents</i><br /><br /><i>In space, the neutral gas pressure is usually negligible, and so the balance between the I × B force and the pressure force cannot occur. The only way the situation can be resolved is for the I × B force to disappear. This implies that I and B (current direction and magnetic field direction) are parallel and, by vector algebra, the cross product is zero.</i><br /><br /><i>If other magnetic fields are present, as they are known to be through much of cosmic space, then the I × B force must be calculated using the total magnetic field, that is, by adding the current’s own B to the general B, added using vector algebra.</i><br /><br /><b><i>Thus in a space plasma, the current I and the total magnetic field B realign so as to be parallel. In other words, the current follows the magnetic field: it is a ‘field-aligned’ current.</i></b><br /><br /><i>Even if there is no external magnetic field, any small elements of current flowing in a plasma will tend to accumulate naturally into larger currents which generate their own magnetic fields and so preserve the filament of current.</i><br /><br /><b><i>What happens is that electrons nearer the centre of the filament flow in almost straight lines and generate an azimuthal magnetic field around them. Electrons further from the centre are influenced by this azimuthal component of the magnetic field and move in a more helical path aligned with the main current direction. This helical motion creates the straighter magnetic field lines near the axis</i></b><i>, as shown in the following diagram. The nearer the centre of the filament, the straighter are the magnetic field lines and the paths of the electrons. [Figure G]</i><br /><br /><i>Any individual electron in the current is thus flowing along the magnetic field direction in its own vicinity, but collectively the filament is preserved even without an external magnetic field. This means that very large currents can be assembled out of small current elements and transmitted over huge distances.</i><br /><br /><b><i>Another way of looking at this is to consider the electrical resistance of the plasma. Current flowing across the magnetic field direction will experience more resistance than current flowing along the magnetic field direction because of the U × B term in the Lorentz Force Law. Effectively, the parallel resistance is less than the perpendicular resistance, so the current tends to flow in alignment with the magnetic field.</i></b><br /><br /><i>6.9  Self-Constriction of Currents</i><br /><br /><i>Detailed mathematical analysis shows that I and B interact in such a way that both I and B tend to spiral parallel to each other around an axis aligned with the external B. </i><b><i>The net effect is that I and B both follow a helical path aligned with the direction of the external B field.</i></b><br /><br /><i>It is also found that the interaction of the axial and azimuthal (ring) components of the helical I and B cause both I and B to be largely confined to a cylinder of definite radius centered on the axis.</i><br /><br /><i>To summarize, the absence of significant pressure in space plasmas causes currents to flow in cylindrical filaments aligned with the general magnetic field direction. Within the cylindrical filament, both the current and the magnetic field will spiral around the axis of the cylinder whilst remaining parallel to each other.</i><br /><br /><i>Note that if for any reason the parallel alignment between I and the total B is disturbed, then an I × B force will arise and cause either radial compression or radial expansion, depending on which of the two components is more axial. Thus pinching of a filament could occur because, for example, of changes in the fields through which the current filament was flowing.</i><br /><br /><i>6.10  Stability of Current Filaments</i><br /><br /><i>Another significant factor emerges from the mathematical analysis. The force-free or field-aligned arrangement is a minimum energy state for the current to flow in. This means that </i><b><i>the field-aligned arrangement is inherently stable.</i></b><i> Unless disturbed by external factors, currents will tend to remain aligned with the magnetic field.</i><br /><br /><i>We can now see how field-aligned currents can persist over vast distances. Field-aligned currents are therefore a much more likely explanation of the collimated (parallel-flow) ‘jets’ seen to be extending for hundreds to thousands of light-years than is the Gravity Model explanation based on conventional fluid flows (refer to 6.6 above). [Figure H]</i><br /><br /><i>[...]</i><br /><br /><i>6.11  Condensation of Matter</i><br /><br /><i>A further effect related to the I × B force can also be determined by analysis.</i><br /><br /><i>Suppose that the current I is caused by an electric field E. Now consider the force arising from the interaction of E and B. Remember that I tends to become aligned with the total B due to the forces on the current itself. Then the E causing the current will not be entirely aligned with the total B, which is the vector sum of the external magnetic field through which the current flows and the azimuthal magnetic field generated by the current itself.</i><br /><br /><b><i>As with the I × B force, there is also an E × B force, whenever E is not parallel to B. This E × B force acts on charged particles in the current cylinder and causes both ions and electrons to move towards the centre of a filament. Plasmas often contain a high proportion of charged dust grains, which will also be drawn into the filament. Viscous drag between the charged particles and neutral atoms will tend to draw the neutral atoms towards the filament as well.</i></b><br /><br /><i>Therefore, current filaments in space will tend accumulate matter in them as a result of the misalignment of the electric field causing the current and the total magnetic field.</i><br /><br /><i>Remembering that pinches can occur if any misalignment of I and B occurs, any matter that has been drawn into the filament will also be compressed if a misalignment of I and B occurs. If the pinch force is large enough, it can fragment the filament into discrete spherical or toroidal plasmoids along the axis of the current. Any matter in the pinch zone would then become compressed into the same form.</i><br /><br /><b><i>Because the electromechanical forces are vastly stronger than gravity, this mechanism offers a means by which diffuse matter can be accumulated and compressed in a much more efficient way than gravitational compression of diffuse clouds of fine dust particles.</i></b><br /><br /><b><i>Of course, once the matter has been sufficiently compressed and if it is neutralized by recombination of ions and electrons, then the electromagnetic forces may be reduced to the point that gravity becomes significant and continues the compression started by the electromagnetic forces.</i></b><br /><br /><i>6.12  Marklund Convection</i><br /><br /><b><i>In the case of a cylindrical current, the E × B force is radially inwards</i></b><i> and results in the self-constriction of a current filament, as we have seen. </i><b><i>This results in an increase in the particle density near the axis of the current.</i></b><i> Two things can then happen.</i><br /><br />- <b><i>The first is that radiative cooling from the regions of increased density can result in a temperature decrease nearer the center, contrary to the increase one might intuitively expect from increasing the density.</i></b><br /><br />- <b><i>The second is that recombination of ions and electrons starts to occur.</i></b><br /><br /><b><i>Every chemical element has a particular energy level, known as its ionization energy, at which it will either ionize or recombine. This is analogous to the boiling point of a liquid such as water: at a particular temperature, the phase or state of the matter will change from one state to another.</i></b><i> [Figure I]</i><br /><br /><i>If the kinetic energy of motion is equated with the ionization energy, then a characteristic velocity, known as the Critical Ionization Velocity (CIV), can be derived for each element. Because temperature is a measure of thermal energy, CIV can be related to temperature. The CIV values of elements commonly found in space are not distributed randomly but are grouped into four distinct bands around certain velocity values. Within each band, all the elements in that band have similar CIVs to each other.</i><br /><br /><b><i>In the vicinity of a field-aligned current, the E × B force causes a radial drift of ions and electrons towards the cooler central axis. Because of their differing CIVs, different ions will recombine at different radii as they move towards the centre and enter progressively cooler regions.</i></b><br /><br /><i>This process is known as Marklund Convection after the Swedish physicist who discovered it, Göran Marklund.</i><br /><br /><b><i>The net result is that Marklund Convection sorts any elements present in the locality into different groups according to their ionization potentials. The groups of elements are arranged in cylindrical shells at different radii within a cylindrical field-aligned current.</i></b><br /><br /><b><i>As hydrogen has a high CIV compared to the other elements, it will recombine first, in a cylindrical shell of larger radius than the shells of the other elements.</i></b><br /><br /><b><i>This type of electrical sorting may be responsible for some of the non-random distribution of elements that we observe in the cosmos. In particular, it may explain the preponderance of neutral hydrogen in thread-like structures throughout the galaxy that have been detected by radio telescopes.&quot;</i></b><br /><br />Astrophysics and Space Science (1978)<br /><br />Interstellar Clouds and the Formation of Stars<br />by Hannes Alfven and Per Carlqvist<br /><br /><a href=\"http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?1978Ap%26SS..55..487A&amp;defaultprint=YES&amp;filetype=.pdf\" class=\"ot-anchor\">http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?1978Ap%26SS..55..487A&amp;defaultprint=YES&amp;filetype=.pdf</a><br /><br /><b><i>&quot;Where the condensation of interstellar clouds has previously been treated in the literature, magnetic fields have generally been considered to obstruct the process.  This point of view, however, is not unconditionally true.  On the contrary, magnetic fields will, under certain conditions, promote the contraction of interstellar clouds or may even constitute the main mechanism for contraction.&quot;</i></b><br /><br />The following paper is one of the few papers ever published on this specific subject, which is quite remarkable given the importance of the subject.  There is clearly an awareness problem when it comes to this alternative theory for accretion of matter into stars.  Do not feel obligated to read the following paper; I include it for the simple reason that it is historically significant, and encourage you to skip ahead to PART 2 if you prefer ...<br /><br /><a href=\"http://www.nature.com/nature/journal/v277/n5695/pdf/277370b0.pdf\" class=\"ot-anchor\">http://www.nature.com/nature/journal/v277/n5695/pdf/277370b0.pdf</a><br /><br />Plasma convection in force-free magnetic fields as a mechanism for chemical separation in cosmical plasmas [Figure J]<br /><br /><b><i>&quot;DURING the past two decades, progress in space research has forced us to abandon the earlier idea of a cosmical plasma as being a homogeneous medium. Filament formation and chemical separation are two important phenomena that are often observed in cosmical plasmas and illustrate the importance of inhomogeneity. An average composition from the Sun&#39;s atmosphere used to be taken as a standard when expressing cosmic abundancies in general. Numerous observations, however, of the chemical composition in the solar wind from in situ measurements and from lunar soil and meteorite samples, indicate a great variability in these ratios</i></b><i> [1]. The proton to alpha-particle flux ratio for solar flare energetic particle events varies widely between and within events [2]. </i><b><i>Recent observations show that this variability is connected with the flare process itself and the earlier history of the flare, rather than propagation effects between the flare site and the observer. Some local chemical differentiation mechanism in the preflare history of the specific active region is probably responsible for the composition of the emitted, energetic particles.</i></b><i> The phenomena of filamentation and chemical separation may be coupled, as in the presence of a temperature gradient, the plasma convection associated with filamentary structures provides an effective means of selective transport. The general principles of this mechanism are described here. These principles may be important not only in solar flares but in many other cosmical plasmas where force-free fields and thermal gradients occur.</i><br /><br /><i>Typical filamentary structures, such as prominences and solar flares, can often be pictured as helical twisted magnetic flux tubes [3-6], where the magnetic field is approximately force free. The electric current flows almost in the direction of the magnetic field.</i><br /><br /><i>Consider for simplicity an axisymmetric case with a force-free helical magnetic field B, and a homogeneous electric field E applied parallel to the axis of the filament, to drive the current (Fig. 1). We assume that we have a transparent filament where the temperature decreases towards the axis of the filament due to a preferential cooling by radiation of the densest regions.</i><br /><br /><i>Under the influence of electric and magnetic fields the ionised component of the plasma will drift inward from the surroundings towards the centre of the filament with a velocity V = E x B / B^2, directed radially inwards. Some steady state models of filamentary structures, where this inward drift of plasma is balanced by an outward diffusion of neutrals, have been described elsewhere [7].</i><br /><br /><i>This E x B / B^2 convection is a very efficient process for collecting material to form the filament. This is true, even if the process is slowed by collisions, because as long as the particles are charged they are forced to drift inwards.</i><br /><br /><i>If the plasma is partially ionised -- as in part of the solar atmosphere and many other cosmical plasmas -- a temperature gradient will cause the radial transport to be different for elements with different ionisation potentials.</i><br /><br /><i>The most abundant elements of a cosmical plasma can be divided into groups of roughly equal ionisation potentials as follows: element, (approximate ionisation potential): He, (24 eV); H,O, N, (13 eV); C, S, (11 eV); Fe, Si, Mg, (8 eV).</i><br /><br /><i>Consider, for example, a case where the ambient plasma consists of hydrogen and helium in some initial proportion p0 = {n(H) + n(H+)} / {n(H-sube) + n(H-sube +) + n(Hsube 2+)} where n() denotes the density of the element within the parentheses. In the configuration considered the ionised components of the plasma will drift inwards and experience a temperature decreasing with radius. When the charged particles have drifted so far that the local temperature is well below the ionisation potential of helium, the rate of recombination of ionised helium is considerable. The neutral helium produced is unaffected by the electric and magnetic forces that drive the ionised particles inwards. On the other hand, the hydrogen is still almost fully ionised, and is therefore still under the influence of the inward force. The protons will continue to drift inwards until they reach a radius where the temperature is well below the ionisation potential of hydrogen. Thus helium and hydrogen will be preferentially deposited in different regions (which may partly overlap). Compared with the composition of the initial ambient plasma, the above process will result in an enrichment of hydrogen in the inner parts (p &gt; p0) and of helium in the outer parts (p &lt; p0).</i><br /><br /><i>Although this discussion has considered a very simple configuration, the process works under very general circumstances. Any E x B / B^2 convection in a multicomponent plasma will tend to separate elements of different ionisation potentials whenever spatial temperature gradients cause the different elements to leave (or join) the convection preferentially in different regions of space. Note also that the E x B / B^2 convection can be a very rapid transport process compared with diffusion. (The latter will in general tend to counteract the separation. However, in thermally inhomogeneous, partly ionised plasmas, even diffusive processes can lead to non-homogeneous chemical distributions [8-10].)</i><br /><br /><i>In the case of the flare process, the separation mechanism discussed above might be of importance for the great variability in composition observed. Because an equilibrium composition requires some time to get established in this way, an individual flare or the first flare in a series of flares will show a different composition of particles than that of a successive flare, which shows a composition characteristic of the partially reestablished equilibrium. However, here the main purpose is to emphasise the existence of a separation mechanism intimately linked to force-free filamentary structure, and to describe the basic operative principles and potentially wide applicability.</i><br /><br /><i>I thank Professor H. Alfven for discussions, and Professor C.-G. Falthammar for reading the manuscript.&quot;</i><br /><br />G. T. MARKLUND<br /><br />Department of Plasma Physics,<br />Royal Institute of Technology,<br />S-100 44 Stockholm, Sweden<br /><br />Received 20 November; accepted 7 December 1978.<br /><br />1. Hirshberg, I. Rev. Geophys. Space Phys. 13, 1059 (1975).<br /><br />2. Briggs, P. R., Armstrong, T. P. &amp; Krimigis, S. M. Astrophys. Lett. (in the press).<br /><br />3. Murty, G. S. Ark. f. Fys. 21, 203 (1 961).<br /><br />4. Alfven, H. Ark. f. Fys. 19, 375 (1961).<br /><br />5. Colgate, S. A. Astrophys. J. (in the press).<br /><br />6. Petrasso, R. P., Kahler, S. W., Kriger, A. S. Sille, J. K. &amp; Vaiana, G. S. Astrophys. J. Lett. (in the press).<br /><br />7. Marklund, G. T. Rep. TRITA-EPP-78-09, (Royal Inst. of Techn., Stockholm, 1978).<br /><br />8. Arrhenius. G. et al. Astrophys. Space Sci. (in the press).<br /><br />9. Bonnevier, B. Plasma Phys. 13, 763 (Pergamon, Oxford, 1971).<br /><br />10. Bonnevier. B. Astrophys. Space Sci. 40, 231 (1976).<br /><br />PART 2: RECENT OBSERVATIONS OF STAR-FORMING REGIONS<br /><br />Now, there are three contextual aspects to realize here:<br /><br />(1) Recent observations, in particular by the Herschel telescope, have revealed the surprise that stars oftentimes form in groups along branching filaments, similar to lightning [Figure K].<br /><br />(2) Despite their manageable complexity, realize that the arguments you&#39;ve just read for the Lorentz force and Marklund convection are not widely considered as a mechanism for protostellar accretion by solar experts or astrophysicists because of their more general assumption that gravity dominates the universe.<br /><br />(3) And realize as well that observations of star-forming regions are only now beginning to accurately penetrate the thick dust which surrounds them.  What they are observing in some of the more energetic cases -- the FU Orionis protostars -- are filaments connecting to the stars and dramatic spikes in brightness.<br /><br /><a href=\"http://www.space.com/32266-young-stars-grow-chaotically-study-shows.html\" class=\"ot-anchor\">http://www.space.com/32266-young-stars-grow-chaotically-study-shows.html</a><br /><br /><i>&quot;For the new work, astronomers focused on protostars known as FU Orionis objects.  These young stars, also known as FUors, are known to experience dramatic spikes in brightness, the researchers said.&quot;</i><br /><br /><a href=\"http://www.space.com/32266-young-stars-grow-chaotically-study-shows.html\" class=\"ot-anchor\">http://www.space.com/32266-young-stars-grow-chaotically-study-shows.html</a><br /><br /><i>&quot;Previous research often envisioned protostars growing in a simple manner, </i><b><i>steadily accumulating or accreting fuel from surrounding clouds.&quot;</i></b><br /><br /><a href=\"http://advances.sciencemag.org/content/advances/2/2/e1500875.full.pdf\" class=\"ot-anchor\">http://advances.sciencemag.org/content/advances/2/2/e1500875.full.pdf</a><br /><br /><b><i>&quot;The envelope of V1057 Cyg shows several spikes, connected with filaments on much more extended spatial scales.&quot;</i></b><i> [Figure L]</i><br /><br /><a href=\"http://www.space.com/32266-young-stars-grow-chaotically-study-shows.html\" class=\"ot-anchor\">http://www.space.com/32266-young-stars-grow-chaotically-study-shows.html</a><br /><br /><i>&quot;The new images of the flaring newborn stars &#39;were surprising and fascinating, and nothing like anything previously observed around young stars,&#39; representatives of the National Institutes of Natural Sciences (NINS) in Japan said in a statement ... The researchers discovered &#39;tails&#39; projecting from the protoplanetary material around the young stars, as well as spikes of gas and dust.&quot;</i><br /><br />The most important aspect of these observations is that they were a surprise.  A surprise implies the potential for a theoretical mistake -- that we&#39;ve fundamentally misunderstood the process, and are approaching the problem with the wrong toolset.<br /><br />In this particular case, the observation of extended filaments and dramatic spikes in brightness are suggestive of a plasma physics explanation.  Neither gravity nor fluids generally form into filaments, yet with the right tweaks to parameters here and there, theorists can cajole a model to say whatever they like ...<br /><br /><a href=\"https://arxiv.org/pdf/1609.03402v1.pdf\" class=\"ot-anchor\">https://arxiv.org/pdf/1609.03402v1.pdf</a><br /><br />On the existence of accretion-driven bursts in massive star formation<br /><br />(... Note that the researchers have created a simulation here ... [Figure M])<br /><br /><i>&quot;Until recently, it was thought that FU-Orionis-type accretion and luminosity bursts were constrained to occur in the solar mass regime of present-day star formation (Audard et al. 2014). However, </i><b><i>recent numerical hydrodynamics simulations</i></b><i> of primordial disc formation around the first very massive stars have also revealed the presence of accretion bursts </i><b><i>caused by disc gravitational fragmentation</i></b><i> followed by rapid migration of the fragments onto the protostar (Stacy et al. 2010; Greif et al. 2012; Smith et al. 2012; Vorobyov et al. 2013; Hosokawa et al. 2016).&quot;</i><br /><br />There is a sustained pattern in the sciences today to look past the empirical differences between the scientific domains, with the net impact being that thought experiments from the most speculative domains are spoken of as facts long before the existence of detailed observations or experiments which might actually validate them.<br /><br />And once the distinction between empirical and speculative becomes blurred; once &quot;facts&quot; become disconnected from the reasons to believe them; once people stop seeking out any critical arguments or history, what happens in practice is a natural bias towards the textbook theories and an uninformed, often hostile rejection of better ideas.<br /><br />GRAPHIC REFERENCES:<br /><br />[1] <a href=\"https://www.plasma-universe.com/Filamentation\" class=\"ot-anchor\">https://www.plasma-universe.com/Filamentation</a><br /><br />[2] Marklund, Göran (February 1979). &quot;Plasma convection in force-free magnetic fields as a mechanism for chemical separation in cosmical plasma&quot;. <i>Nature</i>. 277: 370–371. Bibcode:1979Natur.277..370M. doi:10.1038/277370b0.<br /><br />[3] <a href=\"http://techtv.mit.edu/tags/441-physics/videos/813-mit-physics-demo----forces-on-a-current-carrying-wire\" class=\"ot-anchor\">http://techtv.mit.edu/tags/441-physics/videos/813-mit-physics-demo----forces-on-a-current-carrying-wire</a><br /><br />[4] <a href=\"https://www.thunderbolts.info/wp/2011/12/06/essential-guide-to-the-eu-chapter-6/\" class=\"ot-anchor\">https://www.thunderbolts.info/wp/2011/12/06/essential-guide-to-the-eu-chapter-6/</a><br /><br />[5] <a href=\"http://sydney.edu.au/science/physics/about/pinch.shtml\" class=\"ot-anchor\">http://sydney.edu.au/science/physics/about/pinch.shtml</a><br /><br />[6] <a href=\"https://www.plasma-universe.com/Birkeland_current\" class=\"ot-anchor\">https://www.plasma-universe.com/Birkeland_current</a><br /><br />[7] <a href=\"http://images.nrao.edu/110\" class=\"ot-anchor\">http://images.nrao.edu/110</a><br /><br />[8] <a href=\"http://www.plasma-universe.com/Critical_ionization_velocity\" class=\"ot-anchor\">http://www.plasma-universe.com/Critical_ionization_velocity</a><br /><br />[9] Marklund, Göran (February 1979). &quot;Plasma convection in force-free magnetic fields as a mechanism for chemical separation in cosmical plasma&quot;. <i>Nature</i>. 277: 370–371. Bibcode:1979Natur.277..370M. doi:10.1038/277370b0.<br /><br />[10] <a href=\"http://www.aanda.org/articles/aa/pdf/2011/05/aa16596-11.pdf\" class=\"ot-anchor\">http://www.aanda.org/articles/aa/pdf/2011/05/aa16596-11.pdf</a><br /><br />[11] <a href=\"http://advances.sciencemag.org/content/advances/2/2/e1500875.full.pdf\" class=\"ot-anchor\">http://advances.sciencemag.org/content/advances/2/2/e1500875.full.pdf</a><br /><br />[12] <a href=\"https://arxiv.org/pdf/1609.03402v1.pdf\" class=\"ot-anchor\">https://arxiv.org/pdf/1609.03402v1.pdf</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Constructivist Revolution","summary":"Thomas Kuhn Reduced Modern Science to Two Fundamental States or Modes","image":"https://lh3.googleusercontent.com/-5BMW2Mo-5L4/WB7WpGPaSXI/AAAAAAAAJKw/jZM47ww09FA4aZTbuogyrGv6lC6_n9mDgCJoC/w1800-h2520/the-constructivist-revolution-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/fTuMvbBD3Dg","publishDate":"2016-11-06T07:09:03.247Z","updateDate":"2017-01-03T01:47:42.929Z","text":"<b>The Constructivist Revolution: Thomas Kuhn Reduced Modern Science to Two Fundamental States or Modes: Normal and Revolutionary / The Distinction Appears to Occur for Each Topic of Research / Yet, Scientific Process - Like Peer Review - Remains Monolithic / This Creates a Tension in Modern Science which Will Persist, and Possibly Grow, Until it is Resolved / We Can Speculate on How this Will Turn Out</b><br /><br />There&#39;s a problem with modern science which many people seem to not notice.  It&#39;s an oversight that has created a tension which is going to result in change.<br /><br />It&#39;s not very complicated.<br /><br />The philosopher, Thomas Kuhn, reduced science to two fundamental states: &quot;normal&quot; and &quot;revolutionary&quot; sciences.  The distinction seems to occur in parallel for each topic of research.  Yet scientific method and process remains just one ... completely monolithic: We subject both normal and revolutionary forms of science to the same, single process.  We&#39;ve fundamentally failed to react to Thomas Kuhn&#39;s groundbreaking central thesis through a modification of our process.<br /><br />This oversight creates a tension in modern science which will persist, and possibly even grow, until it is resolved.  Whether or not a person sees this tension is a function of their focus.  It&#39;s regardless there and persistent over time.  Many think that it will simply go away if it&#39;s ignored.<br /><br />Of the small set of people in our culture who understand and see the tension of modern science, there is only one man -- to date -- who has proposed a specific solution: That&#39;s Dr. Gerald Pollack and his idea is called the Institute for Venture Science (IVS).  He wants to create a funding agency that focuses exclusively upon potential challenges to textbook theory.  He&#39;s already advised the world&#39;s largest funding agencies on this same problem (they&#39;re fully aware of peer review&#39;s gatekeeping problem).<br /><br />Dr. Pollack&#39;s approach with the IVS is actually one of a class of solutions.  The broader idea is to simply introduce a second parallel philosophical process which activates once a topic is recognized as challenged.<br /><br />My own short-term approach has also been to simply ignore normal science, but probably what will win out in the long run is that the providers of these parallel paths will in some contexts subsume the normal science programs (funding, journalism and education) -- because the approach will succeed through an increase in innovation.<br /><br />In both of the very separate worlds of science education and philosophy of science, there are two opposing philosophies which have historically split academics: positivism and constructivism.  The debate is very challenging and confusing in that the context -- be it philosophy or science education -- can suggest opposing, contradictory philosophical solutions.  These are the kinds of debates which can rage for many decades.<br /><br />Let&#39;s briefly review the dichotomy ...<br /><br /><a href=\"http://www.calpro-online.org/eric/docs/brown/brown01_03.pdf\" class=\"ot-anchor\">http://www.calpro-online.org/eric/docs/brown/brown01_03.pdf</a><br /><br />What is Unique About Constructivism? (Assumptions)<br /><br /><b><i>&quot;Educators and reformers often worry that today’s students spend too much of their time simply absorbing -- and then reproducing -- information transmitted to them. They fear that students aren’t learning how to make sense of what they are told</i></b><i> and that there is little connection between activities in the classroom and the world beyond school. Students can earn credits, good grades and high test scores, they say, demonstrating a kind of mastery that frequently seems trivial, contrived or meaningless outside the school.</i><br /><br /><i>[...]</i><br /><br /><i>Consider the types of mastery demonstrated by successful adults, such as scientists, musicians, business entrepreneurs, novelists, nurses and designers.</i><br /><br /><i>[...]</i><br /><br /><b><i>The people mentioned face the challenge of constructing or producing meaning or knowledge, instead of merely reproducing meaning or knowledge created by others.</i></b><br /><br /><i>[...]</i><br /><br /><i>Students taught within a conventional curriculum, on the other hand, are usually asked merely to identify the work that others have produced. They may be drilled on the differences between nouns and verbs, for example, or called upon to match authors with their works.</i><br /><br /><i>Disciplined Inquiry</i><br /><br /><i>For achievement to be authentic, it must be grounded in a field of knowledge, which usually includes facts, a specific vocabulary and a set of concepts and theories. Authentic performance in that field reflects an in-depth understanding of a particular problem or issue. That understanding is expressed through elaborate forms of communication that make use of written, visual and/or symbolic language to express ideas, nuances and details.</i><br /><br /><b><i>The conventional school curriculum, on the other hand, is more likely to require students to memorize isolated facts about a wide array of topics, and then use those facts to complete short-answer tests, which don’t require deep understanding or elaborate communication.</i></b><br /><br /><i>[...]</i><br /><br /><b><i>Authentic performance occurs when the student reaches beyond imitation or reproduction of information and analyzes or interprets that information to solve a problem that can’t be solved by information retrieval alone.</i></b><i> We also add the criterion of disciplined inquiry, which requires a student to demonstrate in-depth understanding using substantial knowledge from an authoritative field. </i><b><i>Constructivism on the other hand, doesn’t necessarily require that a student’s construction of knowledge conform to knowledge considered authoritative by others.</i></b><br /><br /><b><i>[Some critics of constructivism advance the perspective] that knowledge and skills exist in workplace practices and are not subject to negotiation.</i></b><i> Instead of viewing knowledge as subjective, these educators contend that there are &#39;givens which morally, prudentially, and educationally cannot be left for learners to construct or misconstruct for themselves&#39; (p. 46). O’Carroll (1997), however, argues that </i><b><i>the issue is not about what forms of knowledge are defendable, but rather how knowledge is constructed and what it means to the learner. In his view, understanding is the critical factor in knowledge construction. The constructivist’s emphasis is not on learning outcomes, but about the ways people learn; and, therefore, about the process of learning that enables a learner to make connections between what is known and what is unknown.&quot;</i></b><br /><br />There&#39;s a sense in the debate over constructivism of people talking past one another, of an interdisciplinary fight between educators and philosophers, and of a dramatic difference in context: Constructivism is focused on perfecting learning and the constructing of new meanings, whereas positivism is essentially focused upon truth-seeking.<br /><br />The distinction is noted in an online email exchange ...<br /><br /><i>&quot;Constructivism is, although fundamentally epistemological in its &#39;terms of reference&#39;, essentially an &#39;applied science&#39;, while philosophy is not. So if it fails to meet the exacting categories of the philosophers, is this really important if it meets the needs and positively changes the practices of science educators (ie classroom teachers)?&quot;</i><br /><br />It seems that the debate has no contextual <b>locus</b>; in other words, the right approach will depend upon the context.  Further, I would personally add, it should be apparent that when science is functioning &quot;normally&quot;, truth-seeking (positivism) is preferable.  But, when a research topic has been seriously challenged, we must prepare to construct new meanings and new theories (constructivism).  The difference between the two contexts is that in the former normal science context, the objective is to align to a particular valid model with a presumed-correct set of assumptions and hypotheses (a worldview).  By contrast, with revolutionary science, there is no presumed worldview because expert authority has been challenged and the worldview itself is undergoing a transition.  Thus, truth-seeking &quot;loses its bearings&quot;, and we have to regress in our approach to perfecting learning and meaning-making.<br /><br />We can look to constructivism to get a feel for what a &quot;venture science&quot; philosophy would look like.  In the light of modern science&#39;s tension, the answer is to simply switch the monolithic process to a dual-track process -- where &quot;normal&quot; science is pursued in a positivistic manner, and &quot;revolutionary&quot; science claims are dealt with in a constructivist manner.<br /><br />Many, many debates have been had based upon the assumption that science must be monolithic.  The lesson of Kuhn should have been that it can&#39;t be.  We need a philosophy of science which seeks out truth.  But when claims become challenged, we should modify our process to more of a constructivist multi-worldview approach.  The details of how to do that should be left to the creator.<br /><br />As Gerald Pollack has argued, we can do it for funding.<br /><br />What is less realized today is that we can apply this same pattern to science journalism.<br /><br />We can do it for website and social network design.<br /><br />And we can do it for science education, as well.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Neutrino-Sunspot Anti-Correlation","summary":"The Sun's Neutrino Output has at Times Varied Inversely with the Sun's Surface Sunspot Count / Were the Neutrinos Produced in the Sun's Nuclear Core, this Relationship would be Inconceivable, Since Solar Physicists Calculate that it Takes About 200,000 Years for the Energy of Internal Fusion to Affect the Sun's Surface / The Observations Seem to Raise the Possibility that Fusion is Occurring Near the Sun's Surface / It Only Took One Non-Correlated Half-Cycle for Theorists to Completely Stop Paying Attention","image":"https://lh3.googleusercontent.com/-hdesDu1im48/WB47yksN3fI/AAAAAAAAJJc/PxYV0awTx446YC1jq6A4wP-IPpbyBz19ACJoC/w3600-h5040/the-neutrino-sunspot-anti-correlation-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/hszXQVEFQSu","publishDate":"2016-11-05T20:08:16.600Z","updateDate":"2017-01-03T01:48:04.063Z","text":"<b>The Neutrino-Sunspot Anti-Correlation: The Sun&#39;s Neutrino Output has at Times Varied Inversely with the Sun&#39;s Surface Sunspot Count / Were the Neutrinos Produced in the Sun&#39;s Nuclear Core, this Relationship would be Inconceivable, Since Solar Physicists Calculate that it Takes About 200,000 Years for the Energy of Internal Fusion to Affect the Sun&#39;s Surface / The Observations Seem to Raise the Possibility that Fusion is Occurring Near the Sun&#39;s Surface / It Only Took One Non-Correlated Half-Cycle for Theorists to Completely Stop Paying Attention</b><br /><br />The excellent chart in the graphic comes from ...<br /><br /><a href=\"https://books.google.com/books?id=El8R8nC5QegC&amp;pg=PA480&amp;lpg=PA480&amp;dq=five-point+running+average+of+the+neutrino+capture+rate&amp;source=bl&amp;ots=77fuvfJxiG&amp;sig=fvhU3HEV7kG-oSi4FWBe6qi5owQ&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwj4jPOVkZLQAhVM0FQKHZacB0QQ6AEIIDAB#v=onepage&amp;q=five-point%20running%20average%20of%20the%20neutrino%20capture%20rate&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=El8R8nC5QegC&amp;pg=PA480&amp;lpg=PA480&amp;dq=five-point+running+average+of+the+neutrino+capture+rate&amp;source=bl&amp;ots=77fuvfJxiG&amp;sig=fvhU3HEV7kG-oSi4FWBe6qi5owQ&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwj4jPOVkZLQAhVM0FQKHZacB0QQ6AEIIDAB#v=onepage&amp;q=five-point%20running%20average%20of%20the%20neutrino%20capture%20rate&amp;f=false</a><br /><br />A review of the controversy is a mix of statistical arguments, assertions rooted in textbook theory, and on occasion some fascinating speculations.  The conversation seems to raise more questions than it answers ...<br /><br /><i>Colliders and Neutrinos: The Window Into Physics Beyond the Standard Model</i>, by Rabindra Nath Mohapatra, Sally Dawson<br /><br /><a href=\"https://books.google.com/books?id=-oVpDQAAQBAJ&amp;pg=PA602&amp;dq=Even+though+later+analyses+showed+that+the+correlation+with+solar+activity+does+not+exist&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjd-4CRmZLQAhVHwFQKHVt8D1gQ6AEIHTAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=-oVpDQAAQBAJ&amp;pg=PA602&amp;dq=Even+though+later+analyses+showed+that+the+correlation+with+solar+activity+does+not+exist&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjd-4CRmZLQAhVHwFQKHVt8D1gQ6AEIHTAA#v=onepage&amp;q&amp;f=false</a><br /><br /><i>&quot;... Even though later analyses showed that the correlation with solar activity does not exist ...&quot;</i><br /><br /><i>Neutrino Astrophysics</i>,<br />by John N. Bahcall<br /><br /><a href=\"https://books.google.com/books?id=8GIP7uNMhlsC&amp;pg=PA29&amp;dq=an+alternative+explanation+of+the+apparent+anticorrelation+of+snus+with+spots+is+that+it+is+due+to+a+rather&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwifzvyZmpLQAhVpw1QKHVwPB08Q6AEIGzAA#v=onepage&amp;q=an%20alternative%20explanation%20of%20the%20apparent%20anticorrelation%20of%20snus%20with%20spots%20is%20that%20it%20is%20due%20to%20a%20rather&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=8GIP7uNMhlsC&amp;pg=PA29&amp;dq=an+alternative+explanation+of+the+apparent+anticorrelation+of+snus+with+spots+is+that+it+is+due+to+a+rather&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwifzvyZmpLQAhVpw1QKHVwPB08Q6AEIGzAA#v=onepage&amp;q=an%20alternative%20explanation%20of%20the%20apparent%20anticorrelation%20of%20snus%20with%20spots%20is%20that%20it%20is%20due%20to%20a%20rather&amp;f=false</a><br /><br /><i>&quot;An alternative explanation of the apparent anticorrelation of SNUs with spots is that it is due to a rather improbable coincidence&quot;</i><br /><br /><i>High Energy Cosmic Rays</i>, by Todor Stanev<br /><br /><a href=\"https://books.google.com/books?id=1y9YEYHAfBMC&amp;pg=PA50&amp;lpg=PA50&amp;dq=In+the+1980s+there+was+a+widely+spread+suspicion+of+the+existence+of+a+second+solar+neutrino+puzzle&amp;source=bl&amp;ots=WuUWWj9yxX&amp;sig=pOWdUMPMzSK9hRGz4idg0112JM0&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwi_v4zLmZLQAhVEzVQKHdxhClUQ6AEIGzAA#v=onepage&amp;q=In%20the%201980s%20there%20was%20a%20widely%20spread%20suspicion%20of%20the%20existence%20of%20a%20second%20solar%20neutrino%20puzzle&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=1y9YEYHAfBMC&amp;pg=PA50&amp;lpg=PA50&amp;dq=In+the+1980s+there+was+a+widely+spread+suspicion+of+the+existence+of+a+second+solar+neutrino+puzzle&amp;source=bl&amp;ots=WuUWWj9yxX&amp;sig=pOWdUMPMzSK9hRGz4idg0112JM0&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwi_v4zLmZLQAhVEzVQKHdxhClUQ6AEIGzAA#v=onepage&amp;q=In%20the%201980s%20there%20was%20a%20widely%20spread%20suspicion%20of%20the%20existence%20of%20a%20second%20solar%20neutrino%20puzzle&amp;f=false</a><br /><br /><b><i>&quot;In the 1980s there was a widely spread suspicion of the existence of a second solar neutrino puzzle -- an anticorrelation of the detection rate of solar neutrinos with solar activity.  A mild effect of that type has been observed in the early 1970s but with the improvement of the stability of the Homestake detector, both in the extraction technique and counting, the correlation after 1980 appeared to become stronger [40].  The further increase of the Homestake statistics and the emergence of other detectors did not confirm the effect, which is most likely due to the difficulties in estimating the statistical significance in cases with very low statistics.  Actually through the decades of its operation the Homestake event rate has been found to correlate with almost everything, from the market value of some famous stocks to the fraction of republican senators in the US Senate.&quot;</i></b><br /><br /><a href=\"http://web.stanford.edu/group/Sturrock/papers/Walther99/Walther99.pdf\" class=\"ot-anchor\">http://web.stanford.edu/group/Sturrock/papers/Walther99/Walther99.pdf</a><br /><br /><i>The Astrophysical Journal</i> (1999)<br /><br />On The Solar-cycle Modulation of the Homestake Solar Neutrino Capture Rate and the Shuffle Test<br /><br />Guenther Walther<br /><br /><i>&quot;There exists no significant correlation between the Homestake neutrino data up to run 133 and the monthly sunspot number, according to a test that is based on certain optimality properties for this type of problem. It is argued that priorly reported highly significant results for segments of the data are due to a statistical fallacy: the usual methods for evaluating the significance of common tests for correlation are not applicable in the sunspot-neutrino context.</i><br /><br /><i>[...]</i><br /><br /><i>Standard tests for correlation are built on strong assumptions that usually do not apply in a time series context. A violation of these assumptions can lead to erroneous, highly significant results. There exist statistical techniques to evaluate nonparametric measures of correlation in a time series context. An appropriate test based on certain optimality properties shows that there is no significant correlation between the sunspot number and the solar neutrino flux up to run 133. A proper reanalysis of reported highly significant correlations for earlier stretches of the data gives results that are compatible with the hypothesis of no correlation. These findings allow one to put together a coherent picture of the somewhat conflicting evidence reported in the literature: the correlation analysis does not contradict the periodogram analysis in Bahcall &amp; Press (1991), where no significant 11 yr component was found in the neutrino data. And the widely reported improved correlation with smoother functions of solar activity is likely due to the statistical effect ...&quot;</i><br /><br /><i>Solar Interior and Atmosphere</i>,<br />edited by Arthur N. Cox, William Charles Livingston, Mildred Shapley Matthews<br /><br /><a href=\"https://books.google.com/books?id=u3EAvRsJoZgC&amp;pg=PA65&amp;lpg=PA65&amp;dq=numerous+authors+have+examined+the+data+from+the+homestake+experiment+as+the+data+evolved+using+various+statistical&amp;source=bl&amp;ots=KkyAHvsjDb&amp;sig=YpDn-xLunYY_foLat_MfbryYkjM&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjVzbTempLQAhVmqVQKHcQSAzUQ6AEIGzAA#v=onepage&amp;q=numerous%20authors%20have%20examined%20the%20data%20from%20the%20homestake%20experiment%20as%20the%20data%20evolved%20using%20various%20statistical&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=u3EAvRsJoZgC&amp;pg=PA65&amp;lpg=PA65&amp;dq=numerous+authors+have+examined+the+data+from+the+homestake+experiment+as+the+data+evolved+using+various+statistical&amp;source=bl&amp;ots=KkyAHvsjDb&amp;sig=YpDn-xLunYY_foLat_MfbryYkjM&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjVzbTempLQAhVmqVQKHcQSAzUQ6AEIGzAA#v=onepage&amp;q=numerous%20authors%20have%20examined%20the%20data%20from%20the%20homestake%20experiment%20as%20the%20data%20evolved%20using%20various%20statistical&amp;f=false</a><br /><br /><i>&quot;Numerous authors have examined the data from the Homestake experiment as the data evolved using various statistical methods to decide whether the data set is consistent with a constant flux of neutrinos.  They have also examined whether there is evidence for a variation in the observed 37 Ar production rates and whether there are unique correlations with solar or astronomical phenomena.</i><br /><br /><i>Below, we refer to these analyses and indicate the statistical treatment and conclusions.  Ehrlich (1982) found no periodicity in the range 0.4 to 5.0 yr. Basu (1982) reported a correlation with the high-level Ap index.  Haubold and Gerth (1985, 1990) made a harmonic analysis of data from 1970 to 1989 and found periods of 8.33, 5.00, 2.13, 1.61, 0.83, 0.61, 0.54 and 0.51 yr.  </i><b><i>Lanzerotti and Raghaven (1981) found a correlation with cosmic-ray fluxes and intense solar flares</i></b><i>.  Gavrin and Kopylov (1984) used Kolomogorov-Smirnov and chi-squared tests to find no variations.  Bahcall et al. (1987a) noticed that the correlations depend on just a few experimental points.  Subramanian and Lal (1987) and Subramanian et al. (1988) found that the correlations were suggestive of neutral-penetrating particles.  Wilson (1987) found no evidence for variations from 1971 to 1981.  Raychaudhuri (1986, 1989) published many papers using various statistical tests that favored variations.  Attolini et al. (1988) using a superposed epochs method found support for a 2.1-yr period.  Davis et al. (1990), Davis (1986), and Rowley et al. (1985), discussed a correlation analysis with sunspots and solar diameter data.  Baltz et al. (1989) did an analysis of correlations and streaks of high and low results.  </i><b><i>Nunokawa and Minakata (1991) found that any anti-correlation with sunspots depends on the treatment of errors and is not definitely established</i></b><i>.  Veselov et al. (1987) pointed out that there is a half-year correation with observed solar latitude in 1979 and 1980.  </i><b><i>Bahcall and Press (1991) showed that there is an anti-correlation with sunspots in 1977-1989, but not in 1970-1977</i></b><i>.  Krauss (1990) found a correlation with variations in low-order acoustic solar oscillations in 1977-1988.  </i><b><i>Bieber et al. (1990) showed that the data is strongly correlated with monthly sunspot numbers</i></b><i>, and the strength of this correlation rises when the semi-annual variation is included.  Filippone and Vogel (1990) found that a time dependence with sunspot numbers is more probable than a constant rate. </i><b><i>Dorman and Wolfendale (1990) suggested that there is an apparent correlation with solar magnetic fields</i></b><i>.  Fiorentini and Mezorani (1991) note that a fit to sunspot occurrences is slightly better than a fit to a constant flux.  Sakurai (1990) showed that a sequential analysis supports quasi-biennial variations.  Finally </i><b><i>Gavryuseva et al. (1990) found that a power spectrum and correlations analysis reveal 3 significant peaks that are unlikely to be of random origin.</i></b><br /><br /><b><i>Most authors that have analyzed the full set of data from 1970 to 1990 find a reasonably significant correlation between the 37 Ar production rate and sunspot occurrences, solar oscillation frequency changes and solar magnetic fields.  Obviously one needs a solar neutrino detector with a higher signal rate to make a definitive test of this phenomenon.&quot;</i></b><br /><br /><i>Stars as Laboratories for Fundamental Physics: The Astrophysics of Neutrinos</i>,<br />by Georg G. Raffelt<br /><br /><a href=\"https://books.google.com/books?id=6esnbt7BfIwC&amp;pg=PA381&amp;dq=because+of+the+statistically+high+significance+of+the+anticorrelations+with+solar+activity+of+the+homestake+results&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjru5D9mpLQAhVqzFQKHcPEC_AQ6AEIHTAA#v=onepage&amp;q=because%20of%20the%20statistically%20high%20significance%20of%20the%20anticorrelations%20with%20solar%20activity%20of%20the%20homestake%20results&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=6esnbt7BfIwC&amp;pg=PA381&amp;dq=because+of+the+statistically+high+significance+of+the+anticorrelations+with+solar+activity+of+the+homestake+results&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjru5D9mpLQAhVqzFQKHcPEC_AQ6AEIHTAA#v=onepage&amp;q=because%20of%20the%20statistically%20high%20significance%20of%20the%20anticorrelations%20with%20solar%20activity%20of%20the%20homestake%20results&amp;f=false</a><br /><br /><b><i>&quot;Because of the statistically high significance of the anticorrelations with solar activity of the Homestake results it is not entirely obvious how to proceed with a quantitative test of the oscillation hypothesis which can cause only a day-night or semiannual time variation.</i></b><i>  In the present section a possible long-term variability of the solar neutrino flux or its detection methods is ignored, i.e. the apparent anticorrelation with solar activity at Homestake is considered to be a statistical fluctuation.  In Sect. 10.7 a possible explanation in terms of neutrino interactions with the solar magnetic field is considered.</i><br /><br /><b><i>One may take the opposite point of view that the variability at Homestake is a real effect, and that it is not related to neutrino magnetic moments because that hypothesis requires fairly extreme values for the dipole moments and solar magnetic fields (Sect. 10.7).  An interpretation of the data in terms of neutrino oscillations then becomes difficult because one admits from the start that unknown physical effects are either operating in the Sun, in the intervening space, or in the detectors.</i></b><i>  One could argue perhaps that the apparent variability of the Homestake data in itself was evidence that something was wrong with this experiment.  In this case one could still test the hypothesis of neutrino oscillations under the assumption that the signal recorded at Homestake was spurious in which case one must discard the entire data set, not only parts of it as has sometimes been done.  Ignoring the Homestake data does not solve the solar neutrino problem, but its significance is reduced.</i><br /><br /><b><i>Still, as no one has put forth a plausible hypothesis for a specific problem with the Homestake experiment it is arbitrary to discard the data.  Admittedly, it is also arbitrary to consider the time variation spurious even though standard statistical methods seem to reveal a highly significant anticorrelation with solar activity&quot;</i></b><br /><br />The following review of the statistics is quite technical, but I think nevertheless important, because <b>it appears to argue that both sides can point to the same statistical data to make their opposing cases!</b><br /><br /><i>Neutrino Astrophysics</i>,<br />by John N. Bahcall<br /><br />(recall that Bahcall was the theorist who insisted upon revising theoretical solar neutrino predictions downwards as the first solar neutrino observations came in, before it was admitted that there was a &quot;solar neutrino problem&quot;)<br /><br /><a href=\"https://books.google.com/books?id=8GIP7uNMhlsC&amp;pg=PA328&amp;lpg=PA328&amp;dq=five-point+running+average+of+the+neutrino+capture+rate&amp;source=bl&amp;ots=VnJbEQ7-4L&amp;sig=qYogoLSx-CKgnxK3oDkUnVo_GIs&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwj4jPOVkZLQAhVM0FQKHZacB0QQ6AEIHTAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=8GIP7uNMhlsC&amp;pg=PA328&amp;lpg=PA328&amp;dq=five-point+running+average+of+the+neutrino+capture+rate&amp;source=bl&amp;ots=VnJbEQ7-4L&amp;sig=qYogoLSx-CKgnxK3oDkUnVo_GIs&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwj4jPOVkZLQAhVM0FQKHZacB0QQ6AEIHTAA#v=onepage&amp;q&amp;f=false</a><br /><br /><i>&quot;An anticorrelation between SNUs and sunspot number is apparent in Figure 10.7. </i><b><i>Davis (1987) pointed out that in the period from 1977 (corresponding to the beginning of sunspot cycle 21) through 1983 the correlation coefficient between yearly average 37 Ar production rate and sunspot numbers is 0.94.</i></b><i> The change in 37 Ar production rate in the three year period 1977 to 1980 corresponds to a change in the rate from 0.84 atoms day-1 (4.4 SNU) to 0.1 atoms day-1 (0.5 SNU), essentially equal to the cosmic ray background rate (0.4 SNU). After the drop, the 37 Ar production rate gradually recovers as the solar activity declines.</i><br /><br /><i>The most remarkable feature of the apparent correlation is that the capture rate is very close to the background level of 0.1 capture per day [see Eq. (10.5)] for the four measured points near 1980.0.</i><br /><br /><b><i>Is this apparent correlation real? Davis (1987) suggests that the effect that is apparent in Figure 10.7 has a statistical significance of about 5 standard deviations. Moreover, he stresses that the largest changes are seen at a characteristic time, when sunspot activity is at a minimum. On the other hand, Bahcall, Field, and Press (1987) have concluded on the basis of an analysis of the same data that the correlation is not significant at a definitive level.</i></b><br /><br /><b><i>How can one draw such different conclusions on the basis of the same facts? The following discussion shows that there are different plausible ways of looking at the data.</i></b><br /><br /><i>Consider a hypothetical linear relation between neutrino capture rate and sunspot number,</i><br /><br /><i>SNU = a + b x sunspots. (10.21)</i><br /><br /><i>The simplest statistic to investigate is whether the x2 [chi-square] distribution for the relation of Eq. (10.21) is significant or not. The numerical values of the pairs of variables (SNU, sunspots) can be taken from the paper of Rowley et al. (1985). </i><b><i>Remarkably enough, the significance of the correlation depends sensitively on exactly how one estimates the standard deviations associated with each measured SNU value.</i></b><i> The upper and lower error estimates are different in Cleveland&#39;s (1983) application of the maximum likelihood method. The reason for this asymmetry is that Cleveland does not allow negative values for the neutrino capture rate.</i><br /><br /><i>Bahcall et al. (1987) defined two different estimates of the individual standard deviations: (1) average error, which is equal to one-half the difference between the upper and lower limits quoted by Rowley et al. (1985); and (2) upper error, which is equal to the difference between the upper limit and the best estimate value. For most experimental situations in which the counting rate is large, observers quote symmetric estimates of the uncertainties, in which case the average error is equal to the upper error.</i><br /><br /><i>The null hypothesis is that the capture rate is constant in time (b = 0). This hypothesis can be marginally rejected (2% significance level) for average errors. However, the null hypothesis is not rejected for upper errors (61% significance level).</i><br /><br /><i>The reader will immediately see how both points of view are consistent with the above cited result. The &#39;stimulating&#39; view of Davis interprets the 2% significance level of rejection for the null hypothesis as quantitative confirmation of the apparent correlation. In Davis&#39; view, 2% is a highly suggestive result. On the other hand, </i><b><i>I am impressed by the fact that the results depend sensitively upon how the errors are estimated.</i></b><i> Even with the most favorable interpretation of the errors, the significance level for rejection of a constant flux is as high as 2%. Since there was no a priori physical reason for expecting an anticorrelation with sunspot number, many different variables could be (and perhaps were) considered in a search for a possible correlation. This makes, in my view, the 2% significance level seem inconclusive.</i><br /><br /><i>Since x2 probabilities depend sensitively on assumed Gaussian errors, Bahcall, Field, and Press (1987) also computed a different, more model-independent test for statistical significance. For the null hypothesis that the SNU flux is constant in time, the data (whatever its error distribution) should be statistically invariant under scrambling in time, as long as the pairing of each measurement with its own standard deviation is preserved. Therefore Bahcall et al. (1987) randomly permuted the data 1000 times. For each permutation, they fit to sunspot number as indicated in Eq. (10.21) and compared the value of x2 for the scrambled data with the value for the observed data. This test determines the acceptability of the null hypothesis: The neutrino capture rates are independent of time.</i><br /><br /><i>The fit of Eq. (10.21) to the real data is better than is obtained with the scrambled data in all but 5% of the permitted cases. This result confirms the conclusion of Davis (1987) (although not at the suggested extremely high significance level) that there is an apparent correlation between SNUs and sunspot number. However, in my view the significance level is not sufficiently small to justify rejecting definitively the null hypothesis of no time variation.</i><br /><br /><i>How robust, under editing of the data set, is the conclusion that there is a correlation? Not very. If the four low neutrino capture rates around 1980 are removed and an identical statistical analysis (scrambling on the data) is performed, the best fit SNU rate is practically independent of sunspot number. In fact, 86% of the scrambled data sets yield a better fit to the sunspot data than do the actual data [see Bahcall et al. (1987)].</i><br /><br /><i>The suggested correlation between neutrino capture rate and sunspot number depends almost entirely upon the four low points near the beginning of 1980. I believe that this sensitivity to a small number of points suggests that the correlation is less likely to be physically real. </i><b><i>On the other hand, the &#39;stimulating&#39; point of view rejoins that the four points that were removed are valid data and are especially significant because they occur near maximum sunspot activity.</i></b><br /><br /><b><i>In any event, the reader can see that there is some analogy between the current controversy and the traditional source of academic polemics, namely: &#39;Is it more correct to describe the coffee cup as half empty or half full?&#39;</i></b><br /><br /><i>B </i><b><i>Correlations with solar flares</i></b><br /><br /><b><i>Bazilevskaya et al. (1982) and Davis (1987) have suggested that solar flares may have produced neutrino pulses that showed up in his 37 Cl detector as unusually high observations.</i></b><i> This result is of great potential importance since conventional theoretical calculations indicate that, while flare dynamics is not understood in detail, there is not enough energy available in neutrinos produced by flares to cause a significant signal in the 37 Cl detector (Lingenfelter et al. (1985) and Bahcall, Field, and Press (1987)). </i><b><i>Moreover, many of the suggested theoretical interpretations of the 37 Cl experiment would have to be revised if it turned out that part of the observed event rate of 2 SNU was due to neutrinos produced by solar flares on the surface of the Sun, rather than by neutrinos produced by nuclear fusion in the solar interior.</i></b><br /><br /><i>The following discussion summarizes the observational evidence on the relation between solar flares and the 37 Cl neutrino capture rate [see Davis (1987) for a fuller discussion of the observations]. </i><b><i>Run 27, the highest experimental run, corresponded in time with the great solar flares of August 1972.</i></b><i> Unfortunately, the flares occurred early in the exposure interval and therefore most of the 37 Ar atoms that might have been produced by flares would have decayed before the extraction was completed. However, </i><b><i>Bazilevskaya et al. (1981) pointed out that run 71 might also be high because a burst of high-energy protons was produced by the Sun in October 1981.</i></b><i> The extraction for this period indeed indicated an enhanced capture rate. In his Table 2, Davis (1987) lists several other solar flares for which there was an excess of observed 37 Ar atoms in the tank. However, not all major solar flares yielded an excess of 37 Ar and the uncertainties in the number of atoms is large in all cases. If flare events were responsible for the increase in the capture rate above the average rate, the approximate numbers of 37 Ar atoms produced in the detector in runs 27, 51, and 71 were 250 ± 130, 20 ± 15, and 56 ± 30, respectively. According to Davis, Monte Carlo simulations of the data indicate that approximately 1 or 2 runs as high as the runs that might be correlated with flares are expected in 60 experiments.&quot;</i><br /><br />Then, what began as simply a hunch appears to gain some credence ...<br /><br /><a href=\"https://apod.nasa.gov/apod/ap010517.html\" class=\"ot-anchor\">https://apod.nasa.gov/apod/ap010517.html</a><br /><br />2001 May 17<br /><br /><i>&quot;Explanation: Neutrinos are subatomic particles generated by the nuclear reactions which power stars like our Sun. Flying outward from the Sun&#39;s core, they easily pass through the Sun (and almost anything else!) unimpeded and should be detectable by earth-based neutrino &#39;telescopes&#39;. Still, to the long-standing consternation of astrophysicists, the observed flux of solar neutrinos is less than expected. </i><b><i>In a new twist to this solar neutrino saga, an analysis of data from the GALLEX / GNO neutrino detector finds a solar neutrino flux that varies over about 27 days ... approximately matching the Sun&#39;s rotation period.</i></b><i> In fact, since different parts of the Sun rotate at different rates, the neutrino flux variations match most exactly the rotation rates of the areas shown in red on this colorful cross-sectional map of the solar interior rotation. </i><b><i>So how could solar rotation affect the neutrino flux? Some theoretical models say that neutrinos can change quantum properties when they interact with tangled solar magnetic fields and become particles that the neutrino experiments were not designed to detect. Then, as the Sun rotates, the neutrinos sometimes come to us unaffected and sometimes come through magnetic fields, diminishing the flux that can be measured.&quot;</i></b><br /><br /><a href=\"http://electric-cosmos.org/sudbury.htm\" class=\"ot-anchor\">http://electric-cosmos.org/sudbury.htm</a><br /><br />Don Scott on the Production of Solar Neutrinos<br /><br /><b><i>&quot;The neutrino problem is a hurdle only for the standard fusion model.  In the Electric Sun model there is no energy produced in the core -- radiant energy is released at the surface by electric arc discharge.  So, there is no &#39;missing neutrino&#39; problem for the electric Sun model. The electron-nuetrinos [sic] that are observed are probably produced by fusion taking place at the solar surface that produces heavy elements (other than hydrogen and helium).</i></b><br /><br /><b><i>For decades the measured deficiency of electron-neutrinos has been a continuing embarrassment for those who want to believe that the accepted H-He fusion model of how the Sun produces its energy is correct.  Because this failure to observe the predicted neutrino flux clearly constitutes falsification of this fusion model, there has been a great effort to explain away the observed deficit.&quot;</i></b><br /><br />Wal Thornhill on the Anti-Correlation<br /><br /><a href=\"http://www.holoscience.com/wp/sciences-looming-tipping-point/\" class=\"ot-anchor\">http://www.holoscience.com/wp/sciences-looming-tipping-point/</a><br /><br /><i>&quot;Experimentally, a DPF [dense plasma focus] produces nuclear fusion and is a copious source of neutrons. Neutron capture in a dense plasma environment of protons and positive ions is necessary to ‘build’ the heavy elements from the lighter elements. So here we have a straightforward possible solution to the continuous production of heavy elements by all stars. The resultant complex nucleosynthesis in the photospheric granules can also be expected to produce a mix of different neutrino types. </i><b><i>The observed neutrino modulation by sunspots is then easily explained because the sunspots clear areas of the photosphere of granulation.&quot;</i></b><br /><br />Wal further suggests an experimental test ...<br /><br /><a href=\"http://www.holoscience.com/wp/solar-neutrino-puzzle-is-solved/\" class=\"ot-anchor\">http://www.holoscience.com/wp/solar-neutrino-puzzle-is-solved/</a><br /><br /><i>&quot;What if the neutrino discovery is correct?</i><br /><br /><i>It says nothing about the correctness of the Standard Solar model. However, it does have &#39;important implications for cosmology and particle physics&#39;. If neutrinos do have mass it will tend to confirm the Electric Universe model. In it, neutrinos are not fundamental particles but are comprised of the same charged sub-particles that make up all matter. They are the most collapsed form of matter known. When a positron and an electron &#39;annihilate&#39;, the orbital energy in both is radiated as a gamma ray and the sub-particles that comprised them both assume a new stable orbital configuration of very low energy, or mass. Matter cannot be created from a vacuum nor annihilated in this model. The differences between the neutrino &#39;flavours&#39; is merely one of different quantum states and therefore different masses.</i><br /><br /><i>The electric Sun model expects far more complex heavy element synthesis to take place in the natural particle accelerators in the photospheric lightning discharges. In that case </i><b><i>the various neutrino &#39;flavours&#39; are all generated on the Sun and do not need to &#39;oscillate&#39; on their way to the Earth to make up an imagined deficit. What is more, fluctuations in neutrino counts are expected in this model to be correlated with electrical input to the Sun, that is, with sunspot numbers and solar wind activity. This has been observed. The standard solar model does not expect any correlation since there is a lag estimated in the millions of years between the nuclear reaction in the core and its final expression at the surface of the Sun.</i></b><br /><br /><b><i>There is an experiment suggested by the SNO results that could confirm the Electric Sun’s photospheric origin of neutrinos. It would require continuous measurement of neutrinos of all flavours as a very large sunspot group rotated with the Sun. In this model, sunspot umbrae are not a source of neutrinos so there should be modulation effects associated with the Sun’s rotation that might be measurable with present equipment. Such an experiment, if sensitive enough, offers the possibility of detecting neutrino oscillations in the Sun as they traverse varying proportions of the body of the Sun. A positive result would falsify the standard nuclear model of the Sun.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Solar Neutrino Problem","summary":"Our Ideas About the Sun Root in Ideas by Eddington from the 1920’s / Yet, the Belief that We Fundamentally Understand the Sun was Never Seriously Re-examined when the Number of Observed Solar Neutrinos Failed to Match the Theoretical Expectation / Instead, Theorists Simply Adjusted their Expectations to Accommodate Properties that Solar Neutrinos \"Must Have\"","image":"https://lh3.googleusercontent.com/-VJNW6zGGU8Q/WBrBxNDu9GI/AAAAAAAAJIU/NDGmdG2jtLAiPNkamKX6UYA97y7vcQTYgCJoC/w3600-h5040/the-solar-neutrino-problem-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/ZNdRcAcrCHA","publishDate":"2016-11-03T04:50:16.866Z","updateDate":"2017-01-03T01:50:19.049Z","text":"<b>The Solar Neutrino Problem: Our Ideas About the Sun Root in Ideas by Eddington from the 1920’s / Yet, the Belief that We Fundamentally Understand the Sun was Never Seriously Re-examined when the Number of Observed Solar Neutrinos Failed to Match the Theoretical Expectation / Instead, Theorists Simply Adjusted their Expectations to Accommodate Properties that Solar Neutrinos &quot;Must Have&quot;</b><br /><br /><i>The Golem: What You Should Know About Science</i><br />by Harry Collins and Trevor Pinch<br /><br />Chapter 7: Set the Controls for the Heart of the Sun: The Strange Story of the Missing Solar Neutrinos<br /><br /><i>&quot;The many stars we see burning in the night sky have one thing in common. They all convert matter into energy by a process known as nuclear fusion. This is the same process that occurs in hydrogen bombs. Because stars are continually eating up their own mass of hydrogen over time, they slowly change. The process of change or evolution is usually gradual, but can have dramatic moments such as the cataclysmic end of a star in a huge explosion, a supernova. The changing history of stars, including our own sun, is described by stellar evolution theory: one of the most fundamental theories in modern astrophysics. This theory successfully explains the different transitions undergone by most stars. </i><b><i>For astronomers and astrophysicists, stellar evolution theory is taken for granted as much as Darwin&#39;s theory of evolution is for biologists.</i></b><br /><br /><i>Yet, despite the undoubted successes of the theory, </i><b><i>its central assumption -- that nuclear fusion is the source of a star&#39;s energy -- has only recently been directly tested.</i></b><br /><br /><b><i>In 1967, Ray Davis, of the Brookhaven National Laboratory, tried to detect solar neutrinos: sub-nuclear particles produced by nuclear fusion in our own sun. This was the first direct experimental test of stellar evolution theory. All other radiation coming from the sun is the result of processes that took place millions of years earlier. For example, light rays take millions of years to escape from the sun&#39;s core as they work their way to the surface. Neutrinos, because they interact so little with matter, travel straight out of the sun. Their detection on the earth would tell us what was happening in the core of the sun only 8 minutes earlier (the time the neutrinos take to travel from the sun to the earth). Solar neutrinos thus provide a direct test of whether our nearest star, the sun, has fusion as its energy source.</i></b><br /><br /><b><i>Precisely because neutrinos interact so little with matter, they are very very hard to detect. On average a neutrino can travel through a million million miles of lead before it is stopped. Detecting neutrinos was always going to be a difficult project! Davis&#39; experiment is rather unusual. He used a vast tank of chlorine-rich cleaning fluid, the size of an Olympic swimming pool, buried deep underground in a disused mine shaft. Every month Davis dredges this tank searching for a radioactive form of argon, produced by the reaction of incoming neutrinos with chlorine. Unfortunately particles from outer space, known as cosmic rays, also trigger the reaction and this is why the experiment must be located deep underground to provide enough shielding to absorb the unwanted rays. The radioactive argon, once extracted from the tank, is placed in a very sensitive radioactive detector (a form of geiger counter), where the exact amount formed can be measured.</i></b><br /><br /><i>This experiment, which must be one of the most peculiar of modern science, has had a baffling outcome. </i><b><i>The predicted neutrino fluxes are not there. A test which was intended as the crowning glory of stellar evolution theory has instead produced consternation.</i></b><i> It is scarcely imaginable that stars do not have nuclear fusion as their energy source, so what has gone wrong? The experiment has been checked and rechecked, theories have been juggled, and models and assumptions have been carefully probed for errors. Thus far, however, no-one is sure what has gone wrong. </i><b><i>Even today, with second-generation experiments &#39;coming on the air&#39; (or more accurately &#39;going underground&#39; since all such experiments need shielding from cosmic rays), it is by no means clear what the outcome will be.</i></b><br /><br /><i>This then is a classic case of a confrontation between experiment and theory. The theoretical realm and the experimental realm cannot, however, be so easily separated. In the solar-neutrino case, theorists and experimentalists have been collaborating for years to reach a conclusion.</i><br /><br /><i>In the first part of our story we will examine how the scientists built the partnership from which the experiment was born. The reception of the results of Davis&#39; experiment, which we deal with in the second part, can only be understood against this background.</i><br /><br /><i>Just as the solar-neutrino experiment is supposed to give us a glimpse into the heart of the sun, </i><b><i>the solar-neutrino episode gives us a glimpse of what happens in the heart of science when things do not turn out as expected.</i></b><i> Strangely, we shall encounter science become &#39;unmade&#39;.</i><br /><br /><i>Although the theoretical models used to calculate the solar-neutrino flux are complex (they are run on large computers), and the experiment is mind-boggling in its sensitivity (a handful of argon atoms are searched for in a tank containing billions upon billions of atoms), when the experiment was first thought of, the procedures were considered routine. Ray Davis, who is a chemist, perhaps over-modestly describes his experiment as merely a matter of &#39;plumbing&#39;. Indeed, </i><b><i>in the early 1960s, so confident was the physics community that Davis would confirm stellar evolution theory that a whole series of underground &#39;telescopes&#39; were planned to detect neutrinos from a range of different cosmic sources. Such plans were shelved after Davis&#39; first results became known. What were formerly taken to be well-understood areas of science became problematic and contentious. The robust became fragile, the closed open, and the certain uncertain. The intricate network of ties between theory and experiment was threatened with rupture. That the rupture has thus far been contained; that the experimental results have stood the test of time; and that the structure of stellar evolution theory remains in place, is a puzzle. We are not witnessing the collapse characteristic of a scientific revolution, yet, on the other hand, it is not quite &#39;business as usual&#39;.</i></b><br /><br /><i>PART 1. BUILDING EXPERIMENTS AND PARTNERSHIPS</i><br /><br /><i>Experiments, like any outcome of human activity, do not arrive de novo. When Ray Davis &#39;switched on&#39; his apparatus in the summer of 1967 to look for the expected flux of neutrinos, the previous 20 years of hard work by himself and others was at stake. Of particular importance was a unique partnership between Davis and a group of nuclear astrophysicists (nuclear astrophysics is nuclear physics applied to astronomy), at the Kellogg Radiation Laboratory of the California Institute of Technology, headed by William Fowler.</i><br /><br /><b><i>The neutrino has always held a special fascination for experimenters because it is one of the most elusive and therefore challenging particles to detect. First postulated by Wolfgang Pauli in 1930 as a purely theoretical entity needed to make theories of radioactive decay consistent, the neutrino was thought to have zero mass and zero electric charge. A kind of reverse radioactivity reaction was one of the few ways to detect this elusive particle. In such reactions a neutrino is absorbed by the nucleus of an atom to form a new element which is unstable and in turn decays. Separating out the few atoms of the newly formed element from the billions upon billions of target atoms is a technically daunting task.</i></b><i> Carrying out this task is what Davis, who had long been interested in neutrino detection, set out to do.</i><br /><br /><i>The experimental technique Davis used came from the hybrid field of radioactivity and chemistry, known as radio-chemistry. The idea was to use chemistry to separate out the newly formed radioactive element (argon) from the target material. Davis achieved this by having the target in liquid form and purging it with helium gas, thus sweeping out the accumulated argon atoms. The exact quantity of argon formed could then be measured by counting its characteristic decays. In order to help separate argon decays from the background radiation Davis shielded his counter in a First-World-War gun barrel made of steel with low natural radioactivity.</i><br /><br /><i>The use of the chlorine-argon reaction for neutrino detection was first suggested by physicists Bruno Pontecorvo and Louis Alvarez. Alvarez&#39;s involvement was particularly important. As was the case for so much post-war physics, the idea stemmed from war-time work. Alvarez had devised a new method to determine whether the Germans were building an atomic bomb. This was a mobile radiochemical detector which could be placed in a bomber and could be used to search for radioactive emissions from smoke-stacks when flown over Germany (none were found).</i><br /><br /><i>Davis took over Alvarez&#39;s ideas and by 1955 he had built a small working detector using 550 gallons of cleaning fluid. Of course, he could not detect anything unless he had a source of neutrinos. One of the most likely sources was thought to be nuclear power plants and thus Davis installed his detector at the Savannah River nuclear power reactor. At the same time and place Frederick Reines and Clyde Cowan were making their now historic measurements which provided the first detection of the free neutrino. Unfortunately for Davis it turned out that his apparatus was not sensitive to the types of neutrino produced in nuclear reactors. Davis now faced a dilemma. He had developed a promising detection system, but he had no neutrinos to detect. It was William Fowler who provided the way out of the dilemma.</i><br /><br /><i>Collaboration with Cal Tech</i><br /><br /><i>At Cal Tech, William Fowler was watching Davis&#39;s work closely. Nuclear astrophysics as a discipline had taken off in the 1930s following on from Eddington and Jean&#39;s pioneering achievement in identifying nuclear reactions as the most likely source of the sun&#39;s energy (this is the same Eddington as encountered in chapter 2). By the end of the 1950s, after many laboratory measurements of nuclear reactions, the detailed cycles of nuclear reactions in our own sun had been determined. In 1957 a comprehensive theory of how lighter elements are synthesised into heavier elements in stars was outlined by Fowler and his co-workers at Cal Tech. This was one of the highpoints of the discipline -- it appeared to explain how all familiar substances were formed from the lightest element of all, hydrogen.</i><br /><br /><i>In 1958 one of the reaction rates crucial to the nuclear reaction cycle in the sun was remeasured and found to have been in error. It looked as if the sun produced some fairly high-energy neutrinos, which Davis should be able to detect. Fowler immediately alerted Davis to the possibility and from then on the two collaborated in order to bring the detector plans to fruition. </i><b><i>At the time Fowler considered the detection of solar neutrinos to be the &#39;icing on the cake&#39; of stellar evolution theory.</i></b><br /><br /><i>Bahcall: a house theorist away from home</i><br /><br /><i>The Cal Tech end of the partnership with Davis centred upon a young post-doctoral student of Fowler&#39;s, John Bahcall. If solar neutrinos were to be Fowler&#39;s icing on the cake, they proved to be John Bahcall&#39;s bread and butter. Solar neutrinos became a dominant theme of Bahcall&#39;s subsequent career. By the time Davis was ready to make measurements it was Bahcall&#39;s career as much as Davis&#39; that was at stake. The need for scientists to act in a concerted manner over a period of time as they pursue their careers is well illustrated by Bahcall&#39;s involvement.</i><br /><br /><i>Bahcall was a theoretical physicist and, as well as providing theoretical expertise, his job was to coordinate the Cal Tech effort. The prediction of the number of neutrinos Davis should expect to measure was a complex task requiring a variety of different sorts of knowledge. When Bahcall did not have the requisite expertise himself, he proved very adept (with Fowler&#39;s support) at recruiting others to help him.</i><br /><br /><i>The detailed prediction involved nuclear physics, astrophysics, neutrino physics, as well as radio-chemistry. The different domains of solar-neutrino physics are shown in figure 7.1.</i><br /><br /><b><i>Nuclear physics was needed both to measure the nuclear reaction rates in the sun and to calculate the interaction between neutrinos and chlorine in the detector. Since all the relevant nuclear reaction rates are measured in the laboratory at much higher energies than occur in the core of the sun, extrapolations to lower energies have to be made. These measurements and consequent estimates are often uncertain, and throughout the history of the solar-neutrino problem reaction rates have been revised as different measurements and calculations have been made.</i></b><i> The Kellogg Radiation Laboratory housed some of the leading nuclear physicists. Bahcall soon enlisted their help in remeasuring many of the crucial nuclear parameters.</i><br /><br /><i>Another key element in calculating the neutrino fluxes is astrophysics. This is required to produce a detailed model of the sun&#39;s structure and evolution. The solar evolution is modelled on a computer over the equivalent of the 4.5 billion years life of the sun. </i><b><i>Many data about the sun, such as the composition of its constituent elements, have to be fed into the model. Such input data were constantly revised.</i></b><i> The solar model was constructed by specialists at Cal Tech. The largest component of the neutrino flux which Davis was expected to detect came from one sub-branch of the main hydrogen fusion chain of nuclear reactions in the sun. </i><b><i>These high-energy neutrinos turned out to be extraordinarily temperature sensitive and critically dependent on the details of the solar-model.</i></b><i> Expertise in neutrino physics was also required in order to determine what happened to the neutrinos in their passage through the sun and on their long journey to the earth.</i><br /><br /><i>As well as working on the prediction of the neutrino fluxes, Bahcall helped Davis with theoretical problems that arose in connection with the design of the experiment, such as, for instance, what the likely cosmic ray background might be, when best to take samples from the tank, and so on. In his own words he became the &#39;house theorist&#39;. He eventually became a consultant to the Brookhaven National Laboratory where Davis worked and thus was paid for his services by the same &#39;house&#39;.</i><br /><br /><i>Funding the experiment</i><br /><br /><i>Davis estimated the cost of the experiment to be approximately $600,000. In the 1960s this was a large sum to obtain for one experiment which, unlike particle accelerators, could be used to produce only one sort of measurement. Fowler was extremely influential in the process of obtaining the funding. He constantly advised Davis and Bahcall and elicited support from his own colleagues. Soundings were taken with all likely sources of support: the Atomic Energy Commission, the National Science Foundation and NASA.</i><br /><br /><i>It is naive to think that scientists obtain funding merely by writing a compelling grant proposal. To get funding for a major facility scientists have to engage in political lobbying and other forms of persuasion. In raising the money for Davis&#39; experiment the following were important: the publication by Davis and Bahcall of their results and plans in the leading physics journal, Physical Review Letters; the use of the highly influential Kellogg Radiation Laboratory &#39;Orange and Lemon Aid&#39; Preprint series in circulating information to the scientific community about what was happening; and coverage in the scientific and popular press -- articles about solar neutrinos appeared in both Time and Newsweek. Of most importance was a letter to the Atomic Energy Commission which Fowler wrote at the bidding of Davis&#39; departmental chairman, Richard Dodson. In this letter Fowler strongly urged that the experiment be funded. Dodson and Fowler were old friends and former colleagues at Cal Tech. It seems that at the time the Atomic Energy Commission did not use formal peer review, and Fowler&#39;s &#39;fine letter&#39; (Dodson&#39;s words) provided the needed technical acclaim to ensure funding.</i><br /><br /><i>Of course, all these efforts to raise funding turned on the scientific merits of the case. The experiment was widely billed as a &#39;crucial&#39; direct test of stellar evolution theory. Two things about the rhetoric used at the time are worth noting. Firstly, </i><b><i>although undoubtedly more direct than other ways of measuring radiation from the sun&#39;s core, the neutrinos Davis expected to detect came from one highly temperature-sensitive sub-branch of the chain of hydrogen reactions. A more direct test would be of the fundamental hydrogen fusion reaction itself. The second-generation experiments now starting to produce results are sensitive to neutrinos from this reaction.</i></b><i> Yet, in the effort to obtain funding, these new experiments too have been billed as &#39;crucial&#39;. The rhetoric of &#39;cruciality&#39; is clearly context dependent. There can be little doubt that when seeking hard-pressed dispensers of funds for large sums of money it helps to be doing something crucial.</i><br /><br /><i>The second thing to note is that many of the scientists whose support was needed for the experiment were nuclear and particle physicists. </i><b><i>Such physicists, brought up on &#39;hard-nosed&#39; laboratory science, were sceptical of astrophysics which they regarded as being much less precise. Many physicists were cautious about funding an experiment based upon astrophysics.</i></b><i> One such sceptic was nuclear physicist Maurice Goldhaber, the Director of the Brookhaven National Laboratory. His support was crucial. Bahcall paid a special visit to Brookhaven to add his voice to the effort to persuade Goldhaber that the predictions were reliable and the experiment feasible.</i><br /><br /><i>In order to convince sceptics such as Goldhaber it was in Bahcall&#39;s interests to have a clear-cut prediction of a large signal which Davis could detect without ambiguity. </i><b><i>There is some evidence that the predictions of the flux of solar neutrinos varied with physicists&#39; need for funding. Figure 7.2 shows the predicted solar neutrino flux in SNUs (solar neutrino units) over time. It reveals that at the moment when the experiment was funded in 1964 the predicted flux was high (40 SNU). It can also be seen that immediately afterwards the predicted flux started to come down, and that by the time Davis got his first results in 1967 it had fallen to a much lower figure (19 SNU). Happily Davis managed to obtain greater sensitivity by improvements in the detection process, but several scientists commented that the experiment would never have been funded if the smaller levels of fluxes predicted in 1967 had been predicted earlier, in 1964 when the experiment was funded.</i></b><br /><br /><i>Many of the changes in the predicted flux came from parameters beyond Bahcall&#39;s immediate control, such as remeasurements of nuclear reaction rates and changes in other parameters. However, the timing of the push for funding, </i><b><i>and the realisation (only after funding had been awarded) that one of the most important nuclear reaction rates had been incorrectly extrapolated to low energies,</i></b><i> tended to work in favour of an overly optimistic prediction in 1964. The interdependence of theory and experiment could scarcely be clearer.</i><br /><br /><i>Building the experiment</i><br /><br /><i>If Bahcall was engaged in unconventional activities for a theoretical physicist, Davis too faced some unusual tasks. Having obtained the necessary funding, he needed to find a deep mine shaft in which to locate his experiment. This proved to be far from easy, not only because of physical constraints such as the depth and stability of rock, but also because most mine owners could see little merit in housing an inconvenient and possibly dangerous experiment in their mines. Davis spent much of 1964 in negotiation with mine owners. Finally the Homestake Mining Company agreed to go ahead with the project once they realised that it was sponsored by the Atomic Energy Commission. It was no accident that the Atomic Energy Commission were consumers of the product of another important Homestake mining venture: uranium.</i><br /><br /><i>The building of the experiment involved Davis in extensive liaisons with the mining company and other commercial companies who helped fabricate the tank and associated equipment and deliver the cleaning fluid to the mine. (The cleaning fluid is actually on loan and may one day be returned for its more usual purpose!) In the end the local miners became enthusiastic about the experiment and several issues of the Homestake newspaper were devoted to the story.</i><br /><br /><i>It is a very impoverished view of science which treats theorists simply as producers of new ideas, and experimenters as the people who test or verify such ideas. As we have seen, doing theory and experiment are far more interesting than that. </i><b><i>Theory and experiment are not independent activities.</i></b><i> They are linked through and through, and form part of a wider network of ties between scientists. If it had not been for the collaboration of theoreticians and experimenters, and in particular the influence of the powerful group of Cal Tech scientists under William Fowler&#39;s leadership, the solar-neutrino experiment would never have happened.</i><br /><br /><i>PART 2. SCIENCE UNMADE</i><br /><br /><i>We will now turn to examine what happened when, against all expectations, Davis reported his negative result. Unlike some of the other cases discussed in this volume which involve a clash between theory and experiment, </i><b><i>Davis&#39; experiment has not lost credibility</i></b><i>. For a while severe doubts were raised about his experimental method -- perhaps somewhere in the vast tank of cleaning fluid he was losing a few atoms of argon. Davis, however, was able to survive such criticism and indeed eventually emerge with an enhanced reputation as a careful experimenter. The theorists too, and Bahcall in particular, have by-and-large managed to live with the result, although, as we shall see, </i><b><i>Bahcall for a time felt that any contradiction between theory and experiment was not compelling.</i></b><br /><br /><i>The main feature of the period, once it was widely recognised that the solar-neutrino problem did indeed exist, has been a process of questioning and examination; just about every assumption upon which the scientific basis of the project was based has come under challenge. It is in such challenges that we can catch a glimpse of science as it becomes &#39;unmade&#39;.</i><br /><br /><i>The first results</i><br /><br /><i>Davis&#39; first results were obtained in August 1967. They indicated a very low signal, that is, a very low flux of neutrinos. Indeed </i><b><i>the signal was so low that it could not be reported as a number (of neutrinos detected) with a possible error, but only as the upper limit: 6 SNU. In other words the signal was no greater than 6 SNU and this signal might result from background radiation alone</i></b><i>. Improvements in the detection technique meant that by early 1968 Davis could set an even lower limit on the neutrino fluxes of 3 SNU. Davis, who had been working with this detection technique for most of his career, did not doubt that his result was correct. In view of its importance, however, he invited two fellow Brookhaven chemists to check his work. They could find nothing wrong. As an added precaution Davis calibrated his experiment by irradiating his tank with a neutron source which also produced the same isotope of argon as he detected. He recovered the expected number of argon atoms which seemed again to indicate that all was working well. However, this test was not taken to be compelling by everyone, as we shall see below. By this stage (May 1968) Davis felt confident enough to publish his result. In preliminary reports of his findings </i><b><i>he stressed that his result was five times below the predicted signal of 19 SNU.</i></b><br /><br /><i>Bahcall&#39;s reaction</i><br /><br /><i>Davis&#39; confidence that his result was out of line with theory was not shared by Bahcall. </i><b><i>As soon as Bahcall learnt of Davis&#39; low result (he was, of course, in continual contact), he went to work with yet more &#39;fine-tuning&#39; of the theoretical &#39;prediction&#39;.</i></b><i> New measurements of various parameters were included which reduced the prediction to 7.5 SNU (with an error bar of 3 SNU). This enabled Bahcall to report by May 1968 that the &#39;present results of Davis ... are not in obvious conflict with the theory of stellar structure&#39;.</i><br /><br /><b><i>At this point Bahcall very much wanted Davis&#39; experiment to be in agreement with his &#39;prediction&#39;. With Davis reporting ever lower results, Bahcall became more and more depressed. Most of the other theorists shared Bahcall&#39;s concerns and were hoping against hope that the conflict would go away. A lot had been invested in this experiment. There was even talk of a Nobel Prize in the offing if only Davis&#39; experiment would &#39;come out right&#39;.</i></b><br /><br /><i>Iben&#39;s reaction</i><br /><br /><i>That a contradiction between theory and experiment existed was first and most forcefully recognised not by Bahcall, but by an old colleague of his from Cal Tech days, Icko Iben. Iben was a solar model specialist who had been part of the Cal Tech team which made the 1964 prediction. Iben now took a rather jaundiced view of what Bahcall was doing. </i><b><i>For him, Bahcall was being disingenuous, abandoning the previously bold predictions of a high flux and using rather arbitrary values for parameters in order to try and lower the flux.</i></b><i> In Iben&#39;s mind there was little doubt that there was a conflict and he used his own solar models to demonstrate its extent. A potentially acrimonious dispute between the two theorists was avoided, however, when shortly afterwards (in 1969) Bahcall, too, went on record proclaiming there to be a discrepancy. The solar-neutrino problem was now officially born.</i><br /><br /><b><i>The disagreement between Bahcall and Iben reminds us once more how flexible the &#39;prediction&#39; with its many inputs could be. This episode also shows that judging the outcome of a test of a theory is not always straightforward. It is not simply a matter of inspecting theoretical prediction and experimental result as some philosophers believe. Interpretation is always involved. Bahcall and Iben were both highly competent theorists who were well-acquainted with the relevant scientific issues, yet in 1967 they drew rather different conclusions.</i></b><br /><br /><i>Bahcall now became one of the leading advocates of the view that there was a discrepancy -- so much so that he has even fought battles with other scientists who were less enthusiastic about the extent and importance of the solar-neutrino problem. Although it is hazardous imputing strategies to individual scientists, and we should be wary of any simple-minded models of scientists as rational calculators who always try to promote what is in their best career interests, we can nevertheless speculate as to what rationale Bahcall might have had for his dramatic change in position.</i><br /><br /><i>His initial resistance to the idea that there was a conflict can be understood as a response to his previous involvement with the experiment. In 1967 Bahcall rightly or wrongly believed that further progress in his career depended upon Davis getting the right answer. However, the longer he held to the view that there was no conflict, while other theorists such as Iben who had less at stake in the project drew opposite conclusions, the more tenuous his position became. An indication of the kind of pressure he was under at the time comes from a conversation which Bahcall recalls having had with the famous Cal Tech physicist, Richard Feynman. </i><b><i>Feynman apparently advised the young Bahcall that he had done nothing wrong and that if there was a contradiction this made the result more rather than less important.</i></b><i> It seems Bahcall took Feynman&#39;s advice to heart. Furthermore, it seems to have been good advice. Bahcall has managed to continue to make a career out of solar neutrinos by stressing the scientific importance of the problem. His career does not seem to have suffered either; he has won prizes for his work on solar neutrinos and currently holds the highly prestigious post of Professor of Astronomy and Astrophysics at the Princeton Institute for Advanced Study.</i><br /><br /><i>Ray Davis: an ideal experimenter</i><br /><br /><i>With the conflict between theory and experiment now publicly recognised by the theorists, the heat was back on Davis to make sure that the problem did not reside with him. In most controversies of this type, as we have seen elsewhere in this volume, other scientists become involved in attempts to repeat the original experiment; in this case the experiment was too daunting and costly. This placed a special onus on Davis. Although he felt further tests of his procedures were largely a waste of time in terms of learning any new science, he saw the need to convince the more sceptical community of astrophysicists.</i><br /><br /><i>Throughout, Davis has made a point of following up and carrying out their suggestions, no matter how outlandish they might appear. The strategy seems to have paid off because over the years Davis has acquired something of a reputation as the ideal experimenter. Indeed at one meeting in 1978 he was heralded as an unsung &#39;scientific hero&#39; and he is widely regarded as one of the best experimenters in modern science. The ideal experimenter&#39;s profile which Davis cultivated is that of openness, caution, and modesty. There can be little doubt that acquiring such a profile helps an experimenter to maintain credibility. That Davis had already built a partnership with the theoreticians can now be seen to go some way towards explaining why his result has been taken so seriously. Having invested so much in him and his experiment, and having worked with him over the years, the theoreticians could not easily abandon him.</i><br /><br /><i>Among the new tests Davis agreed to perform was the introduction of 500 atoms of argon directly into his tank. Davis went on to recover the argon with the expected efficiency. He also persuaded some Brookhaven chemists to look for an anomalous form of argon which might remain trapped in the tank. They could find none. However, to repeat a familiar theme from other parts of this book, no experiment is alone definitive, and loop-holes can always be found by a determined critic. In 1976 such a determined critic appeared in the shape of a sceptical astrophysicist, Kenneth Jacobs.</i><br /><br /><i>Argon trapping</i><br /><br /><i>Jacobs was worried that Davis&#39; experiment had never been repeated and he drew analogies with other experiments, such as Weber&#39;s gravity wave detector (discussed in chapter 5), where a signal near the noise was used as a basis to challenge established theories, and where in the long run the experiment had become discredited. Jacobs maintained that it was most likely that argon was being trapped somewhere, thus explaining the low result. He proposed a possible mechanism for such trapping in the form of weak &#39;polymerisation&#39; which liquid hydro-carbons similar to Davis&#39; cleaning fluid were known to undergo. He remained unconvinced that all the previous tests ruled out this possibility.</i><br /><br /><i>Jacobs&#39; doubts could always be raised because of the logic of the types of calibration experiment Davis was forced to undertake. Calibration tests by their very nature always involve differences when compared with what happens in an actual experimental run. For example, the calibration Davis used relied on ready-made argon atoms rather than atoms formed by neutrino interaction. Even when argon atoms were deliberately formed by fast neutron bombardment the chains of reaction were quite different from those involved in the case of neutrinos from the sun. Differences between a calibration and an experiment itself always exist. A compelling calibration is one where the importance of such differences seems small so that calibration is as much like the &#39;real thing&#39; as possible, but &#39;seeming small&#39; is a variable phenomenon.</i><br /><br /><i>Differences between experiments, as we see in other chapters of this book, always give grounds for doubt. Whether or not such doubts are raised depends on how far a scientist is prepared to challenge the conventional wisdom which holds such differences to be insignificant. Jacobs, as we have seen, was prepared to challenge that wisdom and for him the differences revealed a chink through which his argon-trapping hypothesis could sneak.</i><br /><br /><i>There was a complex test which was widely regarded as ruling out trapping. Davis eventually performed this test successfully. We do not know if Jacobs could have questioned this as well, because he failed to get tenure at his university and quit science altogether. Although the matter has rested here, if someone else as determined as Jacobs were to appear on the scene ...</i><br /><br /><i>Solutions to the problem</i><br /><br /><i>Radio-chemistry aside, the overwhelming response to Davis&#39; results has been to question one or more of the complicated chain of assumptions in the nuclear physics, astrophysics or neutrino physics which form the basis of the predicted neutrino fluxes. By 1978 over 400 papers had been published proposing &#39;solutions&#39; to the solar-neutrino problem. Perhaps unsurprisingly it is in the area of astrophysics that most solutions have been canvassed. We have already remarked on the extraordinary temperature sensitivity of the neutrinos Davis sought to detect and many solutions have involved modifications to the solar model which lower the sun&#39;s central temperature. For instance, mixing of cooler material from the exterior of the sun into the hot core (like a giant cauldron) would reduce the neutrino fluxes. A reduction would also be obtained if the sun had become contaminated by heavy elements early on in its history, perhaps as a result of a collision with another celestial body. In the area of nuclear physics it has been suggested that the extrapolations of reaction rates to low energies are not sound. One of the strongest candidate solutions is that of &#39;neutrino oscillation&#39;. </i><b><i>Over the years Davis&#39; result has consistently been about one third less than the best theoretical prediction. Since there are three types of neutrino and Davis&#39; experiment is only sensitive to one sort, the proposal is that the neutrinos are produced in one state in the sun but oscillate between their three states in the long journey to the earth, and Davis only picks up a third of his expected signal.</i></b><br /><br /><i>Some of the proposed solutions have been almost bizarre: the sun is not burning hydrogen at all; or a fundamental theory, such as weak interaction theory, which governs the interaction between neutrinos and chlorine, is in error. Many solutions have not been rebutted and remain unremarked upon in the literature. Others have been given careful consideration and rejected on a variety of grounds. Often such solutions have proved difficult to rule out altogether and scientists have had to resort to notions such as that they are ad hoc, or &#39;aesthetically unappealing&#39;. Overall none of the solutions has yet gained universal assent. </i><b><i>On the other hand, stellar evolution theory has not been overthrown. The solar-neutrino result has been treated as an anomaly; something to be put aside for the time being.</i></b><br /><br /><i>An experiment into the nature of science</i><br /><br /><i>Although no challenge has yet gained consensus as the solution to the solar-neutrino problem, such challenges are interesting because they reveal a world of doubt and uncertainty which lies behind even the most well-established areas of knowledge. Before 1967, the project to detect solar neutrinos seemed to rest upon a solid structure of theoretical and experimental assumptions -- it was at least solid enough for a large amount of funding and scientific time to be devoted to it. No doubt, once an agreed solution is reached, all the current uncertainties will vanish and scientists will again have the greatest confidence in these areas of science. What now has become &#39;unmade&#39; will be &#39;remade&#39;.</i><br /><br /><i>One way of thinking about what has happened is to treat Ray Davis&#39; result itself as an experiment into the nature of science. It is as if Davis&#39; result cuts a knife of uncertainty through the normal taken-for-granted ideas and practices. For a moment scientists could think the unthinkable, explore the unexplorable and cut loose from the shackles of normal science and just see &#39;what if ...&#39;. But if we take every suggestion seriously then nearly everything we take for granted is challenged. In this world the sun no longer has nuclear fusion as its source of energy; neutrinos oscillate, decay or stay within the sun; the sun experiences periods of mixing which correlate with ice ages; argon is trapped; weak interaction theory does not work; and so forth.</i><br /><br /><i>We have Ray Davis to thank for giving us this thought experiment in the plasticity of scientific culture. Of course, &#39;what-if science&#39; is not conventional science; in ordinary science a million Howers most decidedly do not bloom. The puzzle the solar-neutrino problem leaves us with is the following: </i><b><i>if scientists can in some circumstances think the unthinkable, what stops them doing it most of the time? If the answer does not lie in recalcitrant Nature, and throughout this book we have suggested that Nature imposes much less of a constraint than we normally imagine, this leaves scientific culture. Science works the way it does, not because of any absolute constraint from Nature, but because we make our science the way that we do.&quot;</i></b><br /><br /><a href=\"http://www.thunderbolts.info/tpod/2011/arch11/110727neutrino.htm\" class=\"ot-anchor\">http://www.thunderbolts.info/tpod/2011/arch11/110727neutrino.htm</a><br /><br />Neutrino Quest II<br />by Don Scott<br /><br />Jul 27, 2011<br /><br /><i>&quot;Dealing with less than scrupulous salesmen requires the consumer to employ diligence, a healthy skepticism, and have some knowledge of the items being offered for sale. As the traveling salesman in the Broadway musical The Music Man says, &#39;Ya’ gotta know the territory!&#39;</i><br /><br /><b><i>For about a decade, astrophysicists have been selling the idea (to what they hope is an accepting public) that: &#39;neutrinos have mass and therefore can change flavor.&#39; This, they claim, explains the so-called &#39;neutrino deficit.&#39;&#39; It doesn’t. Things are not quite that simple.</i></b><br /><br /><i>A steady-state thermonuclear reaction of the type they claim powers the Sun </i><b><i>must emit a flood of electron-neutrinos</i></b><i> and so far, nowhere near the requisite number of these neutrinos have been observed.</i><br /><br /><i>Solar fusion advocates are in a near-panic because a series of grandly expensive experiments have failed to find the necessary electron-neutrino flux. Briefly, the problem is as follows:</i><br /><br /><i>According to accepted particle physics, there are three different kinds of neutrinos:</i><br /><br /><i>· electron neutrinos </i><b><i>(which are the ones supposedly produced in the Sun)</i></b><br /><i>· muon neutrinos, and</i><br /><i>· tau neutrinos.</i><br /><br /><i>The fusion reaction in question is:</i><br /><br /><i>4 1H + 2 e --&gt; 4He + 2 electron-neutrinos + 6 photons</i><br /><br /><i>&#39;Some solar neutrinos have indeed been observed -- but only one-third the number required if this fusion reaction really is the main source of the Sun&#39;s energy production. </i><b><i>These negative results from the neutrino experiments have resulted not in any re-examination of solar models. Rather, an intense theoretical effort to discover new properties that solar neutrinos &#39;must have&#39; has occurred. As a result of this effort, it was announced (June 2001) by the Sudbury Neutrino Observatory (SNO) in Canada that neutrinos have mass and can change ‘flavor’. This supposedly accounts for why they have not been fully observed previously.&#39;</i></b><br /><br /><i>I wrote those lines several years ago on my web site: </i><i><a href=\"http://www.electric-cosmos.org/sudbury.htm\" class=\"ot-anchor\">http://www.electric-cosmos.org/sudbury.htm</a></i><i> and went on to analyze inaccuracies in the SNO report. </i><b><i>Since that time, I have been roundly taken to task by pseudo-skeptics claiming that I am ignorant of the latest experimental results that have now put this question to rest.</i></b><br /><br /><i>Really? Let’s see. We must remember that </i><b><i>it is the observed flow of electron-neutrinos that is too low.</i></b><i> For example we read [1] in Wikipedia:</i><br /><br /><i>&#39;For many years the number of solar electron-neutrinos detected on Earth was 1/3 to 1/2 of the number predicted by the standard solar model. This anomalous result was termed the solar neutrino problem. </i><b><i>Theories proposed to resolve the problem ... posited that electron neutrinos could oscillate -- that is, change into undetectable tau and muon neutrinos as they traveled between the Sun and the Earth.&#39;</i></b><br /><br /><i>This statement makes sense logically. If the type that seems to be missing (when the neutrino stream reaches Earth) can change into either of the other two types somewhere along the journey, that might explain the deficit. </i><b><i>So the crucial question is: Do electron-type neutrinos change into either of the other two types?</i></b><i> Reading further in the Wiki article we see:</i><br /><br /><i>&#39;Several neutrino observatories were built in the 1980s to measure the solar neutrino flux as accurately as possible, including the Sudbury Neutrino Observatory and Kamiokande. Results from these observatories eventually led to the discovery that neutrinos have a very small rest mass and do indeed oscillate [change type]. Moreover, in 2001 the Sudbury Neutrino Observatory was able to detect all three types of neutrinos directly, and found that the Sun&#39;s total neutrino emission rate agreed with the Standard Solar Model, although depending on the neutrino energy as few as one-third of the neutrinos seen at Earth are of the electron type. This proportion agrees with that predicted by the Mikheyev-Smirnov-Wolfenstein effect (also known as the matter effect), which describes neutrino oscillation in matter, </i><b><i>and it is now considered a solved problem.&#39;</i></b><br /><br /><i>Fine. But they still did not observe electron-neutrinos changing into either of the other two types. </i><b><i>They said they observed a sufficient total number of neutrinos, such that if they all had been electron-type, the accepted solar fusion model would have been verified. But they weren’t all that type.</i></b><i> The last phrase in the above paragraph, &#39;it is now considered a solved problem,&#39;&#39; is analogous to Napoleon declaring victory at Waterloo and going home to Paris. What would have to happen on the journey to Earth is for electron-neutrinos to change into one or both of the other types. This was not observed in any of these experiments and may indeed be impossible.</i><br /><br /><i>The search went on, but the reverse of what they want to see keeps being observed. </i><b><i>Just last month we read that muon-neutrinos have now been observed changing into electron-neutrinos</i></b><i>:</i><br /><br /><i>ScienceDaily [2] (June 15, 2011)</i><br /><br /><i>&#39;The international T2K collaboration has announced that it has observed an indication of a new type of neutrino transformation or oscillation from a muon-neutrino to an electron-neutrino. In the T2K experiment in Japan, a muon-neutrino beam was produced in the Japan Proton Accelerator Research Complex, called J-PARC, located in Tokai village, Ibaraki prefecture, on the east coast of Japan, and was aimed at the gigantic Super-Kamiokande underground detector in Kamioka, near the west coast of Japan, 295 km (185 miles) away from Tokai. An analysis of the detected neutrino-induced events in the Super-Kamiokande detector indicates that a very small number of muon-neutrinos traveling from Tokai to Kamioka (T2K) transformed themselves into electron-neutrinos.&#39;</i><br /><br /><i>But, again, this is the reverse of what they want to see. </i><b><i>If this kind of type-change occurs on the neutrinos’ journey from the Sun to Earth, we should measure too many electron-neutrinos, not too few.</i></b><br /><br /><i>The magazine The Economist [3], June 25th - July 1st also announced this latest experimental result:</i><br /><br /><i>... </i><b><i>T2K had seen six electron-neutrinos. If no spot-changing oscillations were happening it should have seen only one or two.</i></b><br /><br /><i>So again -- too many electron-neutrinos were measured in this experiment. But that does not seem to bother them. They certainly hope it doesn’t bother you. </i><b><i>They hope you will accept their package-deal -- that because muon-neutrinos may morph into electron-neutrinos, we must assume the reverse transformation occurs too, even when that has never been observed.</i></b><br /><br /><i>Be careful when you read what these traveling neutrino-salesmen are selling. Don’t get bilked by their road-show sleight of hand. The fusion Sun model is in big trouble.&quot;</i><br /><br />Donald Scott author of The Electric Sky<br /><br />The author earned his Bachelor and Master degrees in Electrical Engineering at the University of Connecticut in Storrs, Connecticut. Following graduation he worked for General Electric in Schenectady, New York, and Pittsfield, Massachusetts. He earned a Doctorate in Electrical Engineering at the Worcester Polytechnic Institute, Worcester, Massachusetts, and was a member of the faculty of the Department of Electrical &amp; Computer Engineering at the University of Massachusetts/Amherst from 1959 until his retirement in 1998.<br /><br />[1] <a href=\"http://en.wikipedia.org/wiki/Solar_neutrino_problem\" class=\"ot-anchor\">http://en.wikipedia.org/wiki/Solar_neutrino_problem</a><br /><br />[2] <a href=\"http://sciencedaily.com/releases/2011/06/110615161804.htm\" class=\"ot-anchor\">http://sciencedaily.com/releases/2011/06/110615161804.htm</a><br /><br />[3] The Economist, June 25th – July 1st 2011, p. 93, Delta Force<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"The Textbook Mythologies of Aether and Relativity","summary":"Of 16 Textbook Accounts of the Eddington Observations and their Significance to Relativity, 12 Were Misleading in their Reports of the Accuracy of the Experiment / Of the 26 Accounts of the Michelson-Morley Aether Experiment, 16 are Wrong on Matters of Fact, and 2 are Wrong on Matters of Significance / \"Those Passages Help to Form the Cultural Background Against which an Understanding of the Nature of Science is Set\"","image":"https://lh3.googleusercontent.com/-ljf4LvPUfsc/WBizhVm8qhI/AAAAAAAAJHQ/gmg-e2pQ-KMk3xub4AQGdHDMj9eCtnwIwCJoC/w3600-h5040/the-textbook-mythologies-of-aether-and-relativity-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/cRaCaLWMowm","publishDate":"2016-11-01T15:24:20.823Z","updateDate":"2017-01-03T01:49:02.955Z","text":"<b>The Textbook Mythologies of Aether and Relativity: Of 16 Textbook Accounts of the Eddington Observations and their Significance to Relativity, 12 Were Misleading in their Reports of the Accuracy of the Experiment / Of the 26 Accounts of the Michelson-Morley Aether Experiment, 16 are Wrong on Matters of Fact, and 2 are Wrong on Matters of Significance / &quot;Those Passages Help to Form the Cultural Background Against which an Understanding of the Nature of Science is Set&quot;</b><br /><br />The quote in the graphic comes from a text titled <i>Vibrations and Waves</i>, by Benjamin Crowell ...<br /><br /><a href=\"https://books.google.com/books?id=dzPD5bJEHDkC&amp;pg=PA50&amp;lpg=PA50&amp;dq=the+end+result+is+that+a+long+series+of+experiments+failed+to+detect+any+evidence+for+the+aether&amp;source=bl&amp;ots=plI6oHHbTD&amp;sig=WoMXYwJa0ZaGKXwmWvESTVwIozg&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjphOPy54fQAhVkrVQKHf0FDy0Q6AEIHjAA#v=onepage&amp;q=the%20end%20result%20is%20that%20a%20long%20series%20of%20experiments%20failed%20to%20detect%20any%20evidence%20for%20the%20aether&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=dzPD5bJEHDkC&amp;pg=PA50&amp;lpg=PA50&amp;dq=the+end+result+is+that+a+long+series+of+experiments+failed+to+detect+any+evidence+for+the+aether&amp;source=bl&amp;ots=plI6oHHbTD&amp;sig=WoMXYwJa0ZaGKXwmWvESTVwIozg&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjphOPy54fQAhVkrVQKHf0FDy0Q6AEIHjAA#v=onepage&amp;q=the%20end%20result%20is%20that%20a%20long%20series%20of%20experiments%20failed%20to%20detect%20any%20evidence%20for%20the%20aether&amp;f=false</a><br /><br /><i>The Golem: What You Should Know About Science</i><br />(14th Edition)<br />by Harry Collins and Trevor Pinch<br /><br /><i>&quot;A quick survey of general physics and relativity textbooks revealed some interesting features of textbook history. </i><b><i>Of sixteen textbook accounts of the Eddington observations and their significance, twelve were misleading in their reports of the accuracy of the experiment, with three of these making incorrect positive claims about succeeding experiments. Perhaps most surprising is that the likelihood of an account being historically accurate seems independent of date of publication and the eminence of the scientist-author</i></b><i> (e.g. see page 178). Pauli is a good example of an eminent scientist getting things wrong. The following is from his review of relativity published in German in 1921 and reprinted in English translation in 1958:</i><br /><br /><i>&#39;The theory of relativity has received a still more definitive confirmation than for the perihelion of Mercury for the case of the deflection of light rays. For, according to [a certain source], a light ray passing the edge of the Sun suffers a deviation of</i><br /><br /><i>deviation = 1.75&quot;.</i><br /><br /><i>This can be checked by observing fixed stars near the Sun during total eclipses of the Sun. The expeditions despatched to Brazil and the island of Principe on the occasion of the total eclipse of the Sun on 29 May, 1919 found that the effect which had been predicted by Einstein did in fact exist. Quantitatively, too, the agreement is a good one. The first-mentioned expedition found that the mean stellar deflection, extrapolated to the surface of the Sun, was 1.98&quot; +/- 0.12&quot;, the second 1.61&quot; +/- 0.30&quot;&#39;. (W. Pauli, Theory of Relativity, Where: Pergamon Press, 1958)</i><br /><br /><i>Sciama is an example of an eminent scientist getting things right:</i><br /><br /><b><i>&#39;Unfortunately, eclipse work is difficult and the actual results are rather disappointing.</i></b><i> The first attempt was made in 1919 by Eddington and Dyson. The report that the expedition had been successful created a world-wide sensation, partly for dramatic and scientific reasons. Eddington himself later referred to it as &#39;the most exciting event I recall in my own connection with astronomy.&#39; </i><b><i>Ironically enough we shall see that Einstein&#39;s prediction had not been verified as decisively as was once believed ... it is hard to assess [the] significance of [the results of all eclipse observations] since other astronomers have derived different results from a re-discussion of the same material. Moreover, one might suspect that if the observers did not know what value they were &#39;supposed&#39; to obtain, their published results might vary over a greater range than they actually do; there are several cases in astronomy where knowing the &#39;right&#39; answer has led to observed results later shown to be beyond the power of the apparatus to detect.</i></b><i> (D. W. Sciama, The Physical Foundations of General Relativity, London: Heinemann, 1972 (first edition 1969 by Doubleday and Company Inc.))</i><br /><br /><i>When we turn to the Michelson-Morley experiment, </i><b><i>of twenty-six examples of text-books, nine report or imply that the original experiment was repeated six months later, or &#39;at other times of the year&#39;, by Michelson and Morley.</i></b><i> Repetition at other times of the year was necessary to demonstrate that there was no ether drift, and </i><b><i>Michelson and Morley included this feature in their original protocol, but they did not actually carry it out.</i></b><br /><br /><b><i>Another six accounts state categorically that no positive ether drifts were ever found; this is untrue, the notable exception being the observations of Miller. A further account misreports the positive result of Miller as being statistically insignificant, mixing up Miller&#39;s result and the subsequent interpretation of it. Two more accounts lend the experiment a far greater historical weight in terms of supporting the special theory of relativity than it can bear when considered on its own.</i></b><i> [3]</i><br /><br /><b><i>Thus of these twenty-six accounts, sixteen are wrong on matters of fact, and two are wrong on matters of significance. Of course there are many other treatments which, by mentioning only the Michelson-Morley experiment and none of the replications, lend it misplaced historical significance by default.</i></b><i> Once more, there are more moderate accounts which do not affect what happens subsequently. </i><b><i>For example, in 1967, Herman Bondi wrote that Michelson-Morley and similar experiments of that period &#39;are even nowadays difficult experiments in which no great accuracy can be obtained&#39;, but this had no affect on more recent accounts. It is as though there is a self-contained tradition of textbook writing which maintains a myth about the salience and decisiveness of these experiments beyond which only professional historians and a few scientists will go.</i></b><br /><br /><i>All that said, we must admit to a mistake. The epigraph we selected to head the section of the chapter on Eddington, taken from the book by Einstein and Infeld, was chosen badly. A more careful reading of that book (and here we thank David Mermin for his help) reveals that in the particular context of that discussion, Einstein and Infeld were not claiming that the Eddington measurement defeated the Newtonian claim, only that it showed decisively that there was some bending. Accordingly, in this edition we have removed the epigraph and replaced it with two others. It should be noted, however, that those same authors were not immune to historical distortion; their account of the Michelson-Morley experiment, taken from the same volume, is misleading both in the salience it gives to the experiment and in its claim that all such experiments were negative:</i><br /><br /><i>&#39;In view of the small time-differences following from the theory, very ingenious experimental arrangements have to be thought out. This was done in the famous Michelson-Morley experiment. The result was a verdict of &#39;death&#39; to the theory of a calm aether-sea through which all matter moves. No dependence of the speed of light upon direction could be found ... </i><b><i>Every experiment has given the same negative result as the Michelson-Morley one, and never revealed any dependence upon the direction of motion of the earth.&#39;</i></b><i> [4]</i><br /><br /><i>Einstein and Infeld&#39;s book adds to the overall impression given by the other textbooks where they discuss the Michelson-Morley experiment. </i><b><i>(Note also that David Mermin&#39;s own book gives Michelson and Morley the central role as a special relativity experiment and says that experiments &#39;force us&#39; to accept the special theory.)</i></b><i> [5]</i><br /><br /><i>But none of these textbook accounts is wrong as far as the modern view of physics is concerned. As far as today&#39;s physics is concerned, the experiment might as well have been repeated six months later -- we now know what the result would have been -- and the fact that other similar experiments were repeated at other times of the year makes the misleading quotations still more harmless in terms of physics. As far as physics is concerned, it might as well have been the case that Miller&#39;s experiment, his result, his prize from the American Association for the Advancement of physics, and his 1933 publication in Reviews of Modern Physics never existed, for they represent mistakes. As far as physics is concerned, the history of physics might as well have been as it is represented in the historically incorrect papers. These accounts are not really history at all -- they are physics dressed up in historical guise.</i><br /><br /><b><i>Why then are we being so petty? It is because the history of physics has consequences for more than physics. The history of physics also has consequences for the role of physicists&#39; expertise in a modern technological society. It is because of this role that it is important that the history of physics is put right. A simplified history of physics, which may be perfectly fine for the physics undergraduate when he or she is an undergraduate because it makes the physics easier to learn, serves the citizen ill, whether that citizen is a scientist, an ex-scientist or a non-scientist. The history of physics that is important outside of physics has to be accurate about the process by which physicists reach their conclusions. While there is no harm for physicists, as physicists, having the history of their subject reduced to a series of pocket-sized myths, these myths must not be allowed to give the impression outside of physics that reaching a conclusion within disputed physics is a simple, straightforward matter that does not involve choice, argument and interpretation over an extended period. [6] What is important about physics for physicists is the outcome of a series of experiments; what is important about physics for non-physicists is the way that outcome is reached.</i></b><br /><br /><i>And it is not that the people with whom we are principally concerned are those who will actually have read one or more of the passages we have discussed above; we are concerned with all those to whom the message spreads. </i><b><i>The actual readers of the passages take a certain image of science away with them and spread it to others in their words, actions, dispositions and what they are seen to take for granted. A sociologist would say that those passages help to form the cultural background against which an understanding of the nature of science is set.</i></b><br /><br /><i>As luck would have it, while preparing the second volume of the Golem series, we chanced upon a quotation that illustrates the pervasiveness of the Michelson-Morley myth outside the realms of physical science. Here is a well-known economist, and adviser to the British Government, discussing the experiment in the context of economics:</i><br /><br /><i>&#39;[Economic forecasts] are all probability statements. All econometrics is based on the idea of probability criteria, that you are going to say that, you know, the likelihood of this result not being consistent with theory is below 5% or something, and you have to cut off at some point. </i><b><i>You don&#39;t get the one definitive experiment which show you that, you know, the speed of light is the same in all directions.&#39;</i></b><br /><br /><i>(From Robert Evans&#39;s contribution to Harry Collins and Trevor Pinch, The Golem at Large: What You Should Know about Technology, Cambridge University Press, 1998)</i><br /><br /><i>The impact of the Michelson-Morley myth, and all those other &#39;crucial&#39; experiments in the natural scientists that it stands for, could not have been expressed more clearly.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The Aether Experiments","summary":"Nearly Every Attempt to Measure an Aether Drift Failed / Since the Experiment was Amazingly Sensitive to Both Temperature and Vibrations, Scientists Tended to Construct the Interferometers on Massive Foundations in the Cellars of Strong, Well-insulated Buildings / But Such a Setup Would Not Measure an Aether that is 'Dragged' Along by Massive Opaque Materials / The One Positive Aether Measurement Was the Only Experiment that Did / But, by this Point, the Consensus on Relativity had Already Solidified","image":"https://lh3.googleusercontent.com/-T75ORg_jk_8/WBbCImdkSFI/AAAAAAAAJGU/JEgzTWoIcp4GjuHeLUz7y8NnzsQNPpICQCJoC/w5400-h7560/the-aether-experiments-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/TJs4Gun2qq9","publishDate":"2016-10-31T04:02:22.249Z","updateDate":"2017-01-03T01:49:57.906Z","text":"<b>The Aether Experiments: Nearly Every Attempt to Measure an Aether Drift Failed / Since the Experiment was Amazingly Sensitive to Both Temperature and Vibrations, Scientists Tended to Construct the Interferometers on Massive Foundations in the Cellars of Strong, Well-insulated Buildings / But Such a Setup Would Not Measure an Aether that is &#39;Dragged&#39; Along by Massive Opaque Materials / The One Positive Aether Measurement Was the Only Experiment that Did / But, by this Point, the Consensus on Relativity had Already Solidified</b><br /><br />From a review of the text ...<br /><br /><i>A History of the Theories of Aether and FAectricity from the Age of Descartes to the Close of the Nineteenth Century</i>, By E. T. Whittaker. London, Longmans, Green, and Co., 1910. xiii+475 pp.<br /><br /><a href=\"https://books.google.com/books?id=7dBXAAAAYAAJ&amp;pg=PA424&amp;lpg=PA424&amp;dq=There+is+no+time+at+which+a+well-coordinated+history+of+a+vast+branch+of+science+can+be+considered+inopportune&amp;source=bl&amp;ots=AvEMhKxnh4&amp;sig=E6OEurO5uXcnmKL3JYapIvK1iD8&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwj76Zr884PQAhVDzVQKHS9WBNsQ6AEIHjAB#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=7dBXAAAAYAAJ&amp;pg=PA424&amp;lpg=PA424&amp;dq=There+is+no+time+at+which+a+well-coordinated+history+of+a+vast+branch+of+science+can+be+considered+inopportune&amp;source=bl&amp;ots=AvEMhKxnh4&amp;sig=E6OEurO5uXcnmKL3JYapIvK1iD8&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwj76Zr884PQAhVDzVQKHS9WBNsQ6AEIHjAB#v=onepage&amp;q&amp;f=false</a><br /><br /><i>&quot;There is no time at which a well-coordinated history of a vast branch of science can be considered inopportune, but the years when a great theory has at last conquered the world after considerable opposition and is taught far and wide by that conservative element who, had they been alive and teaching during its incipiency, would have ignored it or fought it, the years when the progressive element are looking forward to new points of view, to new theories, not yet thoroughly formulated, -- these years are indeed the best in which such a history may appear.</i><br /><br /><b><i>Relative to the ether we are now in precisely this sort of period. Maxwell&#39;s theory of action through an all-pervading plenum has had its triumphal acceptance. Those who could not or would not understand the theory have for the most part passed away. One of the greatest and one of the last of them was Lord Kelvin. He was a deep student of fluid and of elastic media, he was ever seeking an intelligible mechanical conception of the ether; he apparently never found one which was completely satisfactory to him, and it is doubtful if he ever became a real sympathizer with Maxwell&#39;s ether.</i></b><i> To all this the publication of his Baltimore lectures in 1904 bears witness.</i><br /><br /><b><i>These difficulties which bothered Kelvin and which troubled everybody in the early days of the theory have by no means all been resolved; they have merely been ignored. The real triumph has not been physical but psychological; we no longer ask those awkward questions which are inimical to the theory, we take the whole fabric as we find it and unquestioningly make application of it. If there be questions, they are of a different sort.</i></b><br /><br /><b><i>In recent years some active minds have been looking forward toward the formulation of new theories, toward the abolition of the ether. The theory of relativity and the hypothesis of energy quanta have been the two ideas upon which they have chiefly focused their attention. If they in their turn shall triumph, it will probably be not for the reason that all the questions which the opponents of the theories now bring forth shall have been satisfactorily answered, but because the questioners shall have ceased to question. We advance by ignoring our known ignorance and by concentrating upon our assumed knowledge.&quot;</i></b><br /><br /><i>The Golem: What You Should Know About Science</i><br />by Harry Collins and Trevor Pinch<br /><br />Excerpt from Chapter 2<br />Does the Earth Sail in an Aetherial Sea?<br /><br /><i>&quot;In 1887 Albert Michelson and Edward Morley carried out a very careful experiment at the Case School for Applied Science in Cleveland. They compared the speed of light in the direction of the earth&#39;s motion with that at right angles to the earth&#39;s motion. </i><b><i>To their surprise, they found they were exactly the same!&quot;</i></b><i> (Stephen Hawking, A Brief History of Time: From the Big Bang to Black Holes, Bantam Books, 1988, p. 20).</i><br /><br /><i>The tranquil aether sea</i><br /><br /><i>Light and the aether</i><br /><br /><i>In the latter part of the nineteenth century it was believed that light waves travel through a universal if insubstantial medium called the &#39;aether&#39;. If this were true, then the velocity of light waves would appear to vary as the earth moves through the aether in its orbit round the sun. Just as when you run fast in still air you create your own breeze, the movement of the earth should create its own &#39;aether wind&#39; in the tenuous &#39;aether sea&#39;. Stand on the surface of the earth looking into the wind, and light coming toward you should appear to move faster than it would if the aether were still. The speed of light should be increased by the speed of the aether wind. Look across the wind, however, and light should appear to move at its normal speed. When Albert A. Michelson conducted his early experiments on the aether wind this is what he expected to find; what he actually found was that light seemed to move at the same velocity in all directions.</i><br /><br /><i>Michelson and relativity</i><br /><br /><i>According to the theory of relativity, light should have a constant velocity in all directions, </i><b><i>but the theory did not surface until some 25 years after Michelson began his observations. Michelson, then, knew nothing of relativity</i></b><i>; he set out to use movement through the aether sea as a kind of speedometer for the earth. Although the experiment is often thought of as giving rise to a problem that Einstein set out to solve, this too is probably false. </i><b><i>It appears that Einstein was little interested in Michelson&#39;s experiments when he formulated his theory. Einstein&#39;s starting point was a paradox in the theory of electrical waves. The link between Einstein and Michelson was forged by Einstein some twenty or more years after the first &#39;decisive&#39; experiments were completed. Michelson, then, had no idea of the importance his results were later to achieve. At the time he was disappointed, for he had failed to find the speed of the earth. As we shall see, Michelson did not even complete the experiments properly; he went straight on to other things after publishing the initial findings.</i></b><br /><br /><i>How to measure the speed of the aether wind</i><br /><br /><i>To measure the velocity of the earth Michelson needed to measure the velocity of light in a variety of directions. The starting assumption was that the maximum speed of the earth with respect to the aether was of the order of the speed of the planet&#39;s movement in its orbit around the sun: about 18.5 miles per second. The speed of light was known to be in the region o f 185,000 miles per second, so the effect to be measured was small -- one part in 10,000. What is worse, direct determinations of the speed of light were too inaccurate to allow such a small discrepancy to be seen, so the only possibility was to compare the speed in two directions.</i><br /><br /><i>The method was to use what we now call &#39;interferometry&#39;. The same beam of light is split into two and recombined. When the split beam recombines it will give rise to &#39;interference fringes&#39;: a series of light and dark bands. The effect is due to the light waves in each half of the beam alternately reinforcing each other (the bright bands) and cancelling each other out (the dark bands). This is a simple geometrical consequence of the superimposition of two wave motions: as one moves across the field upon which the rays converge, the path length of each ray changes slightly. For example, the left hand ray (Ray 1 in figure 2.1) has to travel a certain distance to reach the left hand side of the illuminated area. To reach a point a little to the right, it will have to travel slightly further, and to reach a point on the far right hand side of the field it will have to travel still further. Thus the ray will strike the field at various different stages in its undulation; the peak of Ray 1 strikes one point of the field whereas the trough strikes another point a little further along. Since the same applies to Ray 2, both peaks (or troughs) will strike the same point sometimes and they will combine their energies, whereas at other points a peak will coincide with a trough and they will cancel each other out -- hence the light and dark &#39;interference fringes&#39;.</i><br /><br /><i>Michelson proposed to transmit the interfering beams at right angles to each other and have them reflected back and recombined near the source. Now let us imagine that the orientation of the whole apparatus is at such an angle to the aether wind that the velocity of light along the two paths is equal (see figure 2.2). Imagine yourself looking at the interference fringes. Now imagine that the whole apparatus is rotated with respect to the aether wind so that the velocity of light becomes faster along one path and slower along the other (see figure 2.3). Then, considering just one path for a moment, what was once the point where a peak impinged might no longer be such a point. The same applies to the other half of the beam. The effect would be that the points of reinforcement and cancelling would shift; that is, that the dark and light bands would be displaced sideways.</i><br /><br /><i>In this experimental design, to detect the movement of the earth through the aether there is no need to know which way the aether wind blows at the outset of the experiment, all one needs to do is to rotate the instrument and look for shifts in the fringes. It is possible to calculate both speed and direction once one knows the full range of movement of the fringes.</i><br /><br /><i>The above explanation glosses over a very serious point. In Michelson&#39;s apparatus the light rays were sent out along a path and then reflected back. Thus, if they were swept along fast with the aether in one direction, they would be slowed in the other; it seems as though the effect would cancel out. Well, the arithmetic shows that this is not quite true. The gain is not completely cancelled by the loss, but it does mean that the effect is very much smaller than it would be if there was a way to recombine the beams without bringing them back to the starting point -- which there is not. Effectively this means that, instead of looking for a change in the velocity of light of about 1 in 10,000, one is reduced to looking for an effect in the region of 1 in 100,000,000. It is, then, a very delicate experiment indeed. Nevertheless, as he developed his apparatus, Michelson expected to see the fringes move about four tenths of the width of a single fringe if the aether wind blew at a speed equal to the earth&#39;s velocity in its orbit. This he ought easily to observe.</i><br /><br /><i>The elements of the experiment</i><br /><br /><i>It is important to note that the apparent velocity of the aether wind would depend on the orientation of the apparatus, and this would change as the earth rotated on its axis; sometimes the wind would seem to blow along the light paths, and sometimes upwards or downwards through the apparatus, when it would have little differential effect on the two light paths. Thus the experiment had to be repeated at different times of the day while the earth rotated so that different orientations could be tested. Further, to understand the movement fully it would be necessary to repeat the experiment at various times of the year when the earth would be moving in different directions with respect to the sun. Should it be the case that the aether was stationary with respect to the sun, so that the whole movement of the earth through the aether was due to its orbital velocity, then the velocity would be more or less constant throughout the year at any one time of day. If, however, the whole solar system moved through the aether, then at some times of the year the earth&#39;s orbital movement would be in the same direction as the movement of the solar system, and at other times it would be in the opposite direction. Thus one would expect to find a maximum apparent &#39;wind speed&#39; at one season of the year, and a minimum at another. The difference could be used to determine the movement of the solar system as a whole.</i><br /><br /><i>Note that if the velocity of the solar system through the aether was similar to the velocity of the earth in its orbit, there would be times of the year when the earth&#39;s movement in its orbit would nearly cancel the sun&#39;s movement. At these times the apparent velocity of the aether wind would be very low or even zero. This would be an unlikely coincidence, but to rule it out it was necessary to make observations during two seasons of the year.</i><br /><br /><i>For the experiment to work, the path lengths of the light rays had to be kept constant so that they would be affected only by changes in the direction of the aether wind. The apparent changes in length that were to be observed were of the order of a single wavelength of light. Since the path lengths were of the order of tens of metres, and the wavelengths of visible light is measured in units of a thousand millionths of a metre, it was hard to keep the apparatus stable enough. A slight flexing of one of the arms that supported the mirrors would be more than sufficient to throw the readings out. Michelson was to find that a mass of 30 grams placed on the end of one of the arms of an apparatus weighing tons was enough to upset the results dramatically. As for temperature, it was estimated that differential changes as small as 1/100 of a degree would produce an apparent effect three times that to be expected from the aether wind itself. Magnetic effects on the material of the apparatus caused by surrounding metal or the earth&#39;s magnetic field might be sufficient to wreck the results in designs where the iron or steel was used to give rigidity, whereas slight changes in humidity could vitiate those experiments where attempts were made to keep paths stable with wooden distance pieces. </i><b><i>The need for temperature and vibration control indicated that the experimental apparatus be heavily built on massive foundations in the cellars of strong, well insulated buildings.</i></b><br /><br /><b><i>Unfortunately, massive apparatus and careful insulation created an opposite problem. It was thought that the aether might be &#39;dragged&#39; along by massive opaque materials. Thus it could be argued that a well-insulated room at or below ground level in a large building would comprise, in effect, an aether trap; it would be stagnant pool around which the aether breeze wafted. Worse, hills or mountains, or the surface of the earth itself might carry the aether along with them, just as they carry the air. This way of looking at things indicated that the experiment ought to be performed outside, on top of a high mountain, or at least within a light building, preferably made of glass.</i></b><br /><br /><b><i>There are, then, six elements in the experiment:</i></b><br /><br /><i>1. the light rays must be split and reflected along paths at right angles;</i><br /><br /><i>2. observations of fringes must be made at a number of points as the whole apparatus is rotated on its axis;</i><br /><br /><i>3. the observations must be repeated at different times of the day to take account of the earth&#39;s rotation on its axis;</i><br /><br /><i>4. the observations must be repeated at different seasons to take account of the earth&#39;s changing direction of movement with respect to the solar system;</i><br /><br /><b><i>5. the experiment, it might be argued, should be carried out in a light, open, or transparent building;</i></b><br /><br /><b><i>6. likewise, the experiment should be carried out on a high hill or mountam.</i></b><br /><br /><i>The experimental apparatus</i><br /><br /><i>Michelson conducted a first experiment in 1881 and, with the collaboration of Arthur Morley, a second and much more refined observation in 1887. In essence the experiment is simple; a beam is split into two, reflected along two paths at right angles, recombined near the source, and the fringes observed. The apparatus is rotated and the observations are repeated, shifts in the position of the fringes being noted. The practice was to observe the position of the fringes at sixteen different positions while the apparatus was rotated through a complete circle. In practice, the experiment was delicate in the extreme. Of his first apparatus, which was built in Germany, Michelson reported great difficulty with vibration. The experiment had to be moved from Berlin to the more peaceful town of Potsdam, and even then the fringes could be made to disappear by stamping on the ground 100 metres from the laboratory. The experimental runs had to be made at night, during periods when there were few external disturbances. The first apparatus had comparatively short path lengths. In subsequent experiments the path lengths were increased by multiple reflection back and forth, thus increasing the sensitivity to the aether wind, but inevitably increasing the sensitivity to vibration and other disturbances too.</i><br /><br /><i>The long history of the experiment can be seen, then, as comprising increases in the path length of the two beams, changes in the materials from which the various parts of the apparatus were made, and changes in the location and housing of the experiment.</i><br /><br /><i>The 1881 experiment</i><br /><br /><i>Michelson&#39;s first experiment had a path length of about 12.0 em. According to his calculations, an aether wind having something in the region of the earth&#39;s orbital velocity would give rise to a displacement of about a tenth of the width of a fringe as the apparatus turned. Michelson felt he would be able to observe this easily if it were there. In building and using this instrument he discovered the problems of vibration and the distortions produced in the arms when the apparatus was rotated about its axis. Nevertheless, he published the results of his observations, which were that no movement of the earth through the aether could be detected.</i><br /><br /><i>After publication, the experiment was re-analysed by H. A. Lorentz, who pointed out that in his analysis Michelson had neglected to take account of the non-zero effect of the wind on the transverse arm of the apparatus; even if you row across a current, it will take longer to get there and back than if there is no current at all! When this effect is taken into account, it halves the expected displacement of the fringes. Michelson concluded that, given the difficulties of the original observation and this new estimate for the displacement, it might be that the effect of the expected aether wind was masked in experimental &#39;noise&#39;. This led him to design and build an improved apparatus.</i><br /><br /><i>The Michelson-Morley 1887 experiment</i><br /><br /><i>The next apparatus was much more elaborate. It was built at Michelson&#39;s home university in Cleveland. A cast iron trough of mercury rested upon brick foundations in a basement room. A massive block of sandstone, about 5 feet square and 14 inches thick, floated on the mercury. It could be set in motion by hand, and once started it would turn slowly so as to complete a full turn in about 6 minutes and would continue to turn on its own for more than hour. The light, beam splitter, reflectors, and so forth were mounted on the sandstone block. A number of mirrors were mounted so as to reflect the beams back and forth several times before they were recombined on the screen. This gave a path length of over 10 metres, and an expected displacement of about four tenths of a fringe as the apparatus rotated.</i><br /><br /><i>After the usual trials and tribulations, Michelson and Morley were ready to observe. At noon on 8, 9 and 11 July, and at a round 6 pm on 8, 9 and 12 July, Michelson walked round with the rotating apparatus calling out results while Morley recorded the observations. They were deeply disappointed, for no effect remotely resembling the expected speed of the aether wind was found. Once more, the experiment produced a null result.</i><br /><br /><i>Now, we remarked above that there are six components in the experiment: transmission at right angles, rotation of the apparatus, observations at different times of day, observation at different times of year, lightweight buildings and an elevated site. </i><b><i>What we have described covers only three of the six elements. Michelson seems to have been so disappointed at the result that instead of continuing he immediately set about working on a different problem</i></b><i>: the use of the wavelength of light as an absolute measure of length.</i><br /><br /><i>The only way one can understand this is to see the experiment through Michelson&#39;s eyes, as an earth speedometer. In that case, it would be expected that the speed would be fairly high and that only by a remarkable coincidence -- the cancelling of the velocity of the solar system by the equal and opposite velocity of the earth at the time of the experiment -- would a low value result. One also has to assume that he was not concerned with the problem of a ether &#39;drag&#39;. The interferometer, as Michelson had built it, was not much use as a speedometer, that much was clear. </i><b><i>If, on the other hand, the experiment is thought of as we think of it now -- a test of the theory of relativity -- its theoretical significance is greater, but its experimental significance is much less. To be a test of relativity, the experiment needs to demonstrate not that the earth is not moving with anything like the expected velocity, but that there is absolutely no difference in the velocity of light in whichever direction it is measured.</i></b><i> In the first case, the results were sufficiently disappointing to make it not worthwhile to develop the speedometer further. As a test of relativity, however, the slightest apparent shift in the fringes would be of great moment. And, it would be of enormous importance to try the test at different times of the year because a slight difference in reading at different seasons would have significance for the theory. </i><b><i>The 1887 experiment was not, then, a very good test of relativity</i></b><i>, even though it was adequate as a test of what Michelson and Morley wanted to know. </i><b><i>Only after Einstein&#39;s famous papers were published in the first years of the twentieth century did the experiment become &#39;retrospectively reconstructed&#39; as a famous and decisive proof of relativity.</i></b><br /><br /><i>Morley and Miller in the 1900s</i><br /><br /><i>In spite of Michelson&#39;s own lack of interest in his findings, discussion did not cease. The results were seen as a &#39;cloud&#39; in the otherwise clear sky of physics. Numerous explanations were put forward in an attempt to show how the existence of an aether was compatible with the null results. These ranged from new sources of inaccuracy in the experiment, such as errors introduced by movement of the observer&#39;s eye, to the &#39;Lorentz contraction&#39; -- the suggestion that matter, including the arms of the interferometer, would shorten in the direction of movement to just the right degree to cancel out the effect. The interest was such that by the early 1900s Morley, and Dayton C. Miller, who had succeeded Michelson as a teacher in the university, were building new and improved interferometers. They built an enlarged device, based on wood, to look for differences in the contraction effect, but found results no different from the metal and sandstone instruments.</i><br /><br /><b><i>Still unsettled was the idea that the aether was trapped, or dragged along, by the dense surroundings of the experiment</i></b><i>; the next step was to try the apparatus on high ground. In 1905 Morley and Miller tried the experiment in a glass hut atop a 300 foot hill. They again found what could only be counted as a null result when compared with what might be expected from the earth&#39;s orbital velocity.</i><br /><br /><b><i>As they completed this work, Einstein&#39;s papers were becoming recognised for what they were and setting the scene for the reinterpretation of the &#39;null&#39; result as one of the most significant findings of experimental physics. It should not be thought, however, that Einstein&#39;s ideas were uniformly accepted upon their publication. The battle lasted several decades. Relativity was resisted for many reasons and on many fronts. There was interest in continued re-examinations of the Michelson-Morley result until beyond the end of the Second World War.</i></b><br /><br /><i>Miller claims to have found an aether drift: his 1920s experiments</i><br /><br /><b><i>As the interferometer experiments came to be seen as tests of relativity, rather than measures of the velocity of the earth, what had been done appeared less than complete.</i></b><i> Dayton Miller, partly as a result of encouragement from Einstein and Lorentz, decided to test the results with an apparatus built on top of Mount Wilson, at a height of 6000 feet. When the results of the earlier experiments were examined closely in the context of relativity, they revealed their ambiguity. </i><b><i>There was a small effect in the earlier experiments, though the fringe displacement was about one hundredth of a fringe rather than the expected four tenths.</i></b><i> For relativity, of course, any real effect, however small, was crucial.</i><br /><br /><i>In the early 1920s Miller conducted a number of inconclusive experiments on Mount Wilson, experiencing the usual troubles with temperature control, lack of rigidity of the apparatus, and so forth. He rebuilt the apparatus and took readings again on 4, 5 and 6 September 1924. </i><b><i>Miller now found a persistent positive displacement, and concluded that &#39;the effects were shown to be real and systematic, beyond any further question&#39;.</i></b><br /><br /><b><i>Miller&#39;s experiment was different from the others in that he pressed ahead with the fourth part of the protocol and took further readings in spring, summer and the following autumn. He concluded, in 1925, that he had found an observed motion of the earth of about 10 kilometres per second -- around one third of the result that the original Michelson experiments were expected to find. In 1925, Miller was awarded the &#39;American Association for the Advancement of Science&#39; prize for this work.</i></b><br /><br /><b><i>Thus, although the famous Michelson-Morley experiment of 1887 is regularly taken as the first, if inadvertent, proof of relativity, in 1925, a more refined and complete version of the experiment was widely hailed as, effectively, disproving relativity. This experiment was not conducted by a crank or charlatan. It was conducted by one of Michelson&#39;s closest collaborators, with the encouragement of Einstein, and it was awarded a major honour in the scientific community.</i></b><br /><br /><i>The initial experimental responses to Miller</i><br /><br /><i>There were a number of experimental responses to Miller&#39;s finding, all of them claiming a null result. The biggest effort was that made by Michelson himself. He built a huge interferometer and ran it in an insulated laboratory, again with null results. He and Miller confronted each other at a scientific meeting in 1928 and agreed to differ. An elaborate German experiment was also completed at about the same time, and this too found no significant effect. </i><b><i>Both of these experiments, it must be pointed out, were well shielded, and neither was conducted at significant elevation. The results of these two experiments seem to have quieted the renewed speculation brought on by Miller&#39;s positive results even though they were not carried out under conditions favourable for the recognition of an aether wind.</i></b><i> A further experiment was flown from a balloon, solving the altitude problem, but necessitating heavy shielding. As is often the case in science, </i><b><i>a &#39;critical mass&#39; of clearly expressed experimental voices can outweigh the objections of a critic however carefully argued.</i></b><br /><br /><i>In 1930 Michelson&#39;s huge device was installed at the top of Mount Wilson, in a telescope housing. The housing was made of metal and was, therefore, more of a potential shield than the housings of Miller&#39;s Mount Wilson experiments. In any case, nothing seems to have emerged from these Mount Wilson observations. What is more, although Michelson&#39;s interferometer was supposed to be made of &#39;Invar&#39;, an alloy not subject to expansion due to heat, a later analysis showed that the material was not properly formulated.</i><br /><br /><i>Miller&#39;s 1933 paper and the most recent experiments</i><br /><br /><i>In 1933 Miller published a paper reviewing the field and concluding that the evidence for an aether wind was still strong. We have then a classic situation of so-called replication in physics. Miller claimed a positive result, critics claimed negative results, but Miller was able to show that the conditions under which the negative experiments were conducted were not the same as the conditions of his own experiment. In particular, his was the only experiment that was done at altitude and with a minimum of the kind of shielding that might prevent the aether wind blowing past the test apparatus. Miller argued:</i><br /><br /><b><i>&#39;In three of the four [negative] experiments, the interferometers have been enclosed in heavy, sealed metal housings and also have been located in basement rooms in the interior of heavy buildings and below the level of the ground; in the experiment of Piccard and Stahel (an interferometer carried aloft in a balloon], a metal vacuum chamber alone was used ... If the question of an entrained ether is involved in the investigation, it would seem that such massive and opaque shielding is not justifiable. The experiment is designed to detect a very minute effect on the velocity of light, to be impressed upon the light through the ether itself, and it would seem to be essential that there should be the least possible obstruction between the free ether and the light path in the interferometer ...</i></b><br /><br /><i>In none of these other experiments have the observations been of such extent and of such continuity as to determine the exact nature of the diurnal [due to rotation of earth], and seasonal variation.&#39;</i><br /><br /><i>(Miller, 1933, p. 240)</i><br /><br /><b><i>In spite of this, the argument in physics was over. Other tests of relativity, including the Eddington observations of 1919 (to be discussed below), bolstered the idea that the theory of relativity was correct and that the velocity of light must be constant in all directions. The sheer momentum of the new way in which physics was done -- the culture of life in the physics community -- meant that Miller&#39;s experimental results were irrelevant.</i></b><br /><br /><i>We have travelled a long way from the notion that the Michelson-Morley experiment proved the theory of relativity. We have reached the point where the theory of relativity had rendered the Michelson-Morley experiment important as a sustaining myth, rather than as a set of results. Results that ran counter to what it was believed the Michelson-Morley experiment demonstrated were largely ignored. Think of it this way. </i><b><i>The notion of &#39;anomaly&#39; is used in science in two ways. It is used to describe a nuisance -- &#39;We&#39;ll ignore that; it&#39;s just an anomaly&#39;, and to signify serious trouble -- &#39;There are some troublesome anomalies in the existing theory.&#39; The interferometry results started as serious trouble for the theory of the aether. The null results passed from anomaly to &#39;finding&#39; as the theory of relativity gained adherents. With Miller&#39;s positive claims, interferometry results became, once more, an anomaly, but this time they were treated as a nuisance rather than a trouble. Miller&#39;s results were &#39;just an anomaly that needed to be explained away&#39;. Miller could not change the status of his positive readings from nuisance to troublesome anomaly even though they were the outcome of the best experiment yet completed, perhaps the only one which could truly be said to have tested what it was meant to test. The meaning of an experimental result does not, then, depend only upon the care with which it is designed and carried out, it depends upon what people are ready to believe.</i></b><br /><br /><i>Postscript</i><br /><br /><i>There are, in the scientific community, some with tidy minds who feel uncomfortable even about the anomalies which most think of as merely a nuisance. As late as 1955 a team were re-analysing the whole history of the experiments in an attempt to reconcile Miller&#39;s findings with what everyone believed. They concluded that Miller&#39;s work had been confounded by temperature changes. Repetitions of the experiment continued after this date. In 1963, experiments were done with a &#39;maser&#39;, the forerunner of the laser, to try to settle the experimental issue. Though, as has been explained, all this was in a sense irrelevant to relativity, it is not irrelevant to the thesis being argued here. Michelson and Morley could not have proved relativity, because as late as 1963 the results of the experiments, considered on their own, outside the context of the rest of physics, were not yet clear.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"Science as a Golum","summary":"The Tendency is for People to Think of Science as All Good or All Bad / Sociologists Collins and Pinch Instead Famously Likened Science in 1993 to a Golum / The Golum is a Creature of Jewish Mythology Made by Man of Clay and Water, with Incantations and Spells / It Grows More Powerful by the Day / It Will Follow Orders, Do Your Work and Protect You / But it is Clumsy and Dangerous / Without Control, a Golum May Destroy its Masters","image":"https://lh3.googleusercontent.com/-vp2fMhNogq4/WBVDLBkGzeI/AAAAAAAAJFU/CdSV8-wcDCY7SqFkjcJHiKxpVLB3yF5EwCJoC/w1800-h2520/science-as-a-golum-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Ao2u2CwFVxw","publishDate":"2016-10-30T00:48:51.498Z","updateDate":"2017-01-03T01:50:58.855Z","text":"<b>Science as a Golum: The Tendency is for People to Think of Science as All Good or All Bad / Sociologists Collins and Pinch Instead Famously Likened Science in 1993 to a Golum / The Golum is a Creature of Jewish Mythology Made by Man of Clay and Water, with Incantations and Spells / It Grows More Powerful by the Day / It Will Follow Orders, Do Your Work and Protect You / But it is Clumsy and Dangerous / Without Control, a Golum May Destroy its Masters</b><br /><br /><i>The Golum: What You Should Know About Science</i><br />by Harry Collins and Trevor Pinch<br />14th Ed (2013)<br />1st Published in 1993<br /><br />Excerpts from the Introduction<br /><br /><b><i>&quot;Science seems to be either all good or all bad.</i></b><i> For some, science is a crusading knight beset by simple-minded mystics while more sinister figures wait to found a new fascism on the victory of ignorance. For others it is science which is the enemy; our gentle planet, our feel for the just, the poetic and the beautiful, are assailed by a technological bureaucracy -- the antithesis of culture -- controlled by capitalists with no concern but profit. For some, science gives us agricultural self-sufficiency, cures for the crippled, and a global network of communication; for others it gives us weapons of war, a school teacher&#39;s fiery death as the space shuttle falls from grace, and the silent, deceiving, bone-poisoning, Chernobyl.</i><br /><br /><b><i>Both these ideas of science are wrong and dangerous. The personality of science is neither that of a chivalrous knight nor that of a pitiless juggernaut.</i></b><i> What, then, is science? </i><b><i>Science is a golem.</i></b><br /><br /><b><i>A golem is a creature of Jewish mythology. It is a humanoid made by man from clay and water, with incantations and spells. It is powerful. It grows a little more powerful every day. It will follow orders, do your work, and protect you from the ever threatening enemy. But it is clumsy and dangerous. Without control, a golem may destroy its masters with its flailing vigour.</i></b><br /><br /><i>The idea of a golem takes on different connotations in different legends. In some the golem is terrifyingly evil, but there is a more homely tradition: in the Yiddish brought from the East European ghetto, a golem (pronounced &#39;goilem&#39; in that dialect), is </i><b><i>a metaphor for any lumbering fool who knows neither his own strength nor the extent of his clumsiness and ignorance. For Collins&#39; grandmother it was good to know a golem if you wanted the garden dug up, but the children were advised to stay clear. Such a golem is not a fiendish devil, it is a bumbling giant.</i></b><br /><br /><i>Since we are using a golem as a metaphor for science, it is also worth noting that </i><b><i>in the mediaeval tradition the creature of clay was animated by having the Hebrew &#39;EMETH&#39;, meaning truth, inscribed on its forehead -- it is truth that drives it on. But this does not mean it understands the truth -- far from it.&quot;</i></b><br /><br />[...]<br /><br />Excerpts from the Conclusion<br /><br /><i>&quot;... Look back whence we came in science and there are what seem to be unconquerable peaks -- Mount Newton, Mount Pasteur, Mount Einstein -- a mountain range of truth. But look forward and the terrain is flat. A few new foothills wrench themselves from the plain every time we glance backwards. What are those new peaks? Were they there yesterday? </i><b><i>To understand how science works we must examine how we cause these foothills and mountains to emerge. To do this we must understand science which fails as well as science which succeeds.</i></b><i> Only in this way will we have the courage to climb the forbidding mountains of yesterday, and those which are newly forming behind us. </i><b><i>What our case studies show is that there is no logic of scientific discovery. Or, rather, if there is such a logic, it is the logic of everyday life.</i></b><br /><br /><i>Human error</i><br /><br /><b><i>It is impossible to separate science from society, yet preserving the idea that there are two distinct spheres is what creates the authoritarian image so familiar to most of us. How is it made to seem that the spheres are separate?</i></b><br /><br /><i>When something goes wrong with science, the scientific community reacts like a nest of ants with an intruder in their midst. Ants swarm over an intruder giving their lives for the nest; in the case of science it is human bodies that are sacrificed: the bodies of those responsible for the &#39;human error&#39; that allowed the problem to arise. The space shuttle explodes because of human error; Chernobyl explodes because of human error. Finding the human error is the purpose of post-accident inquiries. By contrast, our conclusion is that </i><b><i>human &#39;error&#39; goes right to the heart of science, because the heart is made of human activity. When things go wrong, it is not because human error could have been avoided but because things will always go wrong in any human enterprise.</i></b><i> One cannot ask of scientists and technologists that they be no longer human, yet only mythical automata -- quite unlike the constituents of a golem -- could deliver the sort of certainty that scientists have led us to expect of them.</i><br /><br /><i>As things stand, we have, as we remarked in the introduction, only two ways of thinking about science; it is all good or all bad. Unstable equilibrium -- flip-flop thinking -- is the inevitable consequence of a model of science and technology which is supposed to deliver complete certainty. </i><b><i>The trouble is that both states of the flip-flop are to be feared. The overweening claims to authority of many scientists and technologists are offensive and unjustified but the likely reaction, born of failed promises, might precipitate a still worse anti-scientific movement.</i></b><br /><br /><i>[...]</i><br /><br /><i>The second malaise is more worrisome. The favourable public reception of unusual sciences such as parapsychology -- the study of &#39;mind over matter&#39;, &#39;telepathy&#39;, and the like -- has given rise to fears that fringe sciences are taking over. </i><b><i>An anti-fringe science movement has been spawned whose members take it on themselves to &#39;debunk&#39; all that is not within the canon, in the name of proper scientific method. Where this effort is aimed at disabusing the public about unsupported claims, it is admirable, but the zeal of these self-appointed vigilantes carries over into areas where they have no business.</i></b><br /><br /><i>Recently, on British television, the public at large was able to witness a stage magician informing a prestigious scientist, head of a famous Paris institute, that his ideas were ridiculous. The motive for this attack was not the professor&#39;s methods but the topic he had chosen to research -- homeopathy; the instrument of the attack was, nevertheless, an idealised version of what scientific method ought to be. </i><b><i>It is no coincidence that those who feel most certain of their grip on scientific method have rarely worked on the frontiers of science themselves. There is a saying in love &#39;distance lends enchantment&#39;; it is true of science too. It is important that these vigilante organisations do not become so powerful that they can stamp out all that is strange in the scientific world.</i></b><i> Saving the public from charlatans is their role, but scientists must not use them to fight their battles for them. If homeopathy cannot be demonstrated experimentally, it is up to scientists, who know the risks of frontier research, to show why. </i><b><i>To leave it to others is to court a different sort of golem -- one who might destroy science itself.</i></b><br /><br /><i>Science and the citizen</i><br /><br /><i>The debate about the public understanding of science is equally confounded by confusion over method and content. </i><b><i>What should be explained is methods of science, but what most people concerned with the issues want the public to know about is the truth about the natural world -- that is, what the powerful believe to be the truth about the natural world.</i></b><i> The laudable reason for concern with public understanding is that scientific and technological issues figure more and more in the political process. Citizens, when they vote, need to know enough to come to some decision about whether they prefer more coal mines or more nuclear power stations, more corn or clearer rivers, more tortured animals or more healthy children, or whether these really are the choices. Perhaps there are novel solutions: wave power, organic farming, drug testing without torture. </i><b><i>The &#39;public understanders&#39;, as we might call them, seem to think that if the person in the street knows more science -- as opposed to more about science -- they will be able to make more sensible decisions about these things.</i></b><br /><br /><b><i>How strange that they should think this; it ranks among the great fallacies of our age. Why? -- because PhDs and professors are found on all sides in these debates. The arguments have largely been invented in universities.</i></b><i> Thus, all sides have expertise way beyond what can ever be hoped of the person in the street, and all sides know how to argue their case clearly and without obvious fallacies. Why such debates are unresolvable, in spite of all this expertise, is what we have tried to show in the descriptive chapters of this book. That is, we have shown that scientists at the research front cannot settle their deep disagreements through better experimentation, more knowledge, more advanced theories, or clearer thinking. It is ridiculous to expect the general public to do better.</i><br /><br /><i>We agree with the public understanders that the citizen needs to be informed enough to vote on technical issues, </i><b><i>but the information needed is not about the content of science; it is about the relationship of experts to politicians, to the media, and to the rest of us.</i></b><i> The citizen has great experience in the matter of how to cope with divided expertise -- isn&#39;t this what party politics is? What the citizen cannot do is cope with divided expertise pretending to be something else. Instead of one question -- &#39;Who to believe?&#39; -- there are two questions -- &#39;Who to believe?&#39; and &#39;Are scientists and technologists Gods or charlatans?&#39;. The second question is what makes the whole debate so unstable because, as we have argued, there are only two positions available.</i><br /><br /><i>What we have tried to do here is dissolve the second question scientists are neither Gods nor charlatans; they are merely experts, like every other expert on the political stage. They have, of course, their special area of expertise, the physical world, but their knowledge is no more immaculate than that of economists, health policy makers, police officers, legal advocates, weather forecasters, travel agents, car mechanics, or plumbers. </i><b><i>The expertise that we need to deal with them is the well-developed expertise of everyday life</i></b><i>; it is what we use when we deal with plumbers and the rest. Plumbers are science.</i><br /><br /><i>[...]</i><br /><br /><b><i>Experiments in real, deeply disputed, science hardly ever produce a clear-cut conclusion -- that is what we have shown ...</i></b><br /><br /><i>[...]</i><br /><br /><b><i>At the very least, be suspicious if one interpretation of such a test is treated as though it were inevitable. Listen for interpretations from different interest groups, and make sure those groups are happy that they each had some control over how the test was set up, and what the result was taken to mean. If they do not agree on this, listen to the nature of their complaints.</i></b><br /><br /><i>[...]</i><br /><br /><i>Science education</i><br /><br /><i>Finally we come to science education in schools. It is nice to know the content of science -- it helps one to do a lot of things such as repair the car, wire a plug, build a model aeroplane, use a personal computer to some effect, know where in the oven to put a souffle, lower one&#39;s energy bills, disinfect a wound, repair the kettle, avoid blowing oneself up with the gas cooker, and much much more. For that tiny proportion of those we educate who will go on to be professional research scientists, knowledge of the content of science must continue to be just as rigorous and extended, and perhaps blinkered, as it is now. </i><b><i>But for most of our children, the future citizens of a technological society, there is another, and easier, lesson to be learned.</i></b><br /><br /><b><i>Every classroom in which children are conducting the same experiment in unison is a microcosm of frontier science.</i></b><i> Each such multiple encounter with the natural world is a self-contained sociological experiment in its own right. Think about what happens: the teacher asks the class to discover the boiling point of water by inserting a thermometer into a beaker and taking a reading when the water is steadily boiling. One thing is certain: almost no-one will get 100 °C unless they already know the answer, and they are trying to please the teacher. Skip will get 102 °C, Tania will get 105 °C, Johnny will get 99·5 °C, Mary will get 100.2 °C, Zonker will get 54 °C, while Brian will not quite manage to get a result; Smudger will boil the beaker dry and burst the thermometer. </i><b><i>Ten minutes before the end of the experiment the teacher will gather these scientific results and start the social engineering</i></b><i>. Skip had his thermometer in a bubble of superheated steam when he took his reading, Tania had some impurities in her water, Johnny did not allow the beaker to come fully to the boil, Mary&#39;s result showed the effect of slightly increased atmospheric pressure above sea-level, Zonker, Brian and Smudger have not yet achieved the status of fully competent research scientists. </i><b><i>At the end of the lesson, each child will be under the impression that their experiment has proved that water boils at exactly 100 °C, or would have done were it not for a few local difficulties that do not affect the grown-up world of science and technology, with its fully trained personnel and perfected apparatus.</i></b><br /><br /><b><i>That ten minutes renegotiation of what really happened is the important thing. If only, now and again, teachers and their classes would pause to reflect on that ten minutes they could learn most of what there is to know about the sociology of science. For that ten minutes illustrates better the tricks of professional frontier science than any university or commercial laboratory with its well-ordered predictable results.</i></b><i> Eddington, Michelson, Morley, Weber, Davis, Fleischmann, Pons, Jones, McConnell, Ungar, Crews, Pasteur and Poucher are Skips, Tanias, Johnnys, Marys, Zonkers, Brians, and Smudgers with clean white coats and &#39;PhD&#39; after their names. </i><b><i>They all come up with wildly varying results. There are theorists hovering around, like the schoolteacher, to explain and try to reconcile. In the end, however, it is the scientific community (the head teacher?) who brings order to this chaos, transmuting the clumsy antics of the collective Golem Science into a neat and tidy methodological myth. There is nothing wrong with this; the only sin is not knowing that it is always thus.&quot;</i></b><br /><br />Brief Excerpts from the Afterward: Golem and the Scientists<br /><br /><b><i>&quot;Inevitably, a new view perturbs aspects of conventional wisdom and makes some people more uncomfortable than others. The science warriors who hate or scorn the Golem notion will countenance no alternative view of science. They seem to think of science as like a fundamentalist religion: mysterious, revealed, hierarchical, exhaustive, exclusive, omnipotent and infallible. The language is that of the crusade or the witch-hunt; victory, confession and retraction are the goals wherever heresy is encountered. Fundamentalists polarise discussion; for fundamentalists, even our serious critics are to be damned for not being critical enough.</i></b><br /><br /><i>[...]</i><br /><br /><i>We did not intend The Golem to be a radical book in terms of philosophy or any other discipline. The Golem is not an attack on science; far from it. That it has been taken to be a weapon in the science wars tells us most, perhaps, about the fundamentalist nature of that debate. </i><b><i>The more scientists engage in this kind of scientific fundamentalism the more does science reduce its standing as an object of respect. Science is a cultural pinnacle and an exhibition of supreme skill and expertise but we believe that it does not have to justify itself as a complete world view, a new religion or a slayer of false Gods.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The Rejection of the Maser, the Precursor to the Laser","summary":"Before, and Even After the Maser Worked, the Leading Scientists of the Day Told its Inventor that it was Impossible / Objections from Scientists like Neils Bohr and John von Neumann were Not Offhand Opinions Regarding Obscure Physics / The Objections Came from the Marrow of these Mens' Bones, and were Based on the Heisenberg Uncertainty Principle / There Are 2 Lessons Here","image":"https://lh3.googleusercontent.com/-lu-tWkY2Qio/WBNrPGARPaI/AAAAAAAAJEU/Q75EN1XuvaUXDf42VA9JqLfMBTOhiXh_gCJoC/w3600-h5040/the-rejection-of-the-maser-the-precursor-to-the-laser-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/RysFLerBrVj","publishDate":"2016-10-28T15:14:31.187Z","updateDate":"2017-01-03T01:51:17.626Z","text":"<b>The Rejection of the Maser, the Precursor to the Laser: Before, and Even After the Maser Worked, the Leading Scientists of the Day Told its Inventor that it was Impossible / Objections from Scientists like Neils Bohr and John von Neumann were Not Offhand Opinions Regarding Obscure Physics / The Objections Came from the Marrow of these Mens&#39; Bones, and were Based on the Heisenberg Uncertainty Principle / There Are 2 Lessons Here: The Best Theorists can be Wrong / And Innovation Sometimes Requires Work by Engineers Who Don&#39;t Care What Textbooks Say</b><br /><br /><i>How the Laser Happened: Adventures of a Scientist</i><br />by Charles H. Townes<br /><br /><i>&quot;Before -- and even after -- the maser worked, our description of its performance met with disbelief from highly respected physicists, even though no new physical principles were really involved.</i><br /><br /><i>Their objections went much deeper than those that had led Rabi and Kusch to try to kill the project in its cradle; fully familiar with oscillators and molecular beams, these two never questioned the general idea.  They just thought it was impractical and that it diverted departmental resources from basic physics and more sensible work.</i><br /><br /><i>Llewelyn H. Thomas, a noted Columbia theorist, told me that the maser flatly could not, due to basic physics principles, provide a pure frequency with the performance I predicted.</i><br /><br /><i>So certain was he that he more or less refused to listen to my explanations. After it did work, he just stopped talking to me.</i><br /><br /><i>A younger physicist in the department, even after the first successful operation of the device, bet me a bottle of scotch that it was not doing what we said it would (he paid up).</i><br /><br /><i>Shortly after we built a second maser and showed that the frequency was indeed remarkably pure, I visited Denmark and saw Neils Bohr, the great physicist and pioneer in the development of quantum mechanics.  As we were walking along the street together, he quite naturally asked what I was doing.  I described the maser and its performance.</i><br /><br /><i>&#39;But that is not possible,&#39; he exclaimed.  I assured him it was.  Similarly, at a cocktail party in Princeton, New Jersey, the Hungarian mathematician John von Neumann asked what I was working on.  After I told him about the maser and the purity of its frequency, he declared, &#39;That can’t be right!&#39;  But it was, I replied, and told him it was already demonstrated.</i><br /><br /><i>Such protests were not offhand opinions concerning the obscure aspects of physics; they came from the marrow of these men’s bones.</i><br /><br /><i>These were objections founded on principle -- the uncertainty principle ...</i><br /><br /><i>Engineers, whose practical tasks up to that time almost never brought them face to face with such esoterica as the uncertainty principle, never had a hard time with the precise frequency the maser produced.  They dealt all the time with oscillators and cavities, based on a wide variety of physical phenomena, which produced rather precise frequencies.  They accepted as a matter of course that a maser oscillator might do what it did.  What they were not so familiar with was the idea of stimulated emission, which gave the maser its amplifying power.</i><br /><br /><i>Birth of the maser required a combination of instincts and knowledge from both engineering and physics.</i><br /><br /><i>Physicists working in microwave and radio spectroscopy, which demanded engineering as well as physics skills, seem to have had the necessary knowledge and experience to both appreciate and understand the maser immediately.  Rabi and Kusch, themselves in a similar field, for this reason accepted the basic physics readily.  But for some others, it was startling.</i><br /><br /><i>I am not sure that I ever did convince Bohr.  On that sidewalk in Denmark, he told me emphatically that if molecules zip through the maser so quickly, their emission lines must be broad.  After I persisted, he said, &#39;Oh, well, yes, maybe you are right,&#39; but my impression was that he was simply trying to be polite to a younger physicist ...&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"The Pressure to Publish","summary":"There is an Acute Pressure to Publish in Academia / This Hyper-Competitive Atmosphere is Stifling Creativity because it Rewards Students for the Wrong Behaviors / \"Current Pressures and Incentives Mean that being First but Wrong Pays Off Better than Being Second and Right\" / One Person Who Went through the Process Thinks \"She Probably Should Have Ignored the Pressure to Publish, and Ruminated More\"","image":"https://lh3.googleusercontent.com/-bMCHZNf6dBE/WBIf44ESPDI/AAAAAAAAJDA/zpofQH3TisgYxfzCvXWyaL-hH2YVAOjNQCL0B/the-pressure-to-publish-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/3uPsnDLdbKi","publishDate":"2016-10-27T15:40:52.771Z","updateDate":"2017-01-03T01:52:30.596Z","text":"<b>The Pressure to Publish: There is an Acute Pressure to Publish in Academia / This Hyper-Competitive Atmosphere is Stifling Creativity because it Rewards Students for the Wrong Behaviors / &quot;Current Pressures and Incentives Mean that being First but Wrong Pays Off Better than Being Second and Right&quot; / One Person Who Went through the Process Thinks &quot;She Probably Should Have Ignored the Pressure to Publish, and Ruminated More&quot;</b><br /><br />We may not see the true effects of this for another 5-10 years, but there are reasons to believe that many of the (currently ignored) issues raised by insider academic whistleblowers will soon go mainstream as a barrage of academics burn out.<br /><br /><a href=\"http://www.nature.com/polopoly_fs/1.20872!/menu/main/topColumns/topLeftColumn/pdf/538446a.pdf\" class=\"ot-anchor\">http://www.nature.com/polopoly_fs/1.20872!/menu/main/topColumns/topLeftColumn/pdf/538446a.pdf</a><br /><br />Young, Talented and Fed-Up<br />by Kendall Powell<br /><br /><i>&quot;Martin Tingley was coming undone. It was late autumn 2014, just over a year into his assistant-professor job at Pennsylvania State University in State College, and he was on an eight-hour drive home after visiting his wife in Boston. He was stressed, exhausted and close to tears. As the traffic zipped past in the dark hours of the early morning, the headlights gave him the surreal feeling that he was inside a video game.</i><br /><br /><i>Usually, Tingley thought of himself as a &#39;pretty stoic guy&#39; -- and on paper, his career was going well. He’d completed a master’s degree in statistics and a PhD in Earth science, both at Harvard University. With these, and four years of postdoctoral experience, he had landed a rare tenure-track faculty position. He thought he would soon be successfully combining statistics and climate science to produce the type of interdisciplinary research that funding agencies say they want.</i><br /><br /><i>In fact, scientific life was proving tough. He found himself working 60-80 hours per week doing teaching and research. His start-up funding had run out, he had yet to secure a major grant and, according to a practice common in US academia, he would not be paid by his university for three summer months. His wife had not been able to move with him, so he was making tiring weekend commutes. It seemed that the pressures had reached unsustainable levels. Something had to give.</i><br /><br /><i>Tingley is one of many young scientists who are deeply frustrated with life in research. </i><b><i>In September, Nature put a post on Facebook asking scientists who were starting their first independent position to tell us about the challenges that they faced. What followed was a major outpouring of grief. Within a week, nearly 300 scientists from around the world had responded with a candid catalogue of concerns. &#39;I see many colleagues divorcing, getting burnt out, moving out of science, and I am so tired now,&#39; wrote one biomedical researcher from Belgium (see ‘Suffering in science’).</i></b><i> Nature selected three young investigators who voiced the most common frustrations; here, we tell their stories.</i><br /><br /><i>But are young scientists whining -- or drowning? Our interviewees acknowledge that they are extremely fortunate to have an opportunity to direct their own creative, stimulating careers, and they are hardly the only professionals who are expected to work hard. It’s easy for each generation to imagine that things are more difficult for them than they were in the past.</i><br /><br /><i>But some data and anecdotal evidence suggest that scientists do face more hurdles in starting research groups now than did many of their senior colleagues 20-30 years ago (see page 444). Chief among those challenges is the unprecedented number competing for funding pools that have remained stagnant or shrunk in the past decade. </i><b><i>&#39;The number of people is at an all-time high, but the number of awards hasn’t changed,&#39;</i></b><i> says Jon Lorsch, director of the US National Institute of General Medical Sciences (NIGMS) in Bethesda, Maryland. </i><b><i>&#39;A lot of people with influence on the system recognize this is a serious problem and are trying to fix it.&#39;</i></b><br /><br /><b><i>Young scientists and senior scientists alike feel an acute pressure to publish and are weighed down by a growing bureaucratic burden, with little administrative support. They are largely judged on their record of publishing and of winning grants -- but without clear targets, they find themselves endlessly churning out paper after paper. The crucial question is whether this is harming science and scientists. Bruce Alberts, a prominent biochemist at the University of California, San Francisco, and former president of the US National Academy of Sciences, says that it is. The current hyper-competitive atmosphere is stifling creativity and pushing scientists &#39;to do mediocre science&#39;, he says -- work that is safe and uninteresting. &#39;We’ve got to reward people who do something differently.&#39;</i></b><br /><br /><i>Our informal survey suggests that the situation is already making research an unwelcoming career. </i><b><i>&#39;Frankly, the job of being a principal investigator and running a lab just looks horrible,&#39; wrote one neuroscientist from the United States. Tingley wouldn’t disagree.</i></b><br /><br /><i>FUNDING FIGHT</i><br /><br /><i>Tingley has always had broad interests. At university in Canada, he switched from art history to physics. For his graduate studies, he was drawn to the vibrant research environment at Harvard, in Cambridge, Massachusetts, where he built statistical methods that helped to make sense of data on past climate gathered from sources such as tree rings and ice cores.</i><br /><br /><i>By the time he was searching for academic positions, he was already working 60-hour weeks, he says: he would be at work by 8 a.m., go home for dinner, and then pull out his laptop again at night. But by 2013, his research was hitting a high: he had published a statistical analysis in Nature [1] and, after applying for jobs worldwide, was offered a joint appointment in meteorology and statistics at Penn State.</i><br /><br /><i>By this point, his wife, Gabrielle, ran the communications programme for Harvard’s Research Computing centre in Cambridge. Positions offered to her at Penn State fell far short of her qualifications, and she opted to stay where she was. They were facing the ‘two-body problem’ -- a long-standing stress point for scientists.</i><br /><br /><i>Like many first-year assistant professors, Tingley immediately felt pressure to publish in top journals, attract funding and students, and innovate in the classroom. He also knew that his roughly US$200,000 in start-up funding from the university -- to cover his summer salary, computing access and more -- wouldn’t last long, and he applied to the US National Science Foundation for grants. That process was &#39;heartbreaking&#39;, he says.</i><br /><br /><i>In one instance, he put in a proposal with his collaborator, organic geochemist Jessica Tierney at the University of Arizona in Tucson, for work on proxies for past sea surface temperatures. On the first round of review, the application got two scores of &#39;excellent&#39; and two of &#39;very good&#39;, yet it still fell short of being funded. The two were encouraged to resubmit, which they did. On the next round, the proposal scored worse. &#39;Part of it is on me, I was unsuccessful,&#39; Tingley says -- but the anecdote shows the frustration that young scientists face when trying to get a research programme off the ground. </i><b><i>&#39;The funding cycle is brutal.&#39;</i></b><i> In the meantime, the pair published the initial stages of the work [2] in an article that has been cited 40 times.</i><br /><br /><b><i>The views of scientists who responded to Nature revealed a generational divide: many feel that today’s senior investigators experienced a more comfortable trajectory in science and now have a competitive advantage. The ‘baby boom’ scientists, who have longer track records and well-established labs, are in a stronger position to win funds. (In September, Nature asked on Twitter: &#39;What are the challenges facing young scientists?&#39; &#39;Old scientists,&#39; one respondent shot right back.)</i></b><br /><br /><i>In December 2014, shortly after his low point in the car, Tingley and his wife took a month-long trip to Australia and Indonesia for some much-needed time together. The next month, Tingley returned to the winter chill at State College and walked across campus feeling as if his head was scraping against the low-hanging clouds. He knew that much of his time was about to be sucked up teaching two advanced courses, leaving little time for research, and he would be back to the tiring commute to see his wife at the weekends. If he didn’t get a grant soon, he would have no summer salary. &#39;My wife and I knew this wasn’t a sustainable way for us to live our lives.&#39;</i><br /><br /><i>Tingley started googling around late at night, and in March, he spied the perfect job posting. Insurance Australia Group in Sydney was looking for someone with experience in meteorology, statistics and climate. He started there two months later, and his wife easily found a position in communications with the University of New South Wales. Now a senior research analyst, Tingley models and quantifies risks from bush fires, cyclones and other storms. The transcontinental move was not without its difficulties, of course - and as a young researcher moving to the private sector, he’s had to prove himself all over again.</i><br /><br /><i>Tingley now advises others to recognize that there are various paths to a successful career. &#39;It’s perfectly legitimate to use your training and skill set in the private sector.&#39; He isn’t missing the stress and high expectations placed on young investigators’ shoulders, he says. On a sunny spring Saturday in September, he and his wife head out for a walk on their neighbourhood beach. &#39;It turns out that weekends are fantastic,&#39; he says.</i><br /><br /><i>NEVER GOOD ENOUGH</i><br /><br /><i>Sometimes, pressures come not from chasing funding or tenure, but from chasing an ideal of what makes a good scientist. Young researchers from all disciplines told Nature that they wrestle with the lack of clear expectations for success - and materials scientist Eddie López-Honorato is one.</i><br /><br /><i>He grew up in Mexico City and studied chemistry there, at the National Autonomous University of Mexico, but for his PhD, he struck out for the University of Manchester, UK. He worked at night and at weekends to complete his experiments, he says, which became more difficult after his son was born. He found it stressful, but his time at Manchester gave him high working standards that he now tries to emulate. Next, he did a postdoctoral fellowship at the Institute for Transuranium Elements in Karlsruhe, Germany, where he worked on developing safer coatings for nuclear fuels used in reactors.</i><br /><br /><i>At the end of his postdoc, he had the opportunity to return to the United Kingdom as a lecturer at the University of Sheffield, but he and his wife, Paola, yearned to go back to Mexico. They weighed up the pros and cons. López-Honorato knew that he would need to build up his professional reputation in Mexico and that the science infrastructure there was less developed than in Europe. But he thought that working in the United Kingdom would be harder for his family, because they faced constant changes in language and culture. The family chose Mexico.</i><br /><br /><i>In March 2012, López-Honorato started at the Center for Research and Advanced Studies of the National Polytechnic Institute (CINVESTAV) in Ramos Arizpe. He felt an amazing sense of independence and potential on standing in front of his brand new empty lab space. &#39;You know that you have to get some students and money fast, really fast, and that’s when the urge to work kicks in,&#39; he says. Although the government paid his and his students’ salaries, he still needed to secure funds to support his research. He sent out a flurry of grant proposals for government funding, without success.</i><br /><br /><i>López-Honorato spent 2012 travelling around Mexico and the United States to build collaborations. He cold e-mailed other scientists to explain his work. The grants started trickling in. By 2014, he had secured enough to cover most of his research expenses and had established a second arm to his lab’s work: developing adsorptive materials to remove arsenic from drinking water, a problem that affected nearly half of all wells in certain parts of Mexico [3]. Since starting at CINVESTAV, he has published 20 research papers and has built up a lab group of 15 people.</i><br /><br /><i>Like many of those interviewed, he says that the work to sustain funding is as tough as winning the first grants. Even though his position is secure, he feels the pressure of maintaining his research projects and launching the careers of younger scientists. &#39;It’s stressful when you don’t have money, and stressful when you do have money, because then you have to deliver. It’s my fault if anything goes wrong.&#39; He points to a recent eight-month bureaucratic delay in purchasing a coating machine that is essential to his nuclear-fuel work; it put the project a year behind schedule, and he feels that he is to blame.</i><br /><br /><b><i>Many scientists, like other professionals, say that there aren’t enough hours in the day. (&#39;My cohort, we feel exhausted,&#39; said one Generation X scientist, who asked to remain anonymous to protect his career.) In the past two months, López-Honorato says, he has averaged four hours of sleep per night.</i></b><i> He and other early-career researchers are &#39;in a stage where our kids and partners need us the most at home&#39;, he says. His second son is now eight months old.</i><br /><br /><i>He wrestles with whether he has valid reasons to complain, and knows the pressures are largely self-generated. &#39;It’s a problem of saying, ‘That’s enough’,&#39; he says. It’s an issue that many young investigators struggle with -- when you’re the one setting the goals, when do you have enough money, students or publications? Philip Guo, a cognitive scientist at the University of California, San Diego, described in a 2014 blogpost how academics often feel as if they are on an accelerating treadmill. In his previous work as a software engineer at Google, Guo said, he had &#39;tremendous clarity about what and how much I was expected to do&#39;. </i><b><i>Academics, however, have obligations to teach, advise, do research, write grants and support departments, universities and the academic community -- and &#39;none of these sources of work know of or care about one another&#39;. Alberts highlights the young investigators who need two major grants, one to supply their salary and one for their research programme. &#39;It’s horrible pressure on young people. How are they going to be excellent at anything? The incentives are all wrong.&#39;</i></b><br /><br /><i>This year, López-Honorato is trying to lower his own expectations, applying for only one industry grant -- compared with the seven he applied for in 2012 -- in the hope that he’ll get home in time to play with his boys. But that internal pressure is hardest to quell. &#39;We want to be the best -- that’s how we got to the job we have right now. It’s a personal pressure. But that’s even more difficult to get rid of.&#39;</i><br /><br /><b><i>NO TIME TO THINK</i></b><br /><br /><i>Computing always attracted Felienne Hermans, who taught herself programming at age 10. She specialized in computer science at university and pursued a PhD at Delft University of Technology in the Netherlands. There, she applied methods of software engineering to spreadsheets, so that end users such as accountants or biologists would have better ways of maintaining and annotating their data [4]. The creative work won her top conference papers, which are key for advancement in this field. When a tenure-track position opened up in her research group of four professors, she asked whether she could apply. She beat internal and external candidates and started as an independent professor in March 2013, at the age of just 28.</i><br /><br /><i>Two years into the position, Hermans was feeling overwhelmed. She was grappling with the responsibilities of managing her two graduate students and one postdoc, prepping for teaching courses, and what felt like endless ‘service’ requests to review papers for journals and colleagues. The spreadsheet work had in some ways run its course, and she wanted to pivot to a more stimulating research area. But the pressure to publish continuously and copiously dogged her. </i><b><i>Her job is formally split between 40% teaching, 40% research and 20% academic service, but the message is that research should trump everything else. &#39;Four papers are better than three. And five are better than four,&#39; she says.</i></b><br /><br /><b><i>Like Alberts, she says the idea that research output is now synonymous with publication quashes all creativity. &#39;Papers are just one form of communicating ideas and experiments.&#39;</i></b><i> She yearns &#39;for an afternoon of looking out the window and thinking, ‘What will I do next?’&#39;.</i><br /><br /><i>Another barrier has been constant throughout her career: being a woman in an overwhelmingly male-dominated field. In 2014, she attended the Code Generation hands-on programming conference in Cambridge, UK, and found herself 1 of only 2 women among roughly 100 attendees. She spent the three days speaking to colleagues about this sad statistic, rather than about her programming, as she would have preferred. &#39;It drags you down and drains your energies,&#39; she says. In the survey, Nature received roughly a dozen comments from young scientists who indicated that sexism, gender bias or lack of support for women held back their careers.</i><br /><br /><i>Hermans eventually developed a fresh research focus through her Saturday volunteer work at a community centre, where she taught programming to inner-city kids. She and a colleague began thinking about how best to teach the children. Rather than just explaining how to make a robot move forward, say, they wanted to communicate how to maintain code quality through properly naming program features and avoiding ‘code smells’, or poorly designed program sections. The pivot wasn’t totally smooth -- her first conference paper about a generic theory for code smells was rejected for not having enough supporting evidence, but now she is hitting her stride.</i><br /><br /><b><i>Looking back, Hermans says that she probably should have ignored the pressure to publish, and ruminated more.</i></b><i> &#39;But I was new in the tenure track and super scared about not being able to pay my mortgage in two years.&#39; Now, she keeps more careful track of her time. If a colleague knocks on her door for help with a student’s paper, she can turn them down: &#39;I’ve already done my 20% to service.&#39; She’s rearranged her week, cramming teaching, grant writing and service into Monday to Thursday so that she can spend Fridays with her lab group, which now comprises six people.</i><br /><br /><i>There are more-organized moves to help young investigators -- to win grants, for example. Alberts says that &#39;there has to be a shift of resources to the younger people&#39;. He points to the European Research Council grant programme that divides applicants into three career stages - Starter (2-7 years post-PhD), Consolidator (7-12 years post-PhD) and Advanced (more than 12 years post-PhD) -- so that applicants from each career stage compete with their peers. In the same vein, this year the NIGMS piloted a grant called Maximizing Investigators’ Research Award, which separates early-stage investigators from established ones, and offers five years of guaranteed funding. That’s an innovation in the US funding system, says Lorsch, because it means no longer &#39;comparing apples and oranges&#39;. And Lorsch says that older investigators should be encouraged to move into alternative stages of their career -- working in teaching, mentoring and science advocacy -- that don’t require research funds. This could help younger researchers to break in.</i><br /><br /><i>Other scientists vehemently oppose such ideas. And Alberts, like many senior scientists, doesn’t see the problem as solely based on age. </i><b><i>&#39;It’s not about fairness. It’s about how to get the best science for the dollar. We’ll get much better science by funding young or old people to do innovative things.&#39;</i></b><br /><br /><i>Hermans is acutely aware that the grumbles of young scientists can be brushed away. </i><b><i>&#39;If people are complaining about an injustice, it’s easy to say they are just moaning,&#39; she says. &#39;But these are not imaginary problems.&#39;</i></b><i> She feels it’s her duty to be vocal about the challenges facing young investigators. &#39;Experienced researchers should be observing if a young scientist is failing and asking, ‘Are you overwhelmed? Why aren’t you inspired?’&#39;</i><br /><br /><i>Lorsch says that he knows first-hand that Generation X scientists are not whiners: &#39;I do not hear complaining from the people who are trying to get their first grant or renew their first grant, the people trying to get a lab running,&#39; he says. &#39;It’s the really well-funded people who’ve lost one of their grants -- that’s who call me and scream.&#39;</i><br /><br /><i>Kendall Powell is a freelance writer based in Lafayette, Colorado.&quot;</i><br /><br />[1] Tingley, M. P. &amp; Huybers, P. Nature 496, 201-205 (2013).<br /><br />[2] Tierney, J. E. &amp; Tingley, M. P. Geochim. Cosmochim. Acta 127, 83-106 (2014).<br /><br />[3] Mondragón-Solórzano, G., Sierra-Álvarez, R., López-Honorato, E. &amp; Barroso-Flores, J. J. Incl. Phenom. Macrocycl. Chem. 85, 169-174 (2016).<br /><br />[4] Hermans, F., Sedee, B., Pinzger, M. &amp; van Deursen, A. Proc. 2013 Int. Conf. Software Eng. 292-301 (2013).<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Pseudo-Mysteries of Lightning","summary":"In Some Ways, Our Understanding of Lightning Hasn't Improved Much Since Ben Franklin / Ordinary Lightning Should be an Order of Magnitude too Weak to Produce the Incredible Observed Release of Energy / Scientists Can't Agree on How Thunderclouds Become Charged, or What Triggers the Spark / But, Could it be that the Mystery is Created by Modeling the Earth as a Closed Electrical System?","image":"https://lh3.googleusercontent.com/-HaUVfRE-jXg/WBAmgMM3ZhI/AAAAAAAAJCY/KA0fOHnyu7AdEAv4CZMhQNMHmDUBR8qCgCJoC/w5400-h7560/the-pseudo-mysteries-of-lightning-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/DGcxywdpRLp","publishDate":"2016-10-26T03:45:03.132Z","updateDate":"2017-01-03T01:52:54.395Z","text":"<b>The Pseudo-Mysteries of Lightning: In Some Ways, Our Understanding of Lightning Hasn&#39;t Improved Much Since Ben Franklin / Ordinary Lightning Should be an Order of Magnitude too Weak to Produce the Incredible Observed Release of Energy / Scientists Can&#39;t Agree on How Thunderclouds Become Charged, or What Triggers the Spark / But, Could it be that the Mystery is Created by Modeling the Earth as a Closed Electrical System?</b><br /><br />What is interesting about all of the mysteries of and recent surprises about lightning is that there is one single explanation readily available which makes sense of it all ...<br /><br />... that atmospheric lightning is simply one leg of a much larger electrical system which the Earth is immersed within ...<br /><br />... and that the cloud is not the site of lightning&#39;s formation, but rather the site of least electrical resistance to the big ball of iron planetary theorists suggest resides at the center of the Earth ...<br /><br />Why again is this such a heresy?<br /><br /><b>Lightning&#39;s Extraordinary Emissions</b><br /><br /><a href=\"http://space.gizmodo.com/lightning-mysterious-gamma-ray-bursts-1549723614\" class=\"ot-anchor\">http://space.gizmodo.com/lightning-mysterious-gamma-ray-bursts-1549723614</a><br /><br /><i>&quot;While lightning strikes about 50 to 70 times a second worldwide, brief bursts of gamma rays are only emitted about once an hour. </i><b><i>Theoretically speaking, ordinary lightning should be at least an order of magnitude too weak to produce a gamma ray burst.</i></b><i> Current theories for how the bursts happen anyway range from wondering if the bursts are associated with red sprites or blue elves, are a side-effect of lightning triggered by cosmic rays, or maybe even have something to do with the weird high-energy particles sometimes observed in the Van Allen radiation belt. Firestation checked out to start collecting observations as of September 2013, so hopefully we’ll be hearing more in coming months as data helps validate some theories while transforming others into discarded relics.</i><br /><br /><i>Why is this so fascinating? The various press releases talk about parties raging in the upper atmosphere, and surprise at finding something associated with supernova and solar flares taking place in a cold atmosphere:</i><br /><br /><b><i>&#39;&#39;Somewhere in the atmosphere momentarily there’s just an incredible amount of energy release and what happens in that region is something of a witch’s brew,&#39; said Doug Rowland, principal investigator for FireStation at NASA’s Goddard Space Flight Center in Greenbelt, Md. &#39;You get antimatter created in the Earth’s atmosphere during this interaction, you get energetic neutrons that basically you never see in the quiet atmosphere, that you only associate with nuclear reactions, that are happening in our atmosphere whenever these things go off.</i></b><i> That’s one of the first fundamental science reasons [to study this phenomenon] -- it’s part of our planet; we don’t understand it; we want to understand it.&#39;&#39;</i><br /><br /><b>What Charges Thunderclouds?</b><br /><br /><a href=\"http://www.bbc.com/future/story/20120926-what-causes-lightening\" class=\"ot-anchor\">http://www.bbc.com/future/story/20120926-what-causes-lightening</a><br /><br /><i>&quot;It’s not yet agreed, for example, how a thundercloud gets charged up in the first place. Somehow the motions of air, cloud droplets, and precipitation (at that altitude, ice particles) conspire to separate positive from negative charge at the scale of individual molecules. It seems that ice particles acquire electrical charge as they collide, rather as rubbing can induce static electricity, and that somehow smaller ice particles tend to become positively charged while larger ones become negatively charged. As the small particles are carried upwards by convection currents, the larger ones sink under gravity, and so their opposite charges get separated, creating an electrical field.</i><br /><br /><i>A lightning strike discharges this field. It is basically a gigantic spark jumping between the &#39;live wire&#39; and the &#39;earth&#39; of an electrical circuit, in which the former is the charged cloud and the latter is literally the earth.&quot;</i><br /><br /><b>What Seeds the Spark?</b><br /><br /><a href=\"http://www.bbc.com/future/story/20120926-what-causes-lightening\" class=\"ot-anchor\">http://www.bbc.com/future/story/20120926-what-causes-lightening</a><br /><br /><i>&quot;Scientists are still puzzled as to what triggers a spark during a thunderstorm.  The latest attempt to answer the question only adds to the intrigue.  It seems hard to believe that we still don’t understand what causes lightning during thunderstorms – but that’s a fact ... In some ways we’ve not got much further since Franklin.&quot;</i><br /><br /><a href=\"http://phys.org/news/2016-02-scientists-clues-mystery-lightning.html#jCp\" class=\"ot-anchor\">http://phys.org/news/2016-02-scientists-clues-mystery-lightning.html#jCp</a><br /><br /><b><i>&quot;Currently, the largest electric fields that have been measured inside thunderstorms are several times weaker than what is needed to break down cloudy air and initiate lightning.&quot;</i></b><br /><br /><a href=\"http://www.bbc.com/future/story/20120926-what-causes-lightening\" class=\"ot-anchor\">http://www.bbc.com/future/story/20120926-what-causes-lightening</a><br /><br /><i>&quot;One idea is that they are triggered by cosmic rays streaming into the atmosphere from space. A study has tried to interrogate that notion and finds that, if there’s anything in it, it’s probably not quite in the way we thought.</i><br /><br /><i>Out-of-this-world idea</i><br /><br /><i>Cosmic rays – mostly protons and electrons – pervade the universe, being produced in awesomely energetic astrophysical processes like supernovae, and they are constantly raining down on Earth. If a cosmic ray collides with an air molecule, this can kick out a spray of fundamental particles and fragments of nuclei. Those in turn interact with other molecules, ionising them and generating a shower of electrons.</i><br /><br /><i>In 1997, Russian scientist Alexander Gurevich and his coworkers in Moscow suggested a way in which cosmic rays could be lightning seeds. In the electric field of a thundercloud, these electrons are accelerated, much as particles are in a particle accelerator, creating yet more energetic collisions in a &#39;runaway&#39; process that builds into a lightning strike. This process is expected to produce X-rays and gamma-rays too, and since bursts of these rays have been detected by satellites during thunderstorms, Gurevich’s idea of cosmic-ray-induced lightning seemed plausible.</i><br /><br /><i>If the cosmic-ray theory is right, the avalanche of electrons should also generate radio waves, which would be detectable from the ground. Three years ago, Joseph Dwyer of the Florida Institute of Technology began trying to detect such radio signals from thunderstorms, as well as using arrays of particle detectors to look for the showers of particles predicted from cosmic-ray collisions. These and other studies by Dwyer and other groups are still being conducted (literally) at the International Center for Lightning Research and Testing at the US Army base of Camp Blanding in Florida.</i><br /><br /><i>Finding avalanches</i><br /><br /><i>But meanwhile, Dwyer has teamed up with Leonid Babich and his colleagues at the Russian Federal Nuclear Center in Sarov to delve further into the theory of Gurevich’s idea. (The Russian pre-eminence in this field of the electrical physics of the atmosphere dates from the cold-war Soviet era.) They have asked whether the flux of high-energy cosmic-rays, with their accompanying runaway electron avalanches, is sufficient to boost the conductivity of air and cause a lightning strike.</i><br /><br /><i>To do that, the researchers have worked through the equations describing the chances of cosmic-ray collisions, the rate of electron production and the electric fields this induces. The equations are too complicated to be solved by hand, but a computer can crunch through the numbers. And the results don’t look good for Gurevich’s hypothesis: </i><b><i>runaway electron avalanches produced by cosmic-ray showers just don’t seem capable of producing electrical breakdown of air and lightning discharge.</i></b><br /><br /><i>However, all is not lost. As well as the particle cascades caused by collisions of high-energy cosmic rays, the atmosphere can also be electrified by the effects of cosmic rays with lower energy, which are more plentiful. When these collide with air molecules, the result is nothing like as catastrophic: they simply ionise the molecules. But a gradual build-up of such ionised particles within a thundercloud could, according to these calculations, eventually produce a strong enough electrical field to permit a lightning discharge.</i><br /><br /><i>That possibility has yet to be investigated in detail. But Dwyer and colleagues think that it leaves an avenue still open for cosmic rays to lie at the origin of lightning bolts.&quot;</i><br /><br /><b>Modeling the Earth as a Closed System</b><br /><br /><a href=\"http://www.scienceclarified.com/everyday/Real-Life-Biology-Vol-3-Earth-Science-Vol-1/Earth-Systems-How-it-works.html\" class=\"ot-anchor\">http://www.scienceclarified.com/everyday/Real-Life-Biology-Vol-3-Earth-Science-Vol-1/Earth-Systems-How-it-works.html</a><br /><br /><i>&quot;An isolated system is one so completely sealed off from its environment that neither matter nor energy passes through its boundaries. This is an imaginary construct, however, an idea rather than a reality, because it is impossible to create a situation in which no energy is exchanged between the system and the environment. Under the right conditions it is perhaps conceivable that matter could be sealed out so completely that not even an atom could pass through a barrier, but some transfer of energy is inevitable. The reason is that electromagnetic energy, such as that emitted by the Sun, requires no material medium in which to travel.</i><br /><br /><b><i>In contrast to an isolated system is a closed system, of which Earth is an approximation. Despite its name, a closed system permits the exchange of energy with the environment but does not allow matter to pass back and forth between the external environment and the system. Thus, Earth absorbs electromagnetic energy, radiated from the Sun, yet very little matter enters or departs Earth&#39;s system.</i></b><i> Note that Earth is an approximation of a closed system: actually, some matter does pass from space into the atmosphere and vice versa. The planet loses traces of hydrogen in the extremities of its upper atmosphere, while meteorites and other forms of matter from space may reach Earth&#39;s surface.</i><br /><br /><b><i>Earth more closely resembles a closed system than it does an open one -- that is, a system that allows the full and free exchange of both matter and energy with its environment.&quot;</i></b><br /><br /><b>Increased Solar Wind Activity Correlates with Frequency of Strikes</b><br /><br /><a href=\"http://www.bbc.com/news/science-environment-27406358\" class=\"ot-anchor\">http://www.bbc.com/news/science-environment-27406358</a><br /><br />Solar wind &#39;triggers lightning on Earth&#39;<br />By Rebecca Morelle<br /><br />Science correspondent, BBC News<br />15 May 2014<br /><br /><b><i>&quot;Activity on the Sun is sparking lightning strikes here on Earth, a study suggests.</i></b><br /><br /><b><i>Scientists have found that when gusts of high-speed solar particles enter our atmosphere, the number of lightning bolts increases.</i></b><br /><br /><i>The research is published in the journal Environmental Research Letters.</i><br /><br /><i>[...]</i><br /><br /><i>As the Sun rotates, the fiery ball of plasma hurls out charged particles that travel at between 400-800km a second (900,000 to 1.8 million miles per hour).</i><br /><br /><i>The arrival of these solar winds in the atmosphere can trigger displays of the Northern Lights, but this research shows how they could influence our weather too.</i><br /><br /><i>&#39;The solar wind is not continuous, it has slow and fast streams. Because the Sun rotates, these streams can be sent out behind each other - so if you have a fast solar wind catching up with a slow solar wind, it causes a concentration to occur,&#39; said Dr Scott.</i><br /><br /><b><i>The scientists found that when the speed and intensity of the solar winds increased, so too did the rate of lightning strikes.</i></b><br /><br /><b><i>The team said the turbulent weather lasted for more than a month after the particles hit the Earth.</i></b><br /><br /><b><i>Using data from northern Europe, the researchers found there was an average of 422 lightning strikes in the 40 days after the high-speed solar wind arrived, compared with 321 strikes in the 40 days prior.</i></b><br /><br /><b><i>The finding was surprising, said Dr Scott, because it had been thought that an increase in the solar wind would have the opposite effect.</i></b><br /><br /><i>He explained: &#39;It&#39;s unexpected, because these streams of particles bring with them an enhanced magnetic field - and this shields Earth from the very high-energy cosmic rays from outside of the Solar System - these are generated when supernovae explode, and they accelerate particles up to the speed of light.&#39;</i><br /><br /><b><i>Previous research has shown that cosmic rays from space can boost the rate of lightning</i></b><i>, and it had been thought that an increased shielding effect from the solar particles would cause a decrease in the number of strikes.</i><br /><br /><i>&#39;Instead what we actually saw was a marked increase in lightning. It turns out these solar winds bring with them a slightly lower energy population of particle - and these are enhancing the lighting rate,&#39; Dr Scott said.</i><br /><br /><b><i>The team is not exactly sure of the mechanism</i></b><i>, but said that the particles may be penetrating storm clouds, making it easier for them to discharge electrical energy as a bolts of lightning.</i><br /><br /><i>[...]</i><br /><br /><i>While the data was collected in Europe, the researchers believe the effect is global.&quot;</i><br /><br /><b>We See Changes in the Van Allen Belts that Sync with Strikes</b><br /><br /><a href=\"http://www.nasa.gov/home/hqnews/2005/mar/HQ_05070_radiation_belt.html#.VDtrQL789ho\" class=\"ot-anchor\">http://www.nasa.gov/home/hqnews/2005/mar/HQ_05070_radiation_belt.html#.VDtrQL789ho</a><br /><br />March 8, 2005<br /><br />NASA Finds Lightning Clears Safe Zone in Earth&#39;s Radiation Belt<br /><br /><b><i>&quot;Lightning in clouds, only a few miles above the ground, clears a safe zone in the radiation belts thousands of miles above the Earth, according to NASA-funded researchers. The unexpected result resolves a forty-year-old debate as to how the safe zone is formed, and it illuminates how the region is cleared after it is filled with radiation during magnetic storms.</i></b><br /><br /><i>The safe zone, called the Van Allen Belt slot, is a potential haven offering reduced radiation dosages for satellites that require Middle Earth Orbits (MEOs). The research may eventually be applied to remove radiation belts around the Earth and other worlds, reducing the hazards of the space environment.</i><br /><br /><i>&#39;The multi-billion-dollar Global Positioning System satellites skirt the edge of the safe zone,&#39; said Dr. James Green of NASA&#39;s Goddard Space Flight Center, Greenbelt, Md. He is the lead author of the paper about the research published in the Journal of Geophysical Research. &#39;Without the cleansing effect from lightning, there would be just one big radiation belt, with no easily accessible place to put satellites,&#39; he said.</i><br /><br /><i>If the Van Allen radiation belts were visible from space, they would resemble a pair of donuts around the Earth, one inside the other, with the planet in the hole of the innermost. The Van Allen Belt slot would appear as a space between the inner and outer donut. The belts are comprised of high-speed electrically charged particles (electrons and atomic nuclei) trapped in the Earth&#39;s magnetic field. The Earth&#39;s magnetic field has invisible lines of magnetic force emerging from the South Polar Region, out into space and back into the North Polar Region. Because the radiation belt particles are electrically charged, they respond to magnetic forces. The particles spiral around the Earth&#39;s magnetic field lines, bouncing from pole to pole where the planet&#39;s magnetic field is concentrated.</i><br /><br /><b><i>Scientists debated two theories to explain how the safe zone was cleared. The prominent theory stated radio waves from space, generated by turbulence in the zone, cleared it. An alternate theory, confirmed by this research, stated radio waves generated by lightning were responsible. &#39;We were fascinated to discover evidence that strongly supported the lightning theory, because we usually think about how the space environment affects the Earth, not the reverse,&#39; Green said.&quot;</i></b><br /><br />Note what is missing from the two suggested options: A direct electrical connection.<br /><br /><i>&quot;The flash we see from lightning is just part of the total radiation it produces. Lightning also generates radio waves. In the same way visible light is bent by a prism, these radio waves are bent by electrically charged gas trapped in the Earth&#39;s magnetic field. That causes the waves to flow out into space along the Earth&#39;s magnetic field lines.</i><br /><br /><b><i>According to the lightning theory, radio waves clear the safe zone by interacting with the radiation belt particles, removing a little of their energy and changing their direction. This lowers the mirror point, the place above the polar regions where the particles bounce. Eventually, the mirror point becomes so low; it is in the Earth&#39;s atmosphere. When this happens, the radiation belt particles can no longer bounce back into space, because they collide with atmospheric particles and dissipate their energy.</i></b><br /><br /><i>To confirm the theory, the team used a global map of lightning activity made with the Micro Lab 1 spacecraft. They used radio wave data from the Radio Plasma Imager on the Imager for Magnetopause to Aurora Global Exploration (IMAGE) spacecraft, combined with archival data from the Dynamics Explorer spacecraft. IMAGE and Dynamics Explorer showed the radio wave activity in the safe zone closely followed terrestrial lightning patterns observed by Micro Lab 1.</i><br /><br /><i>According to the team, there would not be a correlation if the radio waves came from space instead of Earth. They concluded </i><b><i>when magnetic storms, caused by violent solar activity, inject a new supply of high-speed particles into the safe zone, lightning clears them away in a few days.</i></b><br /><br /><i>Engineers may eventually design spacecraft to generate radio waves at the correct frequency and location to clear radiation belts around other planets. This could be useful for human exploration of interesting bodies like Jupiter&#39;s moon Europa, which orbits within the giant planet&#39;s intense radiation belt.&quot;</i><br /><br />If one goes to the papers which are used to justify the connection between solar wind activity and lightning strikes, it is immediately noticed that the claims are based upon the observation of plasma-generated &quot;hiss&quot; ...<br /><br />Like this paper: <b>&quot;The unexpected origin of plasmaspheric hiss from discrete chorus emissions&quot;</b><br /><br />(at <a href=\"http://www.nature.com/nature/journal/v452/n7183/full/nature06741.html\" class=\"ot-anchor\">http://www.nature.com/nature/journal/v452/n7183/full/nature06741.html</a>)<br /><br />What the researchers might be measuring is the hiss of plasma double layers -- a laboratory plasma physics concept which planetary scientists don&#39;t commonly recognize as a tool for inference.  Double layers in the laboratory are known to broadcast radio noise over a wide range of frequencies as a reaction to electrodynamic activity -- things like ... electric currents.<br /><br />Just sayin.<br /><br />This is clearly another controversy which should be mapped out and tracked over time: The notion that the Earth is an electrically &quot;closed&quot; system may have a general, vague truth to it ... But is it possible that the difference between that ideal and reality is the energy required to make all the Earth&#39;s lightning bolts?<br /><br />The answer will likely appear if we follow the issue.<br /><br />On to the next controversy ...<br /><br />(The conventional explanation for lightning in the graphic comes from ...<br /><br /><a href=\"http://www.slideshare.net/vmelachervu/unleashing-the-power-of-lightning-v16\" class=\"ot-anchor\">http://www.slideshare.net/vmelachervu/unleashing-the-power-of-lightning-v16</a>)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Bertrand Russell's 10 Commandments of Teaching","summary":"Bertrand Russell's 10 Commandments of Teaching","image":"https://lh3.googleusercontent.com/-54xSb_aR4d4/WA7U2F7uRNI/AAAAAAAAJBM/6EXUuD--LUINiTUao5e1cBi1HAjsEGMrQCJoC/w3600-h5040/bertrand-russells-10-commandments-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/B8zyvrzJHEi","publishDate":"2016-10-25T03:44:45.877Z","updateDate":"2017-01-03T01:53:15.644Z","text":"<b>Bertrand Russell&#39;s 10 Commandments of Teaching</b><br /><br /><i>&quot;The Ten Commandments that, as a teacher, I should wish to promulgate, might be set forth as follows:</i><br /><br /><i>1. Do not feel absolutely certain of anything.</i><br /><br /><i>2. Do not think it worthwhile to proceed by concealing evidence, for the evidence is sure to come to light.</i><br /><br /><i>3. Never try to discourage thinking for you are sure to succeed.</i><br /><br /><i>4. When you meet with opposition, even if it should be from your husband or your children, endeavor to overcome it by argument and not by authority, for a victory dependent upon authority is unreal and illusory.</i><br /><br /><i>5. Have no respect for the authority of others, for there are always contrary authorities to be found.</i><br /><br /><i>6. Do not use power to suppress opinions you think pernicious, for if you do the opinions will suppress you.</i><br /><br /><i>7. Do not fear to be eccentric in opinion, for every opinion now accepted was once eccentric.</i><br /><br /><i>8. Find more pleasure in intelligent dissent than in passive agreement, for, if you value intelligence as you should, the former implies a deeper agreement than the latter.</i><br /><br /><i>9. Be scrupulously truthful, even if the truth is inconvenient, for it is more inconvenient when you try to conceal it.</i><br /><br /><i>10. Do not feel envious of the happiness of those who live in a fool’s paradise, for only a fool will think that it is happiness.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"Halton Arp, the Modern Galileo","summary":"He Was a Professional Astronomer Who Began his Career as Edwin Hubble's Assistant / While Compiling a List of Peculiar Galaxies, Arp Discovered that High-Redshift Quasars are Commonly Associated with or Even Connected by Filaments to Lower-Redshift Galaxies / Since the Big Bang Requires that Differences in Redshift Place the Objects at Different Locations, Astronomers Commonly Reject Arp's Claims / But if he is Right, then there Was No Big Bang","image":"https://lh3.googleusercontent.com/-UJsVVpygCpg/WA2XbtJflgI/AAAAAAAAJAU/M0vr_EK-krkPjiWqudBnGA1T3loMC6TSgCJoC/w7142-h9999/halton-arp-the-modern-galileo-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/DCDKHXnrdoH","publishDate":"2016-10-24T05:09:14.318Z","updateDate":"2017-01-03T01:54:09.866Z","text":"<b>Halton Arp, the Modern Galileo: He Was a Professional Astronomer Who Began his Career as Edwin Hubble&#39;s Assistant / While Compiling a List of Peculiar Galaxies, Arp Discovered that High-Redshift Quasars are Commonly Associated with or Even Connected by Filaments to Lower-Redshift Galaxies / Since the Big Bang Requires that Differences in Redshift Place the Objects at Different Locations, Astronomers Commonly Reject Arp&#39;s Claims / But if he is Right, then there Was No Big Bang</b><br /><br /><i>The Electric Sky</i><br />Chapter 17: Redshift and the Big Bang<br />by Don Scott<br /><br /><b><i>&quot;Dr. Halton C. Arp is a professional astronomer who began his career as Edwin Hubble&#39;s assistant.</i></b><i> Arp has earned the Helen B. Warner Prize, the Newcomb Cleveland award, and the Alexander von Humboldt Senior Scientist Award. For years he worked at the Mt. Palomar and Mt. Wilson observatories. </i><b><i>While there, he assembled his well-known Catalog of Peculiar Galaxies -- galaxies that are misshapen or irregular in appearance.</i></b><br /><br /><b><i>While compiling his catalog, Arp discovered that many pairs of quasars (&#39;quasi-stellar objects,&#39; or QSOs), which have enormously high redshift values, are physically associated with galaxies that have low redshift values.</i></b><br /><br /><b><i>These quasars are symmetrically located on either side of, and usually near the spin axis of, what he suggests are their parent low-redshift galaxies. According to astronomical doctrine, the high-redshift quasars must be located at the far edges of the universe, and the low-redshift galaxies must be relatively close by.</i></b><br /><br /><i>The astronomical establishment reflexively explains away Arp&#39;s observations of connected galaxies and quasars as being </i><b><i>&#39;lensing illusions&#39;</i></b><i> [158] or </i><b><i>&#39;coincidences of apparent location.&#39;</i></b><i> But the large number of physically closely associated quasars and low-redshift galaxies that he has photographed and cataloged defies that evasion. </i><b><i>It happens much too often to be a chance association.</i></b><br /><br /><i>Arp has also photographed actual physical connections between low-redshift galaxies and higher-redshift companion galaxies. If two objects are connected, they cannot be cosmically distant from one another. It is just that simple. </i><b><i>Arp&#39;s photographs disprove both the &#39;redshift equals distance&#39; and &#39;redshift equals recessional velocity&#39; assumptions.</i></b><br /><br /><i>Researcher Amy Acheson pointed out:</i><br /><br /><i>&#39;Halton Arp&#39;s observations show that the assumption that high redshift implies velocity never really was valid. </i><b><i>Arp shows, one galaxy grouping at a time, that the smaller and fainter (high-redshift) galaxies really are smaller and fainter, but not necessarily farther away.</i></b><i> They are connected to, interacting with, and possibly ejected from the larger and brighter (low-redshift) galaxies. The expanding universe theory fails because higher redshift isn&#39;t only a measure of velocity. </i><b><i>High redshift is an intrinsic property of small faint (recently born) galaxies. As the galaxy grows and evolves, the redshift changes.</i></b><i> The light from high-redshift faint galaxies has traveled the same distance as the light from their low-redshift parent galaxies.&#39;</i><br /><br /><i>Because of Arp&#39;s photos, the assumption that high-redshift objects have to be far away -- which motivates the Big Bang theory and all of presently accepted expanding-universe cosmology -- is demonstrated to be wrong. It is a paradigm-shattering result.</i><br /><br /><i>A prime example of Arp &#39;s evidence is the connected pair of objects known as galaxy NGC 4319 and quasar Markarian 205. These two objects have vastly different redshift values. [159] </i><b><i>Professional astronomers were quick to say that they &#39;cannot see any physical connection&#39; between these two objects.</i></b><i> They have at their disposal the Hubble Space Telescope, the Chandra x-ray orbiting observatory, and all of the big scopes of Kitt Peak, Hawaii, and Chile. But they cannot see the connection that Arp saw. </i><b><i>Lately they have admitted that there may be an &#39;apparent&#39; connection, but that it is only a &#39;mirage.&#39;</i></b><br /><br /><i>Of course, if there actually is a physical connection, then the Big Bang Theory is in trouble.</i><br /><br /><i>On April 4, 2002, amateur astronomer John Smith of Oro Valley, AZ, obtained an image of the two objects mentioned above: NGC 4319 and Markarian 205 (Mrk 205). I then quantized the light levels in that image to show isophote (similar brightness) contours. This result is shown in figure 64.</i><br /><br /><i>In that image, Mrk 205 is the small appendage below the main galaxy. It is clearly connected to the main body of NGC 4319.</i><br /><br /><i>In processing this image, I used level quantization (staircase gray curves in the Picture Window Pro 3.1 software package) followed by the &#39;Edge tool.&#39; [160] Notice that, of all the small objects surrounding the main galaxy, only Mrk 205&#39;s shape is stretched back toward NGC 4319. None of the other objects in close proximity to 4319 is extended in this manner.</i><br /><br /><i>On October 7, 2002, the Astronomy Picture of the Day (APOD) published an HST161 image of these same objects. The orientation is different, but the connection is again clearly visible. After processing the HST image exactly as was done with the first amateur image, I obtained the image shown in figure 65. It is obvious the larger galaxy (NGC 4319) in these images is physically connected to its small companion quasar. Notice, in figure 65, the three small clumps of matter situated along the line connecting the center of Mrk 205 to the larger galaxy.</i><br /><br /><i>The official explanation of the NASA image states:</i><br /><br /><b><i>&#39;Appearances can be deceiving. In this NASA Hubble Space Telescope image, an odd celestial duo, the spiral galaxy NGC 4319 [center] and a quasar called Markarian 205 [upper right], appear to be neighbors. In reality, the two objects don&#39;t even live in the same city. They are separated by time and space. NGC 4319 is 80 million light-years from Earth. Markarian 205 is more than 14 times farther away, residing 1 billion light-years from Earth. The apparent close alignment of Mrk 205 and NGC 4319 is simply a matter of chance.&#39;</i></b><br /><br /><i>Establishment astronomers deny that there is any physical connection between this pair and assert that the obvious bridge between the two objects &#39;must be&#39; an illusion. But this &#39;must be&#39; only arises from their predilection to believe in a cherished theory despite empirical evidence. When a pair of amateur astronomers such as John and myself are able to get and process images that the professionals &#39;cannot see,&#39; one has to wonder whether those experts are just being stubborn in defense of their assumptions or are in a psychological state of total denial. </i><b><i>Interestingly, the HST image of NGC 4319 and Mrk 205 was obtained under the amateur HST request program that NASA had been supporting for a long time prior to when this image was downloaded. Very soon afterward, the request program was terminated.</i></b><br /><br /><i>In his first book [162] Arp says that a favorite practice of his detractors is to look for absorption lines in the spectrum of the low-redshift (supposedly closer) object that correspond to emission lines in the quasar. This, they say, proves that the quasar is behind the low redshift galaxy. But when such absorption lines are not found, this could just as well be interpreted as proof that the quasar is in front of the galaxy. </i><b><i>In the case of NGC 4319 and Mrk 205, no such announcement was made. Arp knew they had looked for the absorption lines, spending a great deal of time with the Hale 200-inch scope on Mt. Palomar doing so. At an open meeting of astronomers, he eventually asked about what had been found. Arp says in his book, &#39;They replied that they had found no absorption. What seemed to me quite devastating was that they had to publicly admit that they had not published information gained from those enormous amounts of large telescope time, that they had withheld this important scientific information, apparently because it did not agree with the position to which they were committed.&#39;</i></b><br /><br /><i>Recently it has been announced that absorption spectra have indeed been observed in NGC 4319 at the redshifted emission lines in Mrk 205. Arp&#39;s opponents say this proves that Mrk 205 is at an enormous &#39;cosmological distance&#39; from Earth. But the observed lines are at an intensity of only about 10% of the absorption lines seen in control galaxy tests. </i><b><i>Arp claims that this observation simply proves that Mrk 205 has not yet completely emerged from the disk of its ejecting parent, so we are seeing about 10% of that parent galaxy still in front of the quasar.</i></b><br /><br /><i>If this were the only piece of evidence, one might conclude that it was insufficient, on its own, to justify throwing out the Big Bang Theory. But Arp has more -- much more.</i><br /><br /><i>Another example of two or more connected objects that have different redshift values is galaxy NGC 7603 and the quasar that is linked to it by a bridge of luminous matter. If redshift really implies distance, then the bright companion quasar (Object 1) must be twice as far away as the main galaxy. However, in figure 66, it is obvious that the two are physically connected.</i><br /><br /><i>Furthermore two additional quasars have been discovered [163] inside the bridge of matter connecting Object 1 to NGC 7603. These are called Object 2 and Object 3 in the figure.</i><br /><br /><i>These two newly discovered objects have redshift values that establishment astronomers claim should put them </i><b><i>ten or more times farther away</i></b><i> than NGC 7603. </i><b><i>Yet all four objects are obviously in close proximity to each other. They are connected to each other. And they lie within the tail of matter leading off from the parent low redshift galaxy.</i></b><br /><br /><i>The discoverers of the new quasars have said:</i><br /><br /><i>&#39;We present new spectroscopic observations of an old case of anomalous redshift -- NGC 7603 and its companion [object 1]. The redshifts of the two galaxies which are apparently connected by a luminous filament are z=0.029 and z=0.057 respectively. We show that in the luminous filament there are two compact emission line objects with z=0.243 and z=0.391. </i><b><i>They lie exactly on the line traced by the filament connecting the galaxies. As far as we are aware, this is the most impressive case of a system of anomalous redshifts discovered so far.&#39;</i></b><br /><br /><i>Eric Flesch [164] of Wellington, New Zealand, has commented on the discovery as follows:</i><br /><br /><i>&#39;NGC 7603 has been described as &#39;the most impressive case of a system of anomalous redshifts discovered so far.&#39; The configuration and redshifts are: two galaxies of different redshifts, connected by a luminous bridge, and where the bridge connects to each galaxy -- precisely exactly at each of the two points -- there is a higher redshift object. </i><b><i>By virtue of the luminous bridge and the perfect positions of the two faint objects, it is unquestionable that these four objects are physically linked and sharing the same space. The point is that any scientific claim must be falsifiable. To hold that redshift is an absolute indicator of distance must be to allow standards of evidence that will overturn that position. NGC 7603 is evidence which meets that standard, and if it is not sufficient to overturn the standard model all on its own, it is strong enough to require addressing by the proponents of the standard model.&#39;</i></b><br /><br /><i>But again an important point is that these cases are not alone. </i><b><i>Arp and his associates have collected hundreds of similar close associations of highredshift quasars and low-redshift galaxies.</i></b><br /><br /><i>Arp has commented [165] on how a similar case was received by the professional astronomical community:</i><br /><br /><i>&#39;A reasonable response would be to notice such a case and say, &#39;If I see a few more cases like this I will have to believe it is real&#39; Most astronomers say, </i><b><i>&#39;This violates proven physics [i.e., their assumptions] and therefore must be invalid. After all, no matter how improbable, it is only one case.&#39; Then, when they see another case, they treat it de novo and reject it with the same argument.</i></b><i> Professional scientists, however, have a responsibility to know about previous cases. And they do. When they block them out, it is a clear case of falsifying data for personal advantage -- a violation of the primary ethic of science.&#39;</i><br /><br /><i>Arp applied to have the orbiting Chandra X-ray observatory look at these two new quasars. He announced in a paper [166] that he published in June 2004, &#39;It must be noted, however, that an application to make an observation of NGC 7603 with Chandra was turned down; as a consequence we do not know if the two HII galaxies in the bridge are X-ray sources nor do we know what else this active Seyfert [167] contains.&#39;</i><br /><br /><i>In Quasars, Redshifts, and Controversies, [168] Dr. Arp discusses the five interacting galaxies, NGC 7317, 7318A, 7318B, 7319, and 7320, that constitute what is known as &#39;Stephan&#39;s Quintet.&#39; The last one, NGC 7320, has a redshift value of 800 km/sec. The other four have redshifts of either 5700 km/sec or 6700 km/sec. [169]</i><br /><br /><i>Astronomers claim those last four are seven to eight times farther away from us than is NGC 7320. Therefore, they say, there cannot be any interaction between 7320 and the others.</i><br /><br /><i>Dr. Arp states in his book, &#39;The deepest 200 inch (Mt. Palomar) plates that I have been able to obtain clearly show a &#39;tail&#39; coming out of the southeast end of NGC 7320.</i><br /><br /><i>&#39;A tail like this from NGC 7320 ... must be an interaction tail -- which could arise only from physical interaction with the adjacent high-redshift members of the Quintet.&#39;</i><br /><br /><i>He then comments that at least one amateur has been able to see the tail but &#39;it is amazing that so many professionals have difficulty seeing it.&#39; </i><b><i>NASA routinely crops their images of Stephan&#39;s Quintet to exclude the area where this tail is located.</i></b><i> [170] Those familiar with the Galileo story will remember the priests who refused to look through his telescope at the night sky.</i><br /><br /><i>Amateur astronomer John Smith has taken a deep (long exposure), full, un-cropped image of the Quintet. This image is shown in figure 67. The large galaxy on the left side of the group is the low redshift (and therefore supposedly nearby) NGC 7320. Proceeding counter-clockwise from the lower right, we have NGC 7317, 7318A, 7318B, and 7319. The small galaxy at the top left is NGC 7320C. All the other objects are stars located within our Milky Way galaxy.</i><br /><br /><i>This un-cropped image was then inverted, and put through a process similar to the one described in the discussion of the HST NGC 4319-Markarian 205 image. The result is shown in figure 68.</i><br /><br /><i>It is apparent that an interaction &#39;tail&#39; does indeed extend out from NGC 7320 toward the left (East). [171] In fact it appears to curve up toward and connect with the small, high redshift galaxy, NGC 7320C. Once again Arp has visual evidence of close interaction between objects having vastly different redshift values.</i><br /><br /><i>And again -- if amateur astronomers can obtain and perform simple digital processing on an image such as this, then why, with the multimillion dollar facilities of NASA, Cal Tech, and Kitt Peak, can the professionals not do the same? The answer is that they can, but it is not in their self-interest to do so, because the results disprove their long accepted theories and endanger the continuation of their research-funding stream.</i><br /><br /><i>Let us consider yet another example of pairs of linked objects that have vastly different redshift values. NGC 1232 [172] is also known as Arp 41. It is a magnitude 9.9 spiral galaxy with a small secondary spiral on the end of one of its arms. The small companion, NGC 1232A, has a redshift four times that of the large galaxy. [173] Under accepted theory, this requires the small companion to be over four times farther away from us than the large galaxy. Yet it is obviously positioned on the end of one ofthe main galaxy&#39;s arms.</i><br /><br /><i>If that were not enough, yet another small offspring galaxy has recently been discovered within the same arm of NGC 1232 that leads out to NGC 1232A. This new one has an extremely high redshift value. Arp states, </i><b><i>&#39;The most compelling argument that the [new] object was at the distance of NGC 1232 was simply that one does not see background galaxies through the disk of a spiral galaxy.&#39;</i></b><br /><br /><i>Arp goes on to say, </i><b><i>&#39;This evidence, even though of the most detailed and quantitative kind, has always been simply ignored.&#39; It appears that, when one discordant redshift relationship is discovered, close examination of the immediate area often reveals one or more others.</i></b><br /><br /><b><i>Until recently, many of the quasars that Arp has found to be associated with nearby low-redshift galaxies have been in close proximity to those parent galaxies. He has presented statistical arguments to demonstrate that such close positioning could not arise from a uniform distribution of quasars over the sky. Mainstream defenders of the Big Bang throw up all sorts of probabilistic arguments against Arp&#39;s connected objects.</i></b><br /><br /><i>One might think, however, that if anyone were ever to find a quasar directly in front of a bright, low-redshift galaxy, then that would clearly constitute absolute falsification of the notion that these high-redshift objects must be at their extremely remote &#39;cosmological&#39; distances (as dictated by the redshift equals distance assumrtion).</i><br /><br /><b><i>Dr. Arp and his associates [174] have discovered just such a quasar (Figure 70a). It lies directly between Earth and galaxy NGC 7319 in Stephan&#39;s Quintet. It is indeed in front of its parent galaxy and is connected to it. The redshift of the quasar is 93.7 times that of the NGC 7319 galaxy that lies immediately behind it. But in accepted astronomy circles, this would mean the quasar is over 93 times farther away from us than is the big galaxy. Arp et al. show in their paper that the quasar is interacting with the material in front of the galaxy. There is a jet of matter reaching out from the center of the galaxy toward the quasar.</i></b><br /><br /><b><i>The centers of Seyfert galaxies like NGC 7319 are opaque -- we cannot see through them. We cannot even see into the cores of such galaxies let alone what is behind them. Thus, this quasar found superimposed on the face of this nearby low-redshift Seyfert galaxy obviously has to be closer to us than the galaxy. This discovery should force the final and absolute demise of the main assumptions that led to the Big Bang theory -- the assumptions that high redshift implies great distance and recessional velocity.</i></b><br /><br /><b><i>Arp and his colleagues submitted a paper for publication announcing the discovery. For many months, the paper was &#39;discussed and re-reviewed&#39; -- to the extent that one can legitimately use the word &#39;stonewalled&#39; -- by editors of prestigious astronomical journals. However, the image shown here received wide circulation on the Internet, where it was viewed by an increasingly vocal number of amateur astronomers and other interested viewers. In my opinion this widespread dissemination forced the Astrophysical Journal finally to publish it (2/10/05).</i></b><br /><br /><i>I asked one of Arp&#39; s detractors about this picture, and he continued to put forward probabilistic doubts that we are seeing what we are obviously seeing. He said that even this picture was not proof the quasar was in front of the galaxy. </i><b><i>Another astronomer opined that there &#39;must be&#39; a hole in the galaxy at just the right position so that we can see the quasar through it at its far &#39;cosmological distance.&#39;</i></b><br /><br /><b><i>As of July 2006, several such images [175] of QSOs in front of the disks of low-redshift galaxies have been published.</i></b><br /><br /><i>Dr. Arp believes that quasars are ejected with large intrinsic redshift values from their parent galaxies. As they move away, their redshift decreases. Often, when the intrinsic redshift evolves down to a certain value, the quasar starts to look like a small galaxy or BL Lac object [176] and begins to fall back, with still decreasing redshift values, toward its parent. Any additional redshift (over and above its intrinsic value) is indeed indicative of the object&#39;s velocity. But the intrinsic component is an indication of the object&#39;s youth and usually makes up the larger fraction of the object&#39;s total redshift. [177]</i><br /><br /><b><i>Other plasma scientists, such as Dr. Anthony Peratt, dispute the &#39;ejection&#39; claim. They think it is more likely that the quasars are &#39;pinches&#39; in the Birkeland current streaming along the galaxy&#39;s spin axis. However, they all agree with Arp that the quasars are physically associated with their parent galaxies.</i></b><i> All agree that the occurrence of the Big Bang is now extremely doubtful.</i><br /><br /><b><i>In addition to the above evidence, the intrinsic redshift z values of quasars seem to be quantized. [178] That is to say, those calculated values are tightly grouped around a series of discrete values.</i></b><br /><br /><i>The existence of such quantization is sufficient proof of the failure of the idea that redshift is only an indicator of recessional speed (and therefore distance). </i><b><i>This quantization means (under the redshift equals distance interpretation) that quasars must lie in several concentric shells, with Earth at the center of the entire arrangement.</i></b><i> Copernicus discovered a long time ago that Earth isn&#39;t at the center of anything.</i><br /><br /><b><i>Recently a group of astronomers, [179] having analyzed a large number of observed high redshift values, announced that they could find no such quantization effect. But the raw (observed) redshift value is not what is quantized. It is the quasars&#39; intrinsic redshift z values [180] that are.</i></b><i> The raw values can be smeared out away from the quantization peaks by two different Doppler velocity effects:</i><br /><br /><i>1. The system&#39;s motion - the family (parent galaxy and its offspring quasars) is moving as a group, either away from us or toward us.</i><br /><br /><i>2. The velocity components of the individual members of each quasar pair. [181] Due to their ejection from the parent galaxy, one is moving toward us, and the other is moving away.</i><br /><br /><i>In his new book [182], Arp shows that, </i><b><i>when the observed z values are corrected for these two Doppler motions, the resulting intrinsic redshift values occur only on the quantization peaks with almost no dispersion.</i></b><i> One must first identify which quasars are associated with each parent galaxy and then transform their observed redshift values into the parent galaxy&#39;s reference frame.</i><br /><br /><i>Figure 71 presents an image taken by the Chandra orbiting x-ray telescope of galaxy M82. (Also see figure 59.) This image shows another example of the quasar ejection phenomenon.</i><br /><br /><i>The official caption 183 says:</i><br /><br /><i>&#39;Chandra Images Seething Cauldron of Starburst Galaxy. Chandra x-ray image of M82, at a distance of 11 million light years from Earth, is the nearest starburst galaxy. Massive stars are forming and expiring in M82 at a rate ten times higher than in our galaxy. The bright spots in the center are supernova remnants and x-ray binaries. These are some of the brightest such objects known. The luminosity of the x-ray binaries suggests that most contain a black hole. The diffuse x-ray light in the image extends over several thousand light years, and is caused by multimillion-degree gas flowing out of M82. A close encounter with a large galaxy, M81, in the last 100 million years is thought to be the cause of the starburst activity.&#39;&#39; [Emphasis added.]</i><br /><br /><i>&#39;Hot gas&#39; must be extremely hot to radiate X-rays. Even orthodox astronomers will admit that all the atoms in such a hot gas are ionized and that the &#39;gas&#39; is actually a plasma. But the electrical properties of a plasma can cause it to emit x-rays at much lower temperatures. Because astronomers are unaware of such properties, they must call upon the much weaker properties of gravity to supply the energy. Mathematical extrapolation indicates that the necessary energy would be available if matter were concentrated into a &#39;black-hole.&#39; Although no black hole has ever been observed, the presence of one is often inferred and then announced as fact -- as it is here.</i><br /><br /><i>In the image shown in figure 71, a string of at least five high-intensity, young, x-ray-emitting plasmoids (quasars?) extends along the spin axis of M82. There is a high level of plasma activity. High-amplitude electric currents undoubtedly are producing large pinch forces that create these objects. No black-hole magic is needed.</i><br /><br /><i>If there is a black hole in the middle of each of these objects, why is matter pouring out rather than in? There is an old saying: &#39;When you hear hoof beats, do not automatically assume the presence of zebras.&#39; </i><b><i>When there is a powerful ejection of energy and material, it is not logical to immediately postulate the presence of a black hole that supposedly sucks everything in</i></b><i> [184].</i><br /><br /><i>There are many exciting images taken by the Hubble space telescope of phenomena astronomers attribute to galactic collisions. Figure 72 is an example. The typical official commentary is as follows:</i><br /><br /><i>&#39;Hubble astronomers conducting research on a class of galaxies called ultra-luminous infrared galaxies (ULIRGs) have discovered that over two dozen of these are found within &#39;nests&#39; of galaxies, apparently engaged in multiple collisions that lead to fiery pile-ups of three, four or even five galaxies smashing together.&#39;&#39; [Emphasis. added.]</i><br /><br /><i>In view of Arp&#39;s observations, what we are most likely seeing is the riotous birth of galaxies and quasars, not their deaths. Instead of witnessing collisions, we are looking at the separation of parent and offspring.</i><br /><br /><i>Hubble astronomers seem to have forgotten how empty &#39;gravitational space&#39; is: Remember the dust motes that are miles apart: </i><b><i>Random collisions in such vacuous spaces are extremely unlikely. And if the universe were really expanding, everything should be getting farther away from everything else. Collisions should be even more improbable.</i></b><br /><br /><i>Many years ago the philosopher Benedict Spinoza commented [185]:</i><br /><br /><i>&#39;It is very rare for men to relate an event simply as it happened, without adding any element of their own judgment. When they see or hear anything new, they are, unless strictly on their guard, so occupied with their own preconceived opinions that they perceive something quite different from the plain facts seen or heard ... for most people think they sufficiently understand a thing when they have ceased to wonder at it.&#39;</i><br /><br /><i>Astronomers are presently trying to explain away observations of quasar-galaxy connections with the hypothesis of &#39;gravitational lensing.&#39; [186] Below, in figure 73, are ten examples of such groupings. The only way such an optical illusion could occur is if Earth, a nearby galaxy, and a distant quasar (all three) fall precisely on a single straight line. Could this happen once?</i><br /><br /><i>Possibly. But dozens of times? Not likely. In fact, the probability is vanishingly small.</i><br /><br /><i>And if Halton Arp is correct, the quasars are not that far away in the first place.</i><br /><br /><i>These sets of objects are not illusions or mirages -- rather, they are visual proof that Arp is correct in what he says: </i><b><i>Young, high-redshift objects are being ejected in jets and arcs of plasma from the centers of active galactic nuclei (AGNs) and Seyfert galaxies.</i></b><i> The images show exactly that happening.</i><br /><br /><i>The eminent astronomer Geoffrey Burbidge, a professor of physics at the University of California at San Diego, has remarked [187] that he is not concerned by the fact that some astronomers find Arp&#39;s data unconvincing, but he objects strongly to their attempts to brush such anomalous data under the rug. His remarks are pertinent to all of science:</i><br /><br /><i>&#39;Evidence of the kind just mentioned which is favorable to the cosmological interpretations [188] of the redshifts does not negate the other evidence. It simply means that the world is a complicated place .... In articles such as that by Weedman, it is somehow considered all right to totally ignore the non-cosmological hypothesis.</i><br /><br /><b><i>The fairest way to deal with the problem is not to fall back on authority (what eminent authorities believe or don&#39;t believe) but to examine the evidence for oneself.</i></b><i> The most extensive collection of this evidence is in the book[s) by Halton C. Arp ... If, after examining the statistics yourself and understanding the evidence, you are unconvinced, so be it. Remember, if the conventional view is correct, all of these apparent juxtapositions must be accidental. Above all, do not be swayed by the views of the authorities, be they Dan Weedman, Allan Sandage, Maarten Schmidt, Chip Arp, or myself. We are fallible, too, and some of us (ask the others.) have axes to grind.&#39;</i><br /><br /><i>Our galaxy, the Milky Way, has two much-smaller companion galaxies called the Large Magellanic Cloud (LMC) and the Small Magellanic Cloud (SMC). These two objects are only readily visible to observers south of the equator. Recently a search was made for quasars &#39;behind&#39; each of these galaxies. Astronomer Tom Van Flandern, previously of the Naval Observatory brought the results [189] to the attention of an email group of which I am a member:</i><br /><br /><i>&#39;Figures ... show the distribution of quasars behind the Large and Small Magellanic Clouds. They are obviously more numerous within the cloud boundaries than outside them. However, the selection criteria are not rigorous and may be biased. The distribution is sufficiently denser close to the clusters as to suggest that many of the quasars are members of these two clusters. This of course presumes that cosmological redshift is not a valid distance indicator.&#39;</i><br /><br /><i>Amy Acheson answered Van Flandern:</i><br /><br /><i>&#39;Thanks for taking the time to send the AJ article. It&#39;s really an amazing article about the LMC. They even sneak in, especially if you look closely at the pictures, </i><b><i>quasars lined up along the minor axis of the SMC, along filaments</i></b><i>, and a histogram ... that shows quantization of redshift, exactly as expected by Chip Arp.&#39;</i><br /><br /><i>If these observations are confirmed to be quasars, they will be the nearest such objects yet discovered.</i><br /><br /><i>It is often amazing how different investigators can look at the same data and come to opposite conclusions about its meaning. Patterns (or quantizations) can be obvious to some observers and invisible to others - especially to those who do not wish to see them.</i><br /><br /><i>Consider the pattern called, &#39;The Fingers of God.&#39; The bottom diagram in figure 74 is supposedly a plot of the distances [190] to many of the galaxies that lie in a ninety-degree field of view centered on the Virgo Galaxy Cluster (whose members are shown in red). The distance measurement for each galaxy was computed by presuming that its actual distance is proportional to its redshift value.</i><br /><br /><i>What is actually plotted is redshift value vs. angular position. </i><b><i>As a result, the Virgo cluster takes on the shape of two long fingers pointed directly at Earth. These have become known as &#39;The Fingers of God.&#39;</i></b><br /><br /><i>Long cosmic-sized fingers pointed directly at Earth? This is absurd on its face. It is independent proof that the &#39;redshift equals distance&#39; assumption is false. Earth is not the center of the universe </i><b><i>(nor is it the focal point for the Virgo galaxy cluster)</i></b><i>.</i><br /><br /><i>If the lower plot in figure 74 had included larger radial &#39;distances,&#39; then the strings of red dots that represent the Virgo cluster objects would have closed as shown in the upper right plot in the figure. The plot would form an elongated ellipse. But, the high-redshift galaxies in the upper regions of this plot are not far away, they are just young -- and are at approximately the same distance from us as the low-redshift galaxies. Arp points out that the Virgo cluster is much more compact than it appears in the lower diagram. A plot of the correct distance distribution is shown in the diagram at the upper left in figure 74.</i><br /><br /><b><i>How astronomers can continue to look at this diagram and not realize that something is wrong with the &#39;redshift equals distance&#39; idea is evidence of how blinded by this assumption they have become.</i></b><i> Like the Church in Galileo&#39;s day, refusing to open its eyes to any version of the order of the cosmos but its own, today&#39;s astrophysicists have closed their eyes and minds to observations that might disprove their dogma.</i><br /><br /><i>Carl Sagan put it this way, &#39;You can&#39;t convince a believer of anything; for their belief is not based on evidence, it&#39;s based on a deep seated need to believe.&#39;</i><br /><br /><i>It is ironic to remember that Galileo got into trouble with the Church by promoting the work of Copernicus who said that Earth is not the center of anything. </i><b><i>In pointing out the erroneous nature of the Fingers of God diagram, Arp is indeed today&#39;s Galileo.</i></b><br /><br /><i>A detailed discussion of Einstein&#39;s General Relativity Theory is well beyond the scope of this book. However, for completeness, we should briefly discuss how Arp&#39;s work impinges on the assumptions that were made in solving Einstein&#39;s famous equation. That equation can be written, in its simplest form, as [191]</i><br /><br /><i>G=T,</i><br /><br /><i>where T represents the energy and momentum of a system of particles and G describes the geometry (e.g., curvature of space-time) of the space in which the particles reside. Note carefully that this postulated relationship does not explicitly include any electrical or magnetic phenomena.</i><br /><br /><i>In 1922 a Russian mathematician, Alexander Friedmann, derived a solution to the general form of this equation, which yielded the now famous result that the spatial separations of the particles in the cosmos have to increase with time. This was the genesis of the notion that space itself was expanding. The very coordinate system in which we measure the positions of objects in the universe was blowing up. Redshift could be decoupled from recessional velocity, distance could be made &#39;stretchable,&#39; and the Big Bang could be anchored to a non-empirical mathematical structure into which any observation could be subsumed. It was overlooked that such a deduced hypothesis was not falsifiable and thus made cosmology into a pseudoscience. It was not until 1977 that mathematician Jayant Narlikar [192] pointed out that Friedmann&#39;s solution had assumed that the total mass of all the particles in the universe remained constant in time. Narlikar did not make that particular assumption and obtained a different solution -- one wherein m = at^2. This result indicates that the mass of the cosmos increases at a rate that is proportional to the square of the age of that mass. It isn&#39;t the size of the universe that is getting larger -- it is its mass.</i><br /><br /><i>Arp points out in his book Seeing Red [193] that &#39;the most useful feature of the Narlikar solution is that it explains [my] preceding book[s] full of observations. </i><b><i>If particle masses are a function of time, then younger (more recently created) electrons have smaller masses. When a less massive electron makes a transition between atomic orbits, the photon involved has a lower energy and the resulting spectral line is redshifted. The consistent lesson of the observations we have discussed is, the younger the object, the higher the intrinsic redshift.&#39;</i></b><br /><br /><i>Einstein was not necessarily wrong when he postulated his relation. The error lay in the assumptions made by others when they solved that equation. It was wrong to presume that the mass of the cosmos is constant. That single faulty assumption has led to the present disaster of curved spacetime, expanding space, Big Bang, Dark Energy, inflation theory, and a host of immeasurable, nonsense quantities -- none of which can be observed. Contrast all this to Narlikar&#39;s elegant, closed-form solution of the Einstein equation. This solution results in a conclusion that has been repeatedly verified by Arp&#39;s experimental evidence -- his observations and his images.</i><br /><br /><i>Dr. Halton Arp&#39;s contributions to observational astronomy and the way he has been treated by the majority of the astronomical community are reminiscent of the story of Plato&#39;s cave [194]. Imagine a dark, cave-like prison in which humans are bound by their necks to a single place from infancy. All they can see are shadows on the opposite wall of the cave. The real objects are beyond their field of vision. A fire behind the real objects creates the shadows. The prisoners do not recognize their imprisonment and are satisfied to live their lives in this way. Then one man gets free and struggles to where he sees the real objects. This freed prisoner now understands the shadows to be imperfect, twisted copies of the full reality now grasped in his mind. But when he is returned to the prison, the freed prisoner becomes the object of ridicule, disbelief, and hostility. His observations are rejected as being &#39;obviously flawed.&#39;</i><br /><br /><i>Any unbiased scientific examination of Arp&#39;s images will quickly lead to the conclusion that it is not his evidence and his images that are flawed. Rather, the flaw is in the established and convoluted theory at whose foundation lies a mistaken solution of Einstein&#39;s postulate, the unwarranted presumption that the total mass of the universe must be a constant and the now falsified assumption that &#39;redshift implies large distance and recessional velocity.&#39;</i><br /><br /><i>Real evidence leads us to conclude that Halton Arp is correct in his contention that redshift is caused mainly by an object&#39;s being young and only secondarily because of its velocity. Therefore, quasars are not the brightest, most distant, and most rapidly moving objects in the observed universe -- but they are among the youngest. And the Virgo galaxy cluster most certainly does not have the shape of a long &#39;Finger of God&#39; pointed directly at Earth. And the universe is not expanding.</i><br /><br /><i>We have noted that Arp&#39;s contention that quasars and BL Lac objects are ejected from active galactic nuclei is somewhat contrary to Peratt&#39;s and Alfven&#39;s theory that they are formed in place by z-pinches in cosmic Birkeland currents. But these two proposed mechanisms are not contradictory: Arp&#39;s ejection is accompanied by much plasma activity; Peratt&#39;s z-pinches occur in plasma streaming out of a galaxy along its axial current. In either case, the hypothesis is antithetical to the Big Bang Theory. Both agree that quasars are in close proximity to (and in dozens of cases connected to) their central galaxies.</i><br /><br /><i>The story of Halton Arp demonstrates the extent to which the astrophysics power structure will go in defending their theories and assumptions even when confronted with clearly falsifying evidence. Worst of all are their attempts to stifle Arp&#39;s publications.</i><br /><br /><i>What can be called &#39;The Arp Affair&#39; should have been stopped early on by heeding Carl Sagan&#39;s judgment: &#39;The suppression of uncomfortable ideas may be common in religion or in politics, but it is not the path to knowledge, and there&#39;s no place for it in the endeavor of science.&#39;&quot;</i><br /><br />FOOTNOTES:<br /><br />[158] See chapter 4.<br /><br />[159] NGC 4319: z = 0.0045 or 1357 km/s. Mrk 205: z = 0.071 or 21,237 km/s.<br /><br />[160] Digital image processing such as was used here is a standard technique. The images were treated globally -- nothing was done to one part of the image that was not done to the entire frame. The technique only emphasizes what is already there in the image -- it adds nothing that was not there originally.<br /><br />[161] Hubble Space Telescope.<br /><br />[162] Quasars, Redshifts and Controversies, Interstellar Media, Berkeley, CA., 1987 p.36.<br /><br />[163] &quot;Two emission line objects with z &gt; 0.2 in the optical filament apparently connecting the Seyfert galaxy NGC 7603 to its companion.&quot; Authors: M. Lopez Corredoira, Carlos M. Gutierrez. <a href=\"http://arxiv.org/PS_cache/astro-ph/pdf/0203/0203466.pdf\" class=\"ot-anchor\">http://arxiv.org/PS_cache/astro-ph/pdf/0203/0203466.pdf</a><br /><br />[164] <a href=\"http://quasars.org/\" class=\"ot-anchor\">http://quasars.org/</a><br /><br />[165] Seeing Red, pp 14-15.<br /><br />[166] arXiv:astro-ph/0401103, v18, Jan 2004. Astronomy &amp; Astrophysics, January 8, 2004, &quot;New optical spectra and general discussion on the nature of ULX&#39;s,&quot; H. Arp [1], C. M. Gutierrez [2], and M. Lopez Corredoira. [3]<br /><br />[1] Max-Planck-Institut f&#39;ur Astrophysik, Karl Schwarzschild-Str.1, Postfach 1317, D-85741 Garching, Germany<br /><br />[2] Instituto de Astrof&#39;isica de Canarias, E-38200 La Laguna, Tenerife, Spain<br /><br />[3] Astronomisches Institut der Universit&#39;at Basel, Venusstrass 7, CH-4102<br />Binningen, Switzerland<br /><br />[167] Seyfert Galaxies are spiral galaxies with unusually bright, tiny cores that fluctuate in brightness. Most are powerful sources of infrared radiation. Some emit intense radio, X ray, and gamma rays.<br /><br />[168] p. 96-101<br /><br />[169] NGC 7320 z = 0.0026; NGC 7317 z = 0.022; NGC 7318A z = 0.022; NGC<br />7318B z = 0.019; NGC 7319 z = 0.023; NGC 7320C z = 0.020.<br /><br />[170] See, for example, <a href=\"http://www.chandra.harvard.edu/photo/2003/stephan/index.html\" class=\"ot-anchor\">http://www.chandra.harvard.edu/photo/2003/stephan/index.html</a><br /><br />[171] Remember, if we stand facing south and look up at the night sky, east is to our left and west to our right. Most (but not all) astronomical images are published in this orientation.<br /><br />[172] Quasars, Redshifts and Controversies, H. Arp, 1987, pp 88-89.<br /><br />[173] NGC 1232 z = 0.0053; NGC 1232A z = 0.022.<br /><br />[174] &quot;The Discovery of a High Redshift X-ray Emitting QSO Very Close to the Nucleus of NGC 7319,&quot; <a href=\"http://arxiv.org/abs/astro-ph/0409215\" class=\"ot-anchor\">http://arxiv.org/abs/astro-ph/0409215</a>, Authors: Pasquale Galianni, E.M. Burbidge, H. Arp, V. Junkkarinen, G. Burbidge, Stefano Zibetti.<br /><br />[175] NGC 3320, 3963, 5248, 5656. See <a href=\"http://www.quasars.org/qso-gals/default.htm\" class=\"ot-anchor\">http://www.quasars.org/qso-gals/default.htm</a><br /><br />[176] BL Lacertae objects (BL Lac for short) exhibit no emission lines but have a strong continuum stretching from radio frequencies through X-ray frequencies.<br /><br />[177] See Appendix B for the details of the calculation.<br /><br />[178] QSO redshifts cluster at z = 0.061, 0.30, 0.60, 0.91, 1.41, 1.96, ... such that (1 + z2)/(1 + z1) = 1.23.<br /><br />[179] Hawkins, Maddox, and Merrifield (M.N.R.A.S. 336, L13, 2002).<br /><br />[180] To make this calculation, we first must identify a pair of quasars symmetrically positioned on either side of a galaxy and measure the redshift of each; then calculate the average value. This average value is the inherent redshift of the pair. This is the quantity that has been observed to be quantized.<br /><br />[181] See Appendix B.<br /><br />[182] Catalogue of Discordant Redshift Associations, H. C. Arp, Apeiron, Montreal, pp. 58, 59.<br /><br />[183] <a href=\"http://chandra.harvard.edu/photo/2001/0094true/\" class=\"ot-anchor\">http://chandra.harvard.edu/photo/2001/0094true/</a><br /><br />[184] To explain this contradiction, astrophysicists now say some black holes can radiate energy, via a mechanism known as Hawking radiation, named after the British physicist Stephen Hawking who first invented the idea. This avoided the necessity of having to propose a wholly new invisible entity -- or accept Alfven&#39;s model.<br /><br />[185] A Theologico-Political Treatise, 1670 (Pub Anon.), Benedict de Spinoza (1632-1677)<br /><br />[186] See the section on Gravitational Lensing in chapter 4.<br /><br />[187] Burbidge, Geoffrey; &quot;Quasars in the Balance,&quot; Mercury, 17:136, 1988.<br /><br />[188] Astronomers use the phrase &quot;cosmological distance&quot; to imply that the redshift equals distance assumption is correct. The phrase &quot;non-cosmological hypothesis&quot; refers to Arp&#39;s ideas.<br /><br />[189] &quot;Quasars. Distribution behind Magellanic clouds,&quot; AJ 125, 1-12 (2003).<br /><br />[190] What is plotted are redshift-dictated indicative velocities. The assumption is made that use of the Hubble constant can then reveal the distance of each of these galaxies.<br /><br />[191] This description is described by Arp, Seeing Red, p. 225-252.<br /><br />[192] Annals of Physics, 107, 325.<br /><br />[193] P. 227.<br /><br />[194] Adam, James. The Republic of Plato. 1902. 2nd Ed. Cambridge: Cambridge University Press, 1963.<br /><br />Annas, Julia. An Introduction to Plato&#39;s Republic. Oxford: Clarendon Press 1981.<br /><br />GRAPHIC REFERENCES:<br /><br />[1] <a href=\"http://www.spaceflightnow.com/news/n0210/04hubble/\" class=\"ot-anchor\">http://www.spaceflightnow.com/news/n0210/04hubble/</a><br /><br />[2] <a href=\"http://www.astr.ua.edu/keel/galaxies/arp.html\" class=\"ot-anchor\">http://www.astr.ua.edu/keel/galaxies/arp.html</a><br /><br />[3] <a href=\"http://electric-cosmos.org/arp.htm\" class=\"ot-anchor\">http://electric-cosmos.org/arp.htm</a><br /><br />[4] <a href=\"https://rinabrundu.com/2016/04/17/galassia-ngc-4319-e-quasar-markarian-205-houston-avete-un-problema-ne-e-convinto/\" class=\"ot-anchor\">https://rinabrundu.com/2016/04/17/galassia-ngc-4319-e-quasar-markarian-205-houston-avete-un-problema-ne-e-convinto/</a><br /><br />[5] <a href=\"http://electric-cosmos.org/arp.htm\" class=\"ot-anchor\">http://electric-cosmos.org/arp.htm</a><br /><br />[6] <a href=\"http://www.areopage.net/Fusion93.html\" class=\"ot-anchor\">http://www.areopage.net/Fusion93.html</a><br /><br />[7] <a href=\"https://apod.nasa.gov/apod/ap140327.html\" class=\"ot-anchor\">https://apod.nasa.gov/apod/ap140327.html</a><br /><br />[8] <a href=\"http://electric-cosmos.org/arp.htm\" class=\"ot-anchor\">http://electric-cosmos.org/arp.htm</a><br /><br />[9] <a href=\"http://annesastronomynews.com/photo-gallery-ii/galaxies-clusters/ngc-1232/\" class=\"ot-anchor\">http://annesastronomynews.com/photo-gallery-ii/galaxies-clusters/ngc-1232/</a><br /><br />[10] <a href=\"http://electric-cosmos.org/arp.htm\" class=\"ot-anchor\">http://electric-cosmos.org/arp.htm</a><br /><br />[11] <a href=\"https://apod.nasa.gov/apod/ap990601.html\" class=\"ot-anchor\">https://apod.nasa.gov/apod/ap990601.html</a><br /><br />[12] <a href=\"https://www.thunderbolts.info/tpod/2004/arch/041018fingers-god.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2004/arch/041018fingers-god.htm</a><br /><br />[13] <a href=\"https://johnhartnettdotorg.files.wordpress.com/2013/12/fig02.jpg\" class=\"ot-anchor\">https://johnhartnettdotorg.files.wordpress.com/2013/12/fig02.jpg</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23person\">#person</a>  ﻿","category":"person"},{"name":"The Big Bang's Big Redshift Assumption","summary":"When Edwin Hubble Demonstrated an Inverse Relationship Between Galaxy Brightness and Distance, the Question was Never Asked","image":"https://lh3.googleusercontent.com/-hzj-5UousNI/WAvCuh8J44I/AAAAAAAAI-4/LPsbTh7h6jgpPnRPiv5cEHnE5jNknxplQCJoC/w3600-h5040/the-big-bangs-big-redshift-assumption-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/9u4cuZ8yxnE","publishDate":"2016-10-22T19:49:47.915Z","updateDate":"2017-01-03T01:55:13.517Z","text":"<b>The Big Bang&#39;s Big Redshift Assumption: When Edwin Hubble Demonstrated an Inverse Relationship Between Galaxy Brightness and Distance, the Question was Never Asked: If it is Dimmer, does it have to be Farther Away? / A Similar Oversight Occurred with Redshift: Although Hubble Saw an Apparent Relationship Between Redshift and Distance, he did not Feel that Redshift was Necessarily Caused by an Object&#39;s Velocity / Nevertheless, the Notion that &#39;Redshift Implies Recessional Velocity&#39; Became Astronomical Dogma</b><br /><br />The Hubble quote is admittedly stripped of its context.  He was making a nuanced point in the paper.  But, the point being made here is simply that even at 6 years after his observation (the date of the quote), he was still not convinced that redshift strictly implies recessional velocity.<br /><br /><i>The Electric Sky</i><br />Chapter 17: Redshift and the Big Bang<br />by Don Scott<br /><br /><i>&quot;In the mid 1920s, astronomers had not yet realized that each of the faint &#39;spiral nebulae&#39; that they had observed through their telescopes was really another galaxy in its own right. They believed that the stars of the Milky Way marked the extent of cosmic space. In 1920 Harlow Shapley (1882-1975), at that time the Director of Harvard College Observatory, still maintained [150] in a public debate that these spiral nebulae were clouds of gas and dust within our Milky Way galaxy. He asserted that those faint swirls of luminosity would eventually shrink down to form new stars and solar systems.</i><br /><br /><i>Assertions made without much in the way of hard evidence to back them up are always precarious. Michael Faraday stated it this way: &#39;I could trust a fact, but always cross-examined an assertion.&#39; Shapley&#39;s unsupported assertion would soon be proven wrong.</i><br /><br /><i>In 1924, astronomer Edwin P. Hubble used the 100-inch telescope at Mt. Wilson to measure the distance [151] to one of the nearest &#39;spiral nebulae&#39; - M31, known today as the Great Andromeda galaxy. He was also able to measure distances to a few other nebulae. They were hundreds of thousands of times farther away than the most distant stars in our Milky Way galaxy. Shapley&#39;s &#39;spiral nebulae&#39; were other island universes -- separate galaxies much like the Milky Way.</i><br /><br /><i>Not surprisingly, Hubble found that generally there was an inverse relationship between the apparent brightness of each of those galaxies and their distance from us. The farther away from us one was, the dimmer it appeared. Of course, this is exactly what anyone would expect: if it was farther away, it was dimmer. </i><b><i>But the obverse question was never asked: If it is dimmer, does it have to be farther away? Couldn&#39;t it be just a small dim galaxy near us?</i></b><br /><br /><i>If the absorption or emission lines in the spectrum of the light from a star or galaxy appear at a lower frequency (shifted toward the red) than where they are observed in the spectrum of a laboratory experiment, astronomers say this object has &#39;positive redshift.&#39; [152] The first galaxy Hubble measured, M31, was found to be blue-shifted, but by 1925 [153] redshifts predominated in the list he had compiled. In 1929 Hubble announced that there was a linear relationship between redshift and the galactic distances [154] he had measured. The dimmer a galaxy was, the more its light was shifted toward the red end of the spectrum -- the higher its &#39;redshift&#39; was.</i><br /><br /><i>Because of Hubble&#39;s work, the inference was universally drawn that &#39;redshift implies distance.&#39; </i><b><i>But one question still remained unasked: Could many of the dimmer galaxies whose distance had not been directly measured be just small, weak galaxies relatively near to us? Did they have to be big galaxies located very far away? The question was ignored. It was also assumed that if an object&#39;s light exhibited a high redshift value, this implied that the object was speeding away from us.</i></b><i> Why did astronomers conclude that?</i><br /><br /><i>The assumption that redshift is caused by a body&#39;s moving away from us is drawn by analogy to the well-known Doppler effect in sound waves -- the downward shift in the pitch of a train whistle as it passes through a railroad crossing and speeds away from us. It sounds reasonable. Light, just like sound, is a wave and, as such, we can measure its frequency. If the source is receding away from the observer, the frequency that is measured (of light or sound) will decrease. </i><b><i>But, can things other than the Doppler effect also cause redshift?</i></b><br /><br /><i>There are really two different important questions that must be answered by astronomers:</i><br /><br /><i>Is the redshift phenomenon a useful tool in measuring the recessional velocities of stars and galaxies?</i><br /><br /><b><i>Is recessional velocity the only cause that can produce a redshift? It has become clear that the answers to these questions are: I) Yes, provided you are careful about how you do it, and 2) an emphatic no. Astronomers have only concentrated on the first one. They ignore the second.</i></b><br /><br /><b><i>Although Hubble saw an apparent relationship between redshift and distance, he did not feel that redshift was necessarily caused by an object&#39;s velocity. He said it might be, but he felt that the relationship of redshift to velocity had not been demonstrated by observation -- at least not clearly enough to accept it as a universal truth. In everything he said and wrote about redshift, Hubble always used the qualifier &#39;if.&#39;</i></b><br /><br /><i>&#39;If the redshifts are a Doppler shift ... the observations as they stand lead to the anomaly of a closed universe, curiously small and dense, and, it may be added, suspiciously young. </i><b><i>On the other hand, if redshifts are not Doppler effects, these anomalies disappear and the region observed appears as a small, homogeneous, but insignificant portion of a universe extended indefinitely both in space and time.&#39;</i></b><i> [155]</i><br /><br /><b><i>But the analogy between the Doppler effect in sound waves and the observed redshift in light was so compelling to astronomers that they quickly dropped Hubble&#39;s cautionary &#39;if.&#39; The directly measured value of redshift, z, was routinely converted to a Doppler velocity. [156] Astronomers reported redshift values of observations in terms of recessional velocities as though they had measured them with a speedometer instead of a theory. To resist this hidden implication, astronomer Fritz Zwicky insisted on using the term &#39;indicative velocity.&#39;</i></b><br /><br /><i>Nevertheless, the notion that &#39;redshift implies recessional velocity&#39; became astronomical dogma. But it is only a dogmatically held assumption. In fact, because of relatively new evidence, it is becoming clear that a high positive redshift value does not imply that an object is moving rapidly away from us - or even that it is extremely distant. [157]</i><br /><br /><i>Yet another conclusion was cantilevered onto all this: if objects are moving away faster in proportion to their distance, then running time backward will bring the objects together at a single point in space sometime in the past. The expanding universe theory was born. Everything must have started with a Big Bang.</i><br /><br /><i>But if a high redshift value does not necessarily mean the object is moving away from us and is far away, then what does it signify? Could there be another cause of high redshift values? If so, that would suggest the universe is not necessarily expanding -- that there was no Big Bang.&quot;</i><br /><br />REFERENCES:<br /><br />[150] April 1920 &quot;Great Debate&quot; between Harlow Shapley and Heber Curtis. See:<br />Nature, Vol. 240, pp 429-430 (1972).<br /><br />[151] By identifying bright Cepheid variable stars in M 31. However, his distance estimate was too close by a factor of 2 - this was corrected in the 1950&#39;s.<br /><br />[152] Wavelength of stellar emission line = (1 + z) * wavelength of same emission line measured in lab. Thus, the reported redshift value, z, is the fractional increase in the wavelength of a star&#39;s light.<br /><br />[153] E. Hubble, The Realm of the Nebulae, Yale University Press, 1936, p. I 02.<br /><br />[154] E. Hubble, Proc. Nat. Acad. Sci., 1929, 15, 168.<br /><br />[155] E. Hubble, Roy. Astron. Soc. M. N., 17, 506, 1937<br /><br />[156] At first v = cz. But when QSOs were observed with redshifts z &gt; 1, the equation was adjusted to keep v from exceeding c, the speed of light: v = c x [((z + 1)^2 - 1)/((z + 1)^2 + 1)].<br /><br />[157] Astronomers use the &#39;Hubble Law&#39;: v = Hd where v = recessional velocity, d = distance, and H = the &#39;Hubble constant&#39; (~80 ± 17 km/s/Mpc), ignoring Hubble&#39;s warnings.<br /><br />Note: I&#39;ve had trouble validating the quote in [155].  It may not be properly sourced (?).<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The Bias of Being in a Thin Atmosphere in a Plasma Universe","summary":"\"While Only a Few Natural Plasmas - Such as Flames or Lightning Strokes - can be Found Near the Earth's Surface, Plasmas are Abundant in the Universe. More than 99% of All Known Matter is in the Plasma State\" - According to the National Research Council of the National Academies / What this Means is that Our Theories Will Naturally Bias Towards the Stuff We See Around Us Rather than the Plasma's Laboratory-Observed Behavior","image":"https://lh3.googleusercontent.com/-If2tuK6IbB8/WArU5yMFU_I/AAAAAAAAI90/HcchYPiTRPEL1KvKM9o4iq5W7CMyv88FQCJoC/w3600-h5040/the-bias-of-an-atmosphere-in-a-plasma-universe-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/7SkuUDrRjnY","publishDate":"2016-10-22T02:54:35.359Z","updateDate":"2017-01-03T01:55:45.529Z","text":"<b>The Bias of Being in a Thin Atmosphere in a Plasma Universe: &quot;While Only a Few Natural Plasmas - Such as Flames or Lightning Strokes - can be Found Near the Earth&#39;s Surface, Plasmas are Abundant in the Universe. More than 99% of All Known Matter is in the Plasma State&quot; - According to the National Research Council of the National Academies / What this Means is that Our Theories Will Naturally Bias Towards the Stuff We See Around Us Rather than the Plasma&#39;s Laboratory-Observed Behavior</b><br /><br />This will not be new to some here, but it will nevertheless help others to make sense of the above claims ...<br /><br /><b><i>&quot;Plasmas play a fundamental role in nature.</i></b><i> Probably more than 99 percent of visible matter in the universe exist in the plasma state. Plasmas exist, e.g. as interstellar gas, in stellar atmospheres, inside the sun, in giant planets, and in white dwarfs.&quot;</i><br /><br /><b>Quantum Statistics of Nonideal Plasmas</b><br /><a href=\"https://books.google.com/books?vid=ISBN3540652841&amp;id=wLN69UmwyJsC&amp;pg=PP1&amp;lpg=PP1&amp;dq=plasma+99%25+universe&amp;sig=DObg7XXvjV_CYCYIAGDnJd_PvDE#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?vid=ISBN3540652841&amp;id=wLN69UmwyJsC&amp;pg=PP1&amp;lpg=PP1&amp;dq=plasma+99%25+universe&amp;sig=DObg7XXvjV_CYCYIAGDnJd_PvDE#v=onepage&amp;q&amp;f=false</a><br /><br /><i>&quot;How was it determined that 99% of the Universe is in a plasma state?</i><br /><br /><b><i>Most of the gas in interstellar space is ionized (astronomers can tell by the wavelengths of light the gas absorbs and emits), and all of the gas in stars [is] ionized, that&#39;s where the 99% comes from.</i></b><i> The 99% ignores any dark matter which might be out there.&quot;</i><br /><br /><b>NASA&#39;s Cosmicopia - Ask Us: Cosmic Rays, Energetic Particles, and Plasma</b><br /><a href=\"http://helios.gsfc.nasa.gov/qa_cr.html#plasma\" class=\"ot-anchor\">http://helios.gsfc.nasa.gov/qa_cr.html#plasma</a><br /><br /><b><i>&quot;Today it is recognized that 99.999% of all observable matter in the universe is in the plasma state</i></b><i> and plasmas are found at temperatures and densities far exceeding those that will support matter in the first three states&quot;</i><br /><br /><b>Plasma Physics</b><br /><a href=\"https://books.google.com/books?vid=ISBN079235527X&amp;id=ZSlJRAeL95sC&amp;pg=PA57&amp;lpg=PA57&amp;ots=jnEIJGfx6t#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?vid=ISBN079235527X&amp;id=ZSlJRAeL95sC&amp;pg=PA57&amp;lpg=PA57&amp;ots=jnEIJGfx6t#v=onepage&amp;q&amp;f=false</a><br /><br /><i>&quot;Because a plasma (partly) consists of free charges, it is a conductor.  Moving electric charges are currents.  These currents induce magnetic fields which in turn influence the motion of the very particles forming the field-generating currents.  Thus the particle motion in a plasma is not only controlled by external electric and magnetic fields, but also creates fields which add to the external ones and modify the motion of the particles: a plasma can interact with itself. </i><b><i>Consequently, dynamics in a plasma are more complex than in a neutral gas.</i></b><i> This is most obvious in the large number of different types of plasma waves ...</i><br /><br /><b><i>In apparently simple situations, a plasma can behave counter-intuitively.</i></b><i> Pouring milk into our coffee, we expect the milk to heat up and mix with the coffee.  A sunspot is a sharply bordered volume of cool gas embedded in the hot solar photosphere; but it stays stable for several months prevented by strong magnetic fields from warming or mixing with its environment.  A cold and dense volume of gas or liquid in a hot environment sinks.  A solar filament is cold and dense compared with the ambient corona but it is held in position against gravity by strong magnetic fields. </i><b><i>Such discrepencies between our daily experience and the behavior of ionized gases clearly show that plasmas do not form a significant part of our environment.</i></b><i> Why then do we study such exotic phenomena?  Are there applications for plasmas?</i><br /><br /><i>First, plasmas are not exotic but quite common.  The interplanetary and interstellar medium and the stars are made of ionized gases.  Thus about 99% of matter in the universe is plasma.  Nearest regions dominated by plasmas are the magnetosphere with its radiation belts, the ionosphere, lightning bolts in the atmosphere, and, in a wider sense, the Earth&#39;s core; thus even in the system Earth plasmas are not uncommon.  Plasma physics, therefore, contributes to the understanding of our environment.  In turn, the natural plasma laboratories, i.e. the ionosphere, the magnetosphere, and interplanetary space, help to test the concepts of plasma physics on spatial scales and at densities unattainable in a laboratory.</i><br /><br /><b><i>Even some everyday materials can be described as plasmas because they show similarities to the free-electron plasma described above:</i></b><i> the conduction electrons in metals and electron hole pairs in semiconductors are charges which can move quasi-freely and lead to a behavior of the matter which can be described in the same way as for a plasma.  The free-electron gas in metals is therefore also included as example of a plasma ...&quot;</i><br /><br /><b>Space Physics</b><br /><a href=\"https://books.google.com/books?vid=ISBN3540206175&amp;id=HnyPP4B4n-4C&amp;pg=PA4&amp;lpg=PA4&amp;ots=JYLozZJ9Sq&amp;dq=plasma+99+universe+date:2003-2007&amp;num=50&amp;sig=CsyG2Lat1MiND3usk1bMYyItd9Q#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?vid=ISBN3540206175&amp;id=HnyPP4B4n-4C&amp;pg=PA4&amp;lpg=PA4&amp;ots=JYLozZJ9Sq&amp;dq=plasma+99+universe+date:2003-2007&amp;num=50&amp;sig=CsyG2Lat1MiND3usk1bMYyItd9Q#v=onepage&amp;q&amp;f=false</a><br /><br /><i>&quot;Plasmas can be divided into two broad categories: natural and man-made.  It is an interesting fact that most of the material in the visible universe, as much as 99% according to some estimates, is in the plasma state.  This includes the Sun, most stars, and a significant fraction of the interstellar medium. </i><b><i>Thus, plasmas play a major role in the universe.</i></b><i> Plasma physics is relevant to the formation of planetary radiation belts, the development of sunspots and solar flares, the acceleration of high velocity winds that flow outward from the Sun and other stars, the generation of radio emissions from the Sun and other astrophysical objects, and the acceleration of cosmic rays.&quot;</i><br /><br /><b>Introduction to Plasma Physics</b><br /><a href=\"https://books.google.com/books?vid=ISBN0521364833&amp;id=VcueZlunrbcC&amp;pg=PA2&amp;lpg=PA2&amp;ots=SwBeJMiXx9&amp;dq=plasma+99%25+universe&amp;sig=YdQ9lsMb-JB1BK19STcBe_vumXw#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?vid=ISBN0521364833&amp;id=VcueZlunrbcC&amp;pg=PA2&amp;lpg=PA2&amp;ots=SwBeJMiXx9&amp;dq=plasma+99%25+universe&amp;sig=YdQ9lsMb-JB1BK19STcBe_vumXw#v=onepage&amp;q&amp;f=false</a><br /><br /><i>&quot;If one wants to produce a good plasma in a laboratory, one usually needs to construct a fairly large apparatus. </i><b><i>If one goes outside the earth, however, the plasma state is the most abundant state of matter.</i></b><i> It is thought that more than 99.9% of matter in the universe is in plasma.  Indeed, plasmic matter begins at about 50km above the earth&#39;s surface in the ionosphere.  There are various types of plasmas in the universe, ranging from very high density (n ~ 10^36/m^3) inside a white dwarf to very low density (n ~ 10^6/m^3) in interstellar space.&quot;</i><br /><br /><b>Plasma Physics</b><br /><a href=\"https://books.google.com/books?vid=ISBN354065285X&amp;id=4cHkd77TSHcC&amp;pg=PA11&amp;lpg=PA11&amp;ots=xdaxcYVbbx&amp;dq=plasma+99.9%25+universe&amp;sig=m9CJtNeOS5UD_v8_C6OC-gxJ6Pk#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?vid=ISBN354065285X&amp;id=4cHkd77TSHcC&amp;pg=PA11&amp;lpg=PA11&amp;ots=xdaxcYVbbx&amp;dq=plasma+99.9%25+universe&amp;sig=m9CJtNeOS5UD_v8_C6OC-gxJ6Pk#v=onepage&amp;q&amp;f=false</a><br /><br /><b><i>&quot;Plasmas in nature should not be forgotten.</i></b><i> It is sometimes said that more than 99 percent of the material in the universe is in the form of a plasma.  Just two examples are given Fig. 1.2.  The Earth is surrounded by several layers of ionosphere.  It starts at about 100 km above the Earth&#39;s sruface and extends up to some 500 km.  Another example is the solar corona surrounding the Sun&quot;</i><br /><br /><b>Plasma Spectroscopy</b><br /><a href=\"https://books.google.com/books?id=w1CJHagviVsC&amp;dq=plasma%2099%20universe%20date%3A2003-2007&amp;source=gbs_book_other_versions\" class=\"ot-anchor\">https://books.google.com/books?id=w1CJHagviVsC&amp;dq=plasma%2099%20universe%20date%3A2003-2007&amp;source=gbs_book_other_versions</a><br /><br /><b><i>&quot;Plasma: A state of matter characterized by unbound negative electrons and positive ions that may conduct electrical current.</i></b><i> Plasma is often called the fourth state of matter, along with the other three: solids, liquids, and gases.  It is estimated that more than 99 percent of matter in the universe exists as plasma; examples include stars, nebulae, and interstellar particles.&quot;</i><br /><br /><b>Burning Plasma</b><br /><a href=\"http://www.nap.edu/openbook.php?record_id=10816&amp;page=188\" class=\"ot-anchor\">http://www.nap.edu/openbook.php?record_id=10816&amp;page=188</a><br /><br /><b><i>&quot;While only a few natural plasmas, such as flames or lightning strokes, can be found near the Earth&#39;s surface, plasmas are abundant in the universe.</i></b><i> More than 99% of all known matter is in the plasma state.&quot;</i><br /><br /><b>Basic Space Plasma Physics</b><br /><a href=\"https://books.google.com/books?vid=ISBN186094079X&amp;id=e4yupcOzJxkC&amp;pg=PP1&amp;lpg=PP1&amp;ots=6xtjT18iOQ&amp;dq=plasma+99%25+universe&amp;sig=i96aK5VoNeZsM9B3rN7igy2umyE#v=onepage&amp;q=plasma%2099%25%20universe&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?vid=ISBN186094079X&amp;id=e4yupcOzJxkC&amp;pg=PP1&amp;lpg=PP1&amp;ots=6xtjT18iOQ&amp;dq=plasma+99%25+universe&amp;sig=i96aK5VoNeZsM9B3rN7igy2umyE#v=onepage&amp;q=plasma%2099%25%20universe&amp;f=false</a><br /><br /><i>&quot;It is estimated that as much as 99.9% of the universe is comprised of plasma.&quot;</i><br /><br /><b>Analytical and Numerical Methods for Wave Propagation in Fluid Media</b><br /><a href=\"https://books.google.com/books?vid=ISBN9812381554&amp;id=v99GV71BNccC&amp;pg=PA14&amp;lpg=PA14&amp;ots=nyCdxEoDvA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?vid=ISBN9812381554&amp;id=v99GV71BNccC&amp;pg=PA14&amp;lpg=PA14&amp;ots=nyCdxEoDvA#v=onepage&amp;q&amp;f=false</a><br /><br /><b><i>&quot;Plasmas, often called the fourth state of matter, are the most common form of matter in the universe.</i></b><i> More than 99% of all matter across the universe is found in the plasma state, that is, a state in which at least one atomic electron has enough energy to escape the coulomb attraction of the atomic nucleus resulting in the independent motions of the free electrons and the atomic ions.&quot;</i><br /><br /><b>Spacecraft Power Technologies</b><br /><a href=\"http://www.amazon.com/Spacecraft-power-technologies-Space-Technology/dp/1860941176/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1436332400&amp;sr=1-1&amp;keywords=9781860941177\" class=\"ot-anchor\">http://www.amazon.com/Spacecraft-power-technologies-Space-Technology/dp/1860941176/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1436332400&amp;sr=1-1&amp;keywords=9781860941177</a><br /><br /><i>&quot;Plasma is not a gas, liquid, or solid -- it is the fourth state of matter.  Plasma often behaves like a gas, except that it conducts electricity and is affected by magnetic fields. </i><b><i>On an astronomical scale, plasma is common.</i></b><i> The Sun is composed of plasma, fire is plasma, fluorescent and neon lights contain plasma.</i><br /><br /><i>&#39;99.99 percent of the Universe is made up of plasma,&#39; says Dr. Dennis Gallagher, a plasma physicist at NASA&#39;s Marshall Space Flight Center. </i><b><i>&#39;Very little material in space is made of rock like the Earth.&#39;&quot;</i></b><br /><br /><b>NASA Science News - Plasma, Plasma, Everywhere - A New Model of the Plasmasphere Surrounding Our World</b><br /><a href=\"http://science.nasa.gov/science-news/science-at-nasa/1999/ast07sep99_1/\" class=\"ot-anchor\">http://science.nasa.gov/science-news/science-at-nasa/1999/ast07sep99_1/</a><br /><br /><i>&quot;It has often been said that 99% of the matter in the universe is in the plasma state; that is, in the form of </i><b><i>an electrified gas</i></b><i> with the atoms dissociated into positive ions and negative electrons. </i><b><i>This estimate may not be very accurate, but it is certainly a reasonable one in view of the fact that stellar interiors and atmospheres, gaseous nebulae, and much of the interstellar hydrogen are plasmas.</i></b><i> In our neighborhood, as soon as one leaves the earth&#39;s atmosphere, one encounters the plasma comprising the Van Allen radiation belts and the solar wind. On the other hand, in our everyday lives encounters with plasmas are limited to a few examples: the flash of a lightning bolt, the soft glow of the Aurora Borealis, the conducting gas inside a fluorescent tube or neon sign, and the slight amount of ionization in a rocket exhaust. </i><b><i>It would seem that we live in the 1% of the universe in which plasmas do not occur naturally.&quot;</i></b><br /><br /><b>Introduction to Plasma Physics</b><br /><a href=\"https://books.google.co.uk/books?id=u8nvAAAAMAAJ&amp;q=99%25+plasma+universe&amp;dq=99%25+plasma+universe&amp;hl=en&amp;sa=X&amp;ei=mfirUIu2DLG00QWy9YH4AQ\" class=\"ot-anchor\">https://books.google.co.uk/books?id=u8nvAAAAMAAJ&amp;q=99%25+plasma+universe&amp;dq=99%25+plasma+universe&amp;hl=en&amp;sa=X&amp;ei=mfirUIu2DLG00QWy9YH4AQ</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The History of the Birkeland Current","summary":"They're also Known as Force-Free Field-Aligned Currents / These are Surprisingly Complex Forms Which Can Emerge Naturally Under Specific Electrical Conditions in Gas / It Was Initially Thought that they Could Not Form in Space, Largely Because Space was Assumed to be a Perfect Vacuum / But, as Our Telescopes have Increased in Both Accuracy and Bandwidth, it Has Become Apparent that the Milky Way May be Threaded by these Structures on All Scales - and Most Stars Appear to Form Along them","image":"https://lh3.googleusercontent.com/-P-4loBCMDLo/WAbWIUfEBEI/AAAAAAAAI70/S4xLCf1qVMov6j9gkH7UJPeZ0xng-cisgCL0B/the-history-of-the-birkeland-current-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/TFL3iHyjnwz","publishDate":"2016-10-19T02:14:17.251Z","updateDate":"2017-01-03T01:56:08.692Z","text":"<b>The History of the Birkeland Current: They&#39;re also Known as Force-Free Field-Aligned Currents / These are Surprisingly Complex Forms Which Can Emerge Naturally Under Specific Electrical Conditions in Gas / It Was Initially Thought that they Could Not Form in Space, Largely Because Space was Assumed to be a Perfect Vacuum / But, as Our Telescopes have Increased in Both Accuracy and Bandwidth, it Has Become Apparent that the Milky Way May be Threaded by these Structures on All Scales - and Most Stars Appear to Form Along them</b><br /><br /><b>A Timeline of the Controversy Over Cosmic Birkeland Currents</b><br /><br />There&#39;s not actually any controversy with regards to the existence of laboratory-observed Birkeland currents; the controversy occurs at the point where it is claimed that Birkeland currents can exist at astrophysical scales, beyond just planetary localized planetary structures.  I&#39;m speaking of the interplanetary, interstellar and intergalactic scales.<br /><br />A timeline which combines discoveries and commentary can help us to better understand the evolving understanding and debate.  I doubt this timeline is comprehensive, but when you are talking about a full century of research, it&#39;s best to avoid vague generalities and narratives.  We must get into the detail ...<br /><br /><b>1700s: Nebular Theory of Star and Planet Formation Proposed by Kant and Laplace</b><br /><br /><a href=\"https://www.thunderbolts.info/tpod/2004/arch/040727stellar-nurseries.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2004/arch/040727stellar-nurseries.htm</a><br /><br /><i>&quot;The nebular theory of star and planet formation from a cloud of gas and dust that collapses under its own gravity was proposed </i><b><i>in the 18th century by Kant and Laplace.</i></b><i> It was modified in the 19th century and again in the 20th century </i><b><i>to accommodate objections. Because gravity is a very weak force, to collapse under its own weight the cloud needs to be cool and free from magnetic fields which might disturb the process. But in the stellar nurseries that have been identified, the new stars are embedded in turbulent regions of hot dust, glowing plasma and magnetic fields.</i></b><br /><br /><b><i>A quick tweak of the theory is all that was needed to adjust it to this requirement: The abundant shockwaves from stellar winds and supernovas in star-forming regions can then trigger the gravitational collapse that begins the star-and-planet forming process. But this ignores other problems. The origin of stellar &#39;winds&#39; remains a mystery. And shock-heated gases should quickly disperse, not collapse.</i></b><br /><br /><i>Nevertheless, these newborn stars are surrounded by what appears to be the disks from which the nebular theory expects planets to be born. </i><b><i>This observation is encouraging theorists to look for mechanisms to dissipate the heat and magnetism during the collapse phase.&quot;</i></b><br /><br /><b>1908: Kristian Birkeland Proposes that Earth&#39;s Auroras Are Powered by Corpuscular Rays Emanating from the Sun</b><br /><br /><a href=\"http://www.ptep-online.com/index_files/2015/PP-41-13.PDF\" class=\"ot-anchor\">http://www.ptep-online.com/index_files/2015/PP-41-13.PDF</a><br /><br />&quot;Birkeland Currents: A Force-Free Field-Aligned Model&quot;, Donald E. Scott<br /><br /><i>&quot;After Kristian Birkeland [1] (1867-1917) suggested in 1908 that Earth’s auroras were powered by corpuscular rays emanating from the Sun that become deflected into Earth’s polar regions by the geomagnetic field, </i><b><i>the existence of such magnetic field-aligned currents was strongly disputed based partially on the idea that currents could not cross the presumed &#39;vacuum&#39; of space&quot;</i></b><br /><br /><i>Physics of the Plasma Universe</i> (2014)<br />Anthony L. Peratt<br /><br /><i>&quot;Birkeland currents have a long and colorful history.  Inspired by his famous terrella experiments at the beginning of the twentieth century (Fig. 2.1) and by his extensive studies of geomagnetic data recorded during magnetic storms, the Norwegian scientist Kristian Birkeland (1867-1917) suggested that the aurora was associated with electric &#39;corpuscular rays&#39; emanating from the sun and deflected to the polar regions of the earth by the geomagnetic field.  Birkeland recognized that the magnetic disturbances recorded on the earth&#39;s surface below the auroral region were due to intense currents flowing horizontally above.  He suggested that these currents, now called &#39;auroral electrojet&#39; currents, were coupled to vertical currents that flowed along geomagnetic field lines into and away from the lower ionosphere.  The system of field-aligned currents suggested by Birkeland is shown in Fig. 2.2a.&quot;</i><br /><br /><b>1927: Sydney Chapman Disputes Birkeland&#39;s Currents with a Mathematically Elegant Model of Currents that Were Contained Completely Within the Ionosphere</b><br /><br /><i>Physics of the Plasma Universe</i> (2014)<br />Anthony L. Peratt<br /><br /><i>&quot;The existence of magnetic field-aligned &#39;Birkeland&#39; currents was disputed because it is not possible to distinguish unambiguously between current systems that are field-aligned and those that are completely ionospheric from a study of surface magnetic field measurements.  Sydney Chapman, the noted British geophysicist, developed mathematically elegant models of currents that were contained completely within the earth&#39;s ionosphere that could adequately account for ground-based magnetic field observations obtained during magnetic storms.  Figure 2.2b shows the &#39;atmospheric current system&#39; developed by Chapman in 1927 (Egeland and Holtet 1968; Hill 1984; Desslet 1984).&quot;</i><br /><br /><b>1939: Hannes Alfven develops theory for the generation of Birkeland currents by the Solar Wind</b><br /><br /><b>1950: Hannes Alfven Proposes that the Universe is Filamentary</b><br /><br /><a href=\"https://books.google.com/books?id=ZSlJRAeL95sC&amp;pg=PA57&amp;lpg=PA57&amp;dq=%22Among+the+earliest+predictions+about+the+morphology+of+the+universe+that+differed+significantly+from+the+common+astrophysical+assumption+is+that+the+universe+be+filamentary%22&amp;source=bl&amp;ots=jqFFFCjw9q&amp;sig=9iZWC3zm3EtT_TZhcKJFdwEHERk&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiGy7Lg3tHPAhVS3GMKHYIwB9oQ6AEIHDAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=ZSlJRAeL95sC&amp;pg=PA57&amp;lpg=PA57&amp;dq=%22Among+the+earliest+predictions+about+the+morphology+of+the+universe+that+differed+significantly+from+the+common+astrophysical+assumption+is+that+the+universe+be+filamentary%22&amp;source=bl&amp;ots=jqFFFCjw9q&amp;sig=9iZWC3zm3EtT_TZhcKJFdwEHERk&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiGy7Lg3tHPAhVS3GMKHYIwB9oQ6AEIHDAA#v=onepage&amp;q&amp;f=false</a><br /><br /><b><i>&quot;Among the earliest predictions about the morphology of the universe that differed significantly from the common astrophysical assumption is that the universe be filamentary</i></b><i> (Alfven, 1950, 1981, 1990).&quot;</i><br /><br /><i>Cosmical Electrodynamics</i> (1950)<br />Hannes Alfven<br /><br /><i>&quot;Most theoretical investigations of cosmical plasmas have been devoted to the study of homogeneous plasmas. </i><b><i>However, observations indicate that most cosmical plasmas are strongly inhomogeneous.</i></b><i> In the ionosphere a small-scale structure is often observed, most pronounced in connexion with aurorae. Auroral rays are often very thin, and the degree of ionization and hence the conductivity may vary by two or three powers of ten within a few kilometres or less. Also the magnetosphere has probably a filamentary structure as indicated by the study of whistler propagation. Similarly, the solar atmosphere has a ray structure. On good eclipse photographs the ray structure of the corona is often very pronounced, and radio observations indicate that the rays continue at least out to 10 or 20 solar radii (&#39;supercorona&#39;). Closer to the surface of the sun we observe prominences, which very often have a threadlike structure. The chromosphere is often regarded as a filamentary network of miniature prominences. In gas nebulae filamentary structures are often conspicuous.</i><br /><br /><b><i>Hence medium-density plasmas (and perhaps also low-density plasmas) seem very often to be strongly inhomogeneous, exhibiting a filamentary structure which often may be parallel to the magnetic field.&quot;</i></b><br /><br /><b>1950/1957: Mathematical Model for Force-Free Magnetic Field Proposed</b><br /><br /><a href=\"http://www.ptep-online.com/index_files/2015/PP-41-13.PDF\" class=\"ot-anchor\">http://www.ptep-online.com/index_files/2015/PP-41-13.PDF</a><br /><br />&quot;Birkeland Currents: A Force-Free Field-Aligned Model&quot;, Donald E. Scott<br /><br /><i>&quot;a mathematical model of a force-free magnetic field was proposed as early as 1950 by Lundquist [3, 4]. He investigated whether magnetic fields could exist in an electrically conducting liquid and his results included presentation of the now well-known Bessel solution for force-free fields. Later in 1957, investigators such as Chandrasekhar and Kendall [5] applied a similar analysis to the spherical geometry of the Sun.&quot;</i><br /><br /><a href=\"http://www.plasma-universe.com/Birkeland_current\" class=\"ot-anchor\">http://www.plasma-universe.com/Birkeland_current</a><br /><br /><i>&quot;Originally Birkeland currents referred to electric currents that contribute to the aurora, caused by the interaction of the plasma in the Solar Wind with the Earth&#39;s magnetosphere. The current flows earthwards down the morning side of the Earth&#39;s ionosphere, around the polar regions, and spacewards up the evening side of the ionosphere. These Birkeland currents are now sometimes called auroral electrojets. The currents were predicted in 1903 by Norwegian explorer and physicist Kristian Birkeland, who undertook expeditions into the Arctic Circle to study the aurora.&quot;</i><br /><br /><b>1964-1981: NASA and Other Investigators Work on Birkeland Currents and Flux Rope Observations</b><br /><br />A list of sample papers ...<br /><br />Gold T. AAS-NASA Symposium on Physics of Solar Flares. Hess W. N., ed., NASA SP–50, 1964, p. 389.<br /><br />Alekseev I. and Shabansky V. A model of a magnetic field in the geomagnetosphere. Planet. Space Sci., 1972, v. 20, 117.<br /><br />Chiu Y. and Hilton H. Exact Green’s function method of solar forcefree magnetic-field computations with constant alpha. I. Theory and basic test cases. Astrophys. J., 1977, v. 212, 873–885.<br /><br />Cloutier P. and Anderson H. Observations of Birkeland currents. Space<br />Sci. Rev., 1975, v. 17, 563–587.<br /><br />Alfven H. Evolution of the solar system. Scientific and Technical Information<br />Office National Aeronautics and Space Administration, Washington, D.C., 1976.<br /><br />Hasegawa A. and Sato T. Generation of Field Aligned Current during Substorm in Dynamics of the Magnetosphere. Akasofu, S-I., ed., D. Reidel, Hingham, MA, 1979, p. 529.<br /><br />Nakagawa J. and Raadu M. A. On practical representation of magnetic field. Solar Phys., 1972, v. 25, 127.<br /><br />Olson W. A model of distributed magnetospheric currents. J. Geophys. Res., 1974, v. 79, 3731.<br /><br />Rostoker G., Armstrong J. C., and Zmuda A. J. Field-aligned current flow associated with intrusion of the substorm-intensified westward electrojet into the evening sector. J. Geophys. Res., 1975, v. 80, 3571–3579.<br /><br />Zmuda A., Armstrong J. C., and Heuring F. Characteristics of transverse magnetic disturbances observed at 1100 kilometers in the auroral oval. J. Geophys. Res., 1970, v. 75 (25), 4757–4762.<br /><br />Dessler A. Corotating Birkeland currents in Jupiter’s magnetosphere - An Io plasma-torus source. Plan. Space Sci., 1980, v. 28, 781–788.<br /><br />Harel M., Wolf R. A., Reiff P. H., Spiro R.W., Burke W. J., Rich F. J., and Smiddy M. Quantitative simulation of a magnetospheric substorm 1. Model logic and overview. J. Geophys. Res., 1981, v. 86, 2217–2241.<br /><br />Alissandrakis C. On the computation of constant alpha force-free magnetic<br />field. J. Astron. Astrophys., 1981, v. 100 (1), 197–200.<br /><br /><b>1966/1967: First Satellite Measurements of Birkeland Currents</b><br /><br /><i>Physics of the Plasma Universe</i> (2014)<br />Anthony L. Peratt<br /><br /><i>&quot;The first satellite measurements of Birkeland currents were provided by Zmuda et al. (1966, 1967) with a single axis magnetometer on board the navigation satellite 1963-1938C at an altitude of ~1,100km.  The magnetic disturbances observed were initially interpreted as hydromagnetic waves, but it was soon realized that their latitudinal extent was not appropriate for waves and they were interpreted as being due to Birkeland currents (Cummings and Dessler 1967).  Today, Birkeland currents are routinely measured by a variety of rocket and satellite instruments.&quot;</i><br /><br /><a href=\"http://articles.adsabs.harvard.edu//full/1969CoASP...1..117W/0000117.000.html\" class=\"ot-anchor\">http://articles.adsabs.harvard.edu//full/1969CoASP...1..117W/0000117.000.html</a><br /><br />Field-Aligned Currents in the Magnetosphere<br />(Cummings, Dessler 1967)<br /><br /><i>&quot;It is now generally accepted that the satellite observations of Zmuda and co-workers are to be interpreted as representing a funnel-shaped distribution of current flowing into and out of the polar auroral zones. [1][2][3] These observations show that substantial magnetic fluctuations occur in a narrow range of auroral latitudes. If these fluctuations were caused by ordinary hydromagnetic waves, their wavelengths would be so long that they could not be confined to auroral latitudes. However, the magnetic fluctuations from field-aligned currents can be closely confined (in the form of TEM waves) as, for example, the confinement of the magnetic disturbance generated by currents flowing in a coaxial cable. The idea that electric currents flow along magnetic field lines connected to the auroral ionosphere is an old one, the original proposal being put forth by Birkeland at the beginning of the century. These currents are commonly called Birkeland currents. </i><b><i>It is interesting to note that, until recently, most efforts in geomagnetism directed toward explaining transient magnetic phenomena contained the assumption that the responsible current systems were confined to the ionosphere. The principal appeal of this rather restrictive assumption is that the magnetic effects of a spherical current surface surrounding the Earth (such as currents confined to the ionosphere) could be described by the exact and elegant methods of spherical harmonic analysis, while the complex system involving both ionospheric and Birkeland currents could not.&quot;</i></b><br /><br /><b>1969: The Modern Quantitative Model for Stellar/Planetary Accretion is Created</b><br /><br /><a href=\"https://en.wikipedia.org/wiki/Accretion_%28astrophysics%29\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Accretion_(astrophysics)</a><br /><br /><i>&quot;The idea proposed in the 19th century that Earth and the other terrestrial planets formed from meteoric material was developed in a quantitative way in 1969 by Viktor Safronov. He calculated, in detail, the different stages of terrestrial planet formation.[3][4] Since then, the theory has been further developed using intensive numerical simulations to study planetesimal accumulation.</i><br /><br /><i>Stars form by the gravitational collapse of interstellar gas. Prior to collapse, this gas is mostly in the form of molecular clouds, such as the Orion Nebula. As the cloud collapses, losing potential energy, it heats up, gaining kinetic energy, and the conservation of angular momentum ensures that the cloud forms a flatted disk—the accretion disk.&quot;</i><br /><br /><b>1974: Discovery of Birkeland Currents in the Earth&#39;s Magnetosphere</b><br /><br />(from same source as above)<br /><br /><i>&quot;The discovery of these Birkeland currents in the earth&#39;s magnetosphere in 1974 (Dessler, 1984) has resulted in the realization that the structure in auroral draperies are caused by the filamentation of Birkelaned charged-particle sheets following the earth&#39;s dipole magnetic-field lines into vortex current bundles.&quot;</i><br /><br /><b>1976: Birkeland Current Locations, Flow Directions and Densities Statistically Mapped</b><br /><br /><i>Physics of the Plasma Universe</i> (2014)<br />Anthony L. Peratt<br /><br /><i>&quot;The location, flow direction and intensity of Birkeland currents have been studied by several satellites, and a statistical map is provided in Fig. 2.3.  This distribution, plotted on an invariant (magnetic) latitude and geomagnetic local time polar dial, was determined from hundreds of orbits of the TRIAD satellite over the polar regions (Iijima and Potemra 1976).  It shows that there are well defined patterns of these currents, and these patterns coincide approximately with the auroral zone.  </i><b><i>The inflowing currents are at latitude 70 degrees and the outflowing currents are at 74 - 78 degrees from about noon to midnight, then reverse directions from midnight to noon.</i></b><br /><br />[...]<br /><br /><b><i>Birkeland currents not only heat the upper atmosphere, increasing the drag on low altitude earth circling satellites, but cause substantial density depletions.</i></b><i>  Electric fields along the dipole magnetic field lines can give rise to field-aligned currents that reduce the ionospheric topside electron and ion densities (Block and Falthammar 1969).  In the topside there is then an excess of ionization in contrast to low altitudes where recombination dominates.  The net effect is an upward flux of neutral particles that are ionized at high altitudes.  Auroral primary particles are charged particles with the proper sign that have been accelerated downward.  Closure of the global circuit is accomplished by particles with the opposite sign, that move out of the magnetosphere and precipitate in another region of the ionosphere with reversed polarity.  </i><b><i>Current measurements suggest that the supply of plasma from the solar wind is negligible in relation to the supply of plasma from the closed loop ionospheric/magnetospheric system.&quot;</i></b><br /><br /><b>1976: Hannes Alfven Remarks on the Lack of Observational Support for Gravitational Collapse</b><br /><br /><i>Evolution of the Solar System</i>, NASA 1976, H. Alfvén &amp; G, Arrhenius, p. 480<br /><br /><i>&quot;There is a general belief that stars are forming by gravitational collapse; in spite of vigorous efforts no one has yet found any observational indication of confirmation. Thus the &#39;generally accepted&#39; theory of stellar formation may be one of a hundred unsupported dogmas which constitute a large part of present-day astrophysics.&quot;</i><br /><br /><b>1979: In Situ Measurements of Venus&#39; Ionosphere Reveals a Flux Rope Structure</b><br /><br /><i>Cosmic Plasma</i> (1981)<br />Hannes Alfven<br /><br /><i>&quot;Elphic et al. (1979) have made in situ [in its original place] measurements of the current system in the ionosphere of Venus.  They observe a &#39;flux rope structure&#39; ... </i><b><i>During quiescent conditions, the flux ropes are observed to be a nearly ubiquitous feature of the dayside ionosphere.  Their diameters are of the order 15-20km.</i></b><i>  The data indicate only a small thermal pressure gradient, so that v x B should be small.  </i><b><i>The observed magnetic field structure can be compared with the theoretical field structure of a flux tube.</i></b><i>  It is evident that the agreement is satisfactory.</i><br /><br /><b><i>This is the first convincing observational evidence for the theoretically expected structure of filamentary currents producing flux ropes.</i></b><i> Due to its basic importance for the electric current structure of cosmic plasmas, further investigations of the flux ropes in the ionosphere of Venus are very important.  One should also try to identify similar structures in other regions.&quot;</i><br /><br /><b>1979: A Field-Aligned Current is Definitively Observed to Connect Jupiter with its Moon, Io</b><br /><br /><a href=\"http://www.igpp.ucla.edu/public/mkivelso/Publications/278-Ch22.pdf\" class=\"ot-anchor\">http://www.igpp.ucla.edu/public/mkivelso/Publications/278-Ch22.pdf</a><br /><br /><i>&quot;From the Discovery of the Io Effect to Mid-1990</i><br /><br /><i>The discovery of the Io effect, i.e. Io’s statistical control over Jupiter’s decametric radiation, by Bigg (1964) gave the first evidence for a strong electrodynamic interaction between Io and the jovian magnetosphere and thereby initiated the research on Io’s plasma interaction. The next big step came with the first spacecraft, Pioneer 10 and Pioneer 11, that visited the Jupiter system in 1973/74. The detection of an ionosphere by Pioneer 10 (Kliore et al. 1975) and the Earthbased observations of neutral clouds of sodium in the vicinity of Io by Brown and Chaffee (1974) (see Chapter 23) related the electrodynamic interaction to a neutral atmosphere on Io. Another major observational building block for Io’s interaction was found by the Voyager 1 and Voyager 2 Jupiter flybys in 1979. Broadfoot et al. (1979) and Bridge et al. (1979) identified the dense, luminous plasma known as the Io plasma torus. Previous hints had already existed from observations of ionized sulfur lines in the inner magnetosphere around Io by Kupo et al. (1976). Several authors had also postulated that the Pioneer 10 plasma and UV observations, initially interpreted in terms of enhanced density of light ions, actually represented heavy ions (see, reviews in Belcher (1983) and Brown et al. (1983)). Voyager 1 also discovered that Io has the most active volcanism in the solar system (Smith et al. 1979, Morabito et al. 1979), driven probably by orbital resonance with Europa and Ganymede, as predicted by Peale et al. (1979) just prior to the actual observation. The Voyager IRIS experiment first detected directly a localized volcanic plume atmosphere of SO2 (Pearl et al. 1979). The Io plasma torus composition provided additional evidence for SO2 as the dominant atmospheric species (Bridge et al. 1979). More than 10 years had to pass until the first Earth-based detection of Io’s SO2 atmosphere by Lellouch et al. (1990). </i><b><i>Direct evidence for the strong electrodynamic interaction came from the magnetic field observations of the Voyager spacecraft (Ness et al. 1979), which inferred an estimated electric current in Io’s flux tube/Alfven wings of 2.8 × 106 Ampere (Acu˜na et al. 1981). Additional recent evidence for a powerful electrodynamic interaction are the observations of near–IR and UV radiation at the footprints of the Io flux tube intersecting Jupiter’s auroral atmosphere (Connerney et al. 1993, Clarke et al. 1996, Prang´e et al. 1996).&quot;</i></b><br /><br /><b>1980s: Large-Scale Filamentation Observed</b><br /><br /><a href=\"https://books.google.com/books?id=ZSlJRAeL95sC&amp;pg=PA57&amp;lpg=PA57&amp;dq=%22Among+the+earliest+predictions+about+the+morphology+of+the+universe+that+differed+significantly+from+the+common+astrophysical+assumption+is+that+the+universe+be+filamentary%22&amp;source=bl&amp;ots=jqFFFCjw9q&amp;sig=9iZWC3zm3EtT_TZhcKJFdwEHERk&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiGy7Lg3tHPAhVS3GMKHYIwB9oQ6AEIHDAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=ZSlJRAeL95sC&amp;pg=PA57&amp;lpg=PA57&amp;dq=%22Among+the+earliest+predictions+about+the+morphology+of+the+universe+that+differed+significantly+from+the+common+astrophysical+assumption+is+that+the+universe+be+filamentary%22&amp;source=bl&amp;ots=jqFFFCjw9q&amp;sig=9iZWC3zm3EtT_TZhcKJFdwEHERk&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiGy7Lg3tHPAhVS3GMKHYIwB9oQ6AEIHDAA#v=onepage&amp;q&amp;f=false</a><br /><br /><b><i>&quot;The suggestion that the universe be filamentary and cellular was generally disregarded until the 1980s, when a series of unexpected observations showed filamentary structure on the Galactic, intergalactic, and supergalactic scale.&quot;</i></b><br /><br /><b>1983: Anthony Peratt Creates Simulation to Show that Two Adjacent Birkeland Filaments Can Produce Proper Galactic Forms, Without the Need for Dark Matter</b><br /><br />The paper ...<br /><br />Peratt, A. L.; Green, J. C., &quot;On the evolution of interacting, magnetized, galactic plasmas&quot;, Astrophysics and Space Science (ISSN 0004-640X), vol. 91, no. 1, March 1983, p. 19-33.<br /><br />A summary of the research is at <a href=\"https://www.plasma-universe.com/Galaxy_formation\" class=\"ot-anchor\">https://www.plasma-universe.com/Galaxy_formation</a> ...<br /><br /><i>&quot;Galaxy formation in the Plasma Universe is modeled as two adjacent interacting Birkeland filaments. </i><b><i>The simulation produces a flat rotation curve, but no hypothetical dark matter is needed, as required by the conventional model of galaxy formation.</i></b><br /><br /><i>The simulations derive from the work of Winston H. Bostick who obtained similar results from interacting plasmoids.[1][2]</i><br /><br /><b><i>In the early 1980s Anthony L. Peratt, a student of Alfvén&#39;s, used supercomputer facilities at Maxwell Laboratories and later at Los Alamos National Laboratory to simulate Alfvén and Fälthammar&#39;s concept of galaxies being formed by primordial clouds of plasma spinning in a magnetic filament.[3]</i></b><br /><br /><i>The simulation began with two spherical clouds of plasma trapped in parallel magnetic filaments, each carrying a current of around 10^18 amperes. The clouds spin around each other until a spiral shape emerges. Peratt concluded that the shapes seen in the simulation appeared similar to observed galaxy shapes, and posited a morphological sequence that corresponded to Halton Arp&#39;s ideas that galaxies formed out of quasars ejected from AGN.[4] Perrat&#39;s spirals had qualitatively flat rotation curves.&quot;</i><br /><br /><b>1984: Galactic Nucleus is Observed in Detail, Exhibits Enormous Current Filaments</b><br /><br /><i>Physics of the Plasma Universe</i> (2014)<br />Anthony L. Peratt<br /><br /><i>&quot;The center of the Galaxy, where twisting plasma filaments, apparently held together by a magnetic field possessing both azimuthal and poloidal components, extend for </i><b><i>nearly 60 pc (10^18 m)</i></b><i> (Fig. 1.14).&quot;</i><br /><br />FYI: <a href=\"https://en.wikipedia.org/wiki/Parsec\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Parsec</a><br /><br /><i>&quot;A parsec (symbol: pc) is a unit of length used to measure large distances to objects outside our Solar System. One parsec is the distance at which one astronomical unit subtends an angle of one arcsecond.[1] A parsec is equal to about 3.26 light-years (31 trillion kilometres or 19 trillion miles) in length. The nearest star, Proxima Centauri, is about 1.3 parsecs (4.2 light-years) from the Sun.[2] Most of the stars visible to the unaided eye in the nighttime sky are within 500 parsecs of the Sun.&quot;</i><br /><br /><a href=\"http://www.thunderbolts.info/thunderblogs/archives/solar08/062308_milky_way_birkeland_current.htm\" class=\"ot-anchor\">http://www.thunderbolts.info/thunderblogs/archives/solar08/062308_milky_way_birkeland_current.htm</a><br /><br />Milky Way&#39;s Birkeland Current Falsifies &quot;Black Hole&quot; Assumption<br /><br />06/23/08<br /><br />&quot;Anatomy of the Milky Way core ...&quot;<br /><br /><a href=\"http://www.annualreviews.org/doi/abs/10.1146/annurev.aa.22.090184.001255\" class=\"ot-anchor\">http://www.annualreviews.org/doi/abs/10.1146/annurev.aa.22.090184.001255</a><br /><br /><i>&quot;... </i><b><i>&#39;At present, we find ourselves in the unsatisfying position of having remarkable new observational insight into the nature of the galactic center but lacking a sturdy interpretive framework.&#39;</i></b><i> - Robert L. Brown and Harvey S. Listz &#39;Sagittarius A and it&#39;s Environment&#39; Annual Review of Astronomy and Astrophysics</i><br /><br /><i>Written in 1984 it appears that since then things have only worsened.</i><br /><br /><i>According to some estimates the bright radio source known as Sagittarius A (pronounced A-star), residing at the heart of our Milky Way galaxy, is </i><b><i>more than 50 light-years wide. It&#39;s radio glow stems from synchrotron radiation, the result of charged particles spiraling around magnetic field lines at relativistic speeds.</i></b><i> Assumptions have been made that the Sag A complex is the result of a massive black hole. Yet, save for mathematical computer models based on gravitational conjecture, </i><b><i>not a single black hole has ever been found&quot;</i></b><i> ...</i><br /><br /><a href=\"http://chandra.harvard.edu/photo/2005/sgra/index.html\" class=\"ot-anchor\">http://chandra.harvard.edu/photo/2005/sgra/index.html</a><br /><br /><i>&quot;... </i><b><i>&#39;According to the standard model for star formation, gas clouds from which stars form should have been ripped apart by tidal forces from the supermassive black hole.</i></b><i> Evidently, the gravity of a dense disk of gas around Sagittarius A offsets the tidal forces and allows stars to form. The tug-of-war between the black hole&#39;s tidal forces and the gravity of the disk has also </i><b><i>favored the formation of a much higher proportion of massive stars than normal.</i></b><i>&quot; - Chandra: &#39;Stars Surprisingly Form in Extreme Environment Around Milky Way&#39;s Black Hole&#39;&quot;</i><br /><br />(it&#39;s curious that the problem hasn&#39;t been more widely discussed)<br /><br /><b>1986: Plasma Universe Term Coined</b><br /><br /><b>1986: Falthammar Notes the Astrophysical Significance of Birkeland Currents Beyond Just the Earth&#39;s Local Space Environment</b><br /><br />Falthammar, C.-G.: &quot;Magnetosphere-ionosphere interactions: near-earth manifestations of the plasma universe&quot;. IEEE Trans. Plasma Sci. 14, 616 (1986)<br /><br /><b><i>&quot;A reason why Birkeland currents are particularly interesting is that, in the plasma forced to carry them, they cause a number of plasma physical processes to occur (waves, instabilities, fine structure formation).  These in turn lead to consequences such as acceleration of charged particles, both positive and negative, and element separation (such as preferential ejection of oxygen ions).  Both of these classes of phenomena should have a general astrophysical interest far beyond that of understanding the space environment of our own Earth.&quot;</i></b><br /><br /><b>Late 1980&#39;s: Research on Birkeland Currents and Flux Rope Observations Picks Up</b><br /><br />A list of some sample papers ...<br /><br />Burlaga L. Magnetic clouds and force-free fields with constant alpha. J. Geophys. Res.: Space Phys., 1988, v. 93 (A7), 7217–7224.<br /><br />Durrant C. J. Linear force-free magnetic fields and coronal models. Aust. J. Phys., 1989, v. 42, 317–329.<br /><br />Falthammer C. G. Magnetosphere-Ionosphere interaction. Near-Earth Manifestations of the plasma universe. IEEE Trans. Plasma. Sci., 1986, v. 14 (6), 616–628.<br /><br />Potemra T. Birkeland Currents I. The Earth’s Magnetosphere (From a special issue dedicated to Hannes Alfven on his 80th Birthday). Astrophys. Space Sci., 1988, v. 144 (1–2), 155–169.<br /><br />Potemra T. Observation of Birkeland currents with the TRIAD Satellite. Astrophys. Space Sci., 1978, v. 58 (1), 207–226.<br /><br />Potemra T. Alfven Waves and Birkeland Currents. Physica Scripta, 1995, v. T60, 107–112.<br /><br /><b>1990: The Orion Nebula&#39;s &#39;String-like&#39; Ionized Features Are Observed</b><br /><br />&quot;Structural Details of the Orion Nebula: Detection of a Network of String-like Ionized Features&quot; by F. Yusef-Zadeh<br /><br /><i>&quot;... The overall morphology of ionized gas at 20cm wavelength shows a number of </i><b><i>striking new substructures</i></b><i> associated with this geometrically complex HII region ... </i><b><i>On a small scale, we report the detection of a web of stringlike features</i></b><i> ...&quot;</i><br /><br />See large-scale filamentary structure in this filtered Hi-rez image here: <a href=\"http://images.nrao.edu/402\" class=\"ot-anchor\">http://images.nrao.edu/402</a><br /><br /><b>1993/1994: Theorist&#39;s 2013 Claim that Planetary Nebulae can All be Explained with Hydrodynamic Models Was Undermined by the Hubble Space Telescope the Very Next Year</b><br /><br /><a href=\"http://www.annualreviews.org/doi/abs/10.1146/annurev.astro.40.060401.093849?journalCode=astro\" class=\"ot-anchor\">http://www.annualreviews.org/doi/abs/10.1146/annurev.astro.40.060401.093849?journalCode=astro</a><br /><br />There is a much shorter, yet free, version here ...<br /><br /><a href=\"http://adsabs.harvard.edu/full/1989IAUS..131...83B\" class=\"ot-anchor\">http://adsabs.harvard.edu/full/1989IAUS..131...83B</a><br /><br />PNMorphologies: The Smirk of the Cheshire Cat<br /><br /><i>&quot;Perhaps the first and youngest of the many &#39;standard models&#39; in astronomy to fall victim to the penetrating spatial resolution and dynamic range of the Hubble Space Telescope (HST) was that for planetary nebulae (PNe). </i><b><i>Historically, in 1993, Frank et al. confidently claimed that the morphologies of nearly all PNe could be understood as the evolving hydrodynamic interaction between fast winds from a central star and the nozzle formed by a dense torus of material presumably ejected earlier in the life of the central star. In 1994, the now-famous HST image of the Cat’s Eye Nebula (Harrington &amp; Borkowski 1994) mocked Frank et al.’s simple paradigm in several ways.</i></b><i> First, no signs of dense tori were seen in close association with either member of an odd pair of orthogonal ellipsoidal features in the nebular core. Second, </i><b><i>the HST image showed an incredible array of meticulously organized knotty or jet-like features that extant hydro models simply cannot explain with any credible set of presumed initial and boundary conditions. Insofar as our comprehension of the shapes of PNe is concerned, the HST image of NGC 6543 is redolent of the frustrating ambiguity of the Cheshire Cat.&quot;</i></b><br /><br /><b>1995: Radio Astronomer Gerrit Verschuur Publishes a Series of Papers that Characterize Interstellar HI Hydrogen at High Galactic Latitudes as Filamentary and Related to Laboratory Plasma Physics Processes</b><br /><br /><a href=\"http://adsabs.harvard.edu/full/1995Ap%26SS.227..187V\" class=\"ot-anchor\">http://adsabs.harvard.edu/full/1995Ap%26SS.227..187V</a><br /><br />&quot;Interstellar Neutral Hydrogen Filaments at High Galactic Latitudes and the Bennet Pinch&quot;, Gerrit Verschuur<br /><br /><i>&quot;Preliminary results from high resolution HI mapping of gas and, dust in an apparent HI &#39;cloud&#39; indicate that </i><b><i>the neutral gas and dust within and around its boundary is itself highly filamentary</i></b><i> ... currently available high-resolution HI data suggest that </i><b><i>interstellar filament physics should take into account the role of currents and pinches for creating and stabilizing the structures.&quot;</i></b><br /><br /><i>&quot;H0827+10 would traditionally be referred to as </i><b><i>a &#39;cloud.&#39;&quot;</i></b><br /><br /><b>Mid-1990s: Surprising Observations of Exoplanets Reveal that they Look Nothing Like Our Own Solar System, Raising Questions About How Stars and Planets Form</b><br /><br /><a href=\"http://www.nature.com/news/astronomy-planets-in-chaos-1.15480\" class=\"ot-anchor\">http://www.nature.com/news/astronomy-planets-in-chaos-1.15480</a><br /><a href=\"http://www.nature.com/polopoly_fs/1.15480!/menu/main/topColumns/topLeftColumn/pdf/511022a.pdf\" class=\"ot-anchor\">http://www.nature.com/polopoly_fs/1.15480!/menu/main/topColumns/topLeftColumn/pdf/511022a.pdf</a><br /><br />Astronomy: Planets in chaos<br /><br /><b><i>&quot;The discovery of thousands of star systems wildly different from our own has demolished ideas about how planets form. Astronomers are searching for a whole new theory.</i></b><br /><br /><i>Ann Finkbeiner</i><br /><br /><i>02 July 2014</i><br /><br /><i>Not so long ago — as recently as the mid-1990s, in fact — there was a theory so beautiful that astronomers thought it simply had to be true.</i><br /><br /><i>They gave it a rather pedestrian name: the core-accretion theory. But its beauty lay in how it used just a few basic principles of physics and chemistry to account for every major feature of our Solar System. </i><b><i>It explained why all the planets orbit the Sun in the same direction; why their orbits are almost perfectly circular and lie in or near the plane of the star&#39;s equator; why the four inner planets (Mercury, Venus, Earth and Mars) are comparatively small, dense bodies made mostly of rock and iron; and why the four outer planets (Jupiter, Saturn, Uranus and Neptune) are enormous, gaseous globes made mostly of hydrogen and helium. And because the same principles of physics and astronomy must apply throughout the Universe, it predicted that any system of &#39;exoplanets&#39; around another star would look pretty much the same.</i></b><br /><br /><b><i>But in the mid-1990s, astronomers actually started finding those exoplanets — and they looked nothing like those in our Solar System.</i></b><i> Gas giants the size of Jupiter whipped around their stars in tiny orbits, where core accretion said gas giants were impossible. Other exoplanets traced out wildly elliptical orbits. Some looped around their stars&#39; poles. Planetary systems, it seemed, could take any shape that did not violate the laws of physics.</i><br /><br /><i>Following the launch of NASA&#39;s planet-finding Kepler satellite in 2009, the number of possible exoplanets quickly multiplied into the thousands — enough to give astronomers their first meaningful statistics on other planetary systems, and to undermine the standard theory for good. Not only were there lots of exoplanet systems bearing no resemblance to ours, but the most commonly observed type of planet — a &#39;super-Earth&#39; that falls between the sizes of our world and Neptune, which is four times bigger — does not even exist in our Solar System. </i><b><i>Using our planetary family as a model, says astronomer Gregory Laughlin of the University of California, Santa Cruz, &#39;has led to no success in extrapolating what&#39;s out there&#39;.</i></b><br /><br /><b><i>The findings have triggered controversy and confusion, as astronomers struggle to work out what the old theory was missing. They are trying ideas, but are still far from sure how the pieces fit together. The field in its current state &#39;doesn&#39;t make much sense&#39;, says Norm Murray of the Canadian Institute for Theoretical Astrophysics in Toronto. &#39;It&#39;s impossible right now to account for everything,&#39; agrees Kevin Schlaufman, an astrophysicist at the Massachusetts Institute of Technology (MIT) in Cambridge. Until researchers reach a new consensus, they will not be able to understand how our own Solar System fits into the grand scheme of things, let alone predict what else might exist.&quot;</i></b><br /><br />[...]<br /><br /><b>2005: Detailed Hubble Imaging of Cygnus Loop Filaments Published</b><br /><br />The filaments&#39; mechanics are still to this day presumed by many astrophysicists to be the result of mechanical &quot;shocks&quot; ...<br /><br /><a href=\"http://iopscience.iop.org/1538-3881/129/5/2268/pdf/204514.web.pdf\" class=\"ot-anchor\">http://iopscience.iop.org/1538-3881/129/5/2268/pdf/204514.web.pdf</a><br /><br />&quot;Hubble Space Telescope Imaging of the Primary Shock Front in the Cygnus Loop Supernova Remnant&quot;<br /><br /><b>2006: &#39;Elephant Trunk&#39; Nebulae Linked to Electromagnetic Causes</b><br /><br /><a href=\"http://www.aanda.org/index.php?option=com_article&amp;access=bibcode&amp;Itemid=129&amp;bibcode=2006A%2526A...454..201GFUL\" class=\"ot-anchor\">http://www.aanda.org/index.php?option=com_article&amp;access=bibcode&amp;Itemid=129&amp;bibcode=2006A%2526A...454..201GFUL</a><br /><br />&quot;Rotating Elephant Trunks&quot;, by G. F. Gahm1, P. Carlqvist2, L. E. B. Johansson3, and S. Nikolic<br /><br /><i>&quot;... As is evident from these images, as well as from a number of images obtained with the Hubble Space Telescope of similar objects, the trunks often have a very intriguing internal structure containing </i><b><i>long, thin filaments and knots</i></b><i> (see Gahm 2003). An interesting finding is that </i><b><i>many filaments show evidence of a twisted or helical structure.</i></b><i> The twisted geometry of the trunks is not accounted for in the previous theoretical models ...</i><br /><br /><i>In case the magnetic field lines inside the filament are not too twisted, part of the filament will attain the shape of a V with its apex pointing towards the centre of the H ii region. The condensation is situated at the apex while the upper parts of the V connect to the shell. In the case the magnetic field lines in the filament are twisted beyond a certain critical limit, a completely new situation will arise. </i><b><i>The two filamentary branches next to the apex are then twisted into a double helix so that the filamentary structure instead resembles a Y. The elephant trunk is identified with the double helix.</i></b><i> The mechanism for forming a double helix out of a magnetic rope was verified both from an analogy model (see Fig. 3 in CGK02) and by a numerical model (CGK03).</i><br /><br /><i>The discovery that the trunks are rotating, sheds new light on how many structures present in elephant trunks can arise. The trunks are continuously stretching and changing shape thereby </i><b><i>implying that electric currents are also generated, which together with the magnetic field play an important role in sculpturing the trunks</i></b><i> ...&quot;</i><br /><br /><b>2006-2012: A Plasma Experiment Used on 18 Missions of the International Space Station Naturally Forms Vortices and Structures that Resemble Galaxies</b><br /><br /><a href=\"http://www.plasma-universe.com/Galaxy_formation\" class=\"ot-anchor\">http://www.plasma-universe.com/Galaxy_formation</a><br /><br /><i>&quot;Experiments with the PK-3 Plus (Plasmakristall-3 Plus) dusty/complex plasmas laboratory on the International Space Station, has shown dusty plasmas in a weightless environment that seem to show &#39;vortices in the plasma resembling a galaxy&#39;, and a &#39;mini-galaxy [that] can be used to study formation of real galaxies&#39;.&quot;</i><br /><br />Partial, Russian-translated transcript from video at <a href=\"https://www.youtube.com/watch?v=kanYuBptuZ0\" class=\"ot-anchor\">https://www.youtube.com/watch?v=kanYuBptuZ0</a><br /><br /><i>&quot;Experiments performed in orbit are changing the way people interpret processes that are modelled on Earth.</i><br /><br /><i>I began this now known experiment, which was at first mostly thought of as a shot in the dark.  And we were completely amazed by the results.  Since I was a child I loved physics and astrophysics, but when I witnessed this new cosmic world being created, it was like seeing the physics itself.</i><br /><br /><i>Plasma crystal, an experiment with the fourth state of matter -- plasma.</i><br /><br /><i>This doesn&#39;t happen on Earth.</i><br /><br /><b><i>It appears that plasma isn&#39;t really a fluid in the physical sense.  Instead, it has a crystalline lattice-like structure.</i></b><br /><br /><i>It became apparent after experiments performed in zero gravity and changed views on the fourth state of matter.</i><br /><br /><i>Dust particles that are suspended within plasma get an electrical charge and experience repulsion from each other. </i><b><i>Because of that repulsion, their space is limited and a crystal lattice like structure is formed.</i></b><br /><br /><i>So, this black bucket ... In orbit the experiment is similar to the one on Earth, except that the vacuum chamber has real space vacuum in it.  A powerful computer is used to process the results.  Plasma is first created in this black bucket-like chamber, micron-sized dust particles are then allowed in.</i><br /><br /><i>This is a view from the inside of the chamber.</i><br /><br /><i>Here you can see the dust particles and beginning of the crystallization process.</i><br /><br /><i>Everything is recorded by video cameras.</i><br /><br /><i>Scientists on Earth will be getting the video from the chamber and will observe the crystallization process.  And everything else that&#39;s been happening in this black bucket.</i><br /><br /><b><i>The received data was completely unexpected.  Instead of a clean lattice, an empty void appeared in the middle.</i></b><i>  During the experiment we contacted the Earth guys, who couldn&#39;t believe it either.  Then one of the scientists from the Max Planck Institute told us that upon receiving the data he locked himself in his office for several weeks just so he could figure out what he saw.  And we got a lot of data just by observing the behavior of the particles.</i><br /><br /><b><i>Observing them is only possible in a weightless environment.  On Earth, gravity squeezes the crystals flat and therefore makes it impossible to study their structure.</i></b><br /><br /><i>This makes studying these aspects of plasma impossible as well.</i><br /><br /><i>In space, however, the crystals sort-of open up and form a 3D object.  </i><b><i>Vortices in the plasma resemble a galaxy.  This mini-galaxy can be used to study formation of real galaxies</i></b><i> ...&quot;</i><br /><br />List of related papers here:<br /><br /><a href=\"https://en.wikipedia.org/wiki/PK-3_Plus_%28ISS_Experiment%29\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/PK-3_Plus_%28ISS_Experiment%29</a><br /><br /><b>2007: More Detailed Observations of Nebulae Reveal that Nebulae are Oftentimes Complex Filamentary Networks</b><br /><br />&quot;Filamentary structures in planetary nebulae&quot;, Hanna Dahlgren, Per Carlqvist, and Gösta F. Gahm<br /><br /><i>&quot;... Planetary nebulae (PNe) have since long held a prominent place in the general literature featuring beautiful, celestial objects. With the Hubble Space Telescope (HST), and the first magnificent images of PNe obtained at higher spatial resolution than before, the beauty and complexity of these objects became even more apparent, and also rose a number of questions concerning the formation, maintenance and evolution of all the remarkable substructures that were discovered. </i><b><i>Many nebulae turn out to contain knots, jets, shells and interwoven filaments</i></b><i>, which in many cases are organized in global, symmetric patterns, like bipolar flows, around the central star ...</i><br /><br /><i>A number of HST images of various types of PNe can be found e.g. in Kwok (2001), where examples of PNe with </i><b><i>helical and sinusoidal subfilaments</i></b><i> are displayed. Kwok (2000) also discussed the origin and evolution of PNe, as well as various kinds of detected </i><b><i>microstructures</i></b><i>. Another comprehensive review of PNe was presented by Balick and Frank (2002) describing the wealth of information now available, </i><b><i>the theoretical challenge posed by the complex morphologies, and also pointing at the role the magnetic fields might have in shaping mass flows and jets. Filaments in PNe were first detected as a result of their low ionization compared to the rest of the nebula. However, they are not just found in the hot, ionized parts of the nebula, but also in the extended halos surrounding PNe. Close to, or in the shells surrounding the hot, ionized gas, one may find cold and dusty filaments</i></b><i>, with photo evaporating surfaces, like the </i><b><i>knots and filaments in the Helix Nebula</i></b><i> investigated by Meaburn et al. (1992). Such filaments have even been detected in radio molecular observations, showing the presence of CO (Huggins et al. 2002) ...&quot;</i><br /><br /><b>2008/2009: Themis Observes in Detail a Flux Rope from the Sun Pumping 650,000 Amperes Down Into the Arctic Auroral Region</b><br /><br /><a href=\"https://science.nasa.gov/science-news/science-at-nasa/2008/30oct_ftes/\" class=\"ot-anchor\">https://science.nasa.gov/science-news/science-at-nasa/2008/30oct_ftes/</a><br /><br /><i>&quot;Magnetic Portals Connect Earth to the Sun</i><br /><br /><i>Oct. 30, 2008: During the time it takes you to read this article, something will happen high overhead that until recently many scientists didn&#39;t believe in. </i><b><i>A magnetic portal will open, linking Earth to the sun 93 million miles away. Tons of high-energy particles may flow through the opening before it closes again</i></b><i>, around the time you reach the end of the page.</i><br /><br /><i>&#39;It&#39;s called a flux transfer event or &#39;FTE,&#39;&#39; says space physicist David Sibeck of the Goddard Space Flight Center. </i><b><i>&#39;Ten years ago I was pretty sure they didn&#39;t exist, but now the evidence is incontrovertible.&#39;</i></b><br /><br /><i>Indeed, today Sibeck is telling an international assembly of space physicists at the 2008 Plasma Workshop in Huntsville, Alabama, that FTEs are not just common, but possibly twice as common as anyone had ever imagined.</i><br /><br /><i>Researchers have long known that the Earth and sun must be connected. Earth&#39;s magnetosphere (the magnetic bubble that surrounds our planet) is filled with particles from the sun that arrive via the solar wind and penetrate the planet&#39;s magnetic defenses. They enter by following magnetic field lines that can be traced from terra firma all the way back to the sun&#39;s atmosphere.</i><br /><br /><i>&#39;We used to think the connection was permanent and that solar wind could trickle into the near-Earth environment anytime the wind was active,&#39; says Sibeck. &#39;We were wrong. </i><b><i>The connections are not steady at all. They are often brief, bursty and very dynamic.&#39;</i></b><br /><br /><i>Several speakers at the Workshop have outlined how FTEs form: On the dayside of Earth (the side closest to the sun), Earth&#39;s magnetic field presses against the sun&#39;s magnetic field. </i><b><i>Approximately every eight minutes, the two fields briefly merge or &#39;reconnect,&#39; forming a portal through which particles can flow. The portal takes the form of a magnetic cylinder about as wide as Earth.</i></b><i> The European Space Agency&#39;s fleet of four Cluster spacecraft and NASA&#39;s five THEMIS probes have flown through and surrounded these cylinders, measuring their dimensions and sensing the particles that shoot through. </i><b><i>&#39;They&#39;re real,&#39;</i></b><i> says Sibeck ...&quot;</i><br /><br /><b>2009: NASA Launches Kepler, Which Identifies 5,000 Potential Exoplanets - Which Collectively Raise Questions About How Planets and Stars Form</b><br /><br /><a href=\"http://www.nature.com/news/astronomy-planets-in-chaos-1.15480\" class=\"ot-anchor\">http://www.nature.com/news/astronomy-planets-in-chaos-1.15480</a><br /><br /><i>&quot;By 2000, astronomers had found 30 exoplanets; by the end of 2008, 330. Then NASA launched Kepler, which spent the next four years searching for exoplanets in a single patch of sky containing some 150,000 Sun-like stars. Kepler identifies planets by detecting the slight dimming in a star&#39;s light that occurs when an object passes in front of it. This &#39;transit&#39; method can find planets much smaller than the radial-velocity technique can, giving astronomers a chance to detect other Earths. Kepler has now found 974 exoplanets, with 4,254 further candidates waiting for confirmation by ground-based measurements. If all of Kepler&#39;s candidates are confirmed — and they do tend to be — then the techniques taken together will have found well over 5,000 exoplanets.&quot;</i><br /><br /><b>2013: Nebulae Assumed to Be Disconnected Exhibit Alignment</b><br /><br />(Alignment can suggest electrical interconnection)<br /><br /><a href=\"http://www.seeker.com/mystery-alignment-of-planetary-nebulae-discovered-1767784004.html\" class=\"ot-anchor\">http://www.seeker.com/mystery-alignment-of-planetary-nebulae-discovered-1767784004.html</a><br /><br /><i>&quot;Mystery Alignment of &#39;Butterfly&#39; Nebulae Discovered</i><br /><br /><i>SEP 4, 2013 09:00 AM ET // BY IAN O&#39;NEILL</i><br /><br /><b><i>Astronomers have discovered something weird in the Milky Way&#39;s galactic bulge -- a population of planetary nebula are all mysteriously pointing in the same direction.</i></b><br /><br /><i>While using the Hubble Space Telescope and the European Southern Observatory&#39;s New Technology Telescope (NTT) to survey 130 planetary nebulae situated near the hub of our galaxy, astronomers from the University of Manchester sorted them into three populations based on their shape: &#39;elliptical,&#39; &#39;either with or without an aligned internal structure&#39; and &#39;bipolar.&#39;</i><br /><br /><b><i>They noticed the mysterious alignment in the long axes of bipolar planetary nebulae.</i></b><br /><br /><i>[...]</i><br /><br /><b><i>Although the surveyed nebulae are completely separate, non-interacting and are of various ages, the researchers noticed a large number of the nebulae long axes are aligned.</i></b><br /><br /><i>&#39;This really is a surprising find and, if it holds true, a very important one,&#39; said Bryan Rees of the University of Manchester, co-author of the paper to appear in the journal Monthly Notices of the Royal Astronomical Society. &#39;Many of these ghostly butterflies appear to have their long axes aligned along the plane of our galaxy. By using images from both Hubble and the NTT we could get a really good view of these objects, so we could study them in great detail.&#39;</i><br /><br /><i>The other two populations of planetary nebulae appear to be randomly oriented in relation to the galactic disk.</i><br /><br /><i>&#39;While any alignment at all is a surprise, </i><b><i>to have it in the crowded central region of the galaxy is even more unexpected</i></b><i>,&#39; said the paper&#39;s second author Albert Zijlstra, also of the University of Manchester, in Wednesday&#39;s Hubble press release.</i><br /><br /><i>[...]</i><br /><br /><i>&#39;The alignment we&#39;re seeing for these bipolar nebulae indicates something bizarre about star systems within the central bulge,&#39; said Rees. &#39;For them to line up in the way we see, the star systems that formed these nebulae would have to be rotating perpendicular to the interstellar clouds from which they formed, which is very strange.&#39;</i><br /><br /><i>Interestingly, bipolar planetary nebulae do not appear to have a preferential orientation in our galactic neighborhood many thousands of light-years from the galactic core. The alignment effect only seems to act near the center of the Milky Way.</i><br /><br /><b><i>The researchers suspect that powerful magnetic fields emanating from the galactic core</i></b><i> as it formed during the evolution of our galaxy may be influencing the direction of the nebulae -- akin to a magnetic field forcing the direction of compass needles ...&quot;</i><br /><br /><b>2014: Cassini Spacecraft Zapped By 1,200 Mile &#39;Electrostatic Shock&#39;</b><br /><br />(Note that these observations were not published for 9 years from date of observation)<br /><br /><a href=\"http://www.jpl.nasa.gov/news/news.php?feature=4340\" class=\"ot-anchor\">http://www.jpl.nasa.gov/news/news.php?feature=4340</a><br /><br /><i>&quot;Cassini Caught in Hyperion&#39;s Particle Beam</i><br /><br /><i>Static electricity is known to play an important role on Earth&#39;s airless, dusty moon, but evidence of static charge building up on other objects in the solar system has been elusive until now. </i><b><i>A new analysis of data from NASA&#39;s Cassini mission has revealed that, during a 2005 flyby of Saturn&#39;s moon Hyperion, the spacecraft was briefly bathed in a beam of electrons coming from the moon&#39;s electrostatically charged surface.</i></b><br /><br /><i>The finding represents the first confirmed detection of a charged surface on an object other than our moon, although it is predicted to occur on many different bodies, including asteroids and comets.</i><br /><br /><i>The new analysis was led by Tom Nordheim, a doctoral candidate at Mullard Space Science Laboratory (MSSL), University College London, and was published recently in the journal Geophysical Research Letters.</i><br /><br /><i>Hyperion is porous and icy, with a bizarre, sponge-like appearance. Its surface is continuously bombarded by ultraviolet light from the sun and exposed to a rain of charged particles -- electrons and ions -- within the invisible bubble generated by Saturn&#39;s magnetic field, called the magnetosphere. The researchers think Hyperion&#39;s exposure to this hostile space environment is the source of the particle beam that struck Cassini.</i><br /><br /><i>Measurements made by several of Cassini&#39;s instruments during a close encounter with Hyperion on September 26, 2005, indicate that something unexpected took place in the charged particle environment around the spacecraft. Among those instruments, </i><b><i>the Cassini Plasma Spectrometer (CAPS) detected that the spacecraft was magnetically connected to the surface of Hyperion for a brief period, allowing electrons to escape from the moon toward the robotic probe.</i></b><br /><br /><i>Most people are familiar with the electrostatic charge buildup that occurs when a balloon is rubbed against hair or a sweater. Objects in space can also become electrostatically charged by exposure to solar ultraviolet light and incoming charged particles. The Cassini data show that a similar process can take place on Hyperion.</i><br /><br /><i>The finding is surprising, as the small but odd-looking moon was thought to be a simple inert object, which would not undergo any strong interactions with the Saturnian magnetosphere. Nevertheless, the team&#39;s analysis indicates that </i><b><i>Cassini remotely detected a strongly negative voltage on Hyperion. &#39;It was rather like Cassini receiving a 200-volt electric shock from Hyperion, even though they were over 2,000 kilometers [1,200 miles] apart at the time,&#39; said Nordheim.</i></b><br /><br /><i>[...]</i><br /><br /><i>&#39;Our observations show that this is also an important effect at outer planet moons and that we need to take this into account when studying how these moons interact with their environment,&#39; said Geraint Jones of MSSL, a member of the Cassini CAPS team who helped supervise the study.&quot;</i><br /><br /><b>2015: The Herschel Telescope Vindicates Alfven&#39;s Plasma Universe</b><br /><br />(Possibly one of the most important articles ever written on the subject of electricity in space, the following piece provides a possible glimpse at what a plasma universe actually looks like.  It seems likely that further observations will confirm that these filaments are Birkeland currents ...)<br /><br /><a href=\"http://sci.esa.int/herschel/55942-herschels-hunt-for-filaments-in-the-milky-way/\" class=\"ot-anchor\">http://sci.esa.int/herschel/55942-herschels-hunt-for-filaments-in-the-milky-way/</a><br /><br /><i>&quot;Herschel&#39;s Hunt for Filaments in the Milky Way</i><br /><br /><i>28 May 2015</i><br /><br /><b><i>Observations with ESA&#39;s Herschel space observatory have revealed that our Galaxy is threaded with filamentary structures on every length scale. From nearby clouds hosting tangles of filaments a few light-years long to gigantic structures stretching hundreds of light-years across the Milky Way&#39;s spiral arms, they appear to be truly ubiquitous.</i></b><i> The Herschel data have rekindled the interest of astronomers in studying filaments, emphasising the crucial role of these structures in the process of star formation.</i><br /><br /><i>Stars are born in the densest pockets of the interstellar medium, a diffuse mixture of gas and dust that pervades galaxies, including our Milky Way. One of the most intriguing questions in astrophysics concerns understanding how this material, which is typically characterised by very low density, can come together, creating denser concentrations that later evolve into compact cores and, finally, give birth to stars.</i><br /><br /><i>In the search for answers, astronomers observe giant molecular clouds, the cosmic incubators where gas and dust are transformed into stars. While these studies are performed using a variety of techniques, </i><b><i>one crucial approach is the observation of infrared light, since the interstellar material shines brightly at these long wavelengths.</i></b><br /><br /><i>In this context, ESA&#39;s Herschel space observatory has been a true game changer. </i><b><i>Probing the portion of the electromagnetic spectrum that ranges from the far-infrared to sub-millimetre wavelengths, it has collected unprecedented data during its three and a half years of observing. One of the key aspects that emerged from these observations is the presence of a filamentary network nearly everywhere in our Galaxy&#39;s interstellar medium. The picture that is emerging is that these structures are closely linked to the formation of stars.</i></b><br /><br /><i>Prior to Herschel, astronomers had already identified several filaments in interstellar clouds and recognised their potential importance for star formation. However, </i><b><i>only with the increased sensitivity and spatial resolution granted by this observatory, combined with its large-scale surveys, could they reveal the full extent of filamentary patterns in the Milky Way.</i></b><br /><br /><i>One of the surveys performed with Herschel – the Gould Belt Survey – focussed on a giant ring of star-forming regions, all located no more than 1500 light-years away from the Sun. The vicinity of these clouds allowed astronomers to obtain exceptionally detailed images using Herschel, unearthing intricate webs of filaments in each region that they examined.</i><br /><br /><b><i>&#39;The greatest surprise was the ubiquity of filaments in these nearby clouds and their intimate connection with star formation,&#39;</i></b><i> explains Philippe André from CEA/IRFU, France, Principal Investigator for the Herschel Gould Belt Survey.</i><br /><br /><i>&#39;But there is more: </i><b><i>these observations revealed that filaments, which may extend to several light-years in length, appear to have a universal width of about one third of a light year. This suggests that something fundamental is lurking underneath.&#39;&quot;</i></b><br /><br />Wal Thornhill remarks on this point at <a href=\"http://www.holoscience.com/wp/alfven-triumphs-again-again/\" class=\"ot-anchor\">http://www.holoscience.com/wp/alfven-triumphs-again-again/</a>:<br /><br /><i>&quot;The constant width over vast distances is due to the current flowing along the Birkeland filaments, each filament constituting a part of a larger electric circuit. And in a circuit the current must be the same in the whole filament although the current density can vary in the filament due to the electromagnetic pinch effect. Therefore the electromagnetic scavenging effect on matter from the molecular cloud, called Marklund convection, is constant along each current filament, which simply explains the consistency of widths of the filaments.&quot;</i><br /><br />Continuing with the Herschel article ...<br /><br /><i>&quot;The astronomers are still trying to understand the details of the star formation processes taking place in these clouds, aided by the abundance and variety of data collected with Herschel.</i><br /><br /><b><i>While most filaments are dotted with compact cores, suggesting that stars are readily taking shape in these dense &#39;fibres&#39; of the interstellar medium, there are also regions that exhibit complex tangles of filaments but no signs of on-going star formation. A study of the most spectacular example of this phenomenon, the Polaris Flare, indicates that filaments must somehow precede the onset of star formation.</i></b><br /><br /><i>The scenario that has emerged from the new Herschel data suggests that star formation proceeds in two steps: first, turbulent motions of the interstellar gas and dust create an intricate web of filamentary structures; then, gravity takes over, causing only the densest filaments to contract and fragment, eventually leading to the formation of stars.</i><br /><br /><i>Indeed, the universal width of filaments seems to correspond, at least in the nearby clouds of the Gould Belt Survey, to the scale at which interstellar material undergoes the transition from supersonic to subsonic state.</i><br /><br /><i>In addition, the material along filaments is not at all static: </i><b><i>astronomers have detected what appear to be accretion flows, with the most prominent filaments drawing matter from their surroundings through a network of smaller filaments. A striking example of such processes is seen in the Taurus Molecular Cloud, where the B211/B213 filament exhibits a series of so-called &#39;striations&#39; perpendicular to the main filament.</i></b><br /><br /><i>This pattern is very similar to that predicted from numerical simulations that model the process of star formation in molecular clouds. According to these simulations, interstellar material flows towards dense filaments along routes that are parallel to the direction of the local magnetic field, as was observed, so the new data indicate the importance of interstellar magnetic fields in shaping these structures.&quot;</i><br /><br />(Note that the author admits the need for magnetic fields, but makes no specific mention of the circumstances which actually create the magnetic fields.  It should be clear that any pre-existing magnetic fields would be dominated by the ones that are electrodynamically created by the moving charges, in the same way that moving a magnet next to some iron filings can be expected to undo the magnetic field which the iron filings were formerly creating.)<br /><br /><i>&quot;However, star formation does not appear to take place only in filaments. </i><b><i>While these structures seem to be the preferred sites for stellar birth, the extraordinary data from Herschel confirmed that a small fraction of stars may also form far away from dense filaments.</i></b><br /><br /><i>In particular, a detailed study of the L1641 molecular clouds in the Orion A complex suggests that </i><b><i>star formation along filaments is the preferential channel to produce typical solar-type stars, while stars that are born away from these dense, elongated structures tend to have lower masses.</i></b><i> This dichotomy could be a result of the greater availability of raw material to protostars that are forming on a filament compared to those that take shape in less dense environments.</i><br /><br /><i>Another of Herschel&#39;s key findings is that the presence and abundance of filaments are not limited to our immediate neighbourhood. In fact, these structures appear everywhere also in the Herschel infrared Galactic Plane Survey (Hi-GAL), which scanned the distribution of the interstellar medium in the huge disc – about 100 000 light-years across – where most of the Milky Way&#39;s stars form and reside.</i><br /><br /><b><i>&#39;We detected a wealth of huge filaments, with lengths ranging from a few to a hundred light-years, revealing what seems to be the &#39;skeleton&#39; of our Galaxy,&#39;</i></b><i> explains Sergio Molinari from IAPS/INAF, Italy, Principal Investigator for the Hi-GAL Project.</i><br /><br /><i>&#39;While it is possible that these structures arose from different physical processes than those giving rise to the small-scale filaments observed in the Sun&#39;s vicinity, </i><b><i>the omnipresent aspect of filamentary structures in the Milky Way is beyond doubt.&#39;</i></b><br /><br /><i>In the post-Herschel era, </i><b><i>one thing is certain: filaments play a leading role in the build-up of galactic material, creating favourable hubs for the formation of stars. This is likely a hierarchical process, starting on very large scales and propagating onwards, to smaller and smaller scales, funnelling interstellar gas and dust into increasingly denser concentrations and thus fostering stellar birth across the Galaxy.&quot;</i></b><br /><br />(Some advocates for a more electrical cosmology, like Anthony Peratt, have argued that plasmas appear to &quot;scale&quot; over enormous magnitudes; in other words, they suggest that plasmas have a fractal quality to them which self-repeats at most or even all scales of the universe.)<br /><br /><i>&quot;Large-scale filaments fragmenting into compact cores that later evolve into stars have been detected all across the Galactic Plane, even in its outermost, peripheral regions. As filaments grow more massive, the material within them contracts and forms smaller structures, </i><b><i>preserving the filamentary pattern on all length scales.</i></b><br /><br /><i>Further investigation of the Hi-GAL survey has revealed new and even more prominent filaments, </i><b><i>extending over hundreds of light-years and weaving their way through the spiral arms of the Milky Way.</i></b><i> The study revealed </i><b><i>nine filaments</i></b><i> in some very dense, inner regions of the Galactic Plane, detecting these for the first time through the direct emission of dust within them, allowing an accurate determination of their mass, size and physical characteristics. </i><b><i>Astronomers believe that almost a hundred similar, gigantic structures are still hiding in the data.</i></b><br /><br /><i>&#39;The intricate distribution of filaments in the interstellar medium revealed by Herschel has definitely revolutionised our view of how stars form in the Milky Way and, presumably, also in other similar galaxies,&#39; comments Göran Pilbratt, ESA Herschel Project Scientist.</i><br /><br /><i>&#39;An increasingly coherent picture is now emerging from combining the analysis of these data with predictions from theory and numerical simulations, as astronomers continue to study the physical processes underlying the fascinating origin of stars and planets.&#39;&quot;</i><br /><br /><b>2016: A Young, Forming Star is Photographed in Detail</b><br /><br />I recommend caution in accepting the theorists&#39; interpretations -- and in particular the bit about &quot;spiral density waves&quot; ...<br /><br /><a href=\"http://phys.org/news/2016-09-spiral-arms-embrace-young-star.html\" class=\"ot-anchor\">http://phys.org/news/2016-09-spiral-arms-embrace-young-star.html</a><br /><br /><i>&quot;Spiral arms: Protoplanetary disk around a young star exhibits spiral structure</i><br /><br /><i>September 29, 2016</i><br /><br /><i>Astronomers have found a distinct structure involving spiral arms in the reservoir of gas and dust disk surrounding the young star Elias 2-27. While spiral features have been observed on the surfaces of protoplanetary disks, these new observations, from the ALMA observatory in Chile, are the first to reveal that such spirals occur at the disk midplane, the region where planet formation takes place.</i><br /><br /><i>[...]</i><br /><br /><i>An international team led by Laura Pérez, an Alexander von Humboldt Research Fellow from the Max Planck Institute for Radio Astronomy (MPIfR) in Bonn, also including researchers from the Max Planck Institute for Astronomy (MPIA) in Heidelberg, has obtained the first image of a spiral structure seen in thermal dust emission coming from a protoplanetary disk</i><br /><br /><i>[...]</i><br /><br /><i>The two sweeping spiral arms in Elias 2-27 extend more than 10 billion kilometers away from the newborn star, at a larger distance than the location of the Kuiper Belt in our Solar System. &#39;The presence of spiral density waves at these extreme distances may help explain puzzling observations of extrasolar planets at similar far-away locations&#39;, Pérez notes, &#39;such planets cannot form in-situ under our standard picture of planet formation ...&#39;&quot;</i><br /><br /><b>October 2018: Launch of James Webb Telescope</b><br /><br />Stay tuned!  This debate will surely heat up in the coming years with the launch of the JWST ...<br /><br /><a href=\"http://jwst.nasa.gov/birth.html\" class=\"ot-anchor\">http://jwst.nasa.gov/birth.html</a><br /><br /><i>&quot;The Dusty Cocoons of Star and Planet Formation</i><br /><br /><b><i>JWST&#39;s amazing imaging and spectroscopy capabilities will allow us to study stars as they are forming in their dusty cocoons. Additionally, it will be able to image disks of heated material around these young stars, which can indicate the beginnings of planetary systems</i></b><i>, and study organic molecules that are important for life to develop.</i><br /><br /><i>Key Questions</i><br /><br /><i>JWST will address several key questions to help us unravel the story of the star and planet formation:</i><br /><br /><i>1. </i><b><i>How do clouds of gas and dust collapse to form stars?</i></b><br /><br /><i>2. </i><b><i>Why do most stars form in groups?</i></b><br /><br /><i>3. </i><b><i>Exactly how do planetary systems form?</i></b><br /><br /><i>4. How do stars evolve and release the heavy elements they produce back into space for recycling into new generations of stars and planets?</i><br /><br /><i>JWST&#39;s Role in Answering These Questions</i><br /><br /><b><i>To unravel the birth and early evolution of stars and planets, we need to be able to peer into the hearts of dense and dusty cloud cores where star formation begins. These regions cannot be observed at visible light wavelengths as the dust would make such regions opaque and must be observed at infrared wavelengths.&quot;</i></b><br /><br />GRAPHIC REFERENCES:<br /><br />[1] <a href=\"https://www.thunderbolts.info/tpod/2004/arch/040727stellar-nurseries.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2004/arch/040727stellar-nurseries.htm</a><br /><br />[2] <i>Physics of the Plasma Universe</i>, 2nd Ed., Anthony Peratt<br /><br />[3] <a href=\"http://www.ptep-online.com/index_files/2015/PP-41-13.PDF\" class=\"ot-anchor\">http://www.ptep-online.com/index_files/2015/PP-41-13.PDF</a><br /><br />[4]-[6] <i>Physics of the Plasma Universe</i>, 2nd Ed., Anthony Peratt<br /><br />[7] <a href=\"https://books.google.com/books?id=ZSlJRAeL95sC&amp;pg=PA57&amp;lpg=PA57&amp;dq=%22Among+the+earliest+predictions+about+the+morphology+of+the+universe+that+differed+significantly+from+the+common+astrophysical+assumption+is+that+the+universe+be+filamentary%22&amp;source=bl&amp;ots=jqFFFCjw9q&amp;sig=9iZWC3zm3EtT_TZhcKJFdwEHERk&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiGy7Lg3tHPAhVS3GMKHYIwB9oQ6AEIHDAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=ZSlJRAeL95sC&amp;pg=PA57&amp;lpg=PA57&amp;dq=%22Among+the+earliest+predictions+about+the+morphology+of+the+universe+that+differed+significantly+from+the+common+astrophysical+assumption+is+that+the+universe+be+filamentary%22&amp;source=bl&amp;ots=jqFFFCjw9q&amp;sig=9iZWC3zm3EtT_TZhcKJFdwEHERk&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiGy7Lg3tHPAhVS3GMKHYIwB9oQ6AEIHDAA#v=onepage&amp;q&amp;f=false</a><br /><br />[8][9] <a href=\"http://www.ptep-online.com/index_files/2015/PP-41-13.PDF\" class=\"ot-anchor\">http://www.ptep-online.com/index_files/2015/PP-41-13.PDF</a><br /><br />[10] <a href=\"http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?1969CoASP...1..117W&amp;defaultprint=YES&amp;filetype=.pdf\" class=\"ot-anchor\">http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?1969CoASP...1..117W&amp;defaultprint=YES&amp;filetype=.pdf</a><br /><br />[11] <a href=\"https://archive.org/details/nasa_techdoc_19720019068\" class=\"ot-anchor\">https://archive.org/details/nasa_techdoc_19720019068</a><br /><br />[12] <a href=\"https://books.google.com/books?id=ZSlJRAeL95sC&amp;pg=PA60&amp;lpg=PA60&amp;dq=The+discovery+of+these+Birkeland+currents+in+the+earths+magnetosphere+in+1974+%28Dessler,+1984%29+has+resulted+in+the+realization+that+the+structure+in+auroral+draperies&amp;source=bl&amp;ots=jqFFICiwbm&amp;sig=4bluQxxA2ejeTu6fxTB6WB4iK5g&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwimhq7WuNrPAhUL5WMKHe59AGgQ6AEIHjAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=ZSlJRAeL95sC&amp;pg=PA60&amp;lpg=PA60&amp;dq=The+discovery+of+these+Birkeland+currents+in+the+earths+magnetosphere+in+1974+(Dessler,+1984)+has+resulted+in+the+realization+that+the+structure+in+auroral+draperies&amp;source=bl&amp;ots=jqFFICiwbm&amp;sig=4bluQxxA2ejeTu6fxTB6WB4iK5g&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwimhq7WuNrPAhUL5WMKHe59AGgQ6AEIHjAA#v=onepage&amp;q&amp;f=false</a><br /><br />[13] <a href=\"http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19770006016.pdf\" class=\"ot-anchor\">http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19770006016.pdf</a><br /><br />[14] <a href=\"https://books.google.com/books?id=wt2CBAAAQBAJ&amp;pg=PA44&amp;dq=%22The+location,+flow+direction+and+intensity+of+Birkeland+currents+have+been+studied+by+several+satellites,+and+a+statistical+map%22&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjSuvaCvdrPAhVCwlQKHc5vD0MQ6AEIHjAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=wt2CBAAAQBAJ&amp;pg=PA44&amp;dq=%22The+location,+flow+direction+and+intensity+of+Birkeland+currents+have+been+studied+by+several+satellites,+and+a+statistical+map%22&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjSuvaCvdrPAhVCwlQKHc5vD0MQ6AEIHjAA#v=onepage&amp;q&amp;f=false</a><br /><br />[15] <a href=\"https://www.researchgate.net/publication/265006614_Global_Magnetospheric_Plasma_Convection\" class=\"ot-anchor\">https://www.researchgate.net/publication/265006614_Global_Magnetospheric_Plasma_Convection</a><br /><br />[16] <a href=\"http://www.holoscience.com/wp/newtons-electric-clockwork-solar-system/\" class=\"ot-anchor\">http://www.holoscience.com/wp/newtons-electric-clockwork-solar-system/</a><br /><br />[17] <a href=\"https://books.google.com/books?id=gAnvCAAAQBAJ&amp;pg=PA26&amp;lpg=PA26&amp;dq=%22During+quiescent+conditions,+the+flux+ropes+are+observed+to+be+a+nearly+ubiquitous+feature+of+the+dayside+ionosphere%22&amp;source=bl&amp;ots=bXhpPQp6L1&amp;sig=Ltzx8pcoGnjNC8KnlBAvg5qfmFA&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiTo_y-wdrPAhVKwmMKHSXVCY4Q6AEIHDAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=gAnvCAAAQBAJ&amp;pg=PA26&amp;lpg=PA26&amp;dq=%22During+quiescent+conditions,+the+flux+ropes+are+observed+to+be+a+nearly+ubiquitous+feature+of+the+dayside+ionosphere%22&amp;source=bl&amp;ots=bXhpPQp6L1&amp;sig=Ltzx8pcoGnjNC8KnlBAvg5qfmFA&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiTo_y-wdrPAhVKwmMKHSXVCY4Q6AEIHDAA#v=onepage&amp;q&amp;f=false</a><br /><br />[18] <a href=\"https://www.newscientist.com/article/mg15420842.900-science--planets-tail-of-the-unexpected/\" class=\"ot-anchor\">https://www.newscientist.com/article/mg15420842.900-science--planets-tail-of-the-unexpected/</a><br /><br />[19] <a href=\"http://www-pw.physics.uiowa.edu/~dag/publications/1977_APlasmaWaveInvestigationForTheVoyagerMission_SSR.pdf\" class=\"ot-anchor\">http://www-pw.physics.uiowa.edu/~dag/publications/1977_APlasmaWaveInvestigationForTheVoyagerMission_SSR.pdf</a><br /><br />[20] <a href=\"https://ase.tufts.edu/cosmos/view_picture.asp?id=1174\" class=\"ot-anchor\">https://ase.tufts.edu/cosmos/view_picture.asp?id=1174</a><br /><br />[21] <a href=\"http://adsabs.harvard.edu/full/1996Ap%26SS.244...89P\" class=\"ot-anchor\">http://adsabs.harvard.edu/full/1996Ap%26SS.244...89P</a><br /><br />[22][23] <a href=\"https://www.plasma-universe.com/Galaxy_formation\" class=\"ot-anchor\">https://www.plasma-universe.com/Galaxy_formation</a><br /><br />[24] <a href=\"http://www.thunderbolts.info/thunderblogs/archives/solar08/062308_milky_way_birkeland_current.htm\" class=\"ot-anchor\">http://www.thunderbolts.info/thunderblogs/archives/solar08/062308_milky_way_birkeland_current.htm</a><br /><br />[25] <a href=\"http://www.annualreviews.org/doi/abs/10.1146/annurev.aa.22.090184.001255\" class=\"ot-anchor\">http://www.annualreviews.org/doi/abs/10.1146/annurev.aa.22.090184.001255</a><br /><br />[26] <a href=\"http://chandra.harvard.edu/photo/2005/sgra/index.html\" class=\"ot-anchor\">http://chandra.harvard.edu/photo/2005/sgra/index.html</a><br /><br />[27] <a href=\"https://www.cambridge.org/core/services/aop-cambridge-core/content/view/S0074180900087647\" class=\"ot-anchor\">https://www.cambridge.org/core/services/aop-cambridge-core/content/view/S0074180900087647</a><br /><br />[28] <a href=\"https://www.researchgate.net/publication/3167411_Magnetosphere-Ionosphere_Interactions-Near-Earth_Manifestations_of_the_Plasma_Universe\" class=\"ot-anchor\">https://www.researchgate.net/publication/3167411_Magnetosphere-Ionosphere_Interactions-Near-Earth_Manifestations_of_the_Plasma_Universe</a><br /><br />[29] <a href=\"http://www.ptep-online.com/index_files/2015/PP-41-13.PDF\" class=\"ot-anchor\">http://www.ptep-online.com/index_files/2015/PP-41-13.PDF</a><br /><br />[30] <a href=\"http://adsabs.harvard.edu/full/1990ApJ...361L..19Y\" class=\"ot-anchor\">http://adsabs.harvard.edu/full/1990ApJ...361L..19Y</a><br /><br />[31] <a href=\"http://www.annualreviews.org/doi/abs/10.1146/annurev.astro.40.060401.093849?journalCode=astro\" class=\"ot-anchor\">http://www.annualreviews.org/doi/abs/10.1146/annurev.astro.40.060401.093849?journalCode=astro</a><br /><br />[32] <a href=\"http://adsabs.harvard.edu/full/1995Ap%26SS.227..187V\" class=\"ot-anchor\">http://adsabs.harvard.edu/full/1995Ap%26SS.227..187V</a><br /><br />[33] <a href=\"http://www.nature.com/polopoly_fs/1.15480!/menu/main/topColumns/topLeftColumn/pdf/511022a.pdf\" class=\"ot-anchor\">http://www.nature.com/polopoly_fs/1.15480!/menu/main/topColumns/topLeftColumn/pdf/511022a.pdf</a><br /><br />[34] <a href=\"http://iopscience.iop.org/article/10.1086/429381/pdf\" class=\"ot-anchor\">http://iopscience.iop.org/article/10.1086/429381/pdf</a><br /><br />[35][36] <a href=\"https://www.thunderbolts.info/wp/2013/12/17/electromagnetic-nebulae-3/\" class=\"ot-anchor\">https://www.thunderbolts.info/wp/2013/12/17/electromagnetic-nebulae-3/</a><br /><br />[37] <a href=\"http://www.aanda.org/index.php?option=com_article&amp;access=bibcode&amp;Itemid=129&amp;bibcode=2006A%2526A...454..201GFUL\" class=\"ot-anchor\">http://www.aanda.org/index.php?option=com_article&amp;access=bibcode&amp;Itemid=129&amp;bibcode=2006A%2526A...454..201GFUL</a><br /><br />[38] <a href=\"https://www.youtube.com/watch?v=kanYuBptuZ0&amp;spfreload=10\" class=\"ot-anchor\">https://www.youtube.com/watch?v=kanYuBptuZ0&amp;spfreload=10</a><br /><br />[39] <a href=\"https://www.academia.edu/28092540/Filamentary_structures_in_planetary_nebulae?auto=download\" class=\"ot-anchor\">https://www.academia.edu/28092540/Filamentary_structures_in_planetary_nebulae?auto=download</a><br /><br />[40] <a href=\"http://science.nasa.gov/science-news/science-at-nasa/2008/30oct_ftes/\" class=\"ot-anchor\">http://science.nasa.gov/science-news/science-at-nasa/2008/30oct_ftes/</a><br /><br />[41] <a href=\"http://www.nature.com/polopoly_fs/1.15480!/menu/main/topColumns/topLeftColumn/pdf/511022a.pdf\" class=\"ot-anchor\">http://www.nature.com/polopoly_fs/1.15480!/menu/main/topColumns/topLeftColumn/pdf/511022a.pdf</a><br /><br />[42] <a href=\"http://news.discovery.com/space/astronomy/weird-planetary-nebula-alignment-discovery-hubble-eso-130904.htm\" class=\"ot-anchor\">http://news.discovery.com/space/astronomy/weird-planetary-nebula-alignment-discovery-hubble-eso-130904.htm</a><br /><br />[43] <a href=\"http://www.jpl.nasa.gov/news/news.php?feature=4340\" class=\"ot-anchor\">http://www.jpl.nasa.gov/news/news.php?feature=4340</a><br /><br />[44] <a href=\"http://sci.esa.int/herschel/55942-herschels-hunt-for-filaments-in-the-milky-way/\" class=\"ot-anchor\">http://sci.esa.int/herschel/55942-herschels-hunt-for-filaments-in-the-milky-way/</a><br /><br />[45] <a href=\"http://www.aanda.org/articles/aa/pdf/2011/05/aa16596-11.pdf\" class=\"ot-anchor\">http://www.aanda.org/articles/aa/pdf/2011/05/aa16596-11.pdf</a><br /><br />[46] Appendix of <i>The Electric Sky</i>, by Don Scott<br /><br />[47] <a href=\"https://public.nrao.edu/news/pressreleases/2016-alma-elias-spiral\" class=\"ot-anchor\">https://public.nrao.edu/news/pressreleases/2016-alma-elias-spiral</a><br /><br />[48] <a href=\"https://arxiv.org/pdf/1312.6232.pdf\" class=\"ot-anchor\">https://arxiv.org/pdf/1312.6232.pdf</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Bias of a Visible Universe","summary":"\"For Millenia, Man has been Priveleged to Observe the Universe Only in the Visible / Thus, in the Development of Astronomy and Astrophysics, What Could be Seen in a Very Narrow Region ... Came to Dominate the Interpretation of the Observed Data ... / While Radio Astronomy Became Important in the 1950s, It had Only Minimal Impact on Already Well-developed World Models / And in Fact, Some of the Data Could be Claimed to Support Existing Models / Not Until the 1980s Did Much of the Electromagnetic Spectrum Become Known (or was Even Suspected to Exist)\" - Anthony Peratt","image":"https://lh3.googleusercontent.com/-mihUmtkaPkg/WAGfvxgAARI/AAAAAAAAI64/fBo-CwBWVBMpoKJAIla__sCQPAMcOuRaQCJoC/w3600-h5040/the-bias-of-a-visible-universe-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/CiNrxj9Noya","publishDate":"2016-10-15T03:19:18.809Z","updateDate":"2017-01-03T01:56:27.395Z","text":"<b>The Bias of a Visible Universe: &quot;For Millenia, Man has been Priveleged to Observe the Universe Only in the Visible / Thus, in the Development of Astronomy and Astrophysics, What Could be Seen in a Very Narrow Region ... Came to Dominate the Interpretation of the Observed Data ... / While Radio Astronomy Became Important in the 1950s, It had Only Minimal Impact on Already Well-developed World Models / And in Fact, Some of the Data Could be Claimed to Support Existing Models / Not Until the 1980s Did Much of the Electromagnetic Spectrum Become Known (or was Even Suspected to Exist)&quot; - Anthony Peratt</b><br /><br />For more information on the controversy over the universe&#39;s dominant state of observable matter, plasma, see ...<br /><br /><b>The Empty Vacuum of Space Mistake</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/QofRTvS6B5b\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/QofRTvS6B5b</a><br /><br />Here&#39;s the original true-to-form quote ...<br /><br /><a href=\"http://plasmauniverse.info/downloads/AdvancesII.pdf\" class=\"ot-anchor\">http://plasmauniverse.info/downloads/AdvancesII.pdf</a><br /><br /><i>&quot;The Bias of a Visible Universe</i><br /><br /><i>For millenia man has been priveleged to observe the universe only in the visible. Thus, in the development of astronomy and astrophysics, what could be seen in a very narrow region, approximately 4000-7000 Angstroms, came to dominate the interpretation of the observed data (Figure 1). </i><b><i>While radio astronomy became important in the 1950s, it had only minimal impact on already well-developed world models</i></b><i> and in fact, some of the data could be claimed to support existing models. </i><b><i>Not until the 1980s did much of the electromagnetic spectrum become known (or was even suspected to exist).&quot;</i></b><br /><br />The graph is in the same paper.<br /><br /><a href=\"https://vimeo.com/45744723\" class=\"ot-anchor\">https://vimeo.com/45744723</a><br />(transcript)<br /><br />Mel Acheson - The New Light<br /><br /><i>&quot;When I was young and stupid and studying astronomy, I was warned not to get mixed up in philosophy.  It was a waste of time, and it would ruin me as a scientist.  Richard Feynman said philosophers are always on the outside making stupid comments.  Well, I didn&#39;t listen, and now I&#39;m old and stupid, and not a scientist.</i><br /><br /><i>But, I found that there&#39;s at least one advantage to being on the outside of the bag of science: I can read the label ... 100% pure steer manure.</i><br /><br /><b><i>Now, that isn&#39;t to be dismissive.  It&#39;s useful.  It can fertilize the growth of new ideas.</i></b><br /><br /><i>So, I&#39;m going to waste your time.  Astronomers have been doing that for over a century ... wasting time ... and tax dollars.</i><br /><br /><i>In the late 1800&#39;s, they had a clue that they were missing something.  Kristian Birkeland figured out that auroras were caused by electricity in space.  But, he wasn&#39;t an astronomer.</i><br /><br /><i>Early in the 1900&#39;s, Irving Langmuir coined the term &#39;plasma&#39; for ... electricity in space.  He wasn&#39;t an astronomer either.</i><br /><br /><i>Later in the 1900&#39;s, Hannes Alfven -- who wasn&#39;t an astronomer -- maybe you can see where this is going ... he told astronomers they didn&#39;t know what they were talking about.  He said: It&#39;s electricity in space, stupid!  That&#39;s a paraphrase.</i><br /><br /><i>By then, astronomers should have caught on.  But, they ignored it because they knew that there was no such thing as electricity in space.</i><br /><br /><i>When I took freshman physics, the professor skipped the chapter on electricity and magnetism.  He told us, you&#39;ll never use this stuff because there&#39;s no such thing as electricity in space.</i><br /><br /><i>I first heard of Birkeland and Langmuir and Alfven and plasma during the Velikovsky Reconsidered days of the 1970s.  I&#39;ve asked myself ever since, did I waste my time going to college?  Maybe I should have changed my major to electrical engineering or basket-weaving, and then I could have acquired knowledge that had something to do with the real world.</i><br /><br /><b><i>The space age has been a problem.  Astronomers have put instruments in space and the instruments reported sightings of ... electricity.  In fact, they were drowning in it.  Everything was electrified.</i></b><br /><br /><i>Astronomers were all like O-M-G and I was all like L-O-L.</i><br /><br /><b><i>Wal Thornhill said they apply a blind eye to the wrong end of the telescope and tell us what they imagine they see.</i></b><br /><br /><i>Well, they imagined impacts, shockwaves, bubbles, rivers, rain, wind ... Who knew astronomy would become a sub-discipline of meteorology?</i><br /><br /><i>They imagine hot gas and cold gas and ultra-dense gas.  It makes you wonder if astronomers have gas ...</i><br /><br /><i>With dark matter and dark energy, they seriously ask us to believe that they can see a universe that&#39;s invisible.  Well, there&#39;s a word for seeing what&#39;s invisible: hallucinating.</i><br /><br /><i>What a waste of a century.</i><br /><br /><i>So, where are we?</i><br /><br /><b><i>Over a century ago, doctors invented a procedure to remove cataracs from peoples&#39; eyes.  Among their first patients were people who had been blind from birth.  The surgery enabled them to see for the first time, as though they were newborn.  But, unlike newborns, they had acquired language and so they could tell the doctors what they were experiencing.</i></b><br /><br /><b><i>They didn&#39;t see things that were &#39;out there&#39;.  They saw meaningless moving patches of color.  The patches weren&#39;t just confusing in themselves; they caused confusion with their other senses as well.</i></b><br /><br /><b><i>The newly-sighted person&#39;s understandings of the world were disrupted.  They had to make sense of it all over again.  They had to learn to interpret those patches in ways that were compatible with their other sensations.</i></b><br /><br /><i>This required them to conceive new ideas of things.  Now, a thing is not a given.  It&#39;s a group of sensations that our minds combine into the concept of a unity.</i><br /><br /><i>We&#39;ve become so accustomed to the process that we take it for granted.  But, a thing is cognitive construction from sensory and conceptual bits and pieces.  </i><b><i>The newly-sighted people were awash in these bits and pieces of sensation, and they had no conceptual structure to make sense of them.</i></b><br /><br /><b><i>They had developed a concept of space in which these things could interrelate.  For the most part, they had no concept of space.</i></b><br /><br /><b><i>Things situated in space was a totally new way of understanding for them.  But before they could learn it, they had to unlearn many of the ways of understanding things and relationships that they had developed without the visual sensations.</i></b><br /><br /><i>The old ideas couldn&#39;t accommodate the new visual sensations.  Well, the task was difficult.</i><br /><br /><i>Some of them gave up, closed their eyes and returned to their old life in the home of the blind.</i><br /><br /><i>The doctors were surprised.  </i><b><i>They discovered that seeing -- the understanding of visual sensations as things in space -- had to be learned.</i></b><i>  By the time most of us can talk about it, we&#39;ve taken it for granted.  We take the metaphor literally: seeing is understanding.  No interpretation or theory seems to be required.</i><br /><br /><b><i>The new instruments of the space age have removed the cataracs of our familiar biological sensory limitations, and we&#39;re seeing for the first time patches of new colors.  We have never before seen the universe in x-rays or radio light.</i></b><i>  We have never before seen the universe from off the Earth&#39;s surface.  We have never before picked up a handful of Martian dirt.  We&#39;ve never before taken a shower in a spray of water from Enceladus.  We&#39;ve never before stuck our finger in a high-voltage socket of the Sun.</i><br /><br /><i>We must learn again how to see new things in a new space.  Is an ion a particle with momentum or a current carrier in a circuit?</i><br /><br /><i>Is a star a speck of gravitationally compacted matter isolated in the midst of emptiness?  Or is it a plasma sheath interacting with interstellar plasma, and the higher density speck of optically-radiating matter at the center is an unimportant residue?</i><br /><br /><b><i>Not surprisingly, the experts in the old way of seeing are having a hard time learning.</i></b><i>  They&#39;re taking refuge in the home of blind astronomy.  What astronomers think they see is mostly think and only a little see.</i><br /><br /><i>The same old thoughts inspired by the trickle of photons from the twinkling stars continue to filter the now-swollen stream of photons from the new light into the same old colors of theories.</i><br /><br /><i>Interposing the computer between the telescope and the eye has enhanced the seeing, but it&#39;s turned the thinking into video games.</i><br /><br /><b><i>To see plasma in its new empirical light, we must first un-think the old thoughts.</i></b><br /><br /><i>Well, that&#39;s where epistemology comes in.  Epistemology is a topic we don&#39;t discuss in polite company.</i><br /><br /><i>It&#39;s one of those basic bodily functions that we take for granted.  It&#39;s kind of like sweating.  </i><b><i>It&#39;s something you do whether you&#39;re aware of it or not.</i></b><br /><br /><i>To study it, you have to be something of a contortionist.  </i><b><i>You have to be able to look inside your own head, look inside your own eyeball.</i></b><i>  It&#39;s insight into insight.  It&#39;s understanding understanding.  It&#39;s knowing knowing.  It&#39;s it&#39;s kinda kinda like stut-stuttering.</i><br /><br /><b><i>It&#39;s becoming aware that you live in Plato&#39;s cave.  You notice that the things you see are sensory shadows projected on a conceptual wall.  You may think you can turn around and look at the projector and see what&#39;s causing the shadows, but you can&#39;t.  Biology is in the way; sensation provides only sensation.</i></b><br /><br /><i>Even with our new senses, we only see the shadows differently.  The modern version of this is that theories are under-determined by observation.  It means that we have more thoughts than there are shadows.  It means there can be more than one thought per shadow.  It means we can have no final thought.</i><br /><br /><i>You can close your eyes and believe you have a final thought, but then you&#39;ll spend the rest of your days in the home for the blind.  Most of us prefer to keep looking.</i><br /><br /><i>Well, I promised to waste your time.  As far as I can tell, this has no practical application.  </i><b><i>Awareness of epistemology can explain why revolutions in science happen, but you still have to go through the struggle.</i></b><i>  It can explain why theories are provisional, but that only seems to be important during revolutions.</i><br /><br /><i>It may restrain you from issuing unconditional press releases that assume the ultimate truth of the Big Bang or quantum mechanics or General Relativity ... or the Electric Universe.</i><br /><br /><i>I think humans are not well adapted to understanding ultimate truth.  We&#39;re better at understanding adapting.  We don&#39;t like that as much; </i><b><i>we like certainty.  We just don&#39;t have the senses for it.</i></b><br /><br /><b><i>But consider this: The universe doesn&#39;t care about truth.  Humans care about truth.  Therefore, truth must fit the human scale.  It must be tailored to the human stature.  It must constantly adapt to the change in human situation.</i></b><br /><br /><i>So, what is the human scale?  Aside from distinctions in size, I think it&#39;s the scale of caring about things -- and remember that things are constructions of human sensations and human thought.</i><br /><br /><i>It&#39;s the scale of uncertainty, the scale of discovery, the scale of creativity.</i><br /><br /><i>In this view, all theories are working hypotheses.  For a working hypothesis, truth depends on how well it works.  It&#39;s another way of talking about how well it works.</i><br /><br /><i>Truth in science is not some mystical, metaphysical property of the theory; it&#39;s whether there is or is not a physical connection between NGC4319 and Markarian 205.</i><br /><br /><i>Truth is not so much about what&#39;s in your head; it&#39;s what&#39;s in your face.</i><br /><br /><i>But, as soon as you grasp that, it slips through your fingers.  How do you determine what it is that&#39;s in your face?  What&#39;s a thing?  What criteria will you use for &#39;in your face&#39;?  How do you interpret facial sensations?</i><br /><br /><b><i>There&#39;s a high-redshift quasar in front of NGC7319.  In conventional theory, this means the quasar is billions of light years on the other side of the galaxy.  So, we must be seeing the quasar through the galaxy.  The only problem is that it&#39;s a Seyfert galaxy.  It&#39;s loaded with dust!  It&#39;s opaque.</i></b><br /><br /><b><i>So, astronomers say there has to be a hole in the galaxy that just happens to align with our line-of-sight to the quasar.&quot;</i></b><br /><br />[... expresses confusion, audience laughs ...]<br /><br /><i>&quot;Now, I&#39;m all like O-M-G.  Theory can always be adjusted to fit the data.  Data can always be interpreted to fit the theory.  The result may look like it was designed by Rube Goldberg, but that&#39;s a problem with nuissance value -- not truth value.</i><br /><br /><i>That&#39;s aesthetics, not logic.</i><br /><br /><b><i>Science IS circular logic -- or, maybe it would be better to say helical logic.</i></b><br /><br /><b><i>Scientists constantly adjust their theories to fit observation, and then they claim the observations verified the theories.  And in this wheel of circular logic, the gerbel of astrophysics runs.</i></b><br /><br /><i>A synonym for circularity is refinement.  Only a puritanical logician would object.&quot;</i><br /><br />[motions &#39;that&#39;s me&#39;]<br /><br /><i>&quot;It&#39;s good.  It enables inventing gadgets -- different theories enable different gadgets.  So, deciding which theory to adopt is not so much a matter of true and false, as a matter of which potential gadget you want, and how cleans you want the lines of your theory.  Aesthetics trumps logic.</i><br /><br /><i>Well, this returns to the idea of science as art.  The Greek root for the word &#39;poetry&#39; ... I don&#39;t know how you pronounce it, it&#39;s Greek to me ... It means &#39;to make or to create&#39;.  This is why they speak of a poet as making a poem.  </i><b><i>As art, science is a making or a creating of sense.</i></b><br /><br /><i>I think of it as cognitive art, brushing measurements onto an imaginitive canvas to create a theory that makes sense of the data.</i><br /><br /><b><i>It&#39;s the idea behind Natural Philosophy: making sense of selected observations more than finding certainty in one true faith.  Certainty in one true faith is scientism.  It&#39;s epistemology in a straightjacket.</i></b><br /><br /><b><i>I think of Natural Philosophy as having a wider ambit than science; it includes bold new ideas -- what I call visions that come before science.</i></b><br /><br /><i>Science, then, would be the methodical investigating and testing of bold new ideas.</i><br /><br /><b><i>This highlights the danger of specialization: People become engrossed in figuring out the next decimal place, and they lose sight of why they adopted the theory in the first place.</i></b><br /><br /><b><i>They become too busy to revisit first assumptions.  They take those assumptions for granted.  The assumptions become unconscious and then they&#39;re surprised when reality throws data at them that isn&#39;t covered by the assumption.</i></b><br /><br /><b><i>They have a certainty that comes from ignoring questions.</i></b><i>  They have the one true faith that comes from tunnel vision.</i><br /><br /><i>This is a job description for technicians, but not for scientists.</i><br /><br /><b><i>Science ... is first of all about the questions.  We need the answers in order to do things, but we need the questions more in order to do new things.</i></b><br /><br /><i>Modern astronomy has become doing old things all over again.  In the words of Wallace Stevens, it&#39;s a repetition and a repetitiousness of men and flies.</i><br /><br /><i>So, the new sensations, the new data, of plasma require us to populate the universe with new things -- with new groupings of sensations into new unities.</i><br /><br /><i>None of our present theories take this into account.  Every discipline must therefore be re-evaluated, probably reconfigured.  In the new light, we&#39;re only deceiving ourselves to cling to the old visions.</i><br /><br /><i>Rupert Sheldrake wondered why he was trying to learn about life by cutting up dead frogs.  He began to photograph a Parrot as it commented on pictures somebody in another room looked at.  He recorded dogs who began wagging their tails when their masters decided to come home from shopping.  He developed a vision of a morphic field.</i><br /><br /><i>In my view, that&#39;s cognitive art.  He&#39;s providing a new canvas for the purpose of behavior -- which is remarkably similar to plasma behavior -- on which to paint the data of biology.</i><br /><br /><i>This is quite different from the old canvas of physiology and dead frogs.</i><br /><br /><i>Michael Meade Steinbacher is a photographer, not a geologist.  He read Worlds in Collision, not The Essentials of Geology.  He&#39;s been traveling around the country actually looking at rocks instead of textbooks.  He sees surfaces in the light of ancient legends -- not buried strata in the light of recent myths.</i><br /><br /><i>He sees the Earth resurfaced in cataclysmic events that deposited mountains as drifts of debris, then global floods washed over them and filled valleys with a slurry run-off -- much of it was lithified by aurora-like discharges that ran through the ground and melted high spots.</i><br /><br /><i>Well, it&#39;s a bold new idea.  </i><b><i>It needs to be refined, circularly.</i></b><br /><br /><i>I would say it&#39;s not science, but what&#39;s important now is the vision ... Does it make sense of the data?  How much sense?  How much data?</i><br /><br /><b><i>Especially, does it make us aware of the nonsense overlooked in received theories?</i></b><br /><br /><b><i>This needs to happen in every discipline.  We must become aware of the fraud of the known -- the nonsense propped up with political pressure.</i></b><br /><br /><i>We&#39;re building a new age of new sciences: Everything is open to question; everything is open to promises.</i><br /><br /><i>Remember that knowledge arises from the human scale, from uncertainty, from discovery, from creativity, from adaptibility.</i><br /><br /><i>We need a plasma geology and a plasma biology, a plasma psychology.  We need plasma history.  We need plasma theology, plasma politics.  We need plasma art.</i><br /><br /><i>There was a song in the 80&#39;s: &#39;The future&#39;s so bright, I&#39;ve gotta wear shades.&#39;  If the shades are consensus theories, then I urge you not to put them on.</i><br /><br /><b><i>Squint if you must, but adapt your vision to the new light.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The Trouble with Neutron Stars","summary":"The Idea of the Neutron Star Dates Back to 1934, at a Time When Space Was Thought to be Empty and Plasmas Weren't Well Known / It Presumes that Pulsar Emissions Must Result from Rotation, and then Accepts Any Explanation that Makes that Possible / Once Stars Were Observed Pulsing at 716 Times Per Second, a New Form of Matter - Unobserved in the Laboratory - was Invented to Explain these Rotation Rates / By Contrast, when an Electrically Conducting Plasma Medium is Considered, the Same Problem can be Solved without Any Need to Invent New Physics","image":"https://lh3.googleusercontent.com/-aS9T5n-TNiQ/V_k5LVGz4sI/AAAAAAAAI5s/7GVLXxuvaws_X7y-usGsERTJx8AUexPbQCJoC/w7142-h9999/the-trouble-with-neutron-stars-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Ymv4LxyQPpB","publishDate":"2016-10-08T18:29:29.642Z","updateDate":"2017-01-03T01:56:47.438Z","text":"<b>The Trouble with Neutron Stars: The Idea of the Neutron Star Dates Back to 1934, at a Time When Space Was Thought to be Empty and Plasmas Weren&#39;t Well Known / It Presumes that Pulsar Emissions Must Result from Rotation, and then Accepts Any Explanation that Makes that Possible / Once Stars Were Observed Pulsing at 716 Times Per Second, a New Form of Matter - Unobserved in the Laboratory - was Invented to Explain these Rotation Rates / By Contrast, when an Electrically Conducting Plasma Medium is Considered, the Same Problem can be Solved without Any Need to Invent New Physics</b><br /><br /><a href=\"https://en.wikipedia.org/wiki/Neutron_star\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Neutron_star</a><br /><br /><b>How Neutron Stars Are Typically Explained</b><br /><br /><i>&quot;A neutron star is the collapsed core of a large star (10–29 solar masses). Neutron stars are the smallest and densest stars known to exist. [1] With a radius on the order of 10 km, they can, however, have a mass of about twice that of the Sun. They result from the supernova explosion of a massive star, combined with gravitational collapse, that compresses the core past the white dwarf star density to that of atomic nuclei. </i><b><i>Most of the basic models for these objects imply that neutron stars are composed almost entirely of neutrons</i></b><i>, which are subatomic particles with no net electrical charge and with slightly larger mass than protons. They are supported against further collapse by neutron degeneracy pressure, a phenomenon described by the Pauli exclusion principle. If the remnant has too great a density, something which occurs in excess of an upper limit of the size of neutron stars at 2-3 solar masses, it will continue to collapse into a form called a black hole.</i><br /><br /><i>Neutron stars that can be observed are very hot and typically have a surface temperature around 6×10^5 K. [2][3][4][5][a] They are so dense that a normal-sized matchbox containing neutron-star material would have a mass of approximately 13 million tonnes, or a 2.5 million m^3 chunk of the Earth. [6][7] They have strong magnetic fields, between 10^8 and 10^15 times that of Earth&#39;s. The gravitational field at the neutron star&#39;s surface is about 2×10^11 times that of the Earth&#39;s.</i><br /><br /><i>As the star&#39;s core collapses, its rotation rate increases as a result of conservation of angular momentum, hence newly formed neutron stars rotate at up to several hundred times per second. Some neutron stars emit beams of electromagnetic radiation that make them detectable as pulsars. Indeed, </i><b><i>the discovery of pulsars in 1967 was the first observational suggestion that neutron stars exist.</i></b><i> The radiation from pulsars is thought to be primarily emitted from regions near their magnetic poles. If the magnetic poles do not coincide with the rotational axis of the neutron star, the emission beam will sweep the sky, and when seen from a distance, if the observer is somewhere in the path of the beam, it will appear as pulses of radiation coming from a fixed point in space (the so-called &#39;lighthouse effect&#39;). </i><b><i>The fastest rotation rate for a neutron star was a rate of 716 times a second [8][9] or 43,000 revolutions per minute, giving a linear speed at the surface on the order of 0.165 c.&quot;</i></b><br /><br /><a href=\"http://www.electric-cosmos.org/hrdiagr.htm\" class=\"ot-anchor\">http://www.electric-cosmos.org/hrdiagr.htm</a><br /><br /><b>Why Neutron Stars are Impossible</b><br />by Don Scott<br /><br /><i>&quot;The concept of the &#39;neutron star&#39; was a baseless invention. </i><b><i>It was proposed because only such a dense material could make up a star that could stand those outrageously high rotation speeds.</i></b><br /><br /><b><i>But, one of the basic rules of nuclear chemistry is the &#39;zone of stability&#39;. This is the observation that if we add neutrons to the nucleus of any atom, we need to add an almost proportional number of protons (and their accompanying electrons) to maintain a stable nucleus. In fact, it seems that when we consider all the natural elements (and the heavy man made elements as well), there is a requirement that in order to hold a group of neutrons together in a nucleus, a certain number of proton-electron pairs are required.</i></b><br /><br /><b><i>The stable nuclei of the lighter elements contain approximately equal numbers of neutrons and protons, a neutron/proton ratio of 1. The heavier nuclei contain a few more neutrons than protons, but the limit seems to be 1.5 neutrons per proton. Nuclei that differ significantly from this ratio spontaneously undergo radioactive transformations that tend to bring their compositions into or closer to this ratio.</i></b><br /><br /><i>Flying in the face of this fact, mainstream astrophysicists continue to postulate the existence of stars made up of solid material consisting only of neutrons, &#39;Neutronium&#39;. This is yet one more example of Fairie Dust entities fantasized by astrophysicists to explain otherwise inexplicable observations.</i><br /><br /><b><i>A proton-free nucleus or &#39;charge free&#39; atom made up of only neutrons has never been synthesized in any laboratory nor can it ever be. Lone neutrons decay into proton - electron pairs in less than 14 minutes; atom-like collections of two or more neutrons will fly apart almost instantaneously.</i></b><br /><br /><b><i>That astrophysicists feel free to postulate and then quickly accept as fact the existence of such preposterous entities provides deep insight into the present state of their science.&quot;</i></b><br /><br /><a href=\"https://www.thunderbolts.info/tpod/2005/arch05/050527variablexray.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2005/arch05/050527variablexray.htm</a><br /><br /><b>The Electrical Perspective of Variable Stars</b><br /><br /><i>&quot;The brightness fluctuations of &#39;variable stars&#39; have long puzzled astronomers. Perhaps the similarity of their brightness curves to those of lightning is the best indicator of the cause.</i><br /><br /><i>[...]</i><br /><br /><i>The brightness curve of Mira (upper diagram) follows the same sudden onset and slow decay as lightning. The star varies over a period of just under a year (the missing part of the brightness curves are the times of the year when the star is not seen at night.) The lower brightness curve is for Delta Cepheus, the flagship star of the Cepheid variables. Cepheid variables have a much shorter period than Mira variables, but </i><b><i>90% of them have the lightning signature of a fast rise and slow decay.</i></b><br /><br /><i>Some variable stars are regular and others are irregular. The irregular ones seem to average power over the bursts. When the bursts are more frequent, the energy per burst is less. </i><b><i>This is to be expected from an electric circuit where the trigger level is variable and the power input is constant.</i></b><br /><br /><i>Don Scott, plasma cosmologist and retired professor of electrical engineering, suggests that most, maybe all, variable stars are binaries. Their variability is caused by electric discharge between the two stars.</i><br /><br /><i>Scott says: &#39;As I see it, [variable] binaries operate generally as follows: Each of the stars has an electrical capacitance. These two capacitances are permanently connected by a plasma (cloud). This plasma exhibits (as do all plasmas) a nonlinear resistance.  If one of the stars charges up to a high enough voltage ... , then the plasma will go into the arc mode and emit brilliant light, perhaps x-rays and gamma-rays. Electrical energy will be transferred from the first capacitor (star) to the other. It doesn&#39;t make any difference which star is bigger.</i><br /><br /><i>&#39;The &#39;bridge&#39; between the two stars probably doesn&#39;t go away.  After the arc discharge is over (the voltage difference between the two capacitors is dissipated), the plasma bridge goes back into its normal glow or dark current mode and waits for the next discharge.&#39;</i>                <br /><br /><b><i>Like a relaxation oscillation circuit, some variable stars reduce their variability and eventually come to equilibrium. We see this happening to the north star, Polaris. Polaris is a binary Cepheid variable whose brightness has increased over the course of the last century, while the level of variability has decreased and the length of time over which it varies has became longer.&quot;</i></b><br /><br /><i>The Electric Sky</i><br />by Don Scott<br /><br /><b>The Heresy of the Relaxation Oscillator</b><br /><br /><i>&quot;Neutron stars are impossible. One of the well-known basic rules of nuclear chemistry is the so-called &#39;band of stability.&#39; This is the observation that, if we add neutrons to the nucleus of any atom, we need to add an almost proportional number of protons (and their accompanying electrons) to maintain a stable nucleus. In fact, it seems that, when we consider all the known elements (even the heavy man-made elements as well), there is a requirement that, in order to hold a group of neutrons together in a nucleus, an almost equal number of proton-electron pairs are required. The stable nuclei of the lighter elements contain approximately equal numbers of neutrons and protons - a neutron/proton ratio of 1. The heavier nuclei contain a few more neutrons than protons, but the limit seems to be about 1.5 neutrons per proton. Nuclei that differ significantly from this ratio spontaneously undergo radioactive decay transformations that tend to bring their compositions closer to this ratio. Groups of neutrons are not stable by themselves.</i><br /><br /><i>A plot showing the number of neutrons and protons in the nuclei of all the known elements is shown in figure 47. Combinations outside the band are not stable. Notice how narrow the band is. Neutronium (a nucleus made up of only neutrons) would have to lie somewhere on the vertical axis in this plot -- and would therefore rapidly decay radioactively. We know from laboratory experiments that any lone neutron decays into a proton, an electron and a neutrino in less than 14 minutes; atom-like collections of two or more neutrons will fly apart almost instantaneously. There is no such thing as neutronium. Therefore there can be no such entity as a neutron star. It is a fiction that flies in the face of all we know about elements and their atomic nuclei.</i><br /><br /><i>[...]</i><br /><br /><i>Electric Pulsars</i><br /><br /><i>Some pulsars oscillate with periods in the millisecond range. Their radio pulse characteristics are:</i><br /><br /><i>1. </i><b><i>The duty cycle is typically 5% (i.e., the pulsar flashes like a strobe light -- the duration of each output pulse, the time it is on, is much shorter than the length of time between pulses).</i></b><br /><br /><i>2. </i><b><i>Some individual pulses are quite variable in intensity.</i></b><br /><br /><i>3. </i><b><i>The polarization of the pulse implies that the origin has a strong magnetic field. Magnetic fields require electric currents.</i></b><br /><br /><b><i>These characteristics are consistent with an electric arc (lightning) interaction between two closely spaced binary stars. Relaxation oscillators with characteristics like this have been known and used by electrical engineers for many years.</i></b><br /><br /><i>It is becoming obvious that pulsars are likely the result of electrical discharges between members of binary pairs: Using the Hubble Space Telescope, astronomers at the Naval Research Laboratory (NRL) have detected five optical companion stars orbiting millisecond pulsars. Three of the companions are among the coolest and oldest white dwarf stars known. One of the fastest pulsars discovered so far is named PSR B1937+21. Don Backer and Shri Kulkarni made the discovery of this first &#39;millisecond pulsar&#39; at Arecibo in 1982. This remarkable star with a spin period of just 1.5578 milliseconds must rotate on its axis almost 642 times per second. In addition, some pulsars are known to be members of binary systems.</i><br /><br /><i>A rotation speed of 642 rotations per second is over 38,500 rpm.</i><br /><br /><i>Some astronomers are quite willing to accept the possibility of a star rotating at 38,000 rpm. They say: &#39;Some stars (named pulsars because of their specific characteristics) are known to rotate very rapidly. I*f a star with the density of our Sun were to spin as fast, it would fly apart. The only way these objects can stay together at those rotation rates is if they have a tremendous gravitational field, hence, are extremely dense. I see nothing wrong with accepting that there are densities in the universe that are much greater than in our solar system.&#39;*</i><br /><br /><i>The only possible logical response to this is:</i><br /><br /><b><i>The entire question about densities is a red herring, e.g., it presumes that the emissions of pulsars are produced because of monstrously rapid rotational beacons -- and then accepts any explanation that makes that possible.</i></b><br /><br /><i>The bursts of radio frequency energy that are observed are quite short in comparison to the time between bursts. In other words they resemble radar signals. The pulsar emits a pulsed radio frequency signal whose modulation frequency is about 600 Hz. The phenomenon is produced electrically (much like a radio station). </i><b><i>A relaxation oscillator can consist of two capacitors (stars) and a nonlinear resistor (plasma) between them. One capacitor charges up relatively slowly and, when its voltage becomes sufficiently high, discharges rapidly to the other capacitor (star). The process then begins again. The rate of this charge/discharge phenomenon depends on the strength of the input (Birkeland) current, the capacitances (surface areas of the stars) and the breakdown voltage of the (plasma) connection. It in no way depends on the mass or density of the stars.</i></b><br /><br /><i>In the plasma that surrounds a star (or planet) there are conducting paths whose sizes and shapes are controlled by the magnetic field structure of the body. </i><b><i>Those conducting paths are giant electric transmission lines and can be analyzed as such. Depending on the electrical properties of what is connected to the ends of electrical transmission lines, it is possible for pulses of current and voltage (and therefore power) to oscillate back and forth from one end of the line to the other. The ends of such cosmic transmission lines can both be on the same object (as occurs on Earth) or one end might be on one member of a closely spaced binary pair of stars and the other end on the other member of the pair similar to the &#39;flux tube&#39; connecting Jupiter and its inner moon, Io.</i></b><br /><br /><b><i>In 1995 an analysis (and several super-computer simulations) was performed on a transmission line system having the properties believed to be those of a pulsar atmosphere. Seventeen different observed properties of pulsar emissions were produced in these experiments.</i></b><i> This seminal work [135] by Peratt and Healy strongly supports the electrical transmission line explanation of pulsar behavior.&quot;</i><br /><br />[135] <a href=\"http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?bibcode=1995Ap%26SS.227..229H&amp;db_key=AST&amp;page_ind=0&amp;data_type=GIF&amp;type=SCREEN_VIEW&amp;classic=YES\" class=\"ot-anchor\">http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?bibcode=1995Ap%26SS.227..229H&amp;db_key=AST&amp;page_ind=0&amp;data_type=GIF&amp;type=SCREEN_VIEW&amp;classic=YES</a><br /><br />GRAPHIC REFERENCES:<br /><br />[1] <a href=\"https://en.wikipedia.org/wiki/Neutron_star\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Neutron_star</a><br /><br />[2] <a href=\"https://www.thunderbolts.info/webnews/neutron_stars.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/webnews/neutron_stars.htm</a><br /><br />[3] <i>The Electric Sky</i>, Don Scott<br /><br />[4] See comments in <a href=\"http://phys.org/news/2016-08-physicists-gravitational-neutron-star-collisions.html\" class=\"ot-anchor\">http://phys.org/news/2016-08-physicists-gravitational-neutron-star-collisions.html</a><br /><br />[5] <a href=\"https://www.thunderbolts.info/tpod/2005/arch05/050527variablexray.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2005/arch05/050527variablexray.htm</a><br /><br />[6] <i>The Electric Sky</i>, Don Scott<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Myth of Galileo","summary":"Elementary School Versions of the Galileo Story Cast the Church as a Villain / But, a More Modern Version of the Story Says the Church Went to Great Lengths to Put Up with Him / This Alternative History Suggests that His Tendency towards Argument, Ridicule and Vanity Undermined His Scientific Genius / If True, Galileo Might Have been the First \"Crank\"","image":"https://lh3.googleusercontent.com/-s-2wMUfU9ys/V-39BdRbHyI/AAAAAAAAI4E/TQBsEGxtQfYZGLGf2zGwrCNeJSw-RpXaACJoC/w1800-h2520/the-myth-of-galileo-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/bYq7zaEPndu","publishDate":"2016-09-30T05:50:57.668Z","updateDate":"2017-01-03T01:57:08.840Z","text":"<b>The Myth of Galileo: Elementary School Versions of the Galileo Story Cast the Church as a Villain / But, a More Modern Version of the Story Says the Church Went to Great Lengths to Put Up with Him / This Alternative History Suggests that His Tendency towards Argument, Ridicule and Vanity Undermined His Scientific Genius / If True, Galileo Might Have been the First &quot;Crank&quot;</b><br /><br /><a href=\"https://www.firstthings.com/blogs/firstthoughts/2011/09/the-myth-of-galileo-a-story-with-a-mostly-valuable-lesson-for-today/\" class=\"ot-anchor\">https://www.firstthings.com/blogs/firstthoughts/2011/09/the-myth-of-galileo-a-story-with-a-mostly-valuable-lesson-for-today/</a><br /><br />The Myth of Galileo: A Story With a (mostly) Valuable Lesson for Today<br />by Joe Carter<br />9.8.11<br /><br /><i>&quot;... Here is the real story about Galileo Galilei. </i><b><i>It’s not the story about an enlightened scientist being persecuted by a narrow-minded Catholic Church because that story is (mostly) a myth.</i></b><i> It’s not a story about a great scientific genius either, though he was that (mainly). It’s also not a story about someone being reincarnated with the soul of the old astronomer like the song by the Indigo Girls that, for a few weeks in ‘92, I thought was (almost) profound. (And I should point out that it [is] not an original story but one that cribbed together from other sources.)</i><br /><br /><i>But like all good stories this one provides a (mostly) valuable lesson.</i><br /><br /><i>In Galileo’s day, the predominant view in astronomy was a model first espoused by Aristotle and developed by Claudius Ptolemy in which the sun and planets revolved around the earth. The Ptolemic system had been the reigning paradigm for over 1400 years when a Polish Canon named Nicholas Copernicus published his seminal work, On the Revolution of the Celestial Orbs.</i><br /><br /><i>Now Copernicus’ heliocentric theory wasn’t exactly new nor was it based on purely empirical observation. While it had a huge impact on the history of science, his theory was more of a revival of Pythagorean mysticism than of a new paradigm. </i><b><i>Like many great discoveries, he merely took an old idea and gave it a new spin.</i></b><br /><br /><i>Although Copernicus’ fellow churchmen encouraged him to publish his work, </i><b><i>he delayed the publication of On the Revolution for several years for fear of being mocked by the scientific community. At the time, the academy belonged to Aristotelians who weren’t about to let such nonsense slip through the &#39;peer review&#39; process.</i></b><br /><br /><i>Then came Galileo, the prototypical Renaissance man -- a brilliant scientist, mathematician, and musician. But while he [was] intelligent, charming, and witty, </i><b><i>the Italian was also argumentative, mocking, and vain.</i></b><i> He was, as we would say, complex. </i><b><i>When his fellow astronomer Johann Kepler wrote to tell him that he had converted to Copernicus’ theory, Galileo shot back that he had too — and had been so for years (though all evidence shows that it wasn’t true). His ego wouldn’t allow him to be upstaged by men who weren’t as smart as he was. And for Galileo, that included just about everybody.</i></b><br /><br /><i>In 1610, Galileo used his telescope to make some surprising discoveries that disputed Aristotelian cosmology. </i><b><i>Though his findings didn’t exactly overthrow the reigning view of the day, they were warmly received by the Vatican and by Pope Paul V. Rather than continuing his scientific studies and building on his theories, though, Galileo began a campaign to discredit the Aristotelian view of astronomy.</i></b><i> (His efforts would be akin to a modern biologist trying to dethrone Darwin.) Galileo knew he was right and wanted to ensure that everyone else knew that the Aristotelians were wrong.</i><br /><br /><i>In his efforts to cram Copernicanism down the throats of his fellow scientists, Galileo managed only to squander the goodwill he had established within the Church. </i><b><i>He was attempting to force them to accept a theory that, at the time, was still unproven. The Church graciously offered to consider Copernicanism [as] a reasonable hypothesis, albeit a superior one to the Ptolemaic system [I think the author means &#39;inferior&#39;, no?], until further proof could be gathered. Galileo, however, never came up with more evidence to support the theory. Instead, he continued to pick fights with his fellow scientists even though many of his conclusions were being proven wrong (i.e., that the planets orbit the sun in perfect circles).</i></b><br /><br /><i>Galileo’s primary mistake was to move the fight out of the realm of science and into the field of biblical interpretation. </i><b><i>In a fit of hubris, he wrote the Letter to Castelli in order to explain how his theory was not incompatible with proper biblical exegesis. With the Protestant Reformation still fresh on their minds, the Church authorities were in no mood to put up with another troublemaker trying to interpret Scripture on his own.</i></b><br /><br /><i>But, to their credit, they didn’t overreact. The Letter to Castelli was twice presented to the Inquisition as an example of the astronomer’s heresy and twice the charges were dismissed. Galileo, however, wasn’t satisfied and continued his efforts to force the Church to concede that the Copernican system was an issue of irrefutable truth.</i><br /><br /><i>In 1615, Cardinal Robert Bellarmine politely presented Galileo with an option: Put up or Shut up. Since there was no proof that the earth revolved around the sun, there was no reason for Galileo to go around trying to change the accepted reading of Holy Scripture. But if he had proof, the Church was willing to reconsider their position. Galileo’s response was to produce his theory that the ocean tides were caused by the earth’s rotation. </i><b><i>The idea was not only scientifically inaccurate but so silly it was even rejected by his supporters.</i></b><br /><br /><i>Fed up with being dismissed, Galileo returned to Rome to bring his case before the Pope. The Pontiff, however, merely passed it along to the Holy Office who issued the opinion that the Copernican doctrine is &#39;foolish and absurd, philosophically and formally heretical inasmuch as it expressly contradicts the doctrine of Holy Scripture in many passages ...&#39; The verdict was quickly overruled by other Cardinals in the Church.</i><br /><br /><i>Galileo wasn’t about to let up, though, and to everyone’s exasperation, pressed the issue yet again. The Holy Office politely but firmly told him to shut up about the whole Copernican thing and forbid him from espousing the unproven theory. This, of course, was more than he was willing to do.</i><br /><br /><i>When his friend took over the Papal throne, Galileo thought he would finally find a sympathetic ear. He discussed the issue with Pope Urban VIII, a man knowledgeable in matters of math and science, and tried to use his theory of the tides to convince him of the validity of his theory. Pope Urban was unconvinced and even gave an answer (though not a sound one) that refuted the notion.</i><br /><br /><b><i>Galileo then wrote A Dialogue About the Two Chief World Systems in which he would present the views of both Copernicus and Ptolemy. Three characters would be involved: Salviati, the Copernican; Sagredo, the undecided; and Simplicio, the Ptolemian (the name Simplicio implying &#39;simple-minded&#39;). And here is where we find our hero making his biggest blunder: he took the words that Pope Urban had used to refute his theory of the tides and put them in the mouths of Simplicio.</i></b><br /><br /><i>The Pope was not amused.</i><br /><br /><i>Galileo, who was now old and sickly, was once again called before the Inquisition. Unlike most suspected heretics, though, he was treated surprisingly well. While waiting for his trial, Galileo was housed in a luxurious apartment overlooking the Vatican gardens and provided with a personal valet.</i><br /><br /><i>In his defense, Galileo tried a peculiar tactic. He attempted to convince the judges that he had never maintained nor defended the opinion that the earth moves and that the sun is stationary and that he had, in fact, demonstrated the opposite by showing how the Copernican hypothesis was in error. The Holy Office, who knew they were being played for fools, condemned him as being &#39;vehemently suspected of heresy&#39;, a patently unjust ruling considering that Copernicanism had never been declared heretical.</i><br /><br /><i>Galileo’s sentence was to renounce his theory and to live out the rest of his days in a pleasant country house near Florence. Obviously the exile did him good because it was there, under the care of his daughter, that he continued his experiments and published his best scientific work, Discourses on Two New Sciences. He died quietly in 1642 at the ripe old age of 77.</i><br /><br /><i>As the philosopher Alfred North Whitehead wrote, &#39;In a generation which saw the Thirty Years’ War and remembered Alva in the Netherlands, </i><b><i>the worst that happened to men of science was that Galileo suffered an honorable detention and a mild reproof, before dying peacefully in his bed.&#39;</i></b><br /><br /><i>As Paul Harvey would say, now we know the rest of the story. So what can we learn from this tale? I think it provides different lessons for different groups of people.</i><br /><br /><b><i>For scientists it shows that if you are in agreement with most of your colleagues, you will most likely be forgotten while history remembers some crank. For advocates of non-consensus positions (e.g., AGW skeptics, Intelligent Design theorists) it teaches that claiming your theory is correct is no substitute for backing it up with experiments and data (even if you are right). For aggressively self-confident people the lesson is that sometimes being persistent and believing in yourself will just get you into trouble.</i></b><i> For Catholics it provides an example of why you shouldn’t insult the Pope (at least when there is an Inquisition going on).</i><br /><br /><b><i>I suspect that there are many more lessons that can be gleaned from this story. But I find that the real moral is not so much in the story itself but in the fact that the story even needs to be told in the first place. While I first heard the story of Galileo in elementary school, it wasn’t until long, long after I had graduated from college that I finally learned the truth. No doubt some people are just now hearing about it for the first time. How is that possible?</i></b><br /><br /><i>I suspect it may have something to do with the fact that for centuries people like Bertrand Russell, George Bernard Shaw, Carl Sagan, Bertolt Brecht, and the Indigo Girls have been passing on the myth. I don’t think any of them were intentionally lying. In fact, </i><b><i>I doubt any of them ever bothered to examine the facts themselves. They didn’t need to. The story fit what they already believed -- that science and religion were natural enemies -- and that was all they needed to know.</i></b><br /><br /><i>It would be easy to mock such gullibility and intellectual laziness. But the truth is that I’m probably guilty of doing the same thing quite often. Perhaps it’s because I am a journalist (sort of) and am more apt to believe whatever version of a story I find more interesting. As a newspaper editor I often favored David over Goliath, even when the powerful Philistine was more credible than the person slinging the stones. &#39;Boy Shepherd Slays Powerful Giant&#39; always makes for a better headline.&quot;</i><br /><br />Out of respect for the research that went into the piece, I&#39;ll leave his conclusion ...<br /><br /><i>&quot;As a Christian, though, I don’t have the option of favoring the position that will sell more newspapers. Instead, my duty is to side with the truth. When I hear a story that fits my agenda I should examine all the relevant facts before accepting it as Gospel. I may not always be absolutely certain which side of the line the truth lays. But I do know on thing for sure. That is the side that God will be on.&quot;</i><br /><br />See also:<br /><br />Mano Singham, “The Copernican myths” (Physics Today)<br /><a href=\"http://scitation.aip.org/content/aip/magazine/physicstoday/article/60/12/10.1063/1.2825071\" class=\"ot-anchor\">http://scitation.aip.org/content/aip/magazine/physicstoday/article/60/12/10.1063/1.2825071</a><br /><br />George Sim Johston, “The Galileo Affair”<br /><a href=\"http://www.d.umn.edu/~svanhave/The_Galileo_Affair.html\" class=\"ot-anchor\">http://www.d.umn.edu/~svanhave/The_Galileo_Affair.html</a><br /><br />John Appeldoorn, “The Myth of Galileo”<br /><a href=\"http://www.ips-planetarium.org/?page=a_appledoorn1990\" class=\"ot-anchor\">http://www.ips-planetarium.org/?page=a_appledoorn1990</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"The Copernican Myths","summary":"One of the Most Famous of all Scientific Revolutions - The Copernican Revolution - is Obscured by Scientific Folklore / When Properly Told, the Story's Lessons Dramatically Change / Switching to a Heliocentric Worldview Meant that a Whole Class of Solved Physics Problems Were Now Suddenly Unsolved / And When the Revolution Began, the Models Were Equally Cumbersome and Equally Accurate / But, Over Time, the Newer Model Gained Adherents Because it was Useful for Resolving Many Puzzles","image":"https://lh3.googleusercontent.com/-PJAMTp22o-I/V-0wArTXSfI/AAAAAAAAI3M/SyInpCm4thEjMNaB7oNdC87UmqVqM6zRQCJoC/w7142-h9999/the-copernican-myths-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/T45LN5GmN8C","publishDate":"2016-09-29T15:16:20.604Z","updateDate":"2017-01-03T01:57:27.886Z","text":"<b>The Copernican Myths: One of the Most Famous of all Scientific Revolutions - The Copernican Revolution - is Obscured by Scientific Folklore / When Properly Told, the Story&#39;s Lessons Dramatically Change / Switching to a Heliocentric Worldview Meant that a Whole Class of Solved Physics Problems Were Now Suddenly Unsolved / And When the Revolution Began, the Models Were Equally Cumbersome and Equally Accurate / But, Over Time, the Newer Model Gained Adherents Because it was Useful for Resolving Many Puzzles</b><br /><br /><a href=\"http://joelvelasco.net/teaching/3330/singham07-copernican_myths.pdf\" class=\"ot-anchor\">http://joelvelasco.net/teaching/3330/singham07-copernican_myths.pdf</a><br /><br />The Copernican Myths<br /><br />by Mano Singham<br /><br /><b><i>&quot;The real story of how the scientific and religious establishments greeted the Copernican revolution is quite different from the folklore. And it&#39;s a lot more interesting.</i></b><br /><br /><i>Mano Singham is director or the University Center for Innovation in Teaching and Education and adjunct associate professor of physics at Case Western Reserve University in Cleveland, Ohio.</i><br /><br /><i>Perhaps the most famous of all scientific revolutions is the one associated with Nicolaus Copernicus (1473-1543). The popular version of the story goes as follows:</i><br /><br /><i>&#39;The ancient Greeks, although they were great philosophers and good at mapping the motions of stars and planets, tended to create models of the universe that were more influenced hy philosophical, aesthetic, and religious considerations than by observation and experiment. The idea that Earth was the stationary center of the universe, and that the stars and planets were embedded in spheres that rotated around Earth, appealed to them because the circle and the sphere were the most perfect geometric shapes.</i><br /><br /><i>In the Christian era, the model also pleased religious people because it gave pride of place to human beings -- God&#39;s special creation. The prestige of Greek philosophers like Aristotle was so great, and commitment to religious doctrine so strong, that many scholars stubbornly tried to retain Ptolemaic astronomy even though increasingly complicated epicycles had to be added to make the system work even moderately well.</i><br /><br /><i>So when Copernicus came along with the correct heliocentric system, his ideas were fiercely opposed by the Roman Catholic Church because they displaced Earth from the center, and that was seen as both a demotion for human beings and contrary to the teachings of Aristotle. Therefore the Inquisition persecuted, tortured, and even killed those who advocated Copernican ideas.</i><br /><br /><i>Because of the church&#39;s adherence to philosophical and religious dogma, scientific progress was held back for a millennium. It was the later work of Tycho Brahe (1546-1601), Johannes Kepler (1571-1630), Galileo Galilei (1564-1642), and Isaac Newton (1642-1727) that finally led to the acceptance of heliocentrism.&#39;</i><br /><br /><i>Variations on this breezy version of the Copernicus story are common in science textbooks. [1] </i><b><i>How much of the story is true? Apart from the final sentence, not much. But it&#39;s a good illustration of how scientific folklore can replace actual history.</i></b><br /><br /><b><i>Let us start with the myth that the Copernican model was opposed because it was a blow to human pride, dethroning Earth from its privileged position as the center of the universe.</i></b><i> Dennis Danielson, in his fine article on the subject, [2] shows how widespread that view is by quoting the eminent geneticist Theodosius Dobzhansky. With Copernicus, Dobzhansky contends, &#39;Earth was dethroned from its presumed centrality and preeminence.&#39; Carl Sagan described Copernicanism as the first of a series of &#39;Great Demotions ... delivered to human pride.&#39; Astronomer Martin Rees has written, &#39;It is over 400 years since Copernicus dethroned the Earth from the privileged position that Ptolemy&#39;s cosmology accorded it.&#39; And Sigmund Freud remarked that Copernicus provoked outrage by his slight against humankind&#39;s &#39;naive self-love.&#39;</i><br /><br /><i>The squalid basement</i><br /><br /><b><i>Danielson, however, points out that in the early 16th century, the center of the universe was not considered a desirable place to be. &#39;In most medieval interpretations of Aristotelian and Ptolemaic cosmology, Earth&#39;s position at the center of the universe was taken as evidence not of its importance but ... its grossness.&#39;</i></b><br /><br /><b><i>In fact, ancient and medieval Arabic, Jewish, and Christian scholars believed that the center was the worst part of the universe, a kind of squalid basement where all the muck collected. One medieval writer described Earth&#39;s location as &#39;the excrementary&#39; and filthy parts of the lower world.&#39; We humans, another asserted, are &#39;lodged here in the dirt and filth of the world, nailed and rivetted to the worst and deadest part of the universe, in the lowest story of the house, and most remote from the heavenly arch.&#39; In 1615 Cardinal Robert Bellarmine, a prominent persecutor of Galileo, said that &#39;the Earth is very far from heaven and sits motionless at the center of the world.&#39; [2]</i></b><br /><br /><b><i>In Dante Alighieri&#39;s The Divine Comedy, hell itself is placed in Earth&#39;s innermost core. Dante also speaks of hell in ways consistent with Aristotelian dynamics -- not full of flames, which would be displaced skyward by the heavier Earth, but as frozen and immobile.</i></b><br /><br /><b><i>By contrast, heaven was up, and the further up you went, away from the center, the better it was. So Copernicus, by putting the Sun at the center and Earth in orbit around it, was really giving its inhabitants a promotion by taking them closer to the heavens.</i></b><br /><br /><i>When and why did the history become distorted? Danielson doesn&#39;t pinpoint when the erroneous view gained supremacy. But he says that </i><b><i>from 1650 onward one can find some writers making this revisionist claim. By the late 18th century it had taken hold completely. Johann Wolfgang von Goethe (1749-1832), for example, wrote: &#39;Perhaps no discovery or opinion ever produced a greater effect on the human spirit than did the teaching of Copernicus. No sooner was the Earth recognized as being round and self-contained, than it was obliged to relinquish the colossal privilege of being the center of the world.&#39; Here Goethe managed to propagate another major distortion: the notion that before Copernicus (and Columbus) it was not known that Earth was a sphere.</i></b><i> [3,4]</i><br /><br /><i>Aristotle&#39;s cosmology</i><br /><br /><i>Even Aristotle did not believe Earth to be the center of the universe. He thought it rather to be </i><b><i>at the center</i></b><i>. This fine distinction was not driven by religious dogma or human self-importance but by physics arguments: In Aristotle&#39;s cosmology the universe was finite and the heavens existed beyond its outermost sphere. The universe had a center -- defined as the center of the large outer sphere in which the stars were embedded — and matter was drawn to that center. </i><b><i>In that cosmology, &#39;up&#39; and &#39;down&#39; were well defined. &#39;Down&#39; was toward the center of the universe and &#39;up&#39; was away from it, toward the sphere containing the stars.</i></b><br /><br /><b><i>The elements were earth, air, water, and fire, and each element had its natural affinity for a location in the universe. As could be seen from the fact that rocks fell to the ground, earth, being heavy, was drawn to the center. Flames leaping upwards showed that fire, being light, was drawn towards the heavens. The model explained many things, such as why objects fell to the ground when released from any point and why Earth&#39;s surface was spherical. It also explained why Earth was motionless at the center. For it to move, there would have to be something that took it away from the center. And no such agent was in evidence.</i></b><br /><br /><b><i>In his book The Copernican Revolution, historian Thomas Kultn pointed out that Aristotle was clearly saying that Earth was at the center of the universe not because it was especially important but simply because it was massive: &#39;It so happens that the Earth and the Universe have the same center, for the heavenly bodies do move towards the center of the Earth, yet only incidentally, because it has its center at the center of the universe.&quot;</i></b><i> [5]</i><br /><br /><i>Problems with heliocentricity</i><br /><br /><b><i>Copernicus&#39;s heliocentric model, on the other hand, created all manner of difficulties. It required Earth to be in motion, but it did not say what caused it to move away from the center. If Earth was not stationary at the center but was midway in the sequence of planetary orbits around the Sun, how could you define &#39;up&#39; and &#39;down&#39;? Why would objects fall &#39;down&#39; if Earth were not at the center of the universe? How could objects thrown upward fall back to the same point if Earth was not at rest? Earth was still believed to be the most massive object in the universe. So if it was not drawn to a fixed point at the center, did that mean that the universe had no center? Could that mean that the universe was infinite?</i></b><br /><br /><i>Kuhn argues that there were thus excellent reasons for rejecting the upstart Copernicus and retaining Aristotelian cosmology and its elaboration in Ptolemaic astronomy. </i><b><i>Accepting Copernicus would not simply replace one astronomical model with another. It also meant that a whole class of physics problems that had been considered solved were now suddenly unsolved. Therefore much of the initial resistance came from within the physics and astronomy contmunities rather than from the church.</i></b><br /><br /><i>In fact, awareness of Copernicus&#39;s work was at first largely restricted to the community of astronomers. Only they were interested in improving the calculation of planetary motions. Copernicus was widely respected as one of Europe&#39;s leading astronomers, and reports about his work, including his heliocentric hypothesis, had been circulating since 1515. So when his De Revolutionibus Orbium Coelestium (On the Revolutions of the Celestial Spheres) was published 28 years later, it was hardly a surprise to other astronomers. They accepted it as the most comprehensive account of celestial motions since Ptolemy.</i><br /><br /><i>But most astronomers also felt that the Ptolemaic system, although complicated, could ultimately he made to work. So while they hailed Copernicus&#39;s work and used his tables and methods, </i><b><i>they were skeptical of his central idea of a moving Earth. They dismissed it as an ad hoc trick</i></b><i> (much as Max Planck&#39;s quantum hypothesis was initially viewed centuries later) that turned out to be a useful tool for calculations. Thee idea that the motion described by some artificial model was a convenient fiction was not unprecedented. </i><b><i>Ptolemy himself had said that not all of his epicycles had to be considered physically real. Some were to be thought of as merely mathematical devices that gave sound results.</i></b><br /><br /><b><i>Initially, however, the Copernican system did not give better numerical results than the Ptolemaic.</i></b><i> Part of the problem was that some of the existing astronomical observations were simply erroneous, a problem that plagued Ptolemaic and Copernican astronomy alike. Although better observations soon eliminated some of those problems, other problems remained obdurate [stubborn, unyielding] for a long time. Furthermore, at the level of accuracy available to Copernicus, the introduction of ellipses in place of circular orbits would not have helped. What Copernicus needed to do, as historian Owen Gingerich puts it, was to &#39;treat Earth and Mercury the same way as the other planets.&#39;</i><br /><br /><i>Kuhn says of Copernicus: </i><b><i>&#39;His full system was little if any less cumbersome than Ptolemy&#39;s had been. Both employed over thirty circles; there was little to choose between them in economy. Nor could the two systems be distinguished by their accuracy. When Copernicus had finished adding circles, his cumbersome sun-centered system gave results as accurate as Ptolemy&#39;s, but did not give more accurate results.</i></b><i> Copernicus had failed to solve the problem of the planets.&#39; [5]</i><br /><br /><i>Advantages</i><br /><br /><b><i>The Copernican model did have some aesthetic and qualitative advantages. It provided a more natural qualitative explanation for the zigzag motion of planets like Mars as observed from Earth, and it answered some important questions about the ordering of the planets. That&#39;s why heliocentrism was eventually accepted. As Kulin puts it, &#39;De Revolutionibus did convince a few of Copernicus&#39; successors that sun-centered astronomy held the key to the problem of the planets, and these men finally provided the simple and accurate solution that Copernicus had sought.&#39;</i></b><i> [5]</i><br /><br /><b><i>That&#39;s an important point about scientific revolutions. At the start, the new theory rarely gives convincingly better results than its predecessor. What usually happens is that it has some appeal, often aesthetic, that attracts others to work within the new model. And if, over time, the new model proves fruitful in resolving many puzzles, it gains adherents.</i></b><i> [6]</i><br /><br /><i>The success of the Copernican model was aided by the work of the Danish astronomer Tycho Brahe, who died a few years before the invention of the telescope. Tycho is considered the greatest of the naked-eye observers. His wide-ranging and accurate observations had an enormous impact.</i><br /><br /><i>Although Tycho&#39;s pivotal role is recognized, what is less well known is that he, like most astronomers at the time, rejected Copernicus&#39;s ideas of a moving Earth. It created more problems, he thought, than it solved. But despite Tycho&#39;s opposition, his observations provided two major benefits for the heliocentric model: They got rid of some erroneous old data that had plagued all the earlier models and thus helped to remove some of the anomalies that the Copernican system couldn&#39;t explain. More important, the precision of Tycho&#39;s data provided puzzles that enabled Kepler, a convert to Copernicanism, to come up with the key idea that the motions of the planets were not circular -- as Ptolemy, Copernicus, and Tycho had all assumed -- but elliptical.</i><br /><br /><i>In the folklore that surrounds Copernicus, the introduction of elliptical orbits is rightly recognized as a crucial development that led to ultimate acceptance of his model. The pre-Keplerian astronomers, however, are unfairly characterized as insisting on circular motion because of aesthetic considerations, slavish adherence to the authority of the Greeks, and so forth. But at the time, the reasons for assuming circular motions were quite sensible. Because there were no good theories of force or gravity, one needed to have an explanation of motion. Circular motion could be explained by a plausible hand-waving argument. One could say that it was an initial condition -- that once an object had been set in circular motion it would, if undisturbed, continue circling forever.</i><br /><br /><i>More complicated motions like elliptical orbits would mean that the planets&#39; speeds and distances from the Sun were constantly changing. But that required a dynamical theory that simply did not exist in those pre-Newtonian times. Just introducing the idea of a moving Earth created all kinds of unsolved problems for the physical theories of the day. Adding noncircular motion would have compounded those problems, providing even stronger grounds for rejecting Copernicus.</i><br /><br /><i>Kepler&#39;s innovative idea of elliptical orbits, coupled with his law of areas, did let the Copernican model dispense with cumbersome epicycles. But his accurate Rudolphine Tables for planetary motion, published in 1627, were difficult to use. It was Newton&#39;s theories of motion and gravity, not published until 60 years later, that sealed the scientific case in favor of Copernicus by putting his model on a firm theoretical footing.</i><br /><br /><i>Religious objections</i><br /><br /><i>The actual religious reaction to the heliocentric model also differs from the folklore. For one thing, </i><b><i>Copernicus did not seem to fear religious opposition to his ideas. After all, he was a reputable cleric himself. He even dedicated his book to Pope Paul III with a letter in which he apologized for the seeming outlandishness of his suggestion that the Earth moved. He explained that he was forced to that hypothesis by the inadequacy of the Ptolemaic system for constructing calendars and predicting the positions of stars. A cardinal and a bishop were among those who urged him to publish his book. In fact, for 60 years after Copernicus&#39;s death just two months after its publication, De Revolutionibus was read and at least partially taught at leading Catholic universities.</i></b><br /><br /><i>In 1600 the church did burn at the stake the philosopher Giordano Bruno, an adherent of Copernicus, for heresy. </i><b><i>But Bruno was condemned for other heresies against Christian doctrine rather than explicitly for being a Copernican.</i></b><i> However, the fact that Bruno had been an advocate and popularizer of heliocentrism may have led to the later perception that he was the first martyr of the new science.</i><br /><br /><i>For many years after the publication of De Revolutionibus, while Copernicus&#39;s ideas remained within the mathematical astronomy community, authors of more popular books on astronomy and cosmology were either unaware of his work or chose to ignore it. A few nonastronomers did ridicule it -- not for being heretical but for promulgating the patently absurd idea of a moving Earth.</i><br /><br /><b><i>It was through popularizers, some of them poets, that Copernicus&#39;s ideas eventually became more widely known and began to spark religious opposition.</i></b><i> But here too, the actual history is surprising. Opposition arose initially among Protestant groups rather than from the Roman Catliolic Church.</i><br /><br /><i>Kuhn suggests that this was because Martin Luther (1483-1546) and other leaders of the Reformation were emphasizing the Bible as the fundamental source of Christian knowledge and authority. And there were manifest contradictions between the Bible and Copernicus. </i><b><i>The Catholic Church, by focusing more on doctrinal issues, actually had greater flexibility in dealing with science.</i></b><br /><br /><i>Luther spoke out against heliocentrism in 1539, saying that the idea of a moving Earth going around a stationary Sun clearly went against the account in the book of Joshua that says Joshua commanded the Sun to stand still. Luther&#39;s deputy Philipp Melanchthon followed up by finding other biblical verses that described Earth as stationary.</i><br /><br /><i>The conflict between scripture and Copernicanism was not limited to verses that involved the motion of Sun or Earth. The realization was growing that acceptance of Copernicanism raised other profound theological difficulties as well. As Kuhn points out, the problems just kept multiplying:</i><br /><br /><i>&#39;When it was taken seriously, Copernicus&#39; proposal raised many gigantic problems for the believing Christian. If, for example, the Earth were merely one of six planets, how were the stories of the Fall and of the Salvation, with their immense bearing on Christian life, to be preserved? If there were other bodies essentially like the Earth, God&#39;s goodness would surely necessitate that they, too, be inhabited. But if there were men on other planets, how could they be descendants of Adam and Eve, and how could they have inherited the original sin? ... Again, how could men on other planets know of the Savior who opened to them the possibility of eternal life? Or, if the Earth is a planet and therefore a celestial body located away from the center of the universe, what becomes of man&#39;s intermediate but focal position between the devils and the angels? If the Earth, as a planet, participates in the nature of celestial bodies, it cannot be a sink of iniquity [gross injustice or wickedness] from which man will long to escape to the divine purity of the heavens. Nor can the heavens be a suitable abode for God if they participate in the evils and imperfections so clearly visible on a planetary Earth. Worst of all, if the universe is infinite, as many of the later Copernicans thought, where can God&#39;s Throne be located? In an infinite universe, how is man to find God or God man?&#39; [5]</i><br /><br /><b><i>As time went on, Copernicus&#39;s ideas were seen as seriously disturbing to Christianity; they had to be countered. Soon the Bible became the main weapon used against Copernicus. Protestant and Catholic clerics in the 17th century started combing through it for ammunition. People started calling the Copernicans infidels and atheists and urged their repression. But the new Protestant churches did not have the powers of suppression and enforcement that the long-established Catholic Church had.</i></b><br /><br /><i>Kuhn argues that it was probably the menace of burgeoning Protestantism that caused the Catholic hierarchy </i><b><i>in 1616 to switch abruptly from tolerance of Copernicanism to repression.</i></b><i> &#39;Copernican doctrines were, in fact, condemned during the Counter Reformation, just when the Church was most convulsed by internal reforms designed to meet Protestant criticism. Anti-Copernicanism seems, at least in part, one of these reforms. </i><b><i>Another cause of the Church&#39;s increased sensitivity to Copernicanism after 1610 [the year Galileo first turned a telescope to the heavens] may well have been a delayed awakening to the fuller theological implications of the Earth&#39;s motions. In the I6th century those implications had rarely been made explicit.&#39;</i></b><i> [5]</i><br /><br /><b><i>The idea of the Copernican model being a demotion for humanity probably first developed around 1650, after the scientific community had already accepted heliocentrism.</i></b><i> Religious bodies undertook what was essentially a propaganda war against Copernicus. What probably happened was that after the heliocentric model had been well established, the location of the Sun did come to be perceived as a privileged place. So people read back into history the newly believed excellence of the center and attributed that belief retrospectively to the pre-Copernicans. The demotion idea may have been introduced as part of the effort to rally nonscientific religious people to turn against Copernicanism by appealing to their pride as human beings.</i><br /><br /><b><i>The Protestant churches abandoned their opposition to Copernicanism fairly quickly when it became clear that the evidence in favor of a Sun-centered system was overwhelming. But the Catholic Church, being a much larger and more tradition-bound and bureaucratic institution, was left clinging to its anti-Copernican views for a long time. Its ban on Copernicus remained until 1822, and his book remained on the forbidden list until 1835. In fact it was only in 1992 that Pope John Paul II lifted the edict of inquisition against Galileo. Thus the Roman Catholic Church is now generally regarded as the principal villain in perhaps the most notorious episode in the history of science.</i></b><br /><br /><b><i>What can we learn from all this? The story of the Copernican revolution shows that the actual history of science often bears little resemblance to the popular capsule versions that are learned in school or college or portrayed in textbooks and the popular media. Steven Weinberg calls them &#39;potted history.&#39; The true story is much more complicated, but it&#39;s also a lot more interesting.</i></b><br /><br /><i>My thanks to Ouvn Gingerich for an enlightening discussion and many helpful suggestions.</i><br /><br /><i>References</i><br /><br /><i>1. See, for example, P. Fishbane, S. Gasiorowicz, S. Thornton, Physics for Scientists and Engineers, 2nd ed., Prentice Hall, Upper Saddle River, NJ (1996), pp.1, 320, 321.</i><br /><br /><i>2. D. R. Danieison, Am. J. Phys. 69, 1029 (2001).</i><br /><br /><i>3. M. Singham, Phi Delta Kappan 88, 590 (2007).</i><br /><br /><i>4. J. B. Russell, Inventing the Flat Earth: Columbus and Modern Historians, Praeger, New York (1991).</i><br /><br /><i>5. T. Kuhn, The Copernican Revolution: Planetary Astronomy in the Development of Western Thought, Harvard U. Press, Cambridge, MA (1957).</i><br /><br /><i>6. I. Lakatos, The Methodology of Scientific Research Programmes, Cambridge U. Press, New York (1978).&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"Changing Radioactive Decay Rate as a Solar Flare Predictor","summary":"Neutrinos are Thought to Have No Mass or Charge, Hence the Idea that they Could be Interacting with Anything is Foreign to Physics / Yet, it is Currently the Favored Mainstream Explanation for How a Change in Decay Rate Can Precede a Large Solar Flare by 39 Hours / Whatever the Cause, the Precursor Signal has been Observed by Now for 10 Solar Flares / There's Possibly New Physics Here, but Theorists Insist Upon Interpreting the Observations Using Existing Theory","image":"https://lh3.googleusercontent.com/-QGeIhIqO4dU/V-tVWgEoQdI/AAAAAAAAI1s/cWsnxT_LvUIYXjsja8AKLgAtfczRc_bHgCJoC/w5400-h7560/changing-decay-rate-solar-flare-predictor-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/ZANhKQpb4Zp","publishDate":"2016-09-28T05:30:30.854Z","updateDate":"2017-01-03T01:58:40.632Z","text":"<b>Changing Radioactive Decay Rate as a Solar Flare Predictor: Neutrinos are Thought to Have No Mass or Charge, Hence the Idea that they Could be Interacting with Anything is Foreign to Physics / Yet, it is Currently the Favored Mainstream Explanation for How a Change in Decay Rate Can Precede a Large Solar Flare by 39 Hours / Whatever the Cause, the Precursor Signal has been Observed by Now for 10 Solar Flares / There&#39;s Possibly New Physics Here, but Theorists Insist Upon Interpreting the Observations Using Existing Theory</b><br /><br /><a href=\"http://physicsworld.com/cws/article/news/2008/oct/02/the-mystery-of-the-varying-nuclear-decay\" class=\"ot-anchor\">http://physicsworld.com/cws/article/news/2008/oct/02/the-mystery-of-the-varying-nuclear-decay</a><br /><br /><i>&quot;The Purdue researchers submitted a paper on the solar flare correlations to Physical Review Letters but it was rejected, they say, because there was no mechanism to back it up&quot;</i><br /><br /><a href=\"http://phys.org/news/2012-08-solar-flares-advance.html\" class=\"ot-anchor\">http://phys.org/news/2012-08-solar-flares-advance.html</a><br /><br /><i>&quot;New system could predict solar flares, give advance warning</i><br /><br /><i>August 13, 2012 by Emil Venere (Phys.org) -- </i><b><i>Researchers may have discovered a new method to predict solar flares more than a day before they occur</i></b><i>, providing advance warning to help protect satellites, power grids and astronauts from potentially dangerous radiation.</i><br /><br /><b><i>The system works by measuring differences in gamma radiation emitted when atoms in radioactive elements &#39;decay,&#39; or lose energy. This rate of decay is widely believed to be constant, but recent findings challenge that long-accepted rule.</i></b><br /><br /><i>The new detection technique is based on a hypothesis that radioactive decay rates are influenced by solar activity, possibly streams of subatomic particles called solar neutrinos. This influence can wax and wane due to seasonal changes in the Earth&#39;s distance from the sun and also during solar flares, according to the hypothesis, which is supported with data published in a dozen research papers since it was proposed in 2006, said Ephraim Fischbach, a Purdue University professor of physics.&quot;</i><br /><br />The science journalist leaves the impression that the observation of changing radioactive decay is explained.  They also miss the fact that Russian scientist Simon Shnoll has been studying the effect since 1958.  That said, it seems that the Purdue researcher, Jenkins, was the first to notice the correlation with solar flares.<br /><br /><i>&quot;Fischbach and Jere Jenkins, a nuclear engineer and director of radiation laboratories in the School of Nuclear Engineering, are leading research to study the phenomenon and possibly develop a new warning system. </i><b><i>Jenkins, monitoring a detector in his lab in 2006, discovered that the decay rate of a radioactive sample changed slightly beginning 39 hours before a large solar flare.</i></b><br /><br /><i>Since then, researchers have been examining similar variation in decay rates before solar flares, as well as those resulting from Earth&#39;s orbit around the sun and changes in solar rotation and activity. The new findings appeared online last week in the journal Astroparticle Physics.&quot;</i><br /><br />Once again, ignoring Shnoll&#39;s work ...<br /><br /><i>&quot;&#39;It&#39;s the first time the same isotope has been used in two different experiments at two different labs, and it showed basically the same effect,&#39; Fischbach said. The paper was authored by Jenkins and Fischbach; Ohio State University researchers Kevin R. Herminghuysen, Thomas E. Blue, Andrew C. Kauffman and Joseph W. Talnagi; U.S. Air Force researcher Daniel Javorsek; Mayo Clinic researcher Daniel W. Mundy; and Stanford University researcher Peter A. Sturrock.</i><br /><br /><i>Data were recorded during routine weekly calibration of an instrument used for radiological safety at Ohio State&#39;s research reactor. Findings showed a clear annual variation in the decay rate of a radioactive isotope called chlorine 36, with the highest rate in January and February and the lowest rate in July and August, over a period from July 2005 to June 2011.</i><br /><br /><i>The new observations support previous work by Jenkins and Fischbach to develop a method for predicting solar flares. Advance warning could allow satellite and power grid operators to take steps to minimize impact and astronauts to shield themselves from potentially lethal radiation emitted during solar storms.</i><br /><br /><i>The findings agree with data previously collected at the Brookhaven National Laboratory regarding the decay rate of chlorine 36; changes in the decay rate were found to match changes in the Earth-sun distance and Earth&#39;s exposure to different parts of the sun itself, Fischbach said.</i><br /><br /><i>Large solar flares may produce a &#39;coronal mass ejection&#39; of highly energetic particles, which can interact with the Earth&#39;s magnetosphere, triggering geomagnetic storms that sometimes knock out power. The sun&#39;s activity is expected to peak over the next year or so as part of an 11-year cycle that could bring strong solar storms.</i><br /><br /><i>Solar storms can be especially devastating if the flare happens to be aimed at the Earth, hitting the planet directly with powerful charged particles. A huge solar storm, called the Carrington event, hit the Earth in 1859, a time when the only electrical infrastructure consisted of telegraph lines.</i><br /><br /><i>&#39;There was so much energy from this solar storm that the telegraph wires were seen glowing and the aurora borealis appeared as far south as Cuba,&#39; Fischbach said. &#39;Because we now have a sophisticated infrastructure of satellites, power grids and all sort of electronic systems, a storm of this magnitude today would be catastrophic. </i><b><i>Having a day and a half warning could be really helpful in averting the worst damage.&#39;</i></b><br /><br /><i>Satellites, for example, might be designed so that they could be temporarily shut down and power grids might similarly be safeguarded before the storm arrived.</i><br /><br /><b><i>Researchers have recorded data during 10 solar flares since 2006, seeing the same pattern.</i></b><br /><br /><b><i>&#39;We have repeatedly seen a precursor signal preceding a solar flare,&#39;</i></b><i> Fischbach said. &#39;We think this has predictive value.&#39;</i><br /><br /><i>The Purdue experimental setup consists of a radioactive source -- manganese 54 -- and a gamma-radiation detector. As the manganese 54 decays, it turns into chromium 54, emitting a gamma ray, which is recorded by the detector to measure the decay rate.</i><br /><br /><i>Purdue has filed a U.S. patent application for the concept.</i><br /><br /><b><i>Research findings show evidence that the phenomenon is influenced by the Earth&#39;s distance from the sun; for example, decay rates are different in January and July, when the Earth is closest and farthest from the sun, respectively.</i></b><br /><br /><b><i>&#39;When the Earth is farther away, we have fewer solar neutrinos and the decay rate is a little slower,&#39; Jenkins said. &#39;When we are closer, there are more neutrinos, and the decay a little faster.&#39;</i></b><br /><br /><i>Researchers also have recorded both increases and decreases in decay rates during solar storms.</i><br /><br /><i>&#39;What this is telling us is that the sun does influence radioactive decay,&#39; Fischbach said.</i><br /><br /><i>Neutrinos have the least mass of any known subatomic particle, yet it is plausible that they are somehow affecting the decay rate, he said.</i><br /><br /><i>English physicist Ernest Rutherford, known as the father of nuclear physics, in the 1930s conducted experiments indicating the radioactive decay rate is constant, meaning it cannot be altered by external influences.</i><br /><br /><b><i>&#39;Since neutrinos have essentially no mass or charge, the idea that they could be interacting with anything is foreign to physics,&#39; Jenkins said. &#39;So, we are saying something that doesn&#39;t interact with anything is changing something that can&#39;t be changed. Either neutrinos are affecting decay rate or perhaps an unknown particle is.&#39;</i></b><br /><br /><b><i>Jenkins discovered the effect by chance in 2006, when he was watching television coverage of astronauts spacewalking at the International Space Station. A solar flare had erupted and was thought to possibly pose a threat to the astronauts. He decided to check his equipment and discovered that a change in decay-rate had preceded the solar flare.</i></b><br /><br /><i>Further research is needed to confirm the findings and to expand the work using more sensitive equipment, he said.</i><br /><br /><i>Jenkins and Fischbach have previously collaborated with Peter Sturrock, a professor emeritus of applied physics at Stanford University and an expert on the inner workings of the sun, to examine data collected at Brookhaven on the decay rate of radioactive isotopes silicon-32 and chlorine-36. The team reported in 2010 in Astroparticle Physics that the decay rate for both isotopes varies in a 33-day recurring pattern, which they attribute to the rotation rate of the sun&#39;s core.</i><br /><br /><i>The group found evidence of the same annual and 33-day effect in radium-226 data taken at the Physikalisch-Technische Bundesanstalt (PTB) in Braunschweig, Germany, and those findings were published in 2011. They also found an additional 154-day recurring pattern in both the Brookhaven and PTB data, published in 2011, which they believe to be solar related and similar to a known solar effect called a Rieger periodicity.&quot;</i><br /><br />The following article contains a variety of other details, which I&#39;m including because I&#39;m not entirely sure about their chain of logic ...<br /><br /><a href=\"http://news.stanford.edu/news/2010/august/sun-082310.html\" class=\"ot-anchor\">http://news.stanford.edu/news/2010/august/sun-082310.html</a><br /><br />Stanford Report, August 23, 2010<br /><br /><i>&quot;The strange case of solar flares and radioactive elements</i><br /><br /><i>When researchers found an unusual linkage between solar flares and the inner life of radioactive elements on Earth, it touched off a scientific detective investigation that could end up protecting the lives of space-walking astronauts and maybe rewriting some of the assumptions of physics.</i><br /><br /><i>by Dan Stober</i><br /><br /><i>It&#39;s a mystery that presented itself unexpectedly: The radioactive decay of some elements sitting quietly in laboratories on Earth seemed to be influenced by activities inside the sun, 93 million miles away.</i><br /><br /><i>Is this possible?</i><br /><br /><i>Researchers from Stanford and Purdue University believe it is. But their explanation of how it happens opens the door to yet another mystery.</i><br /><br /><i>There is even an outside chance that this unexpected effect is brought about by a previously unknown particle emitted by the sun. &#39;That would be truly remarkable,&#39; said Peter Sturrock, Stanford professor emeritus of applied physics and an expert on the inner workings of the sun.</i><br /><br /><i>The story begins, in a sense, in classrooms around the world, where students are taught that the rate of decay of a specific radioactive material is a constant. This concept is relied upon, for example, when anthropologists use carbon-14 to date ancient artifacts and when doctors determine the proper dose of radioactivity to treat a cancer patient.</i><br /><br /><i>Random numbers</i><br /><br /><b><i>But that assumption was challenged in an unexpected way by a group of researchers from Purdue University who at the time were more interested in random numbers than nuclear decay.</i></b><i> (Scientists use long strings of random numbers for a variety of calculations, but they are difficult to produce, since the process used to produce the numbers has an influence on the outcome.)</i><br /><br /><b><i>Ephraim Fischbach, a physics professor at Purdue, was looking into the rate of radioactive decay of several isotopes as a possible source of random numbers generated without any human input.</i></b><i> (A lump of radioactive cesium-137, for example, may decay at a steady rate overall, but individual atoms within the lump will decay in an unpredictable, random pattern. Thus the timing of the random ticks of a Geiger counter placed near the cesium might be used to generate random numbers.)</i><br /><br /><b><i>As the researchers pored through published data on specific isotopes, they found disagreement in the measured decay rates – odd for supposed physical constants.</i></b><br /><br /><i>Checking data collected at Brookhaven National Laboratory on Long Island and the Federal Physical and Technical Institute in Germany, they came across something even more surprising: </i><b><i>long-term observation of the decay rate of silicon-32 and radium-226 seemed to show a small seasonal variation. The decay rate was ever so slightly faster in winter than in summer.</i></b><br /><br /><i>Was this fluctuation real, or was it merely a glitch in the equipment used to measure the decay, induced by the change of seasons, with the accompanying changes in temperature and humidity?</i><br /><br /><i>&#39;Everyone thought it must be due to experimental mistakes, because we&#39;re all brought up to believe that decay rates are constant,&#39; Sturrock said.</i><br /><br /><i>The sun speaks</i><br /><br /><i>On Dec 13, 2006, the sun itself provided a crucial clue, when a solar flare sent a stream of particles and radiation toward Earth. Purdue nuclear engineer Jere Jenkins, while measuring the decay rate of manganese-54, a short-lived isotope used in medical diagnostics, </i><b><i>noticed that the rate dropped slightly during the flare, a decrease that started about a day and a half before the flare.</i></b><br /><br /><i>If this apparent relationship between flares and decay rates proves true, it could lead to a method of predicting solar flares prior to their occurrence, which could help prevent damage to satellites and electric grids, as well as save the lives of astronauts in space.</i><br /><br /><i>The decay-rate aberrations that Jenkins noticed occurred during the middle of the night in Indiana – </i><b><i>meaning that something produced by the sun had traveled all the way through the Earth to reach Jenkins&#39; detectors.</i></b><i> What could the flare send forth that could have such an effect?&quot;</i><br /><br />(This inference may in fact be the weakest chain of their logic.  Think carefully about what they are saying: They are suggesting that two things which are not supposed to be related in fact are.  They do not yet understand the nature of this relationship, or the nature of the mistake which has obscured this relationship.<br /><br />But, then they claim that the Sun must nevertheless be the source for the effect.  Is it not possible that the solar flare is the result of some other physical process?  What if the density of electrons in the vicinity of the Sun is not perfect, and if they are at all times falling into the Sun?  Then, couldn&#39;t this incoming &quot;electron drift&quot; alternatively explain the observations without recourse to neutrinos?<br /><br />When we are contemplating the possibility of a mistake in the textbook theory, we should not be so quick to use the existing theory to constrain our explanations.  If we&#39;re smart, we&#39;ll more broadly consider competing paradigms.)<br /><br /><b><i>&quot;Jenkins and Fischbach guessed that the culprits in this bit of decay-rate mischief were probably solar neutrinos, the almost weightless particles famous for flying at almost the speed of light through the physical world – humans, rocks, oceans or planets – with virtually no interaction with anything.</i></b><br /><br /><i>Then, in a series of papers published in Astroparticle Physics, Nuclear Instruments and Methods in Physics Research and Space Science Reviews, Jenkins, Fischbach and their colleagues showed that the observed variations in decay rates were highly unlikely to have come from environmental influences on the detection systems.</i><br /><br /><i>Reason for suspicion</i><br /><br /><i>Their findings strengthened the argument that the strange swings in decay rates were caused by neutrinos from the sun. </i><b><i>The swings seemed to be in synch with the Earth&#39;s elliptical orbit, with the decay rates oscillating as the Earth came closer to the sun (where it would be exposed to more neutrinos) and then moving away.</i></b><br /><br /><i>So there was good reason to suspect the sun, but could it be proved?</i><br /><br /><i>Enter Peter Sturrock, Stanford professor emeritus of applied physics and an expert on the inner workings of the sun. While on a visit to the National Solar Observatory in Arizona, Sturrock was handed copies of the scientific journal articles written by the Purdue researchers.</i><br /><br /><b><i>Sturrock knew from long experience that the intensity of the barrage of neutrinos the sun continuously sends racing toward Earth varies on a regular basis as the sun itself revolves and shows a different face, like a slower version of the revolving light on a police car. His advice to Purdue: Look for evidence that the changes in radioactive decay on Earth vary with the rotation of the sun. &#39;That&#39;s what I suggested. And that&#39;s what we have done.&#39;</i></b><br /><br /><i>A surprise</i><br /><br /><b><i>Going back to take another look at the decay data from the Brookhaven lab, the researchers found a recurring pattern of 33 days. It was a bit of a surprise, given that most solar observations show a pattern of about 28 days – the rotation rate of the surface of the sun.</i></b><br /><br /><b><i>The explanation? The core of the sun – where nuclear reactions produce neutrinos – apparently spins more slowly than the surface we see. &#39;It may seem counter-intuitive, but it looks as if the core rotates more slowly than the rest of the sun,&#39; Sturrock said.</i></b><br /><br /><i>All of the evidence points toward a conclusion that the sun is &#39;communicating&#39; with radioactive isotopes on Earth, said Fischbach.</i><br /><br /><b><i>But there&#39;s one rather large question left unanswered. No one knows how neutrinos could interact with radioactive materials to change their rate of decay.</i></b><br /><br /><b><i>&#39;It doesn&#39;t make sense according to conventional ideas,&#39; Fischbach said. Jenkins whimsically added, &#39;What we&#39;re suggesting is that something that doesn&#39;t really interact with anything is changing something that can&#39;t be changed.&#39;</i></b><br /><br /><i>&#39;It&#39;s an effect that no one yet understands,&#39; agreed Sturrock. &#39;Theorists are starting to say, &#39;What&#39;s going on?&#39; But that&#39;s what the evidence points to. It&#39;s a challenge for the physicists and a challenge for the solar people too.&#39;</i><br /><br /><i>If the mystery particle is not a neutrino, &#39;It would have to be something we don&#39;t know about, an unknown particle that is also emitted by the sun and has this effect, and that would be even more remarkable,&#39; Sturrock said.&quot;</i><br /><br />To make my point that we must broaden our perspectives when we consider anomalous data, here is a more electrical inference for the observation ...<br /><br /><a href=\"https://www.thunderbolts.info/tpod/2005/arch05/050404antarctic-fossil.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2005/arch05/050404antarctic-fossil.htm</a><br /><br /><i>&quot;... Russian researchers S. E. Schnoll, et al, have been studying the effects of celestial cycles on the decay constant for over 30 years. They document changes in the decay constant and in chemical reaction rates that correlate with moonrise/moonset, eclipses, the sidereal and synodic day, the year, and the sunspot cycle.</i><br /><br /><i>Each of these cycles shows that the decay rate is connected to something. </i><b><i>From an Electric Universe point of view, that something is probably electric currents in space. The synodic day variations (sunrise to sunrise) would correlate to the Sun&#39;s electric currents, while the sidereal day variations (from star-rise to star-rise, just under four minutes shorter than the synodic day) would correlate to something from beyond the solar system, such as the galactic electric currents.</i></b><br /><br /><b><i>Schnoll, et al, document that solar activity affects the decay rate of Plutonium239. This activity peaks when there are active solar flares or CME&#39;s. The decay rate spike occurs immediately after the solar activity, or 2 to 3 days before the solar wind arrives to change the Earth&#39;s geomagnetic field. This may mean that the cause of the changed decay rate arrives at Earth at the speed of light (or faster). Or it may mean that both the changes in decay rate and the solar activity have a common cause in galactic-scale electric currents</i></b><i> ...&quot;</i><br /><br />For those interested, there is a bit more analysis here:<br /><br /><a href=\"http://physicsworld.com/cws/article/news/2008/oct/02/the-mystery-of-the-varying-nuclear-decay\" class=\"ot-anchor\">http://physicsworld.com/cws/article/news/2008/oct/02/the-mystery-of-the-varying-nuclear-decay</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Changing Radioactive Decay Rate","summary":"A 2011 Purdue Paper Confirms that Radioactive Decay Rate is Not Completely Random / The Anomaly has been Studied by a Russian Scientist Named Simon Shnoll Since 1958 / Shnoll Found a Striking Similarity in the Fine Structure of Histograms for Different Types of Simultaneous Natural Processes - Even Across Labs Separated by 100's of Miles, Implicating a Cosmophysical Cause / One of the Most Important Discoveries of the 20th Century is Still Not Widely Recognized by Western Physicists","image":"https://lh3.googleusercontent.com/-58SfACMx6nM/V-ipLjToX6I/AAAAAAAAIz8/1N2U7hn9bMkEsRYMHPtqd6FLf2XNL2M0QCJoC/w7142-h9999/the-changing-radioactive-decay-rate-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/9HMwdFrUPQx","publishDate":"2016-09-26T04:51:39.052Z","updateDate":"2017-01-03T01:58:56.036Z","text":"<b>The Changing Radioactive Decay Rate: A 2011 Purdue Paper Confirms that Radioactive Decay Rate is Not Completely Random / The Anomaly has been Studied by a Russian Scientist Named Simon Shnoll Since 1958 / Shnoll Found a Striking Similarity in the Fine Structure of Histograms for Different Types of Simultaneous Natural Processes - Even Across Labs Separated by 100&#39;s of Miles, Implicating a Cosmophysical Cause / One of the Most Important Discoveries of the 20th Century is Still Not Widely Recognized by Western Physicists</b><br /><br /><a href=\"https://arxiv.org/pdf/1106.1678.pdf\" class=\"ot-anchor\">https://arxiv.org/pdf/1106.1678.pdf</a><br /><br />Analysis Of Experiments Exhibiting Time-varying Nuclear Decay Rates: Systematic Effects Or New Physics?<br /><br />J.H. Jenkins<br />School of Nuclear Engineering, Purdue University<br /><br />E. Fischbach<br />Department of Physics, Purdue University<br /><br />P.A. Sturrock<br />Center for Space Science and Astrophysics, Stanford University<br /><br />D.W. Mundy<br />Department of Radiation Oncology Physics<br /><br />June 8, 2011<br /><br /><i>&quot;It has long been a universal belief that radioactive decay is a random process, one that is almost completely insensitive to external influences. There have been a few special cases in recent times where minor changes in decay rates have been measured due to artificially produced changes in the physical environment of the decaying nuclides [1,2,3,4,5,6,7], but on the whole the assumption has been that radioactive decays follow the standard exponential decay law which is based on these decays being a random process. In recent years, however, a few independent groups have identified some interesting behaviors in measured nuclear decay rates that did not arise from a change in the physical or chemical environment of the decaying nuclei. [8,9,10,11,12,13,14,15] </i><b><i>In these results, there appears to be some structure in what should be randomly distributed data points. More recently, however, Recent work by our group [16,17,18,19,20,21,22,23] has gone further and detailed the existence of periodicities and other non-random behaviors in measured nuclear decay data from Purdue University [16], Brookhaven National Laboratory (BNL) [24], and the Physikalisch-Technische Bundesanstalt (PTB) [25]. The suggestion of this recent work is that there is a solar influence on the these measured decay rates, via some particle or field of solar origin such as solar neutrinos.</i></b><br /><br /><b><i>Such a proposal is, without question, going to generate criticism from the physics community, based on the belief that the observed effects were the result of changes in the environment of the detector systems (i.e., temperature, background, etc.) or systematic effects. [27,28,29,30] However, a thorough analysis by our group of the Purdue, BNL and PTB detector systems has effectively refuted essentially all of this criticism. [31] In this report we will further strengthen this view by providing additional perspective and results that support the conjecture that whatever is influencing the measured decay rates is external to the terrestrial environment, and could in fact have a solar origin.&quot;</i></b><br /><br />But, truthfully, these American researchers are really just replicating Russian academic work that has advanced far beyond just radioactive decay into the domains of both chemistry and biology.<br /><br /><a href=\"http://www.21stcenturysciencetech.com/articles/time.html\" class=\"ot-anchor\">http://www.21stcenturysciencetech.com/articles/time.html</a><br /><br />Russian Discovery Challenges Existence of ‘Absolute Time’<br />by Jonathan Tennenbaum<br /><br />(Full text of article from summer 2000 21st Century)<br /><br /><i>&quot;Russian scientists discover unexpected regularities in radioactive decay, linked to astronomical cycles</i><br /><br /><i>Two years ago, nearly unnoticed in the West, the Russian biophysicist S.E. Shnoll published a paper in the prominent Russian physics journal Uspekhi Fisicheskikh Nauk1 summing up the results of more than three decades of investigations of </i><b><i>anomalous statistical regularities in a wide range of physical, chemical, and biological processes, from radioactive decay to the rates of biochemical reactions.</i></b><br /><br /><b><i>The evidence points unambiguously to the existence of a previously unknown relationship between fluctuations in the rates of radioactive and other processes in the laboratory, and major astronomical cycles, including the day, month, and year. The implication is, that many phenomena which until now have been regarded as purely statistical in character -- such as the distribution of fluctuations in the momentary rates of radioactivity measured in a sample -- are somehow controlled or at least strongly influenced by an astrophysical factor, which varies in time in the same way at all points on the Earth.</i></b><br /><br /><i>Vladimir Voeikov, a colleague of Shnoll, comments in the Spring 2000 issue of 21st Century: &#39;Shnoll’s work shows that time is heterogeneous. It is not a Newtonian time. </i><b><i>Each moment in time is different from another, and this can be seen in any physical process that you study.&#39;</i></b><br /><br /><i>Albert Einstein, who rejected claims by Niels Bohr and others that the fundamental microphysical processes are essentially, irreducibly random in character, liked to say that &#39;God does not play dice.&#39; Einstein and others pointed to the arbitrary nature of Bohr’s argument: Just because physicists in Bohr’s time could not penetrate beyond the apparent randomness of radioactive decay and other microscopic processes, to find a deeper lawfulness and regularity underlying such processes, does not mean that science is doomed to remain in that state of ignorance forever!</i><br /><br /><b><i>By demonstrating the existence of a universal, astronomical factor influencing the fine structure of supposedly random fluctuations, Shnoll et al. have opened up an entirely new field of scientific investigation which is not supposed to exist, according to Bohr.</i></b><br /><br />A Simple Experiment<br /><br /><i>We now give a very brief description of the basic phenomenon discovered by Shnoll and his collaborators. The phenomenon itself is so astonishingly simple, that it is amazing that it has not attracted more attention until now. </i><b><i>The simplest case is the measurement of radioactive decay, where Shnoll has conducted thousands of experiments</i></b><i> of the following simple type. We take a radioactive sample, and place it in front of a suitable detector (such as a Geiger counter), which counts the individual acts of radioactive decay of nuclei in the sample by detecting the emitted particles. Assuming the half-life of the radioactive element involved is relatively long, the count-rate of the detector, in counts per second or per minute, will fluctuate around a certain average value, which is related to the number of radioactive atoms in the sample and their half-life.</i><br /><br /><b><i>This phenomenon of continual fluctuations in the number of counts per unit time, around a relatively fixed average value, is normally accounted for by assuming that the radioactive decay of any given atom is a random event, and the assumption that decay of a given atom occurs independently of the other atoms in the sample. Thus, each atom which has not yet decayed up to a certain moment in time, has a certain probability of decaying during the next minute -- a probability which is fixed for any given isotope by the character of that isotope, and virtually independent of the temperature, chemical environment, and activity of neighboring atoms.</i></b><br /><br /><b><i>An extraordinary phenomenon emerges, however, when we examine the fluctuations more carefully, with the help of a histogram: We fix a certain period of time (10 seconds, or a minute for example), and record the number of counts during each of a series of consecutive intervals of the given length. This gives us a sequence of whole numbers. We construct a histogram, by plotting the number of times a given whole number appears in the sequence, as a function of the number.</i></b><br /><br /><b><i>Now, from the standpoint of simple statistics we would expect the histogram curve to have a simple bell shape, with a maximum around the number corresponding to the overall average number of counts, and then declining gradually on both sides. Naturally, if the number of measurements is small, the histogram will look more irregular, owing to the effect of random fluctuations; but we would expect that as we increase the total time of measurement, the curve would become closer and closer to the ideal mathematical bell curve.</i></b><br /><br /><b><i>However, real measurements of radioactivity and many other processes, carried out by Shnoll and others over many years, give a completely different result! The histograms typically show several clearly defined peaks, which do not &#39;smooth out&#39; as we increase the number of measurements, but which actually become more and more pronounced!</i></b><br /><br /><i>In four histograms, each plotting the results of 1,200 consecutive measurements of the radioactivity of a sample of the iron isotope Fe-55, over 36-second intervals, the largest peak corresponds to the average count, of about 31,500 pulses per 36 seconds; but there are a number of other peaks, which we can see emerging more and more clearly as we follow the cumulative results of the first 100, 200, 300, and so on, measurements as &#39;layers&#39; under the main curve (Figure 1).</i><br /><br /><i>Change in Shape over Time</i><br /><br /><i>The histograms, made from more than two days from four successive 12-hour-long series of measurements, show another typical phenomenon discovered by Shnoll: </i><b><i>The shapes of the histograms change over time (Figure 2). Most remarkably, the shapes of histograms for independent measurements taken over the same time period, tend to be very similar.</i></b><br /><br /><b><i>For example, simultaneous measurement of the reaction rate of ascorbic acid, dichlorophenolindophenol (DCPIP), and beta activity of carbon-14 show histograms of very similar shape.</i></b><br /><br /><i>These and a large number of other experiments carried out by Shnoll and his collaborators over many years, point unambiguously to the existence of a universal factor influencing the shapes of histograms, and which varies in time. </i><b><i>Furthermore, the Russian researchers have discovered well-defined periods, over which similar histogram shapes tend to recur</i></b><i> (Figure 3).</i><br /><br /><b><i>To do this, they devised a computer-based algorithm for measuring the relative degree of &#39;closeness&#39; or similarity of histogram shapes, and on this basis carried out a computer analysis of hundreds of histograms taken over a long period. Examining the distribution of time intervals between &#39;similar&#39; histograms, they found strong peaks at 0 hours (that is, histograms made independently at the same time tend to be similar), at approximately 24 hours, at 27.28 days (probably corresponding to the synodic rotation of the Sun), and at three time intervals close to a year: 364.4, 365.2 and 366.6 days.</i></b><br /><br /><b><i>More recent data, just reported to the author, indicate that the &#39;24-hour&#39; period is actually slightly shorter, and corresponds quite precisely to a sidereal day! The latter would suggest, that at least one astronomical factor influencing histogram shape may originate outside the solar system, being associated with the orientation of the measuring station relative to the galaxy, and not only relative to the Sun.</i></b><br /><br /><i>Shnoll concludes: </i><b><i>&#39;From the data presented above, it follows that the &#39;idea of shape&#39; -- the fine structure of distributions of results of measurements of processes of diverse nature -- is determined by cosmological factors.&#39;</i></b><i> He does not put forward a definite hypothesis concerning the nature of these factors, but suggests as a possibility the notion of a global &#39;change of space-time structure,&#39; and notes that &#39;a sound analysis of such a hypothesis will possibly require experiments under different gravitational conditions.&#39;</i><br /><br /><i>Clearly, these results should be intensively followed up by scientists around the world.</i><br /><br /><i>Jonathan Tennenbaum, based in Wiesbaden, Germany, is a member of the scientific advisory board of 21st Century Science &amp; Technology magazine. He heads the Fusion Energy Foundation in Europe.</i><br /><br /><i>Notes</i><br /><br /><i>1. See S.E. Shnoll, V.A. Kolombet, E.V. Pozharskii, T.A. Zenchenko, I.M. Zvereva, and A.A. Konradov, 1998. &quot;Realization of discrete states during fluctuations in macroscopic processes,&quot; in Uspekhi Fisicheskikh Nauk, Vol. 41, No. 10, pp. 1025-1035. A new paper is currently in preparation. Shnoll’s group is based at Moscow State University.</i><br /><br /><i>When the &#39;Scientific Method&#39; Obstructs Science</i><br /><br /><i>Excerpts from the &quot;Conclusion&quot; of Shnoll et al., &quot;Realization of discrete states during fluctuations in macroscopic processes,&quot; in Uspekhi Fisicheskikh Nauk, Vol. 41, No. 10, pp. 1025-1035.</i><br /><br /><i>Concluding this brief account of studies performed at our laboratory, we would like to anticipate some naturally arising questions. </i><b><i>Forty years have passed since our first publication in 1958. Why then have there been no results from other laboratories? We believe that the main reason is that other researchers are too well aware of the &#39;principles of science.&#39; We are talking of the &#39;spread of readings&#39; of measurements. The &#39;spread of readings&#39; is something to be eliminated rather than studied. When physicists or chemists get a scatter of data greater than anticipated on account of inaccuracies of individual stages of investigations, the physicist will reach out for his soldering iron and screwdriver, and the chemist will check the purity of reactants and the quality of distilled water.</i></b><br /><br /><i>Another reason is that the accepted methods of statistical data processing based on the central limit theorems are not suited for analysis of the fine structure of the distributions. The criteria of conformity of hypotheses just &#39;overlook&#39; this fine structure. The distributions are averaged and smoothed ... Moreover, the majority of problems do not require knowledge of the fine structure of the distributions.</i><br /><br /><b><i>A third reason is a lack of confidence that this phemomenon is at all possible. The scatter of data is associated with the concept of &#39;error.&#39;</i></b><i> We have spent many years looking for possible artifacts. Our main task therefore consisted in proving the &#39;theorem of existence.&#39; This task may be deemed completed. The acceptance of the phenomenon itself -- </i><b><i>the realization of the discrete spectrum of allowed states, which at any given time is similar for processes of entirely different nature, and which is attributable to cosmophysical forces -- requires some psychological effort</i></b><i> ...</i><br /><br /><i>There are many interesting problems that have to be studied. A number of theorems need to be proved, and new computer techniques developed. Experiments must be performed on satellites and space stations. A network for simultaneous measurements at different geographcial locations ought to be organized. Finally, and most importantly, we need to develop a theory that will explain the nature of this phenomenon. All this is to be done in the future. The task of this paper is accomplished -- we have introduced the object of future research ...</i><br /><br /><i>D.S. Chernavskii, editor of Physics-Uspekhi, added a postscript to the article by Shnoll et al., which is excerpted here:</i><br /><br /><i>The paper that you have just read is somewhat out of the ordinary. </i><b><i>Professor Shnoll is a known biologist, but the paper deals not so much with biology as with pure physics -- radioactive decay.</i></b><i> Many years of experiments have led to the discovery of several (to be more precise, two) new phenomena.</i><br /><br /><i>The purpose of this comment is to discuss why these phenomena may be of interest to physicists, and what role they may play in the development of science ...</i><br /><br /><i>Two conclusions follow.</i><br /><br /><i>1. The histograms of S.E. Shnoll et al. contain new information about the nature of a random process which until now has passed unnoticed.</i><br /><br /><i>2. </i><b><i>The postulate of measurement in quantum mechanics is at least not complete.</i></b><i> Indeed, when we say that &#39;alpha decay occurs at random, so that the probability of detecting ... etc.&#39; we ought to specify what kind of randomness it is, and what chaos it is based upon. Otherwise we are not able to predict a number of phenomena observed ...</i><br /><br /><i>This proves the importance of the first phenomenon described in the paper. The second phenomenon consists in the periodical change of the fine structure of histograms. It is demonstrated that the fine structures of histograms for quite diverse random processes (physical, chemical, biological, etc.) are similar and vary in sympathy. Moreover, these periodical changes correlate with the changes in our solar system, and possibly in our universe. To evaluate properly this phenomenon we first ought to understand the cause and mechanism of the first phenomenon.</i><br /><br /><i>The authors do not suggest any explanation of the phenomena discussed, and make no hypotheses concerning their possible mechanisms, and quite rightly so! The reader must start thinking on his own, which certainly is the main intent of this publication.&quot;</i><br /><br />GRAPHIC REFERENCES:<br /><br />[1] <a href=\"http://www.21stcenturysciencetech.com/articles/time.html\" class=\"ot-anchor\">http://www.21stcenturysciencetech.com/articles/time.html</a><br /><br />[2] <a href=\"https://www.researchgate.net/publication/238723521_Realization_of_discrete_states_during_fluctuations_in_macroscopic_processes\" class=\"ot-anchor\">https://www.researchgate.net/publication/238723521_Realization_of_discrete_states_during_fluctuations_in_macroscopic_processes</a><br /><br />[3] <a href=\"http://www.21stcenturysciencetech.com/articles/time.html\" class=\"ot-anchor\">http://www.21stcenturysciencetech.com/articles/time.html</a><br /><br />[4]-[6] <a href=\"https://arxiv.org/pdf/1106.1678.pdf\" class=\"ot-anchor\">https://arxiv.org/pdf/1106.1678.pdf</a><br /><br />[7][8] <a href=\"https://www.researchgate.net/publication/238723521_Realization_of_discrete_states_during_fluctuations_in_macroscopic_processes\" class=\"ot-anchor\">https://www.researchgate.net/publication/238723521_Realization_of_discrete_states_during_fluctuations_in_macroscopic_processes</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Nuclear Half-Life Modification Technology","summary":"The Nuclear Dogma of Our Day is that Radioactive Isotopes have a Specific Half-Life and We are Told it has not been Proven that Nuclear Isotope Half-Lives can be Changed by any Physical and/or Chemical Methods known to Mankind / Yet, Some Nuclear Researchers Claimed in 1964 to have Fully Deactivated Radioactive Nuclear Waste Samples in Garage Laboratories in Just Minutes Using Nothing More than a Geiger Counter, Vacuum Pump, Bell Jar and a High-Power DC Source Driven by a Homemade Variac Transformer","image":"https://lh3.googleusercontent.com/-UMPJEGEAIeQ/V-bgiYf28FI/AAAAAAAAIy8/1H9GpZE1ytMqBNLpuaWCVKJsUlFATifIgCJoC/w3600-h5040/nuclear-half-life-modification-technology-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/gFDurg3ytEj","publishDate":"2016-09-24T20:23:27.315Z","updateDate":"2017-01-03T01:59:23.821Z","text":"<b>Nuclear Half-Life Modification Technology: The Nuclear Dogma of Our Day is that Radioactive Isotopes have a Specific Half-Life and We are Told it has not been Proven that Nuclear Isotope Half-Lives can be Changed by any Physical and/or Chemical Methods known to Mankind / Yet, Some Nuclear Researchers Claimed in 1964 to have Fully Deactivated Radioactive Nuclear Waste Samples in Garage Laboratories in Just Minutes Using Nothing More than a Geiger Counter, Vacuum Pump, Bell Jar and a High-Power DC Source Driven by a Homemade Variac Transformer</b><br /><br /><a href=\"http://www.gdr.org/nuclear_half.htm\" class=\"ot-anchor\">http://www.gdr.org/nuclear_half.htm</a><br /><br /><i>&quot;Nuclear Half-life Modification Technology</i><br /><br /><i>GDR GREEN NEWSLETTER 001</i><br /><br /><i>Submitted by Larry Geer for GDR</i><br /><br /><i>Radioactivity Deactivation at High Temperature in an Applied DC Voltage Field Demonstrated in 1964</i><br /><i>by Larry Geer &amp; Cecil Baumgartner</i><br /><br /><b><i>Anomalies in isotope decay rates have been observed throughout the 1900&#39;s, but have been under investigated, under reported, and suffered from neglect. Late 1950&#39;s in the development of ceramic nuclear fuel manufacturing, an anomaly of 35% loss in nuclear fuel content was observed and was written off as a processing loss of the nuclear fuel.</i></b><br /><br /><b><i>One of the engineers involved, Cecil Baumgartner, recognized that the 35% nuclear content loss as being some type of induced change of isotope decay rate that reduced the nuclear fuel content. This led Cecil Baumgartner to start a multi-year search for a method to eliminate nuclear waste by a deactivation process.</i></b><br /><br /><i>Some years later in early 1964, Cecil succeeded in accomplishing partial and then complete deactivation of gamma radiation in cobalt, alloys containing cobalt, and other radioactive metals. Currently, over the last forty years, many other investigators/experimenters have reported deactivation processes and even preservation of radioactive isotopes, including other anomalies.</i><br /><br /><i>Radioactive isotope decay rate or half-life can be increased or decreased as needed to deactivate radioactivity or to increase shelf life of radioactive isotopes. Currently many investigators/experimenters have reported half-life anomalies and have demonstrated repeatability of the various processes. The deactivation/neutralization of radioactivity in isotopes by the several demonstrated processes clearly suggest the possibility of full scale processing of radioactive nuclear materials to deactivate radioactive nuclear materials.</i><br /><br /><i>Why does DOE insist on saving every precious isotope of nuclear waste for some possible unknown future use? It should be recognized as nuclear hazardous radioactive waste deserving of permanently deactivation. Is it not clear that radioactive nuclear waste is being released and distributed into the air, water, land? Yet, the current Secretary of Energy announced, as was reported April 9, 2003 in the Los Angeles Times, that the DOE Secretary knew of NO other way of disposing of the radioactive nuclear waste other than burying it at Yucca Mountain. DOE nihilistic nuclear radioactive waste materials approach is releasing nuclear radioactive materials daily 24/7 into the earth environment.</i><br /><br /><i>The first deactivation process that I am aware of that was developed for the expressed purpose of permanently deactivating radioactivity in nuclear materials. </i><b><i>The Baumgartner process was accomplished and demonstrated in early 1964. Baumgartner process demonstrated that C60 gamma radiation could be reduced to background radiation levels. With an additional step in the Baumgartner process, the radioactivity was reduced to a nil radiation level. This process and many other processes that modifies an isotope half-life are designated Nuclear Half-life Modification Technology.</i></b><br /><br /><i>The Baumgartner process is described under Nuclear Half-life Modification Technology at </i><i><a href=\"http://www.gdr.org\" class=\"ot-anchor\">www.gdr.org</a></i><i> so that can be performed by any interested party.</i><br /><br /><i>The Baumgartner process can easily be duplicated by any Nuclear Half-life Modification researcher/investigator/experimenter. While deactivation of radioactivity was accomplished at a high temperature in an applied DC voltage field, an interesting alternative experiment is the investigation of isotope half-life changes at ambient temperature in a +/- DC voltage fields. Suggest using Am241, since its log lambda/lamdba nought versus DC voltage plot shows a significant slope. With this Am241 data in hand one can easily see the preservation possibilities for isotopes that respond to DC voltage.</i><br /><br /><i>SHORT VERSION ... continue for full version ...</i><br /><br /><i>Nuclear Half-life Modification Technology</i><br /><br /><i>GDR GREEN NEWSLETTER 001</i><br /><br /><i>Radioactive isotope half-lives can be decreased to neutralize nuclear waste and weapons. This phenomenon can be accomplished by using equipment available since the late 1800’s and this technique has been known for the past 40 years. Incorporating this technique can result in vast financial savings, solve the current insurmountable environmental problems of radioactive waste storage and ensure this planet a future free from the threat of deadly nuclear radiation contamination.</i><br /><br /><i>Data on these processes have been accumulating for the past 40 years and the database continues to grow.  More and more unfunded independent experimenters are reporting their findings and substantiating various methods of Nuclear Half-Life Modification. A basic demonstration of Half-Life Modification can be easily demonstrated in almost any of the high school science laboratories.  Currently, the cold fusion (now called Low Energy Nuclear Reactions, LENR or Chemical Assisted Nuclear Reactions, CANR) experimental reports are disclosing increased energy release, and also confirm deactivation when radioactive isotopes are used in the solutions, which results in transmutations in the solutions and on the electrodes.</i><br /><br /><b><i>Many renowned scientists may find this half-life modification statement beyond all reason. This I have been told many times by many PhDs over the past 40 years.</i></b><i>  The Secretary of Energy announced, as was reported April 9, 2003 in the Los Angeles Times, that the DOE Secretary knew of no other way of disposing of the nuclear waste other than burying it at Yucca Mountain (and like sites).  This declaration created impediments that must be overcome to provide for the safety and health of people on this planet Earth.  The continued release of nuclear radioactive materials into the environment happens to involve the entire planet.  This presents imminent danger to the entire planet population.  Madame Curie is a well-known example of the devastation caused by nuclear radiation.  Should the current radioactive nuclear waste disposal practices be continued by the authorities, the steady and mounting degradation of a healthy life on planet Earth will continue to accelerate.  Recently this degradation has been significantly increased by the use of depleted uranium (DU) on the military test ranges and on the battlefields covering large areas of this earth, land, air and water.</i><br /><br /><i>This newsletter will present the discoveries for modifying radioactive isotope half-lives.  Initially in my case Cecil Baumgartner of Glendora, CA experimentally performed the radioactive isotope deactivation for the permanent elimination of radioactive nuclear waste. </i><b><i>The first indication of half-life modification occurred in the early 1900’s and very likely, unknowingly.  The first half-life modification to be reported occurred in April 1932 (experimenter(s) and location specified later).</i></b><br /><br /><i>My curious nature had to know why the radioactive isotopes half-lives were a specific assigned value and why not a variable value.  </i><b><i>I was taken aside and told that isotopes had been frozen, heated, crushed, pressurized, grounded, electrocuted, and any thing else that they could think of doing to an isotope, but the isotope always had its assigned half-life value afterwards-just as published.  Therefore, I was duly informed to knock it off and just use the published book values.</i></b><br /><br /><i>That worked out very well in my nuclear project assignments.  That is until I met Cecil Baumgartner while working on some nuclear projects.  I had known Cecil for some time, then one day he decided to share one of his previous nuclear experiences.  Cecil had always been very thorough and tenacious in all of his engineering activities.  C*ecil, as a research engineer, in the late 1950’s was experimentally processing ceramic nuclear fuel elements.  This ceramic material was processed at a very high pressure and heated to a very high temperature by an induction coil in vacuum.  The finished processed fuel element tested 35% short of the specified fuel mixture.  This was immediately interpreted as a weight error in preparing the initial mixture.*  There was very strict accountability of the fuel, however, and the residual unprocessed nuclear fuel was weighed and verified the correct amount specified for the fuel element had been removed.  The vacuum chamber and evacuation filter were checked but no trace of the missing fuel could be found.  Was the material stolen?  Well, there were no traces of radioactivity and all personnel radiation exposure badges were clear so no one could have carried the radioactive fuel off the premises.  </i><b><i>No one thought of analyzing the fuel element for transmutations to non-radioactive elements ... The missing fuel remained a mystery.</i></b><br /><br /><i>During the 1950’s several companies were working on many nuclear projects and some probably had experienced the missing radioactive isotopes for some of the fabrication processes.  </i><b><i>One such incident may have occurred at NUMEC when 8 kg of U235 (total amount of material being processed is not revealed) became missing after some fabricated parts being subjected to some high pressure forming at high temperature.  Zalman M. Shapiro and probably others were accused of stealing the U235 by the investigating federal agencies.</i></b><i>  NUMEC had received many orders and took on first-of-a-kind contracts for nuclear fuels.  The nuclear half-life could have very likely been modified by transmutation of the 8 kg of radioactive U235 to non-radioactive isotopes in the high pressure and high temperature processes.  </i><b><i>Again the investigations failed in finding or even beginning to identify the culprit for the missing 8 kg. It was just lost, but how?</i></b><br /><br /><i>The missing 35% of the radioactive fuel in the ceramic hot pressing process became a data point for Cecil that would generate great tenacity in his search for modifying the half-life of radioactive isotopes. One day in late 1963, just before JFK was assassinated, Cecil came by my office and asked if I wanted to work on the nuclear waste projects for the rest of my life.  I did not find that to be appealing for my interest with so many more interesting projects to be involved in during this time period.  Cecil then said he did not want to be working on nuclear waste projects the rest of his life.  How could Cecil and I know at that time many people considered playing with nuclear waste to be generational continuous employment for themselves and their children?&quot;</i><br /><br /><b>Wikipedia definition of diathermy device:</b> <i>&quot;In medicine, diathermy is electrically induced heat or the use of high-frequency electromagnetic currents as a form of physical or occupational therapy and in surgical procedures.&quot;</i><br /><br /><i>&quot;Then Cecil shared even more of his nuclear experiences. He was sure that he could deactivate radioactive isotopes.  </i><b><i>He had a modified diathermy device, voltage equipment, and vacuum equipment that he was using to treat small, low-level cobalt 60 gamma radiating specimen in his garage.  They had been subjected to various AC voltage levels at various frequencies at moderate to high temperatures within the limits of the equipment being used.  He had been doing these experiments in his garage</i></b><i>, not a well-equipped, generously funded research laboratory! At this point he had exhausted his personal allotted funds for his independent research.  This then became the start of the brainstorming and discussion sessions regarding the possibility of eliminating radioactivity in nuclear isotopes, especially for the ever-accumulating daily nuclear waste in all forms.</i><br /><br /><b><i>At that time, and still today in the nuclear industry, the nuclear dogma was/is that radioactive isotopes had/have a specific half-life and it has not been proven that nuclear isotope half-lives can be changed by any physical and/or chemical methods known to mankind.  Radioactive isotopes are earth’s timekeepers for dating and clocks.  Currently the encyclopedia and nuclear textbooks still state that the half-life of radioactive nuclear isotopes can NOT be changed by any physical and /or chemical methods.</i></b><br /><br /><b><i>Lord Rutherford reported on April 28, 1932 at a meeting of the Royal Society, that J. D. Cockcroft and E T. S. Watson in the Cavendish Laboratory had successfully demonstrated the accelerated release of radioactive energy from lithium elements and other light elements.  The most surprising feature at the time was the relatively low voltage necessary.  Could this have been the first accelerated decay transformation?  Could this be the first indication that the current nuclear dogma that isotope half-lives are immutable is false????</i></b><br /><br /><b><i>Segre and Wiegart established decay rate mutability in 1949.  They showed that decay rate was dependent on the chemical environment for Be7, which decays by electron capture.</i></b><i> This was extended to the physical environment by investigators for other isotopes, which can decay either by electron capture or isomeric transitions.</i><br /><br /><b><i>The earth is a negative charged (positive charged for electrical engineers) orb traveling in space with an associated magnetic field.  The earth has a surplus of electrons and varies in voltage potential to the ionosphere by 300,000 to 400,000 volts (some times more or less) with respect to the ionosphere.  The steady loss of electrons from the earth is called ionic current and amounts to about 9 micro amps per square mile of the earth’s surface.  The entire earth’s surface and the ionosphere is the oppositely-charged plates of a vast capacitor with the air between them acting as a rather inferior insulator, for it leaks continuously.  This ionic current can be rather variable due to many influencing conditions that exist over time.  Electrical storms can locally increase the electrical potential many times over the so called normal value.  Additionally the magnetic field can vary over the earth’s surface.  These various earth conditions cause the decay rate of radioactive isotopes to vary over the earth’s surface and over eons of time.  Once one realizes that this is the case, then the next step is to vary certain environmental conditions specifically on an isotope that one would prefer to exhibit an increase in half-life or a decrease in half-life.  All that can easily be done.</i></b><br /><br /><i>Infinite Energy magazine, Issue 53, 2004, in the article by Gary Christopher Vezzoli entitled &#39;Gravitational Data During the Syzygy of May 18, 2001 and Related Studies&#39; presented data for changes in the earth gravity field. Amazingly, </i><b><i>another section in the article presented data measuring the change of decay rate for Po210 corresponding to a lunar variation.  Pu239 decay variations were presented for daily monthly, and yearly time frames.  Decay rate variations were observed for the period of September 7-11, 2002, when Jupiter eclipsed a deep space quasar.  Not only local earth environment affects decay rate of radioactive isotopes, now data is presented showing cosmic influences on the decay rate on earth from the sun, the moon, planets, and one quasar.</i></b><br /><br /><i>Then in late 1963 it was agreed to change Cecil’s equipment configuration by adding an operating base DC field, negative or positive, for testing the radioactive specimen.  This was accomplished by using a full wave rectified center tap transformer that Cecil specified for the power available in a residential garage and to operate with the diathermy equipment.  The full wave transformer produced a variable DC voltage due to 60 Hz feed.  The initial plan was to let the various frequencies produced by the diathermy to ride on the varying DC field.  A Variac was used to control the DC voltage to the specimens being tested.</i><br /><br /><i>Early 1964 the new transformer had been fabricated.  Cecil then installed the new full wave rectification transformer and was ready to start experiments on his cobalt gamma radiation specimens again.  A cobalt gamma radiating specimen finally yielded to the available electrical and temperature conditions in a vacuum.  It was found that the field provided by the diathermy equipment was not required to produce a permanent change in level of radiation on the cobalt gamma radiating specimen.  </i><b><i>It was found that a positive DC field even at high temperature did not produce any measurable change in the specimen activity.  It was found that a negative DC field of sufficient voltage potential at a sufficient specimen temperature did produce significant measurable changes in the radiation levels measured by a Geiger Counter.  By increasing the temperature in what one could call &#39;the critical neutralization deactivation temperature&#39; the treated specimen was reduced to measured background readings with a Geiger Counter.  Additional monitoring showed a continued background reading.  The radioactive level had been permanently reduced.</i></b><br /><br /><i>Cecil Baumgartner came in one morning early in 1964 and reported that he had deactivated the gamma radiation in the cobalt specimen to background radiation level.  The readings for the specimen after a specific treatment were at normal background levels.  Eureka!!!!</i><br /><br /><i>Can I do the same operation with the equipment for a gamma radiating cobalt specimen and get a similar result?  A time was agreed to and with Cecil’s directions and under his watchful eye I operated the equipment to try to obtain a similar result.  A specimen that had a high active count was selected for the test.  Equipment was checked for radioactivity and indicated background levels.  The specimen was placed on the pedestal, arc gap adjusted, bell jar was greased and placed on its platform.  Electrical connections were checked for the correct polarity, i.e., a negative charge (positive charge for all you electrical engineers) on the pedestal where the specimen was placed.  Electrical equipment was switched on for the vacuum pump and for the electric controls to the specimen.</i><br /><br /><b><i>The vacuum pump had to create a reduced pressure in the bell jar for the purpose of creating an electrical arc (3000 volts maximum from the center tap transformer) and thus the heating effect on the specimen once the electrical arc was established and at the same time placing a negative electric field (less than 3000 volts when operating with the electrical arc) on the specimen for deactivation that was placed on the pedestal in the evacuated bell jar. The major concern was obtaining an adequate temperature (critical deactivation temperature) on the specimen by the electric current in the vacuum and power available from the electrical equipment, since it was limited by available electrical power current.  The specimen attained a nice orange glow, probably somewhere around 2400 degrees Fahrenheit.  Held at temperature for a minute or so and turned off the vacuum pump.  The electric arc was then broken as the bell jar lost its vacuum.  The heating stopped since the electrical arc was broken.  The electrical field was maintained  during cool down of the specimen. The electric field was discontinued after the specimen had cooled down and the bell jar was removed.</i></b><br /><br /><i>The Geiger Counter was then used to make radiation measurements.  The specimen showed background readings.  The result was very few counts as compared to the high number of counts prior to treatment.  The pedestal, bell jar, HEPA filter located between the bell jar and the vacuum pump, vacuum hose, and the vacuum pump were still at their initial background readings.  The data convinced me that elimination of radioactivity and deactivation/neutralization had occurred for the treated cobalt specimen.</i><br /><br /><i>These cobalt specimens had been irradiated in the Northrup Research reactor to a permitted level of activity not requiring a license or health physics monitoring.  Cecil and I were well aware of the hazards of radiation at any level and used very safe procedures in handling of the material for the deactivation experiments.</i><br /><br /><i>ANY IONIZING RADIATION IS UNSAFE TO LIVING TISSUE AT ANY LEVEL.  This includes X-rays, cell phones, microwave, cathode ray tubes, radioactivity in food, DU, uranium mine tailings, etc.</i><br /><br /><i>More testing was confirming for various other materials like alloys of steel and aluminum. </i><b><i>The materials were demonstrating that a critical deactivation temperature had to be attained in a negative voltage potential to reduce the activity to a measured background level. When the critical deactivation temperature was not attained, only a partial deactivation/neutralization was recorded for the specimens with varying activity levels.</i></b><br /><br /><i>Starting in March 1964 Cecil Baumgartner conducted the initial experiments on irradiated stainless steel and the radioactivity was reduced 50%.  A higher temperature was used, melting the specimen, and the radioactivity was reduced 75%.  Carbon steel was tested and the radioactivity count was reduced 98.5%.  Cobalt 60 isotope levels were reduced from 3000 counts per minute to a level indistinguishable from background radiation levels.  Aluminum specimens at melt temperatures were reduced to background radiation levels.</i><br /><br /><i>Materials that were vaporized in the high voltage corona and condensed onto the inner surface of the bell jar and in the HEPA filter to the inlet of the vacuum pump were totally devoid of any radioactivity. Have you ever seen a radiation monitor count go below background or essentially no counts??  The first time can be scary.  You immediately think &#39;instrument failure&#39;.  </i><b><i>The equipment (transformer, bell jar, vacuum pump, HEPA filter, carbon crucibles used in the bell jar, pedestal, bell jar mounting surface) used for these deactivation experiments were thoroughly surveyed by competent safety technicians and found the equipment devoid of any radioactivity.</i></b><br /><br /><i>Year 1964 marked the demonstration of intentional deactivation of radioactive materials for the sole purpose of eliminating radioactivity in waste.  This was the foundation for the Baumgartner Nuclear Half-life Modification Technology which has remained dormant these many years.</i><br /><br /><b><i>In 1964 we thought and believed that radioactivity in nuclear waste would soon be history on planet earth.  As history has proven us wrong, we now know and understand that there is a fortune, billions yearly, to be made by saving every scrap of radioactive nuclear waste and trying to bury it in Yucca Mountain and in cleaning up spills, leaks, and escaping radioactive particles from decaying containment schemes.  We were just looking at the wrong goal post.  No one receiving the funds has any interest in eliminating radioactivity in nuclear waste.</i></b><i>  Nuclear Half-Life Modification Technology could reduce the cost to a fraction of the cost that is experienced today.  Someone tried to tell me that the money was mother’s milk of politics.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"Distributed or Multiple Representations in Multiple Agents","summary":"It's the Common Circumstance where Multiple People Have Dissimilar Mental Representations About an Object, Person, Interaction or Situation / This Simple Conceptual Framework Explains How to Coordinate Teams of Specialists / It Helps to Explain Why Discussions Between Experts and Laypeople are So Challenging / And it's a Crucial Concept for Any Social Network which Seeks to Manage a Clash of Worldviews","image":"https://lh3.googleusercontent.com/-0Z73ALNXpIY/V-YjH5YvuWI/AAAAAAAAIyI/DL-NelNdWzsuVUcicuO70qXeIn-tpKrXwCJoC/w1800-h2520/multiple-representations-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/GANLSkUbBGD","publishDate":"2016-09-24T06:55:40.525Z","updateDate":"2017-01-03T01:59:53.150Z","text":"<b>Distributed or Multiple Representations in Multiple Agents: It&#39;s the Common Circumstance where Multiple People Have Dissimilar Mental Representations About an Object, Person, Interaction or Situation / This Simple Conceptual Framework Explains How to Coordinate Teams of Specialists / It Helps to Explain Why Discussions Between Experts and Laypeople are So Challenging / And it&#39;s a Crucial Concept for Any Social Network which Seeks to Manage a Clash of Worldviews</b><br /><br />Many people will understandably find this to be getting into the weeds of social network theory and design.  It&#39;s nevertheless important to mention because it commonly pops up as a source for design decisions when trying to construct a scientific social network.<br /><br />The first excerpt is the more important one; the second one is quite a bit longer and elaborates upon the first.<br /><br />From pages 30-32 of <i>Visualizing Argumentation: Software Tools for Collaborative and Educational Sense-Making</i> ...<br /><br /><i>&quot;Misunderstandings due to differences in representation are not easy to avoid, especially between experts who have spent thousands of hours mastering their fields and novices whom they may be working with. </i><b><i>Discussions between experts and novices have been shown to be notoriously difficult (Bromme, Nuckles, and Rambow, 1999). Specific issues cannot be explained at the required level of understanding to a novice, and when the expert tries to explain more, the novice is overwhelmed.</i></b><i> This is not only due to the nature of the knowledge to be conveyed, but also because experts tend not to be able to &#39;tune in&#39; to the level of novice understanding. This is not only true when dealing with very different fields like law and microbiology where one person can be an expert in one field and know absolutely nothing about another. </i><b><i>Boshuizen and Tabachneck-Schijf (1998) found that cooperation problems also occur when different experts have fields of expertise that partially overlap and partially differ in content, concerns and/or paradigm. They call this &#39;distributed or multiple representations in multiple agents&#39; and define it as the circumstance where multiple human or artificial agents have dissimilar representations about an object, person, interaction or situation.</i></b><i> They describe how representations may vary along the dimensions of data, format and operators.</i><br /><br /><b><i>When people think differently about an object or have different perspectives on it, their representations vary along the data dimension.</i></b><i> Data is what we call &#39;content&#39; in everyday life. Since the content mastered by an expert is vast and highly integrated, it is virtually impossible for a novice to learn that which is relevant for solving a certain problem. This is compounded by the fact that even when two people have mastered the same concepts, they still might not understand each other. </i><b><i>Different people can have different perceptions or have different prototypes of the same concept. This is painfully apparent in the differences exhibited between the understanding of concepts via &#39;real physics&#39; and &#39;lay physics&#39;</i></b><i>, a phenomenon often labeled misconception. Other differences stem from different domain-specific representations: </i><b><i>stakeholders of a wicked problem often represent problems in different ways, thus a problem in the production of a lawn mower is a logistics problem for the planner, a design problem for the industrial designer, a personnel problem for the human resources manager and a financial problem for the accountant.</i></b><i> Recognizing such differences at the conceptual level can often take more time, result in more surprises and be more of a nuisance than is the case when people entirely lack each other&#39;s concepts. Different contents of representations can also be the source of many misunderstandings, for example the scheduling of a meeting between an English and a Dutch delegation at &#39;half ten&#39;, which in Dutch means 9:30.</i><br /><br /><b><i>A second dimension along which representations may vary is format, for example when one representation is propositional and the other visual (e.g., mathematical notation vs. Venn diagrams). Note that argument visualization is a means of forcing the use of the same format, or &#39;surface representation&#39; as Stenning (1998) calls it, on different people. Stenning points out that using the same surface representation may help to unravel differences in conceptual structures.</i></b><br /><br /><b><i>The third dimension reflects that representations may differ regarding the operators applied. Differences along this dimension may appear unexpectedly and may lead to conflicts because one person thinks that the other&#39;s reasons are &#39;unfair&#39;. Boshuizen and Tabachneck-Schijf (1998) give an example of the use of different sets of operators: a legal approach toward solving a misunderstanding between people versus using more common-sense ideas about solving it.</i></b><br /><br /><b><i>Another source of misunderstanding stems from differences in the criteria that agents use to evaluate solutions and arguments.</i></b><i> While a blueprint for a containment construction may convince engineers that the container is an acceptable solution to a environmental problem, it is not likely an environmentalist will accept this as a solution. </i><b><i>Differences in criteria between agents can be so fundamental that they lead to the complete rejection of specific approaches to problem solving or to certain reasoning patterns.</i></b><i> An example is an abortion rights activist who considers life to begin at birth and a member of the clergy who considers life to begin at conception, if not earlier.</i><br /><br /><b><i>Data, formats, and operators can form extended procedures or lines of reasoning (macro-operators) which are needed for problem solving. We define such macro-operators as sequences of operators learned during past (problem solving) experience which can be shared by domain area experts (i.e., mathematical procedures, juridical reasoning). Argumentation strategies for a specific line of reasoning reflect common values and goals for solving problems in a specific domain and the role of argumentation therein.</i></b><i> Scientific researchers, for example, attempt to achieve consensus based upon the exchange of arguments that are open for critical debate while lawyers try to win a conflict by convincing a ruling authority that their claims should be honored. However, even within a discipline, striking differences can be found in argumentation. More formal reasoning styles, for example, are tightly linked to scientific paradigms and are often not accepted by scientists with a different background.</i><br /><br /><i>The second author for this chapter recalls a situation where two different macro-operators from two scientific domains led to utter misunderstanding. While at university, she participated in a seminar on psycholinguistics and text-processing for psychology and linguistics students. The two groups had a hard time learning the relevance of each others&#39; argumentation structures. The psychologists could only be convinced by hard empirical evidence and wanted experiments to judge theoretical claims, while the linguists were masters in generating examples and connterexamples such as Chomsky&#39;s master sentence &#39;Colorless green ideas sleep furiously&#39;. With these examples, they evaluated the claims of linguistic theories. It took several sessions and a mini-lecture by the professor before the two groups of students could begin to understand the reasons for the mutual misunderstandings. It took even longer to learn to appreciate that the example and counter-example argumentation worked very well for linguistic theories, but that empirical argumentation was more valid for text-processing and understanding.</i><br /><br /><b><i>What this suggests, is that there are serious coordination problems when dealing with multiple agent, multiple representation situations.&quot;</i></b><br /><br />From the 1988 text titled <i>Learning with Multiple Representations</i>, p137-151:<br /><br />&quot;Chapter 8: Problem Solving with Multiple Representations by Multiple and Single Agents: An Analysis of the Issues Involved&quot;<br /><br />Henny P. A. Boshuizen and Hermina J. M. (Tabachneck-)Schijf<br /><br /><i>&quot;As young children people have already learned that they have five senses: their eyesight, hearing, smell, taste and sense of touch. The receptors of all these senses transform information into a format that can be dealt with by the human brain. People use these different kinds of information in a coordinated way; e.g. we know a person from his or her appearance, sound of voice and movements such as footsteps or coughs and sneezes, smell, sometimes touch and maybe even taste. These different &#39;parts&#39; together form the full &#39;description&#39; of this person. They are one whole and people are very surprised when different parts do not match. For instance, suppose you hear your colleague Frank coming up to your room (footsteps, coughs, knock on the door), but instead of Frank, Francy comes in. What would you think? Probably something like &#39;Where did you leave Frank?&#39;, &#39;Am I becoming senile?&#39; or &#39;Are they playing a trick on me?&#39;</i><br /><br /><i>In Western societies people grow up with the idea that multiple representations in the form of texts with illustrations are helpful for building up an adequate understanding of the topic at hand. At least, that is how school books and books for young children are designed. The Frank/Francy example shows how integrated these different representations are. </i><b><i>Normally the use of multiple representations feels like a very smooth process.</i></b><i> The same applies to the process of building up multiple representations of new objects or events. </i><b><i>Only in extreme situations may we experience that this process can be very difficult.</i></b><i> The following quote illustrates this difficulty. It has been taken from a chapter by Oliver Sachs (1995) in which he describes the &#39;adventure&#39; of a middle-aged man, Virgil, who became blind at a very young age, but whose eyesight was partly restored after a glaucoma operation.</i><br /><br /><i>&#39;As we settled down, Virgil&#39;s cat and dog bounded in to greet and check us — and Virgil, we noted, had some difficulty in telling which was which. This comic and embarrassing problem had persisted after he returned home from surgery: both animals, as it happened, were black and white, and he kept confusing them — to their annoyance — until he touched them, too. Sometimes, Amy (his wife, HB) said, she would see him examining the cat carefully, looking at its head, its ears, its paws, its tail and touching each part gently as he did so. I observed myself the next day — Virgil feeling and looking at Tibbies with extraordinary intentness, correlating the cat. (Sachs, 1995: 121- 122, italics added)&#39;</i><br /><br /><i>This man not only has problems with the visual information as such, but also with the integration and translating back and forth of the visual and the tactile information. Another patient who became blind during adolescence and recovered after about 10 years reported similar problems.</i><br /><br /><i>Examples like Virgil&#39;s have been known for quite some time, but </i><b><i>so far little is known about the everyday-life difficulties of learning and problem solving with multiple representations.</i></b><i> We do not know if the naive belief that illustrations &#39;help&#39; with building an understanding is right. The fact that normally no effort is felt does not mean that such is indeed the case. This is exactly the leading thread of the present chapter and the others in this section. </i><b><i>Here, the assumption that multiple representations help and hence that coordinating or integrating multiple representations is more or less effortless, is questioned.</i></b><br /><br /><i>In this part of the book several chapters are brought together which explore this rather contradictory situation in different domains, asking the question if even under less extreme circumstances integration of multiple representations and solving problems with multiple representations requires an extra effort and if so what the extra effort is. As an extra problem the integration of multiple representations held by different people (multiple representations in multiple agents) is investigated. In the present chapter a formal analysis of the different ways representations can be integrated is given, followed by an extrapolation to the multiple agent situation.</i><br /><br /><i>1 Some Essentials of Representations</i><br /><br /><i>A representation is a format for recording, storing and presenting information together with a set of operators for modifying the information (Tabachneck &amp; Simon, 1995). Neither component, information format nor operators, alone defines a representation. A representational format can be almost anything. Data can be words, pictures, numbers, parts of an X-ray, etc. Operators modify the information stored in the data. &#39; + &#39; and &#39;-&#39; are well-known arithmetic operators. &#39;Scan&#39; and &#39;zoom&#39; would work on pictures. For example, Roman numerals (M, D, C, L, X, V, I), are data pieces that combine in strings that represent cardinal numbers. They are meaningless without definitions of the individual symbols (format) and rules (operators) for performing arithmetic operations on them (e.g. XXXXX -&gt;• L; XX + XX -» XL). The effectiveness of a representation depends both on its operators and on the speed with which the operations can be performed. The latter depends also on familiarity with the representation: expert diagnosticians will draw conclusions more rapidly and accurately than novices from X-rays.</i><br /><br /><i>Both the data structures and the operations in distinct representations will be very different. However, inferential goals may be achievable in both representations. One can for instance find the solution of a set of two simple linear equations by solving them or by drawing them and reading off the coordinates of the intersection of the two lines. The fact that some goals will be much easier to achieve in one representation than in the other provides the rationale for using the superior representation: it is more efficient than the other. The effectiveness of representations for communicating and instructing depends first and foremost upon their computational efficiencies. Consequently we must distinguish the informational from the computational equivalence of representations.</i><br /><br /><i>Two representations that are informationally equivalent (the information in the one can be obtained from the other and vice versa) will usually not be computationally equivalent (will not obtain the information with the same amount of computation), for their efficacy depends on the operators that act on them. Two sets of operators may differ in their capabilities for recognising patterns, in the inferences they can carry out and in their strategies for controlling search. Diagrammatic and sentential representations, for example, employ operators that differ in all of these respects (Larkin &amp; Simon, 1987). Efficacy is not necessarily a fixed attribute of the representation format. Ways can be found to do the operations faster — for instance, long division on Roman numerals with a calculator for that purpose would be faster than long division on Arabic numerals by hand. Efficacy of a representation can alter drastically when ways are found to perform the operations more rapidly. These ways can be found through exercise, invention, or &#39;help&#39;.</i><br /><br /><i>A special type of operator is the inference operator. Inference operators help us get information out of data that is implicitly, but not explicitly present. The process of going from a question to an answer is an inference. Inference operators define a kind of logic on the database — although not, in general, formal logic. There are several types of inferences. For example, going from &#39;1 + 1 = ?&#39; to &#39;2&#39; using knowledge about arithmetic is a deductive inference. Going from seeing 20 geese, all white, to &#39;all geese are white&#39; is an inductive inference. Inferring cancer from a small blob on an X-ray requires experiential inference — the diagnostician has learned, from books but more so from being exposed to many, many X-rays, that a certain type of blob can mean cancer. He can, of course, be wrong — the blob may merely be an innocent growth. Unlike the tautological inference rules of logic (e.g. syllogism, modus ponens), which remain the same in every domain of reasoning, inference rules that incorporate matters of fact must be specially tailored for each knowledge domain, and will usually not remain invariant over time. Usually, it is worth the cost to acquire a substantial set of inference operators tailored to any domain in which we wish to reason and solve problems frequently, because such operators may accelerate enormously the speed with which we can extract knowledge from a database.</i><br /><br /><i>An example from medicine is the following. Suppose we have a patient with pain in the upper abdomen. The medical student knows, from his book learning, that this type of pain can originate in many organs in that area: the liver, gall bladder, stomach, pancreas, etc. and from many different processes, like a tumour, inflammation, stone or trauma. While it takes a medical student quite a while to come to a diagnosis, experienced diagnosticians know what to look for in order to come to a likely correct diagnosis. They will not only look at physical symptoms, like degree of pain, and place of pain, but also at the patient&#39;s background. Confronted with a 45-year-old male complaining about severe continuous pain in the upper abdomen, many possibilities exist. However, when questioning reveals him to be an alcoholic, then chances that he has an inflammation of the pancreas or the liver are high. If a 10-year-old child came in with the same complaint, the doctor would most probably not even think of the pancreas, simply because it is a known fact that children of that age do not get that type of disease. Inference rules incorporating age of the patient and lifestyles are tailored to diagnostics. But such rules must change with altering circumstances. For example, 10 years ago, a general practitioner would question a patient with a peptic ulcer extensively about stress and might even refer this person for psychiatric treatment. Nowadays, it is known that peptic ulcers are in the first place a result of bacterial infection with Helicobacter pyloris (first identified in 1983), alcohol and aspirins. Medical practice has changed accordingly.</i><br /><br /><i>1.1 Interactions Between Different Kinds of Representations</i><br /><br /><i>Modes of representation are aligned with natural human input modes — hearing, seeing, touch, smell and taste. We now know that seeing constitutes at least two modes — spatial and objects modes, which are handled by different areas of the brain, and are eventually reunited again (for an overview of the research, see Farah, 1990). For this chapter, we will concentrate on hearing (verbal communication, be it spoken or written) and seeing. We realise that touch and smell are also very important senses, especially in medical diagnostic tasks, but will for the moment ignore these. We hypothesise that each mode of representation has its own format and its own operations, and that one mode cannot directly modify a representation of another mode. However, one mode can activate, or connect to, a representation of another mode (for a description of a computational model embodying these principles, see Tabachneck, Leonardo, &amp; Simon, 1997).</i><br /><br /><i>Although different representations cannot alter one another directly, it is possible to translate one representation into another. </i><b><i>One representation may be computationally efficient for dealing with one part of a problem-solving or reasoning task, while another representation may be more advantageous for another task.</i></b><i> For instance, a graph of a set of experimental outcomes may be quite fast in conveying that there is an interaction between two dependent measures — the lines cross. However, finding out whether this interaction is significant is much easier using statistical mathematics. Translating from one representation of data to an informationally equivalent one may be a non-trivial task. For example, it took several topflight physicists a number of months to show that Heisenberg&#39;s and Schroedinger&#39;s independently discovered representations of quantum mechanics were informationally equivalent. Both representations continue to be used in physics today (nearly 70 years later), because each has computational advantages over the other in solving particular classes of problems.</i><br /><br /><i>1.2 Natural and Artificial Learning Settings</i><br /><br /><i>We have already suggested that in humans, </i><b><i>dealing with multiple representations is very common. In effect, multiple representations probably seem the rule rather than the exception, especially for everyday learning.</i></b><i> On eating a pancake, we see the characteristic shape and colour, we smell the typical pancake smell, and we taste the typical taste. All three representations are stored. While all three together offer the largest likelihood of recognising that we are dealing with a pancake, each one separately also makes a good chance. How things are learned and which representations are involved, is linked to the input senses, to the task a person has, to his or her expertise and to the characteristics of the communication setting. </i><b><i>However, learning in more artificial settings, like a classroom, can sometimes, surprisingly, lack representation forms that can turn out to be very important for future performance.</i></b><i> For instance, medical students are initially taught from books, and do not come in contact with the patients. The pictures in these books do not afford smelling and touching, or hearing a patient wheeze or groan. Furthermore, textbooks show typical signs of a single illness; patients often have more than one disease at a time. When medical students first start applying their book knowledge to patients, or, rather, when medical students first have to lay connections between their book knowledge and the patients&#39; signs and symptoms they have to translate the new representations into the better known format. For instance, they must recognise that the specific sound heard, is a &#39;crepitation&#39; that according to the books should sound as footsteps in the snow: correlating the patient, one might say, just like Virgil.</i><br /><br /><i>Although multiple representations seem the rule in humans, there are several complications that will be highlighted in this chapter. The next two paragraphs are meant to investigate these complications by analysing the boundary conditions that affect &#39;cooperation&#39; between different representations in multiple agents (e.g. teams of specialists) and multiple representations in a single agent. We start with the multiple agents, because this domain magnifies certain problems that are also inherent, but less easily detected in single agents. In this approach multiple representations in multiple agents are not only dealt with for their own sake; the multiple agents are also, so to speak, a metaphor for the single agent situation, with all the pitfalls a metaphor can have.</i><br /><br /><i>2 Distributed or Multiple Representations in Multiple Agents</i><br /><br /><b><i>&#39;Distributed or multiple representations in multiple agents&#39; is defined here as the circumstance where multiple human or artificial agents have dissimilar representations about one object, person, interaction or situation. These representations may be dissimilar in terms of their data.</i></b><i> This is very often the case, for instance when people think differently about an object, or when they have different perspectives on an object or situation. An example of the latter can be found in research by Baum and Jonides (1979), who showed that inhabitants of one part of the country have more detailed representations of their own part than of other parts farther off. Hence inhabitants of the same country can have very dissimilar representations of locations and distances between cities depending on their own place of living. </i><b><i>Representations can also be different in terms of the formats, e.g. when one representation is prepositional and the other visual. And finally they may differ regarding the operators that are applied, e.g. logical vs. experience based.</i></b><i> In everyday life the different contents of representations are most eye-catching and can be the source of many misunderstandings. For instance, just the other day a colleague, Jim, told us about showing up in The Hague for a meeting, while the person he was supposed to meet was in Groningen. They had agreed to meet in John&#39;s office. Unfortunately, John had changed offices half a year before, and was now stationed in Groningen, 2 hours from The Hague by train. Both men confidently assumed that the meeting place was well known. This was a large difference in representation content, but small differences are an everyday occurrence. Sending a child for &#39;a loaf of bread&#39; may result in anything from white to brown, to raisinbread, depending on the child&#39;s representation of &#39;a loaf of bread&#39;. Different operators can appear very unexpectedly and can lead to real quarrels, because one person thinks that the other reasons &#39;unfair&#39;. An example of the use of different sets of operators is using a legal approach toward solving a misunderstanding between people vs. using more common sense ideas about it.</i><br /><br /><b><i>A typical situation where people work together and have different representations is the team of specialists who work together on one task.</i></b><i> The following is an example. It has been taken from the domain of surgery where teams of specialists with very different backgrounds work together.</i><br /><br /><i>2.1 Advantages and Disadvantages of Multiple Agents in a Team</i><br /><br /><i>Teamwork has many advantages over the situation where one person has to do all the work. A single person does not need to have all knowledge and all skills necessary to perform the task. This makes the learning process before the task can be done at the required level much shorter than when every expert would have to do all sub-tasks. In some cases, it is even impossible for one person to meet the standards set for all participating specialisations. Also, one person cannot be at several places simultaneously, which might be a task requirement as well.</i><br /><br /><b><i>If knowledge were completely different in each agent, cooperation would not be possible. They would not know what the other requires. Experts working together must share at least part of their representations. This may require that they extend their own representations.</i></b><i> For example, the surgeon in the operating theatre team must have basic understanding of anaesthetic procedures and problems and the anaesthesiologist must understand the needs and problems of the surgeon. Both must know which part of the representations is shared and which they do not share. </i><b><i>But even that does not guarantee that they always understand each other immediately; communication is not automatically smooth and spotless, especially in cases of uncommon situations or procedures.</i></b><i> For example, a 4-month-old child with sickle cell anaemia (a very uncommon condition in The Netherlands where this happened) needs a hernia operation. From the perspective of the surgeon this is a standard operation that only requires some extra blood cells because of the child&#39;s anaemia; a &#39;piece of cake that can be done on a Friday afternoon&#39;. For the anaesthesiologist it is a very complicated situation, not only because the time span in which blood has to be given is very short, but especially because problems with the blood circulation may occur afterward and require extra precaution. Hence she refuses to do the procedure in a local hospital that does not have the equipment to keep the child properly monitored after the procedure.</i><br /><br /><i>An example from another domain is the different representations that a team of people working on new product design will have of the product. The engineer will look at the product aspects that make the product work smoothly; the designer will focus on whether the product looks and feels pleasing; the marketing person will look at the product in the light of how it should look and act in order to sell better; the human factors specialist at those aspects that have to do with how the customer will interact with it. </i><b><i>All are looking at the same product; all have a different representation of it, with different data sets, and different operators.</i></b><i> It is no wonder that team members sometimes have a hard time talking to each other — in a way, they really are thinking about a completely different product. Similarly, the surgeon and anaesthesiologist are dealing with different (aspects of) patients and draw different conclusions. The misunderstandings that may result from such differences in perspective are a serious disadvantage that can only be solved through communication. This, unfortunately, takes time, another disadvantage. On the other hand, time can be saved as well. The product design team can complete the assignment in less time than one single person can. A disadvantage can be that a team member sometimes has to wait until the other has completed his or her share in the task.</i><br /><br /><i>This tentative analysis of the advantages and disadvantages of people working together in teams and hence of distributed representations in multiple agents reveals several factors that can affect this cooperation: (a) (dis)similarity of representations, (b) the (lack of) coincidence of individual tasks and goals and the common goal of the team, (c) the necessity of a good timing and coordination of the activities. Another factor that has not yet come up in the analysis but that may largely affect the success of a team is (d) the responsibilities of the team members and the social structure that coordinates these individual responsibilities. We see this very often in medical teams responsible for the care of multi-problem patients. The specialities in such teams are not organised hierarchically (each has its own responsibilities that cannot be overruled by the others). Sometimes it is possible to find a way out of a dilemma by taking extra measures, but sometimes, when evaluation procedures fail to come up with one definitive solution, the team may end up in a situation where there is no good solution. They are caught in an approach-avoidance dilemma that people try to prevent as much as possible. An example of such a situation is the discussion whether to operate or not on a very important person, e.g. the Pope or the president of the former Soviet Union. In the case of Boris Yeltsin in the mid-1990s, who was said to have local and generalised problems with the heart and blood vessels, long and serious discussions took place, because not only the health or even life of the Russian President was at stake, but also the professional status and prestige of many doctors.</i><br /><br /><i>The heart surgeon said that he could not take the responsibility for longer postponement of the operation; the anaesthesiologist and the physician said that the man&#39;s condition was so poor (he had problems with the kidneys and liver, and generalised atherosclerosis) that his chances of surviving the operation were minimal. Finally they consulted a very famous and old American heart specialist to come to a mutual agreement. In the following paragraphs each of the factors that could affect cooperation will be analysed.</i><br /><br /><i>2.2 (Dis)similarities of Representations</i><br /><br /><b><i>Representations can be dissimilar in the data, the format and the rules that operate on them.</i></b><i> Experts working together must at least share some of their representations. How far this similarity must go, is unknown. In daily practice it is impossible, yes even undesirable to have complete similarity of representations. It would annihilate the advantage of saving learning time. </i><b><i>In practice we see a combination of shared representations and trust in the validity, feasibility, etc. of the other&#39;s representations.</i></b><i> An operating theatre team (partly) shares their representation of the standard procedure applied; their representations of the patient, however, largely deviate since both specialities take different aspects into account. The anaesthesiologist primarily focuses on the vital functions of the patient with the help of hi-tech equipment monitors, while the surgeon focuses on the patient&#39;s anatomy, a part the anaesthesiologist often even hardly sees because of his or her position in the room.</i><br /><br /><i>Two people can &#39;cooperate&#39; to make data from their representation available to the other person. We discuss two different ways in which this can occur: coordination and integration.</i><br /><br /><i>2.2.1 Coordination Between Two Representations</i><br /><br /><b><i>What does it mean for two people to coordinate between representations?</i></b><i> Anaesthesiology and surgery require completely different skills, and therefore operators and databases. So different, that although both are needed for a successful task execution, one can do his or her job nearly independent of the other. The part that is not independent, however, is crucial and coordination is a necessity. The surgeon and anaesthesiologist are fortunate in that there are only a relatively small number of pieces of information that one needs from the other during the operation. Mostly, data exchange suffices. With the term data exchange, we mean the furnishing of pieces of information from one representation to another, which the other representation can use. Pure data exchange does not, a priori, require translation of the data; although most often, for data exchange to be successful, one of the partners will have to translate in order for the data to fit the other&#39;s representation. For the director of a business, it is crucial to know what profits the company made. The accountant gives him or her a summary of the company&#39;s operation, translated to terms the director has learned to understand. The director does not need to learn accounting, only to learn to understand the summary. The accountant does not need to know how to run the business, just how to compose the summary and how to provide other information needed for the director to make sound decisions. In most cooperations, there are clear rules as to which data is needed and in what format, and who will do the translation.</i><br /><br /><i>For successful data exchange, therefore, the cooperating partners must know for what reason which data need to be exchanged, and the format in which they need to be exchanged. Some data need to be exchanged because the other person needs to know that to perform his own procedures in the right way. In the surgical team that might be the information from the anaesthesiologist that the patient is anaesthetised at a proper level so that the surgeon can start his or her procedure. Some data need to be communicated to the other team members because they indicate an emergency coming up, for example cardiac fibrillation that requires a resuscitation. The pieces of data that need to be exchanged between the team members form the &#39;links&#39; in the team process; they are needed to continue it. </i><b><i>In terms of representations, these data are the places where their representations touch each other — where they coordinate.</i></b><br /><br /><i>Operator exchange is another form of representation coordination. However, giving someone an operator is only useful if the other has the correct data structure. Let us say you receive this operator: &#39;If a lower number appears before a higher number, the two numbers form a unit and you have to subtract the lower number from the higher number to get the correct number.&#39; If you do not know what Roman numerals are, and what value the basic numbers have, then you could be given the string XLIX, but you cannot apply the operator. </i><b><i>Hence, operator exchange is only useful if there is a data structure to apply it to.</i></b><br /><br /><b><i>Also, non-trivially, people need to know that a cooperation needs to exist. The anaesthesiologist and the surgeon have both gone to medical school, and have both been exposed to basic issues in anaesthesiology and surgery. Through acquiring partly the same representations, they have learned that they need each other, what for, and probably what the rules of data exchange are. If this initial exposure to other representations does not occur, people may have no idea that they need someone else to successfully complete a task.</i></b><i> For instance, consider computer programming — especially the user-friendliness aspect. In the early years of computer programming, and unfortunately even often today, programmers were rarely exposed to human factor, social or cognitive psychology issues relevant to software usability. Consequently, they constructed software that is easy to use for them, but not for the (non-programmer) people who have to use it. Most often users of software do not share the representations and knowledge of the software designers and programmers. Trying out the software on one&#39;s colleagues does of course not help. </i><b><i>The software designers are not to blame though — they have no way to even know that they are constructing non-user-friendly software, or of realising that cooperation with human-computer interaction (HCI) experts may dramatically improve their product. Cooperation was not stimulated at the organisational level; it was not part of the programmers&#39; formal training experience.</i></b><br /><br /><i>2.2.2 Integration of Two Representations</i><br /><br /><b><i>Data exchange, and an understanding of each other&#39;s needs by (often very) limited shared representations is generally all that is needed for cooperative partners to do the task.</i></b><i> When someone has to take over another&#39;s task, or continue another&#39;s work, integration of representations has to occur: the whole or partial takeover of a representation so that newly incoming information can be interpreted similarly. One can only integrate representations, i.e. make it possible to freely exchange operators and data, if representations have the same data structures and operators (they do not need identical contents; but the more overlap of content, the easier the exchange). The more complex the task, the more difficult this is. Academic students, in general, need at least 10 years of preparatory work to be able to continue parts of the work of their professor. In work situations, passing on &#39;experience&#39; is often very difficult. If this knowledge is considered critical, a knowledge engineer can be called in to record the representation of an expert. This is difficult and time-consuming work, and requires many iterations with the expert to ensure that the representation has been recorded correctly. Presenting the knowledge in such a way to the next person so that it can be acquired is another problem again, but as volumes upon volumes have been written about education, one we will leave to others.</i><br /><br /><i>2.3 Other Factors that Affect Cooperation Between Multiple Agents</i><br /><br /><i>2.3.1 Task and Goal Structure</i><br /><br /><i>The previous paragraphs have shown that coordinating and integrating representations can be very difficult and time consuming. How important it is that representations are coordinated and integrated, and how and when this has to be done depends to a large extent on the task characteristics.</i><br /><br /><i>Let us return to our team of medical specialists concerned with the preparation and performance of surgery on a patient. This team has one common, overarching task to which every team member contributes in his or her own way. Everyone is responsible for his or her own sub-tasks in the whole, plus exactly knowing when and what data to exchange in what format. </i><b><i>In such a task situation these coordinations should not be done on the job. It could even be disastrous. A substantial part of the success of the team lies in preparation and in the prevention of problems</i></b><i> (Gaba, 1992). This can be done by testing procedures and the communication lines in the team by running simulations of emergencies. An important aspect of doing surgery is that the job must be done in a specified period. The shorter the better. Problems will always lead to a longer operation and more anaesthetic, and more stress to the patient. Problem prevention is to be preferred over problem solving. Surgery is not unique in this respect. The same applies to aviation. While in the air it is impossible to park the aeroplane and perform trouble shooting.</i><br /><br /><i>Domains like these have their own dynamics and timing. Deadlines are very, very real, as someone will literally be dead, unlike with the project &#39;deadlines&#39; that a design team is confronted with. Those might be merely expensive. Another difference is that the normal task must go on during trouble shooting, however difficult it may be. The aeroplane must stay in the air, and the patient must be kept alive.</i><br /><br /><i>2.3.2 Working Procedures</i><br /><br /><i>Although cooperation in surgical and aviation teams is heavily practised, </i><b><i>other collaborative tasks have less clearly worked-out structures and standard procedures.</i></b><i> An example is the team of road accident analysis in chapter 9 by Alpay, Giboin and Dieng. But even new groups, working on tasks the individual members have never done before, could do better by using a more or less standard way of project management to organise their activities, manage their agenda and allocate resources. </i><b><i>Many innovation projects have about five phases (Cumming &amp; Worley, 1993). In the first phase an exploration of the problem or task and of the situation takes place, also including the environment that can dictate boundary conditions for future solutions. This soon narrows down the field to a limited set of possible solutions. The analysis then follows; pros and cons of the solutions are sorted out, and possibilities are estimated. As a next step, a small-scale try-out can be initiated. After this phase has been successfully completed, larger scale implementations can take place. The final phase involves an evaluation of the innovation.</i></b><br /><br /><b><i>Working procedures need to describe what has to be done, when, by whom, who is responsible for what, who takes which decision, who communicates with whom, etc. Clearly, standard procedures and scripts dictate the communication between participants and how their representations should be tuned to the task.</i></b><br /><br /><i>2.3.3 Expertise as a Team</i><br /><br /><i>Multi-disciplinary teams often consist of experts with different backgrounds. We have seen several examples in this chapter: the surgeon, anaesthesiologist, cardiologist and physician who are responsible for the management of a multiple-problem patient or the product development team consisting of a designer, an engineer, a human factor specialist and a marketing specialist. All these people have to learn to work together, not only by coordinating and/or integrating their representations, but also by learning how the work of the other is done and how it affects their own work.</i><br /><br /><i>An example is found in the work by Gaba (1992) who investigated the cooperation of operating room teams. In his opinion every time an anaesthetic is given, it should be perceived as a disaster waiting to happen; good teams prevent disasters as much as possible. They do this by intensive investments in team training, by doing pre-use checkout of the equipment, preoperative evaluation of the patient and by monitoring the care and skill of the team members. The better they know each other&#39;s strong and weak points and idiosyncratic peculiarities (e.g. a left-handed surgeon) and how these affect their own task, the better they can prevent eventual problems. Prevention of these problems is to be preferred over emergencies that have to be dealt with on the spot.</i><br /><br /><i>3 Distributed or Multiple Representations in Multiple Agents Revisited</i><br /><br /><i>All the examples given and analysis performed were meant to serve one goal: the analysis of the essence and of the factors affecting the cooperation of multiple agents having multiple representations. Looking back, a few important points seem to have emerged in our analysis and examples:</i><br /><br />. <i>Most prominent is of course the ways multiple representations can be dealt with: coordination between representation, including data and operator exchange, and real integration consisting of (partially) handing on knowledge and experience.</i><br /><br />. <b><i>Boundary conditions are the recognition by one agent of which data or operators are needed by the other agent, or the other way around when the agent him- or herself needs which data from the other agent.</i></b><i> This rather metacognitive aspect of the cooperation is made much easier when all participating agents have a common understanding of the task to be performed and the procedures that have to be followed. These standard procedures and protocols coordinate the tasks and responsibilities of the different team members; they also dictate and monitor the timing of the task performance. Since these procedures are common knowledge, they also regulate the interaction between the agents. Expertise built up by the team in running these procedures will result in faster and smoother communication and in prevention of problems rather than solving problems after the fact.</i><br /><br /><i>Two chapters in this part of the book explore aspects that are fundamental in the cooperation between multiple agents. Bromme and Niickles (chapter 10) investigate what determines the problems professionals of different backgrounds have when working together, i.e. doctors and nurses working together in a paediatric oncology ward. They suggest the notion of &#39;perspectives&#39; to refer to knowledge which is distributed among different persons and shaped by different (professional) approaches. </i><b><i>Relevant differences seem to be: knowledge about the task demands of the other, representations of the presumed perspectives of the other, responsibility for effective interprofessional communication, power differences, different languages used by differently trained professionals and the existence of one dominant language</i></b><i>, i.e. the doctors&#39; language. Alpay, Giboin and Dieng (chapter 9) basically address the same phenomenon: cooperating experts with different professional training, i.e. different specialists in a team that analyses road accidents. Their chapter describes a quest after the dimensions along which the representations of different specialists vary and how coordination of these representations can be best described. Ostwald&#39;s model of representations for mutual understanding, but as Alpay et al. also found for mutual agreement, turned out to be very helpful in explaining the differences and indicating directions for improvement. </i><b><i>Their work also shows that unclear task structures magnify the problems due to a lack of knowledge about the information needs of other team members.</i></b><i> Although approaching from very different angles these two chapters allow similar conclusions concerning the factors that jeopardise shared cooperation in a team due to noncoordinated multiple representations.</i><br /><br /><i>4 Again Multiple Representations in One Agent</i><br /><br /><i>From the above analyses, it is clear that it takes work and effort to coordinate different representations in multiple agents; it should come as no surprise that we assume that coordinating different representations in a single agent is no less effortful though less &#39;visible&#39;. In one agent, representations can also be coordinated through connections via data, or, more rarely, operators, and representations can be integrated. Coordination occurs by connecting representations through a piece of data, which then becomes common to both representations. </i><b><i>Integration occurs by interconnecting two similar but separate representations into one.</i></b><i> An example of this process in a single agent is the following. Suppose that you have learned to do statistics in the context of a mathematics class. Now, you take a few psychology classes, and after doing a few experiments, you collect a set of data. You can then learn to apply the statistics learned in the mathematics class to the psychology data, which have been translated to a form that now fit the statistical formulas. In this way, you can integrate the psychological data structures with your statistical representations.</i><br /><br /><i>In textbooks of nearly all disciplines, pictures, graphs, text and equations are often mixed — each, hopefully, being used to most efficiently convey knowledge to the reader. </i><b><i>When reasoning between representations is required, i.e. when one must connect knowledge from different representations into one body of knowledge, one must map knowledge from one (the base) onto another (the target), preserving among the target objects a system of relations that holds among the base objects.</i></b><i> This can be quite difficult for novices (Tabachneck, 1992). In case of between-representation mapping, the same domain knowledge is mapped from one representation to a different one. For instance, in a graph of an economic market, demand is represented by a downward-sloping line and supply by an upward-sloping line, with price on the yaxis and quantity on the A&#39;-axis. The equilibrium price and quantity are found by noting the coordinates of the intersection of the demand and supply lines. The graphical information can also be expressed verbally, for example, the supply information by the statement that the quantity of a commodity that consumers buy will be smaller, the higher the price. A successful representational mapping does not only add flexibility to the reasoning process; it may also offer new insights into problem structure and new computational possibilities. In one study of a physics and a geometry problem, diagrammatic and verbal representations facilitated different search control strategies, different recognition patterns, and different inferential processes (Larkin &amp; Simon, 1987).</i><br /><br /><i>Novices&#39; difficulties in within-domain mapping from one representation into another have been well documented both in basic research (e.g. isomorphs of the Tower of Hanoi puzzle (Hayes &amp; Simon, 1974-1976) and the mutilated checkerboard (Kaplan &amp; Simon, 1990)), and in applied research. </i><b><i>In economics, the base representation is verbal, and students come in with little or imprecise knowledge or with misconceptions. They do have some general knowledge about graphs, but the graph elements are meaningless until mapped onto the particular concepts they represent in economics. The operations required to interpret graphs include both general domain-independent (perceptual) operations and special operations that are meaningful only in the domain under consideration. A similar distinction applies to other representations, including the verbal (e.g. knowledge of English syntax vs. knowledge of terms used in economics). In medicine, it is hard to say which is the base representation: verbal, spatial, mental models, etc. To really grasp medical knowledge all these representations must be coordinated. This process goes on for many, many years. The diagnostic and management tasks medical students are trained for require that medical events are analysed using all these representations: how a tumour or inflammation, or other process, affects the spatial relations between organs, how they affect their functioning, and as a result the functioning of other organs, how the body will detect these disturbances and give alarm signals, and the other way round, how to interpret these alarm signals and signs of malfunction, and/or spatial changes, and integrate that into a clear image of what is wrong. Evidently the diagnostic task requires that students can apply their knowledge starting from the signs and symptoms, but medical education most often starts from the other end, describing the disease, requiring extra coordination effort from the student.</i></b><br /><br /><i>Four chapters in this part of the book deal with the problems of the coordination and integration of multiple representations in one single agent. All three do this from the perspective of expertise building, clearly signalling that they see this change as an important part of a learning process that can take very long. </i><b><i>Savelsbergh, De Jong and Fergusson-Hessler (chapter 13) show how the problem representations generated by poor and proficient novices and experts, and hence the knowledge structures these representations are based on, show remarkable differences especially in structure and flexibility. Expert knowledge is more coherent and experts have better knowledge about properties of situations that make that knowledge applicable or not. Furthermore, expert knowledge allows for multiple redundant representations, which is a powerful instrument to recover from errors and to find alternative solutions, once a first solution has failed. Expert knowledge also seems better linked to application situations.</i></b><i> Both Tabachneck-Schijf and Simon&#39;s and Boshuizen and Van de Wiel&#39;s chapters address the question how this process takes place. The chapter by Tabachneck-Schijf and Simon (chapter 11) gives a detailed example of how novices in economics learn when confronted with materials in different representations and how little success these learners had. The chapter by Boshuizen and Van de Wiel (chapter 12) also shows how difficult this integration process is. It shows how a more advanced medical student struggles with the coordination of different verbal and spatial representations when solving a problem, while the expert uses both representations flexibly, in a coordinated way. The student only happened to shift from a verbal, clinical knowledge base to a spatial, non-clinical, biomedical one when she got stuck in her problem solving, trying to support her verbal reasoning with spatial knowledge. Van Someren, Torasso and Surma (chapter 14) discuss the problem of optimising knowledge that can be represented in different representational forms. The context is diagnosis that can be performed by reasoning from a causal model or by reasoning from a memory of cases that were solved before. The case memory is &#39;functionally subsumed&#39; by the causal model but in general retrieving a case is more efficient. However, this is not always true. The problem now is to optimise the case memory.</i><br /><br /><i>No chapter, either dealing with multiple or single agents, focuses the importance of procedures, strategies, protocols and scripts that appeared crucial to good task performance in multiple agent teams. In a real paradigmatic shift in research, this aspect has slipped out of the focus of psychological research into the acquisition of expertise and advanced knowledge acquisition. Problem-solving research in the 1960s and 1970s investigating the use of algorithms and heuristics has been replaced by research into the use of knowledge in problem solving and the restructuring that is required for it. Only a few researchers (e.g. Schraagen, 1994) have investigated the role of more &#39;generic&#39; strategies in expert performance, indicating that experts integrate their domain knowledge in generic procedures that are also used by professionals who do not have knowledge of and experience with the specific domain. These procedures that are packed with or linking relevant knowledge in the expert largely guide their actions. Hence, the following chapters unravel aspects of working with multiple representations in single and multiple agents by making the obvious non-obvious. This is a very important contribution to the field. However, we do not hesitate to say that this is only part of the work to be done. Tasks, task structures, procedures, and maybe even the old research findings on algorithms and heuristics, have to be given a place too, later.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Stoner Thought Experiment","summary":"They are Sometimes Backed by the Most Sophisticated Mathematics Known to Man / It Works Because it's an Unfounded Conjecture that Makes Anything Possible / When We Take the Bait, We're Rewarding Fantasy and the Asking of Questions which Science Can Never Answer / If We're Smart, We'll Just Say No","image":"https://lh3.googleusercontent.com/-c4uM-jVap2s/V-No5qANaoI/AAAAAAAAIws/NizGgaile-Y1LjkJoD-4FBD6-L--Vy7CwCJoC/w5400-h7560/the-stoner-thought-experiment-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Hi3VDS2TZyM","publishDate":"2016-09-22T05:16:37.727Z","updateDate":"2017-01-03T02:00:18.133Z","text":"<b>The Stoner Thought Experiment: They are Sometimes Backed by the Most Sophisticated Mathematics Known to Man / It Works Because it&#39;s an Unfounded Conjecture that Makes Anything Possible / When We Take the Bait, We&#39;re Rewarding Fantasy and the Asking of Questions which Science Can Never Answer / If We&#39;re Smart, We&#39;ll Just Say No</b><br /><br /><a href=\"http://blogs.scientificamerican.com/cross-check/how-physics-lost-its-fizz\" class=\"ot-anchor\">http://blogs.scientificamerican.com/cross-check/how-physics-lost-its-fizz</a><br /><br /><i>&quot;... Greene’s suggestion that our universe may be a simulation run on the computer of an alien civilization is also old hat. </i><b><i>These ideas, in fact, are just pseudo-scientific versions of stoner thought experiments: What if our whole world is just a grain of dirt in the pocket of a giant? And there is a whole universe inside one grain of dirt in our pockets? What if our world is really just an experiment created by evil machines? And so on.</i></b><br /><br /><i>Physicists&#39; fantasies about parallel and virtual realms are not just stale. </i><b><i>Increasingly, they strike me as escapist and even irresponsible, because they are so lacking in evidence.</i></b><i> Scientists shouldn&#39;t have to serve the public good any more than poets or musicians. But if theories are being passed off as science, shouldn’t they have at least a remote chance of being empirically corroborated? </i><b><i>Otherwise, how do they differ from pseudoscientific ideas like intelligent design?</i></b><br /><br /><i>Susan Sontag&#39;s 2002 essay &#39;Looking at War&#39; captures my jaded attitude toward speculation in physics, and especially the notion that our cosmos is virtual. Sontag castigated philosopher Jean Baudrillard, among others, for claiming that there is no reality anymore; there are only media &#39;representations,&#39; &#39;spectacles&#39; and &#39;simulated realities.&#39; This sort of philosophical claptrap, Sontag argued, is reprehensible in a world filled with real people suffering from real injustice, tyranny and wars.</i><br /><br /><b><i>To recapture its fizz, physics desperately needs not new ideas but new facts. Discoveries, not inventions. Ideally, physicists will stumble on something so startling that they abandon their pursuit of multiverses, strings and other fantasies and return to reality ...&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The Safe Space","summary":"It's a 50-year-old Term which Originated in Gay and Lesbian Bars in the mid-60's / It Started in an Era of Anti-Sodomy Laws as a Place Where an LGBT Could be Out and in Good Company / In Other Words, it Was a Place where Political and Social Repression were Resisted / But, as the Term has Gone Mainstream, its Scope has Expanded to Universities, and its Meaning has Dramatically Shifted to a Place Where People (Oftentimes Students) can be Free of Ideas which Challenge their Beliefs","image":"https://lh3.googleusercontent.com/-RdIoDObI-p0/V9_0hAZ92HI/AAAAAAAAIvU/JQtljE3VVJsTGZmaMExDv-M1NAn2nDkgQCJoC/w3600-h5040/the-safe-space-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/cW69BPhyg8M","publishDate":"2016-09-19T14:26:10.547Z","updateDate":"2017-01-03T02:00:52.523Z","text":"<b>The Safe Space: It&#39;s a 50-year-old Term which Originated in Gay and Lesbian Bars in the mid-60&#39;s / It Started in an Era of Anti-Sodomy Laws as a Place Where an LGBT Could be Out and in Good Company / In Other Words, it Was a Place where Political and Social Repression were Resisted / But, as the Term has Gone Mainstream, its Scope has Expanded to Universities, and its Meaning has Dramatically Shifted to a Place Where People (Oftentimes Students) can be Free of Ideas which Challenge their Beliefs</b><br /><br />Barak Obama on safe spaces ...<br /><br /><a href=\"https://www.whitehouse.gov/the-press-office/2016/05/07/remarks-president-howard-university-commencement-ceremony\" class=\"ot-anchor\">https://www.whitehouse.gov/the-press-office/2016/05/07/remarks-president-howard-university-commencement-ceremony</a><br /><br /><i>&quot;So don’t try to shut folks out, don’t try to shut them down, no matter how much you might disagree with them.  There&#39;s been a trend around the country of trying to get colleges to disinvite speakers with a different point of view, or disrupt a politician’s rally.  Don’t do that -- no matter how ridiculous or offensive you might find the things that come out of their mouths.  Because as my grandmother used to tell me, every time a fool speaks, they are just advertising their own ignorance.  Let them talk.  Let them talk.  If you don’t, you just make them a victim, and then they can avoid accountability.&quot;</i><br /><br /><a href=\"https://whyevolutionistrue.wordpress.com/2016/09/07/lecture-invitation-to-watson-rescinded-because-of-remarks-on-race-he-made-in-2007/\" class=\"ot-anchor\">https://whyevolutionistrue.wordpress.com/2016/09/07/lecture-invitation-to-watson-rescinded-because-of-remarks-on-race-he-made-in-2007/</a><br /><br /><b>&quot;Lecture invitation to Jim Watson rescinded because of remarks about race he made in 2007</b><br /><br /><i>Three years ago I had the chance to chat with J. D. Watson (yes, the DNA Watson) for well over an hour, and it was an enlightening conversation spanning a lot of diverse topics (see my summary at the link). Watson has a reputation, rightly deserved, for being controversial, and he’s said a number of things that sometimes verge on the odious. The most infamous are his comments on race and intelligence given in an interview with the Sunday Times in 2007, reported by the New York Times in a 2007 piece article in 2007 and summarized in Wikipedia. </i><b><i>In short, he suggested that Africans had genetically-based lower IQs than other groups.</i></b><i> That’s not the first provocative, hurtful, and unevidenced thing he’s said.</i><br /><br /><i>Watson’s statements, although preceded by a history of similar biologically-deterministic remarks, brought him down: he was universally vilified and </i><b><i>forced to resign as Chancellor of the Cold Spring Harbor National Laboratory. A number of speaking invitations he’d received were withdrawn. Accusations of racism have followed him ever since. His name will always be associated with those Sunday Times remarks. I didn’t report this in my earlier post on this site, but Watson brought up the racism issue himself, and it was clear that he had been deeply upset by it, and felt blindsided and unfairly accused. I should add that he apologized for his remarks shortly after he made them.</i></b><br /><br /><i>But is Watson a racist? I don’t know. He is a biological determinist, and he took the phenotypic data on IQ differences as reflecting hard-wired genetic differences -- an unwarranted assumption. </i><b><i>We still have no good data on the genetic basis of ethnic differences in IQ.</i></b><i> But certainly his remarks were unwise and hurtful. </i><b><i>So if saying that performance differences are correlated with ethnicity is racism, then yes, he’s a racist. But I’m not sure that he bears a personal prejudice against blacks and favors discrimination against them (my definition of racism)</i></b><i>, though one can, I suppose, make inferences about that from his remarks about employees. At any rate, he should have kept his mouth shut, and Cold Spring Harbor had little choice but to fire him.</i><br /><br /><b><i>But now it’s 9 years on, and he’s still vilified, to the extent that a lecture he was going to give at New York University (NYU) has just been canceled because some student groups objected.</i></b><i> Here’s a letter sent by the NYU lecture committee informing people about the rescinded invitation:</i><br /><br /><i>&#39;We are writing to inform you that the lecture by Professor James Watson, scheduled for September 12, 2016 has been cancelled. We received the attached letter that had been </i><b><i>written by medical and graduate students at NYU School of Medicine</i></b><i> to express their feelings regarding the invitation of Dr. Watson for this distinguished lecture. In the letter, the students raised the point that Dr. Watson had made public claims to diminish respect for black, female and obese individuals. We agree with the students that this runs counter to our mission of diversity and inclusion at NYU Langone Medical Center and have thus elected to cancel the lecture.</i><br /><br /><b><i>We would like to take this opportunity to commend the students, deans, and faculty who have been involved in this discussion for their devotion to our shared community.</i></b><i> At NYU, we have a strong commitment to equality as well as freedom of speech, and the right balance between these is not always easy to determine. While we may have differences of opinion, we also have tolerance. The Neuroscience Institute will be partnering with students and administration in holding an open forum on inclusion and diversity in the sciences. Please be alert to future notifications for this event as we invite all voices to be heard.</i><br /><br /><i>We enthusiastically join our student community in </i><b><i>safeguarding an environment that promotes diversity</i></b><i> and that respects all people for their capacity to contribute. We will continue to honor this moving forward in our selection of speakers, professors, and trainees and will seek to include a wide range of perspectives during the selection process.</i><br /><br /><i>Sincerely,</i><br /><i>The 2016 Llinas Lecture, Organizing Committee&#39;</i><br /><br /><i>And here is the letter that got the Organizing Committee to cancel the lecture:</i><br /><br /><i>&#39;August 31, 2016</i><br /><br /><i>To the organizers of the Rodolfo Lllnas lecture series:</i><br /><br /><i>As you know, 2016 marks the 3rd annual Rodolfo Llinas Lecture Series at NYU Langone Medical Center. Llinas, an award winning neuroscientist, is credited for landmark discoveries characterizing the diversity of neurons and the Llinas law. The lecture series is meant to honor scientists who have made similarly powerful contributions to science.</i><br /><br /><i>This year&#39;s honored speaker, Dr. James Watson, has certainly made admirable and important contributions to science, most notably the first description of the DNA double helix. However, this is not his only source of notoriety.</i><br /><br /><b><i>In 2007, Dr. Watson was quoted by the UK&#39;s Sunday Times saying that he was &#39;inherently gloomy about the prospect of Africa&#39; because &#39;all our social policies are based on the fact that their intelligence is the same as ours.&#39; He continued on to explain that, while our systems assume human beings are all equal, &#39;people who have to deal with black employees find this not true.&#39;</i></b><i> This comment, met with opposition by the scientific community, preceded his retirement as Chancellor of Cold Spring Harbor Laboratories.</i><br /><br /><i>We believe that Dr. Watson&#39;s views on race and ethnicity are particularly troubling, scientifically inaccurate, and run counter to efforts launched by the National Institutes of Health, National Science Foundation, multitudes of private foundations, and NYU Langone Medical Center to create a more Inclusive environment in the sciences. </i><b><i>His selection as an esteemed speaker sends the wrong message to our university and the greater scientific community.</i></b><br /><br /><i>Many studies demonstrate that social prejudices still thrive in academia, and that racial and gender disparities persist. While there is no scientific basis underlying Dr. Watson&#39;s claims, statements like these have historically and continue to exclude potentially paradigm-shifting scientists from entering our field -- simply on the perception that their color or race will be met with hostility. Now, when a distinguished speaking invitation is extended, despite public remarks diminishing women, obese individuals, and all of Africa, we find it important to express our concerns to the administration and the greater research community.</i><br /><br /><i>We stand as students and scientists of color, as women in scienoe, and as allies against these declarations of our diminished intellect. Our presence and excellence in academia, between the very walls of this prestigious institution, are testament to their falsehood, and we will continue to thrive in spite of the statistics. To reflect on the words of Dr. Ulnas himself:</i><br /><br /><i>&#39;The Neurosciences do not exist exclusively to understand man&#39;s nature. They also serve a social function, such as in the treatment of the cerebral disease or when helping us to have a more pleasant and constructive life.&#39;</i><br /><br /><i>As scientists, we are charged with the dual obligations of discovery and service to humanity. The symbolism in these invitations reflects the character of our community; we must choose carefully what messages we are sending to the world. As the next wave of aspiring scientists and physicians, we refuse to be complicit in a toxic atmosphere towards minorities and women.</i><br /><br /><i>We aim to derail this trend in honor of fostering a more inclusive generation in science, where all people are respected and honored for their capacity to contribute. We ask for our leaders to respect and share in this goal as they select distinguished speakers moving forward.</i><br /><br /><i>With respect and admiration for science,</i><br /><br /><i>Student Diversity Initiative for NYUSoM</i><br /><i>Sackler Diversity Initiative at the Sackler Institute</i><br /><i>NYU Sackler Student Council</i><br /><i>NYUSoM Student Council&#39;</i><br /><br /><i>While NYU had every right to decide whether or not to invite Watson, once he was invited I see this kind of cancellation as censorship. </i><b><i>For one thing, I doubt that Watson would have said anything about race. Even if he’s bigoted, he’s learned to keep his mouth shut.</i></b><br /><br /><i>Second, I think he’s been punished enough, and deserves at least a chance to be heard. </i><b><i>Losing the Chancellorship of Cold Spring Harbor -- losing his job -- is about the worst thing that can happen to a scientist, and although there was no option about that, does he deserve to be shunned and punished for the rest of his life? (He’s now 88.)</i></b><br /><br /><b><i>Finally, Watson still has useful things to impart, as I discovered when chatting with him. Now the students have no choice about whether they can hear him.</i></b><br /><br /><b><i>Should the students protest his talk? Certainly! They can have a counter-talk, peaceful demonstrations, and so on: that is the &#39;free speech&#39; that NYU claims it values. And nobody has to go listen to him if they want. But now there’s no choice, for Watson cannot speak. Note, too, that NYU is already organizing a &#39;counter-forum&#39; on inclusion and diversity in the sciences, while banning Watson.</i></b><i> You can be sure that only one ideological view will be promoted at that counter-forum, and I doubt that anyone will defend Watson’s right to speak once he was invited.</i><br /><br /><i>This is censorship, pure and simple. If these students had their way, Watson would never be able to speak in public again, for he is tarred for life. And who knows -- maybe he’s changed his views in light of the absence of data supporting his views on racial differences. </i><b><i>His detractors have, in effect, given Watson a life sentence of being shunned for what he said nine years ago, and for which he’s apologized; and I don’t think that’s fair. Enough is enough. Student protest, certainly; student censorship, no.&quot;</i></b><br /><br /><a href=\"http://fusion.net/story/231089/safe-space-history/\" class=\"ot-anchor\">http://fusion.net/story/231089/safe-space-history/</a><br /><br /><b>What’s a ‘safe space’? A look at the phrase’s 50-year history</b><br />by Malcolm Harris<br /><br /><i>&quot;Biologist Richard Dawkins, in addition to being an Oxford fellow and a bestselling author, is a dick on Twitter. He relishes his uncompromising gadfly act, willing to go so far against the popular grain as to call famously oppressed teen Ahmed Mohamed a fraud because he didn’t really &#39;invent&#39; a clock. Dawkins frequently takes stands against so-called political correctness, especially when it comes to higher education. </i><b><i>&#39;A university is not a ‘safe space.’&#39; he wrote in a recent and characteristically gym-teacher-like tweet. &#39;If you need a safe space, leave, go home, hug your teddy &amp; suck your thumb until ready for university.&#39;</i></b><br /><br /><i>But what is a &#39;safe space&#39; and why shouldn’t a university be one? This tweet from Dawkins would have been a psychotic response to a school shooting or campus rape, but that’s not the kind of safety he’s talking about. The safe spaces that Dawkins doesn’t like are encroachments onto his turf by queer and feminist activists. All of the sudden a self-styled public intellectual like Dawkins has to use &#39;they&#39; as a singular gender-neutral pronoun or risk censure. He signed up for science, not social studies.</i><br /><br /><i>And Dawkins isn’t alone in his frustration. At the University of Missouri, the president and chancellor have both been forced to resign by student protesters who accused them of failing to create a safe space for Black students. At Yale, a residential &#39;master&#39; earned national condemnation after he and his wife stood up for the principle of racially offensive Halloween costumes. </i><b><i>&#39;Safe space&#39; has become a rallying cry for student activists who want to change the way their campus communities operate, but it has an older history.</i></b><br /><br /><b><i>In her book &#39;Mapping Gay L.A.,&#39; scholar and activist Moira Kenney traces the beginning of the &#39;safe space&#39; idea to gay and lesbian bars in the mid-60s. With anti-sodomy laws still in effect, a safe space meant somewhere you could be out and in good company -- at least until the cops showed up. Gay bars were not &#39;safe&#39; in the sense of being free from risk, nor were they &#39;safe&#39; as in reserved. A safe place was where people could find practical resistance to political and social repression.</i></b><br /><br /><b><i>According to Kenney, the term &#39;safe space&#39; first gets used consistently in the 60s and 70s women’s movement, where safety began to mean distance from men and patriarchal thought and was used to describe &#39;consciousness raising&#39; groups. &#39;Safe space,&#39; she writes, &#39;in the women’s movement, was a means rather than an end and not only a physical space but a space created by the coming together of women searching for community.&#39; Kenney quotes Kathy Sarachild, a founder of the early-70s organization New York Radical Women, on those consciousness-raising groups: &#39;The idea was not to change women, not to make ‘internal’ changes except in the sense of knowing more. It was and is the conditions women face, it’s male supremacy, we want to change.&#39; A safe space was not free of internal disagreement, but it did mean a devotion to a common political project. Those who attempted to undermine the movement -- consciously or unconsciously -- would be kept outside.</i></b><br /><br /><i>As the identity politics and anti-war radicals of the 60s and 70s were defeated and digested by the system in the past couple of decades left-wing groups have adopted safe space aspirations. With the dream of a grand confrontation between rebels and society fading, anti-capitalists (and anti-globalization anarchists in particular) looked toward &#39;prefigurative&#39; models, in which they tried to embody the changes they wanted to see.</i><br /><br /><b><i>By the time I showed up in left-wing spaces in the early 2000s, that meant horizontal organization and consensus instead of majority rule. It has also meant gender-neutral bathrooms, asking people’s preferred pronouns, trigger warnings, internal education &#39;anti-oppression&#39; trainings, and creating separate auxiliary spaces for identity groups to organize their particular concerns. Occupy Wall Street gave these ideas international exposure, but they’re not new. Among the likeminded, the &#39;safe space&#39; designation came to signify a set of standard respectful practices.</i></b><br /><br /><i>Theory, in addition to and in the context of activism, has helped shape the development of &#39;safe space.&#39; In the wake of their defeat in the 60s, many left-wing organizers retreated to the academy, particularly the humanities and social sciences, where they developed increasingly nuanced political schematics based on their experience. Perhaps they could work out on paper where exactly they went wrong. French theorist and queer activist Michel Foucault developed a wide multidisciplinary following with his vision of power as a web of everyday relationships. &#39;Power is not something that is acquired, seized, or shared, something that one holds on to or allows to slip away,&#39; he wrote in 1976, &#39;power is exercised from innumerable points, in the interplay of nonegalitarian and mobile relations.&#39;</i><br /><br /><i>For radicals in the decades following Foucault’s popularization, this insight had profound practical implications: it would no longer be enough to support the right organization or hold the right positions, we are also responsible for the ways in which we reproduce existing power relations at their most micro levels. </i><b><i>A space isn’t &#39;safe&#39; just because everyone is committed to the same movement. The dominant power relations still find their way into the room.</i></b><br /><br /><i>In the 80s and 90s, American thinkers diligently worked through the particulars of our national inequalities. In 1989, legal scholar Kimberlé Crenshaw published a paper called &#39;Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine, Feminist Theory and Antiracist Politics,&#39; in which she defined and popularized the term &#39;intersectionality.&#39; Crenshaw was spurred by a gap in antidiscrimination law, which protected women and Black people separately, but not Black women in particular. General Motors used this loophole to dodge an employment lawsuit: They were only hiring men for manufacturing jobs open to Black applicants and only hiring whites for secretarial jobs open to women. Black women were being excluded, but they had no case either as women or as Black people. From this insight, Crenshaw spun out a larger understanding of how Black women are doubly marginalized in the feminist and Black freedom movements, an understanding that continues to grow in popularity and influence, especially among young campus activists.</i><br /><br /><i>Crenshaw’s work pointed to a fundamental problem with the project of inclusion. Sociologist Patricia Hill Collins in her landmark 1990 book &#39;Black Feminist Thought &#39;developed similar ideas, writing that &#39;ordering schools, industries, hospitals, banks, and realtors to stop discriminating against Black women does not mean that these and other social institutions will comply. Laws may change, but the organizations that they regulate rarely change as rapidly. … As these women gained new angles of vision on the many ways that organizations discriminate, organizations searched for new ways to suppress Black women.&#39; This idea of suppression once inside implicated inclusive liberals even more than it did conservatives.</i><br /><br /><i>For people who take these ideas to heart, they provide quite a practical challenge when it comes to fostering safe space. No policy or law can imagine the full range of intersectional identities, and this impossibility has driven conservatives into a decades-long frenzy, pushing them to imagine super-hyphenated minorities they will be forced to accommodate in new ways. But creating separate space for &#39;people of color&#39; and &#39;women&#39; within an organization quickly reveals intersectional divisions within the accommodated groups. If, to take Crenshaw’s example, Black women are marginalized as women in the PoC auxiliary, and again as Black people in the women’s group, where are they supposed to go? And that only considers two of many real and relevant power relations. Inclusion programs (also called &#39;diversity&#39;) have the same problem; they can never be inclusive enough. Neither accommodation nor diversity -- the preferred liberal solutions -- are good answers to an intersectional critique.</i><br /><br /><b><i>With this new conception of how power operates, the standards for what constitutes a safe space have increased. There’s virtually no way to create a room of two people that doesn’t include the reproduction of some unequal power relation, but there’s also no way to engage in politics by yourself.</i></b><i> Realizing the full scale of what they’re up against and not wanting to engage in false advertising, some organizations like the radical feminist Bluestockings bookstore in Manhattan have switched to the phrasing &#39;safer space.&#39;</i><br /><br /><b><i>Despite what conservatives might imagine, safe space rhetoric is not universally accepted anywhere. Some of the fiercest attacks have come from inside queer theory itself.</i></b><i> Scholar and author of &#39;Female Masculinity&#39; Jack Halberstam has written forcefully against safe spaces and trigger warnings so much that he inspired the parody Twitter alter-ego &#39;Jock Halberslam.&#39; Representative tweet:</i><br /><br /><i>Jock Halberslam</i><br /><i>@halberslam</i><br /><br /><i>&#39;always tell my students, i’m not a counsellor. they won’t even let me supervise at summer camp after i tossed those whiner kids in the lake&#39;</i><br /><br /><b><i>&#39;What we need are new and inventive modes of protest not more safe space,&#39;</i></b><i> he wrote in a blog post about whether or not The Vagina Monologues excludes trans women.</i><br /><br /><i>Halberstam’s criticism, with its calls for students to toughen up to defeat the gender binary, is something like left-wing Dawkinism. American studies scholar Christina Hanhardt in her book &#39;Safe Space: Gay Neighborhood History and the Politics of Violence&#39; has a more compelling line. As gays were incorporated into mainstream society during the last quarter of the 20th century, she writes, their calls for safe space became part of a real estate development agenda and abetted the process of urban gentrification. &#39;Mainstream LGBT political discourse has substantively transformed the category of anti-LGBT violence from the social to the criminological,&#39; Hanhardt writes. Absent a revolutionary political project, safe space could be understood as a demand for crime control, more neighborhood watch than Stonewall.</i><br /><br /><i>Though the ideal of a safe space seems increasingly complicated, the language has proliferated. The university campus is one place, and perhaps the best publicized, but there are others. Kareem Reid wrote for Fader magazine about trying to run a London nightlife party as a safe space: &#39;I promoted the night as a safe space for queer, black, and brown bodies, stipulating ‘no homophobia, no transphobia, no patriarchal flexing’ -- and it kind of worked for a while, but it wasn’t perfect. </i><b><i>The reality of being in public spaces is full of inequalities</i></b><i>, and while I was dismayed to hear that women had been harassed at my night, I wasn’t that surprised because of how often this happens. Eventually, I stopped billing my event as ‘safe.’&#39;</i><br /><br /><i>There are dangers to turning &#39;safe space&#39; into a label of compliance, the way a juice might call itself &#39;organic.&#39; One is that, since the ideal is unachievable, people will give up on the aspiration. Another is that it’s alienating to the uninitiated, especially when those in the know come to believe that true respect can only be articulated in their proprietary dialect. A third is, if you’re not careful, the demand for safe space can itself play into existing power relations. But as the evolution of the phrase over the past 50 years shows, it’s flexible and enduring. It still means something.</i><br /><br /><b><i>Even most advocates will admit that literal safe space is a utopian idea.</i></b><i> Without a unified radical movement, utopianism can look like petty intransigence </i><b><i>or an inability (rather than refusal) to cope with the world as it is.</i></b><i> But with insights gleaned from decades of experimentation, scholarship, and struggle, </i><b><i>most leftists understand that in the web of power relations there is no real shelter to be found. No one can be so conscious and circumspect as to cleanse themselves of all oppressive ideology before entering a meeting or a party or a concert or classroom. As a result, the meaning of safe space has shifted again.</i></b><br /><br /><b><i>What Richard Dawkins hates about the idea of the university as a safe space is that the label is like a sign that hangs outside the classroom saying &#39;Warning: Politics inside.&#39; He wants to explain things, not be party to a ruthless critique of social relations and knowledge production. A safe space, despite the denotation of the phrase, is somewhere people come together and -- in addition to whatever else they’re doing -- wrestle with the chicken-and-egg problem of how to change themselves and the world at the same time.</i></b><i> It’s an adventure that Dawkins would rather not risk, but he should be honest about who precisely is afraid.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"String Theory as Not Even Wrong","summary":"Not Even Wrong is a Witty Scientific Slight Coined by Wolfgang Pauli / It's a Critique of Theories that are So Incomplete and Ill-defined that they Can't be Used to Make Firm Predictions whose Failure Might Show them to be Wrong / String Theory has Come to Epitomize the Proverb, but the Critiques Apply to Many Accepted Scientific Theories as Well","image":"https://lh3.googleusercontent.com/-nqLpGUlr5AI/V97O0s7EUGI/AAAAAAAAIuc/RNhLC5-DGvUhChzeXY9pk0Ul18P_PfPqwCJoC/w1800-h2520/string-theory-as-not-even-wrong-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/1sEe9npchZm","publishDate":"2016-09-18T17:28:56.234Z","updateDate":"2017-01-03T02:01:14.283Z","text":"<b>String Theory as Not Even Wrong: Not Even Wrong is a Witty Scientific Slight Coined by Wolfgang Pauli / It&#39;s a Critique of Theories that are So Incomplete and Ill-defined that they Can&#39;t be Used to Make Firm Predictions whose Failure Might Show them to be Wrong / String Theory has Come to Epitomize the Proverb, but the Critiques Apply to Many Accepted Scientific Theories as Well</b><br /><br />(AS ALWAYS, BOLDING IS PROVIDED TO HELP YOU SKIM THROUGH THE HIGHLIGHTS)<br /><br /><a href=\"http://www.scientificamerican.com/article/wronger-than-wrong/\" class=\"ot-anchor\">http://www.scientificamerican.com/article/wronger-than-wrong/</a><br /><br /><i>&#39;In belles lettres the witty literary slight has evolved into a genre because, as 20th-century trial lawyer Louis Nizer noted, &#39;A graceful taunt is worth a thousand insults.&#39; To wit, from high culture, Mark Twain: &#39;I didn&#39;t attend the funeral, but I sent a nice letter saying I approved of it.&#39; Winston Churchill: &#39;He has all the virtues I dislike and none of the vices I admire.&#39; And from pop culture, Groucho Marx: &#39;I&#39;ve had a perfectly wonderful evening. But this wasn&#39;t it.&#39; Scientists are no slouches when it comes to pitching invectives at colleagues. Achieving almost canonical status as the ne plus ultra [&quot;the ultimate&quot;] put-down is theoretical physicist Wolfgang Pauli&#39;s reported harsh critique of a paper: &#39;This isn&#39;t right. It&#39;s not even wrong.&#39; I call this Pauli&#39;s proverb.</i><br /><br /><i>Columbia University mathematician Peter Woit recently employed Pauli&#39;s proverb in his book title, a critique of string theory called Not Even Wrong (Basic Books, 2006). </i><b><i>String theory, Woit argues, is not only based on nontestable hypotheses, it depends far too much on the aesthetic nature of its mathematics and the eminence of its proponents. In science, if an idea is not falsifiable, it is not that it is wrong, it is that we cannot determine if it is wrong, and thus it is not even wrong.</i></b><br /><br /><i>Not even wrong. What could be worse?&quot;</i><br /><br /><i>Not Even Wrong: The Failure of String Theory and the Search for Unity in Physical Law</i><br />by Peter Woit<br /><br />(from the book&#39;s introduction)<br /><br /><i>&quot;... From the beginning, there was both no experimental evidence for superstrings and various obvious problems in the way of ever being able to use them to make experimental predictions. </i><b><i>The theory required postulating the existence of many extra unobserved dimensions, and by different choices of the properties of these extra dimensions, one could get just about anything one wanted.</i></b><br /><br /><b><i>Remarkably, the lack of any progress in achieving a predictive version of the theory that could be tested by experiment did not lead to theorists giving up on the superstring idea. Instead, it achieved a sort of critical mass, as a whole new research field grew up, largely disconnected from the rest of physics. The great complexity and poorly understood nature of superstring theory provided many topics for theorists to work on, while at the same time avoiding any possibility of showing that the idea was wrong.</i></b><i> A large number of popular science articles, books, and even TV programs promoted superstrings to the general public. By 2004, there was an undergraduate textbook devoted to the subject.</i><br /><br /><i>While the failure of superstring theory as a unified theory of elementary particles grew ever more difficult to ignore, up until the turn of the century, consistent progress was being made toward better understanding some of the implications of the superstring idea. </i><b><i>This sort of internal progress slowed down dramatically</i></b><i>, and in recent years attempts to connect up superstrings with reality have taken a rather bizarre turn. Many string theorists have become convinced that superstring theory inherently must allow an astronomically large number of physical possibilities, so many that it is difficult to see how the theory can ever be tested. </i><b><i>Normally, this sort of conclusion would cause physicists to abandon a theory, but some theorists have instead chosen to claim this as a virtue. They see the existence of this &#39;landscape&#39; of possibilities as justifying the use of something called the &#39;anthropic principle.&#39; Maybe we really live in a &#39;multiverse&#39; of different possible universes, and the one we are in has the particular laws of physics we observe just because those laws are among the few possibilities hospitable to life. This way of thinking about physics does not seem to lead to any falsifiable predictions, and so is one that physicists have traditionally considered to be unscientific.</i></b><br /><br /><i>The willingness of some physicists to give up on what most scientists consider the essence of the scientific method has led to a bitter controversy that has split the superstring theory community. Some superstring theorists continue to hold out hope that a better understanding of the theory will make the landscape problem go away. Others argue that physicists have no choice but to give up on long-held dreams of having a predictive theory, and continue to investigate the landscape, hoping to find something about it that can be used to test the idea experimentally. The one thing both camps have in common is a steadfast refusal to acknowledge the lesson that conventional science says one should draw in this kind of circumstance: if one&#39;s theory can&#39;t predict anything, it is just wrong and one should try something else.</i><br /><br /><i>The physicist Wolfgang Pauli was known for his often less than polite criticism of the work of some of his colleagues. He would sometimes exclaim &#39;wrong&#39; (falsch) or &#39;completely wrong&#39; (ganz falsch) when he disagreed with someone. Near the end of his life, when asked his opinion of an article by a young physicist, he sadly said &#39;it is not even wrong&#39; (Das is nicht einmal falsch). </i><b><i>The phrase &#39;not even wrong&#39; is a popular one among physicists, and carries two different connotations, both of which Pauli likely had in mind. A theory can be &#39;not even wrong&#39; because it is so incomplete and ill-defined that it can&#39;t be used to make firm predictions whose failure would show it to be wrong. This has been the situation of superstring theory from its beginnings to the present day.</i></b><br /><br /><i>This sort of &#39;not even wrong&#39; is not necessarily a bad thing. Most new theoretical ideas begin in this state, and it can take quite a bit of work before their implications are well enough understood for researchers to be able to tell whether the idea is right or wrong. </i><b><i>But there is a second connotation of &#39;not even wrong&#39;: something worse than a wrong idea, and in this form the phrase often gets used as a generic term of abuse. In the case of superstring theory, the way some physicists are abandoning fundamental scientific principles rather than admit that a theory is wrong is something of this kind: worse than being wrong is to refuse to admit it when one is wrong.</i></b><br /><br />[...]<br /><br /><i>One of the leading proponents [of the anthropic principle] is Leonard Susskind, a professor at Stanford and one of the codiscoverers of string theory, explains,</i><br /><br /><i>&#39;Mostly physicists have hated the idea of the anthropic principle; they all hoped that the constants of nature could be derived from the beautiful symmetry of some mathematical theory ... Physicists always wanted to believe that the answer was unique. Somehow there was something very special about the answer, but the myth of uniqueness is one that I think is a fool&#39;s errand ... If there were some fundamental equation which, when you solved it, said that the world is exactly the way we see it, then it would be the same everywhere. On the other hand you could have a theory which permitted many different environments, and a theory which permitted many different environments would be one in which you would expect that it would vary from place to place. What we&#39;ve discovered in the last several years is that string theory has an incredible diversity -- a tremendous number of solutions -- and allows different kinds of environments. A lot of the practitioners of this kind of mathematical theory have been in a state of denial about it. They didn&#39;t want to recognize it. They want to believe the universe is an elegant universe -- and it&#39;s not so elegant. It&#39;s different over here, it&#39;s that over here. It&#39;s a Rube Goldberg machine over here. And this has created a sort of sense of denial about the facts about the theory. </i><b><i>The theory is going to win, and physicists who are trying to deny what is going on are going to lose.&#39;&quot;</i></b><br /><br /><a href=\"https://www.amazon.com/gp/customer-reviews/R1ZJZAZXO7G9QA/ref=cm_cr_arp_d_rvw_ttl?ie=UTF8&amp;ASIN=0465092764\" class=\"ot-anchor\">https://www.amazon.com/gp/customer-reviews/R1ZJZAZXO7G9QA/ref=cm_cr_arp_d_rvw_ttl?ie=UTF8&amp;ASIN=0465092764</a><br /><br />(from a review of Peter Woit&#39;s book)<br /><br /><i>&quot;... I&#39;m a qualified theoretical physicist, no longer practicing. For many years, I heartily supported the efforts of string theorists to achieve either a further illumination of the standard model or a synthesis of general relativity and quantum mechanics. I am sad that they have really achieved neither. Perhaps one day they will achieve something. I still wish them every success.</i><br /><br /><b><i>But the string community (or any other scientific group) cannot be allowed to turn their chosen approach into a fad, a cult, a religion or - worst - an inquisition. That might be sociologically amusing if it weren&#39;t so pernicious for physics and physicists. The effect is that physics departments become the monopoly of self-perpetuating, self-congratulating clans of homogenous thinkers. (Lee Smolin uses the psychological term &#39;groupthink&#39;.) Young physicists are informed that string theory &#39;is the only game in town&#39;. Sign up for it or go away.</i></b><br /><br /><i>Historically, scientists have countered our natural human hubris to believe whatever we think in two ways. One is by challenges from fellow scientists, who invent new theories, at odds in large or small ways with the standard thinking. The other is by nature, whose measured phenomena often contradict established theories. Of course, in the physical sciences, experimental evidence always holds the trump card over human speculation.</i><br /><br /><i>String theory is currently in an unfortunate place. There is no imminent experiment that could bring it into contest with nature. </i><b><i>And, because of the iron curtain within which the string community seems to have walled itself, there is little contact between the adherents of string theory and their potential human challengers</i></b><i> ...&quot;</i><br /><br /><i>Bankrupting Physics: How Today&#39;s Top Scientists Are Gambling Away Their Credibility</i><br />by Alexander Unzicker and Sheilla Jones<br /><br />Chapter 15: Goodbye Science, Hello Religion<br />String Theory: How The Elite Became A Sect And A Mafia<br /><br /><b><i>&quot;&#39;String theory is the hypothesis that space-time consists of very tiny structures in the range of 10^-35 meters. These one-dimensional strings (and their higher-dimensional analog, branes) are supposed to extend along additional dimensions of space-time. The vibrations of such strings are assumed to represent elementary particles. Initially, the theory aspired to calculate all the physical properties of particles, together with being able to explain the different interactions of physics in a unified picture.</i></b><br /><br /><i>String theory pretends to be nothing less than the ultimate unified theory of physics, the Theory of Everything (TOE), a unique accomplishment of human intellect. It is no wonder that the leading institutions of physics almost exclusively follow that road of research. Brian Greene&#39;s book The Elegant Universe explains why:</i><br /><br /><i>&#39;[A] unified theory has become the Holy Grail of modern physics. And a sizeable part of the physics and mathematics community is becoming increasingly convinced that string theory may provide the answer. From one principle -- that everything at its most microscopic level consists of combinations of vibrating strands -- string theory provides a single explanatory framework capable of encompassing all forces and all matter.&#39;</i><br /><br /><b><i>And yet the theory is all-encompassing only to the extent that we define what the &#39;everything&#39; in a Theory of Everything is supposed to be.</i></b><i> Shouldn&#39;t scientists at least know what they want to discover? The science author Gary Taubes expressed it like this: </i><b><i>&#39;The string theorists don&#39;t yet have a clue where this progress will lead them.&#39;</i></b><i> [2] Is it really progress then? Undoubtedly, string theorists have revealed surprising mathematical theorems. Unfortunately, most physicists just don&#39;t seem to know what they&#39;re talking about.</i><br /><br /><i>Jeffrey Harvey, a professor of string theory at the University of Chicago, said, </i><b><i>&#39;The string theorists have however still to figure out what the hell it all has to do with reality.&#39;</i></b><i> [3] </i><b><i>&#39;The answer might be &#39;nothing,&#39; right?&#39;</i></b><i> wrote Dieter Zeh, a chair of quantum theory at the University of Heidelberg. Like many others, Zeh was irritated by the clash of the bold promises of string theory and the concrete results at hand -- none at all, that is to say. Gerardus &#39;t Hooft, the Physics Nobelist of 1999, hit the nail on the head: </i><b><i>&#39;Imagine that I give you a chair, while explaining that the legs are still missing, and that the seat, back and armrest will perhaps be delivered soon; whatever I did give you, can I still call it a chair?&#39;</i></b><br /><br /><i>Phenomenal Painting in an Empty Frame</i><br /><br /><i>The physicist Lee Smolin tore the whole approach to pieces in his 2006 book The Trouble with Physics, arguing that </i><b><i>string theory is basically lacking in everything -- fundamental principles, consistent mathematics, and especially experimental testability, which is out of reach and therefore being replaced by vague hopes and speculations.</i></b><i> Peter Woit, a mathematician at Columbia University, recently characterized the heavily promoted ideology of the last 30 years as follows: </i><b><i>&#39;We must have supersymmetry, so must have supergravity, so must have string theory, ... so must have a multiverse where we can&#39;t predict anything about anything, thus finally achieving success.&#39;</i></b><i> [5]</i><br /><br /><i>In his book Not Even Wrong, Woit judges string theory even more harshly than other critics. Woit&#39;s title refers to a remark made by the quantum pioneer Wolfgang Pauli, notorious for his sharp tongue. Pauli had categorized physical theories as &#39;correct,&#39; &#39;wrong&#39; and &#39;not even wrong,&#39; expressing utter contempt for the last type of theory, which was even worse than wrong due to its lack of testability. </i><b><i>In the 1960s, Pauli derided an overly optimistic announcement of a theory of his friend Werner Heisenberg by sending a postcard to colleagues. It showed an empty frame with the caption, &#39;This is to show the world I can paint like Titian. Only technical details are missing.&#39; Oblivious to the irony Pauli pointed out, many theoreticians nowadays speak of &#39;frameworks&#39; if they don&#39;t have any tangible results.</i></b><br /><br /><b><i>Mathematically speaking, string theory is, Woit said dryly, &#39;the set of hopes that a theory might exist.&#39;</i></b><i> The famous mathematical physicist Roger Penrose, though in a polite tone, also rubs salt into the wound: &#39;How impressed am I that the very striking mathematical relationships ... indicate some deep closeness to Nature? ... I am certainly not convinced about it.&#39; [6]</i><br /><br /><i>Many practitioners of physics feel the same way, so that </i><b><i>the Theory of Everything is sometimes called a Theory of Something or, still more to the point, the Theory of Nothing</i></b><i>. And &#39;t Hooft goes one step further. </i><b><i>&#39;Actually, I would not even be prepared to call string theory a &#39;theory&#39; but rather a &#39;model&#39; or not even that: just a hunch.&#39;</i></b><i> However, most string theorists don&#39;t even have much of a hunch about the rest of physics.</i><br /><br /><i>The Science Of The Revolutionary Guards</i><br /><br /><i>&#39;Knowledge is concrete.&#39;</i><br />- <i>Bertolt Brecht, German writer</i><br /><br /><i>At a legendary string conference in 1995, Edward Witten, indisputably the leading mind of string theory, suggested that certain versions of string theory might be &#39;dual&#39; to one another, and thus deep mathematical similarities may be inherent in them. This must have been an enlightenment for a great number of people, because everybody talked about the &#39;second string revolution.&#39; The title of &#39;first revolution&#39; was reserved for the mere spreading of superstring ideas from the 1980s. I may be missing the punch line, but </i><b><i>is the suggested assumption of a yet-to-be-defined similarity between two types of general hypotheses a result that could actually nourish physics?</i></b><br /><br /><i>And yet, more revolutions were to come. In 2001, David Gross, at the meeting of the American Association for the Advancement of Science, gave a talk entitled &#39;The Power and Glory of String Theory,&#39; in a session called &#39;The Coming Revolutions in Particle Physics.&#39; Three years later Gross would be awarded the Nobel Prize -- though not for the announced revolutions. Maybe they just haven&#39;t been recognized yet. Or, the revolutions just lacked the results. I don&#39;t know.</i><br /><br /><i>Nevertheless these ideas have dominated the entire activity of the string community of the last decades. I believe </i><b><i>6,000 publications</i></b><i> should speak for themselves (assuming they had to say something). Just savor that number like the quantum field theorist Bert Schroer did: </i><b><i>&#39;I don&#39;t know any non-metaphoric statement in the history of particle physics that would allow 6000 researchers to write an article about. You need a subject which is sufficiently vague and flexible to be massaged by so many people.&#39;</i></b><i> [7]</i><br /><br /><i>As far as physics is concerned, these string &#39;revolutions&#39; are a storm in a teacup which has to date not poured a single drop of predictions. The lack of testability -- once a death sentence for a scientific theory -- is obvious. But the less the connection to reality, the bolder the aspirations. The Nobelist Sheldon Glashow noted, </i><b><i>&#39;Superstring theory is so ambitious that it can only be totally right, or totally wrong. The only problem is that the mathematics is so new and difficult that we won&#39;t know which for decades to come.&#39;</i></b><i> [8] Consequently, 30 years ago, string theory was already declared the yet-unexplored brilliant mathematics of the twenty-first century, a capital attribute that somehow has become less fashionable in the last decade. We are waiting for further superlatives to be developed.</i><br /><br /><i>&#39;The gift to the 21st century which fell by luck already into the 20th century.&#39;</i><br />- <i>Edward Witten on string theory</i><br /><br /><i>Math Too Difficult Or Theorists Too Frivolous?</i><br /><br /><i>I sometimes feel that I should not criticize string theory &#39;just&#39; because it is not testable. The mathematics is really, really tough. To be precise, however, its toughness is also hypothetical. That&#39;s because to date, nobody knows anything about the equations to be solved one day, not to mention the theory&#39;s lack of fundamental principles, which makes it unclear what it can ever be based upon. </i><b><i>Instead of getting the mathematics done, string theorists have instead turned to creating legends by carelessly citing incomplete results.</i></b><i> Lee Smolin pointed this out in detail, mentioning, for example, the oft-repeated claim that string theory is &#39;finite,&#39; an idea planted by an article in 1980 that relies on flawed proof. [9] Bert Schroer writes that </i><b><i>string theorists created &#39;a new culture of establishing a scientific truth starting from a conjecture and ending after several reformulations and turns with the acceptance within a community at the level of a theorem.&#39; [10] The entire reasoning has become, as Schroer pointed out in a series of articles, merely metaphorical.</i></b><br /><br /><i>Though not justified by anything, there is a widespread belief that elementary particles necessarily relate to string-shaped objects. Besides that vague idea, according to Sheldon Glashow, there are no convincing arguments in favor of string theory. He writes:</i><br /><br /><b><i>&#39;Until string people can interpret perceived properties of the real world they simply are not doing physics. Should they be paid by universities and be permitted to pervert impressionable students? Will young PhDs, whose expertise is limited to superstring theory, be employable if, and when, the string snaps? Are string thoughts more appropriate to departments of mathematics, or even to schools of divinity, than to physics departments? How many angels can dance on the head of a pin? How many dimensions are there in a compacted manifold, 30 powers of ten smaller than a pinhead?&#39;</i></b><br /><br /><i>Glashow&#39;s concern about jobs has luckily proven to be unsubstantiated, because &#39;also unfruitful ideas become immortal&#39; if a sufficient number of people build their careers upon it, as Smolin scoffed. Even vague assumptions blown up into absurdity have their nutritive value.</i><br /><br /><i>Dimensionful Thinking - it May Become Science</i><br /><br /><i>Despite the difficulty of string theory&#39;s mathematics, there are more than a few people who are not simply too stupid to understand it. Richard Feynman was one. He said: &#39;For example, the theory requires ten dimensions. Well, maybe there&#39;s a way of wrapping up six of the dimensions ... but why not seven? When they write their equation, the equation should decide how many of these things get wrapped up, not the desire to agree with experiment.&#39;[11]</i><br /><br /><i>Feynman summarized his argument rather tartly. </i><b><i>&#39;String theorists don&#39;t make predictions, they make excuses.&#39;</i></b><i> [12] When he happened to meet the inventor of strings, John Schwarz, in the elevator at Caltech, he teased him with the question, </i><b><i>&#39;How many dimensions do you live in today, John?&#39;</i></b><br /><br /><i>Putting the finger on that sore point of how arbitrarily string theorists play with dimensions is a pain in the neck for them. In his book The Elegant Universe, </i><b><i>Brian Greene heartrendingly describes how the theorists Brandenberger and Vafa have recently struggled to justify why we live in a three-dimensional reality. It is five long pages, and ends in a trail of coulds, woulds, and might-bes</i></b><i>, but allow me to refrain from summarizing. In popular books you may have seen the neat drawings of infinite surfaces, decorated with many small pellets, tiny cylinders, or even exotic pretzellike-objects representing the extra dimensions. The less likely it is to come to know the exact fabric of these dimension carpets, the more intensely it is woven onto the theoretical looms since, according to Greene, it is almost dead sure that this is the &#39;fabric of the cosmos.&#39;</i><br /><br /><i>In the world of string theory, the reader is guided through worlds with bizarre objects like o-heterotic strings, and Greene is not talking about a sex shop! For Greene soberly recounts Einstein&#39;s famous photoelectric effect (which doesn&#39;t work with red light ...) and explains at length why electrons leave the metal with the same velocity. [13] You don&#39;t have to be familiar with it, but electrons don&#39;t do this by any means, as a high-school experiment can show. But maybe such niggling details don&#39;t apply to the extra dimensions where Greene seems to lodge.</i><br /><br /><i>Methods of String Theory</i><br /><br /><i>Being a protagonist of string theory, Greene delivers some insights from his important contributions. A story of such revolutionary findings could, in theory, go like this:</i><br /><br /><b><i>&#39;Talking to my dog, I had this idea that I initially dismissed as utterly lunatic. But at a seminar at Harvard I happened to tell it to Professor Deepthinker, who reflected on it for a moment, then raised his eyebrows and said: &#39;This sounds highly interesting. You should take duality into account!&#39; Having been encouraged by this, I immediately set to work with my colleague Jeff Loophater from MIT. Unfortunately our calculations predicted that every elementary particle must consist of 26 pink elephants, which was at first a very depressing result. But after great effort and developing a new renormalization method we managed to reduce this number to 11. One merely has to allow trunkless elephants, a result that was independently proven by John Fastscribller and Jim Nitpicker from the Institute for Advanced Studies. Shortly afterward, Andy Wunderkind of Stanford University showed by means of a duality relation that the color could also be light blue instead of pink. These successes and the wide recognition we got very clearly indicated that we were on the right track ...&#39;</i></b><br /><br /><b><i>String theorists, the real ones, tend to rejoice like little kids about the most marginal reduction of the absurdities they had invented just a moment before. They seem to argue that &#39;we know all this seems utter nonsense, but believe us, it is an absolutely logical consequence of the nonsense we did before.&#39;</i></b><i> Many accounts are written in such a fashion and make you wonder whether this is still Galileo&#39;s book of Nature, which, as the Italian heretic once stated, is written in the language of mathematics.</i><br /><br /><i>String theory has not, of course, been honored with a Nobel Prize, and the science critic John Horgan has wagered $2,000 with the string supporter Michio Kaku that this will also not happen until 2020. </i><b><i>Rather, string theory was acknowledged with a Fields Medal, the most respected award in mathematics. It qualifies a mathematician to be a physicist as much as a Nobel Prize in Literature qualifies someone to be a politician.</i></b><i> This would not be all too tragic, but at the moment, it seems that if physics were a democracy, we would say that elections have been suspended and the physics leaders are being appointed by a circle of literary critics.</i><br /><br /><i>The Lord Of The Strings</i><br /><br /><i>&#39;Forgive him, for he believes that the customs of his tribe are the laws of Nature.&#39;</i><br />- <i>George Bernard Shaw</i><br /><br /><i>You can&#39;t understand string theory without Edward Witten. The early string theorists, Michael Green and John Schwarz, were considered stubborn, lone fighters in the 1970s and 1980s until Witten discovered them and brought them to </i><b><i>the &#39;Hollywood&#39; of physics</i></b><i>. It is said that Witten&#39;s phenomenal intelligence frustrated an entire generation of Princeton graduates working next to him, and his doctoral adviser, David Gross, ran out of math problems because Witten offhandedly solved all of them.</i><br /><br /><i>Witten&#39;s mathematical intelligence must be so all-encompassing that there seems to be not much left even for witty language. John Horgan gives an account of an interview with him. &#39;This is another mark of the na1ve ironic scientist: when he says &#39;truth&#39; there is never any ironic inflection, no smile; the word is implicitly capitalized.&#39; [11] Dare Antoine de Saint-Exupery say once more, that intelligence ruins the sense for the essential! What is more essential than the truth in physics?</i><br /><br /><i>Nobody will quibble about Witten&#39;s mental capacity. Even the string critics Smolin and Woit agree that they have never met a more intelligent person than Witten. Moreover, I have never heard anybody who had encountered Witten and not declared him the most intelligent person they had ever met. Brian Greene casually notes that Witten is regarded as Einstein&#39;s successor, as if we were talking about a new institute&#39;s chair. Yes, Witten is possibly the greatest genius of all time.</i><br /><br /><i>Of course, this also puts into perspective the casual encounters of Greene and Witten of which we are told. It is considered a great accolade among theoretical physicists if they are able to inspire Witten to contemplate a problem for one minute. I fear that 10 minutes of thinking might result in a Nobel Prize (once the Nobel committee eventually has recognized his capacities).</i><br /><br /><i>Witten&#39;s creativity is legendary, his productivity unmatched, and his intelligence unique. Those whom he listens to are anointed to a special realm, the books he lays his finger on are read by many, and his keyboard will no doubt someday be auctioned off on eBay. Witten the genius, Witten the new Einstein, Witten the greatest physcist of all time. The problem? He is not doing physics. </i><b><i>Witten stands out among those who surround him. He may work faster, better, and be more creative than others, but as the Chinese proverb states, &#39;The talent hits the target missed by everyone, but the genius hits the target no one has seen.&#39;</i></b><i> There is nothing more to say about Witten and Einstein.</i><br /><br /><i>Stringy Liturgy</i><br /><br /><i>Now let us venture into the unthinkable. </i><b><i>Can an entire intellectual elite like string theorists have permanently devoted themselves to foolishness? Can string theory really be nonsense if the best and brightest are convinced of it? Yes, it can.</i></b><br /><br /><i>It&#39;s not the ability to rush through complicated calculations that we must consider, but rather a lesson from sociology and history: big brains can be brainwashed, as well as little ones. Take, for instance, the medieval theologians. Weren&#39;t they the elites back then? Didn&#39;t the brightest guys of that era stall science for centuries? They did so because the best minds of that time had no choice but to sharpen their intellect through the problems of medieval theology. Today&#39;s theoretical physicists are trapped in nearly the same way. </i><b><i>A non-string theorist in a leading theoretical physics institution has become as exotic as an atheist in an Episcopal conference.</i></b><br /><br /><i>Unfortunately, the parallels don&#39;t end there. The wondrous stories in popular books on string theory mainly differ from creation myths in their time of origin. Just as the fascination with the godhood has its psychological origin, the fascination with strings is mainly a reflection of dreams and hopes. Physicists are served by string theories as humankind is by the world religions. It is a matter of faith.</i><br /><br /><i>What we are told is that there is a new era of mathematics our minds cannot yet grasp that will lead us to the Promised Land of strings. Those who delve into the subject in a like-minded community of experts -- let&#39;s call them &#39;monks&#39; -- are devoting themselves to abstruse calculation with great effort, an intellectual self-flagellation that is undoubtedly perceived as mind expanding. The absence of the gene of irony, as Horgan diagnosed for Witten,15 also characterizes religion. Or, as the Swiss writer Max Frisch phrased it, &#39;Given that you believe in God, is there anything that makes you believe he has a sense of humor?&#39; American string theorists could not crack a smile when they were called a faith-based initiative. And indeed, it isn&#39;t a joke. String theory has long since become a religion.</i><br /><br /><i>Tracing Tracks - Where Was The Experiment Again?</i><br /><br /><i>&#39;Errors using inadequate data arc much less than those using no data at all.&#39;</i><br />- <i>Charles Babbage, English philosopher</i><br /><br /><i>Reviewing the evidence for string theory only confirms the impression of religiosity. The biblical gospels at least give a credible account of the feeding of the 5,000, while string theory just provides wondrous professors&#39; salaries. &#39;String phenomenology&#39; is instead is a field for building careers, as being the head of the commission investigating the Eucharistic miracles. You may dispute how much string theory has entered that stage, but even grand unification sympathizers like the Harvard professor Howard Georgi are well aware of string theory&#39;s missing link to experiments and how dangerous that is:</i><br /><br /><b><i>If we allow ourselves to be beguiled by the siren call of the &#39;ultimate&#39; unification at distances so small that our experimental friends cannot help us, then we are in trouble, because we will lose that crucial process of pruning of irrelevant ideas which distinguishes physics from so many other less interesting human activities.</i></b><i> [16]</i><br /><br /><b><i>To wriggle out from such accusations, once in a while some fig leaves are provided that claim to be able to test string theory &#39;in principle,&#39; &#39;in the future&#39; or with &#39;indirect evidence,&#39; an abused term that helps to escape the here-and-now of observations.</i></b><br /><br /><i>Among all the predictions for experimental results from the Large Hadron Collider (LHC) results at CERN, the vaguest ones are from string theory, along the lines of &#39;if the extra dimensions are large enough, one might see something.&#39; One of the most brazen after-the-fact predictions of string theory was hyped by Gordon Kane, the director of the Michigan Center for Theoretical Physics. </i><b><i>Once all other energy regions were excluded as possible solutions by LHC experiments, and the first rumors started circulating late in 2011 that a signal at 125 gigaelectron volts might indicate the Higgs, Kane published an article [17] in Nature that concluded that it was precisely this value that supports strings!</i></b><i> At this point, you may ask what it actually means to hold a theoretical physics chair in a prestigious university if you can just follow your fancies into the realm of fantasy.</i><br /><br /><i>Another sport practiced in string theory is predicting violations. In this game, you take any good theory -- special relativity, laws of energy conservation, or whatever -- and any deviation from it, no matter what, where, and how, is deemed to be a triumph for string theory. Again we note that religion starts where science ends.</i><br /><br /><i>&#39;Theoretical ideas discussed must be supported by experimental facts. Neither supersymmetry nor string theory satisfy this criterion. They are figments of the theoretical mind.&#39;</i><br />- <i>Martinus Veltman, Nobel laureate</i><br /><br /><i>Other Successful Postdictions</i><br /><br /><i>&#39;Predictions are difficult - especially for the future.&#39;</i><br />- <i>Niels Bohr, Nobel laureate 1921</i><br /><br /><b><i>Do we really need particle colliders inaccessible by today&#39;s technology to test string theory? Actually, there are some particles that we can study without resorting to such high energies, such as the ones we consist of. It is therefore a misstatement that string theory does not have any experiments at hand with which to establish its glory. More than 20 values in particle physics, mostly masses, are awaiting an explanation.</i></b><i> Frank Wilczek, the Nobel Prize winner in 2004, said: &#39;The early promise of superstring theory to calculate these quantities has faded after decades of disappointing experience in attempts to construct phenomenologically adequate solutions, together with the discovery of multitudes of theoretically unobjectionable but empirically incorrect solutions.&#39; [18]</i><br /><br /><i>The failure to explain masses is now bizarrely being oversold by the claim that string theory &#39;reproduces the standard model.&#39; That&#39;s like claiming to draw a beautiful straight line and starting with a lot of bumps.</i><br /><br /><i>But let&#39;s focus on the big picture. Instead of grappling with these tedious masses, Edward Witten put forward a revolutionary insight that string theory predicted gravity! The moment at which this occurred to him, he said, was the intellectually most satisfactory one in his life. Later on, he even predicted the existence of the proton. The string community appeared spellbound by these epiphanies, but gravity is not at all a mystical experience. We can measure it with a gravimeter. Notice to string theorists: please come back down to Earth! It would be slightly more satisfying if string theory could predict something we don&#39;t already know about.</i><br /><br /><i>Say A Little Prayer</i><br /><br /><i>For ideas that hardly seem to have any real meaning, such as &#39;string cosmology,&#39; Wikipedia offers this: &#39;String cosmology is a relatively new field that tries to apply equations of string theory to solve the questions of early cosmology.&#39; This actually describes one of the bogus science subsidiaries of string theory. It opened after the speculation bubbles of its main enterprise of particle physics deflated.</i><br /><br /><i>I once asked one of these conference-to-conference salespeople for string theory, who had given a talk entitled &#39;String Theory and Cosmology,&#39; about predictions. The question was if we should now expect -- in the same vein as Witten&#39;s discoveries -- that string theory will soon have predicted the Big Bang. &#39;I can imagine what you want to say,&#39; he replied, &#39;but despite Smolin, I still believe in the predictive power of string theory.&#39; Yes, he believes! Sustaining one&#39;s faith is such a difficult thing today. In order to emerge strengthened from such blasphemous attacks as the books of Woit and Smolin, I recommend you cross yourself in 11 dimensions and repeat the following cleansing gospel three times.</i><br /><br /><i>21st Century Lord&#39;s Prayer</i><br /><br /><i>Our Witten who art in Princeton,</i><br /><i>published be thy name.</i><br /><i>Thy brane come,</i><br /><i>Thy theory be done,</i><br /><i>On earth as in other dimensions.</i><br /><i>Give us this day our daily idea,</i><br /><i>and forward us our progresses,</i><br /><i>as we do forsake the path of science,</i><br /><i>and lead us not to contemplation,</i><br /><i>but deliver us from observation.</i><br /><i>For thine is the string,</i><br /><i>and the power, and the irrefutability,</i><br /><i>for ever and ever</i><br /><i>Amen.</i><br /><br /><i>The Value Of Secularization</i><br /><br /><i>&#39;No more mercy with those who haven&#39;t explored but talk nevertheless.&#39;</i><br />- <i>Bertolt Brecht</i><br /><br /><i>The crudest method of escaping the principles of empirical science is, of course, to simply discard them. Since string theory doesn&#39;t provide any testable predictions, we have to get rid of the apparently old-fashioned, strict limitations of scientific methodology, right? But there are much more subtle ways to get around the rules. Instead of launching a barefaced attack on Karl Popper&#39;s idea that scientific theories must be falsifiable, some scientists are tricky enough to quote Thomas Kuhn, another philosopher of science and a critic of Popper. According to Kuhn, science undergoes paradigm shifts like the transition from the geocentric to the heliocentric world view. Here we are: </i><b><i>Strings are an epochal paradigm shift that must leave behind the testability of &#39;old physics&#39;!</i></b><i> [19] Pope Edward Witten is transformed into a Galileo-like heretic by crooks wearing police uniforms. Witten himself, in a papal manner, is &#39;reflecting on the fate of spacetime.&#39; [20] That&#39;s certainly safer than reflecting on the fate of string theory.</i><br /><br /><b><i>String theory is beautiful. Extraordinarily beautiful. At least that&#39;s what everybody says, so I won&#39;t bother you with my supposedly underdeveloped sense of aesthetics. It is, however, extraordinary for science that beauty has become a self-contained argument. String theory is just too marvelous, too beautiful, to not occur in Nature. In the eleventh century, Anselm of Canterbury had &#39;proven&#39; the existence of God in much the same way. Since God possessed all conceivable qualities of greatness, a nonexistence would be a lack of at least one of these features and result in the being not being God.</i></b><i> Anselm is regarded as the founder of scholasticism. He should actually no longer be ignored in the history of string theory.</i><br /><br /><i>Irrational Exuberance: Dying After Thousands Of Superlatives</i><br /><br /><i>It is remarkable how coordinated is the herd of string theorists who attack the same problems, led, of course, by their prophetic bellwether Witten. Roger Penrose was not the only physicist to observe that </i><b><i>&#39;wherever Witten goes, the rest will follow soon.&#39;</i></b><i> Julian Barbour remarked subtly, </i><b><i>&#39;Science has its fashions. String theorists are a bit like a pack of hounds following an extremely promising scent. But it is a particular scent. If they lose the trail, nothing will come of the great chase.&#39;</i></b><i> [21]</i><br /><br /><b><i>If string theorists do lose the trail, it will be the greatest waste of intelligence in history. There are only two possibilities. Either string theory will be the ultimate unified theory, or it will be the biggest load dropped into the garbage dump of science since Ptolemy&#39;s geocentric universe that dominated science for 1,500 years. Let&#39;s hope that it won&#39;t take that long. &#39;One day we may understand what string theory truly is,&#39; said Witten.</i></b><i> [22] Sometimes you just need to understand a little history.</i><br /><br /><i>How To Become History Without A Story</i><br /><br /><i>&#39;Thy is to maunder, since you are the bell of your actions.&#39;</i><br />- <i>Friedrich Schiller, German writer (&#39;Mary Stuart&#39;)</i><br /><br /><b><i>Historians of science have often criticized Einstein for devoting 20 years of his life to a theory (unification of gravity with electromagnetism) that is considered a dead end. He would have been better off going sailing, was a biographer&#39;s posthumous counsel. [23] What is considered bullheadedness in one person turns into something celebrated if thousands of researchers follow that bullheadedness for decades. String theorists are now becoming historians of their own failed ideas. The first &#39;I was there&#39; papers are beginning to appear. Take, for example, the former director of the Aspen Center for Physics, who bravely outed himself by publishing an article entitled &#39;Memoirs of an Early String Theorist&#39; [24] about the &#39;intellectually most satisfying years of his life.&#39; It&#39;s quite an accomplishment to stay satisfied without results.</i></b><br /><br /><i>One of the nicest circular arguments floating around is to say that string theory is &#39;the only game in town.&#39; Oh, we say sadly, there are no reasonable alternatives to string theory! Nathan Seiberg, Witten&#39;s collaborator, announced, </i><b><i>&#39;If there is something that goes beyond, we&#39;ll call it string theory.&#39;</i></b><i> This wasn&#39;t a joke! I would call it a form of pantheistic assurance, where &#39;all&#39; is comfortably and assuredly string theory. This kind of intellectual arrogance has, unfortunately, been corroding the credibility that true science relies on to survive. From a historical perspective, one has to reflect on how physics can get rid of string theory. Maybe one needs to establish a faculty for metaphorical mathematics or creative writing with mathematical methods. </i><b><i>The problem is, as recently Jim Baggott put it, that string theory is still labeled as science.</i></b><br /><br /><i>&#39;The public should be more patient about the truth&#39;.</i><br />- <i>Lisa Randall</i><br /><br /><i>Euphoria Becomes Censorship, Not An Innocent Game Any More</i><br /><br /><i>&#39;Freedom - the first-born daughter of science.&#39;</i><br />- <i>Thomas Jefferson</i><br /><br /><b><i>Paul Ginsparg, a string theorist from Cornell University, has the merit of having developed the platform ArXiv.org, which makes scientists&#39; research openly available to anyone who wants to see it. But he has unfortunately also transformed it into one of the worst monopolies of the Internet. There is no way for a serious researcher to get around ArXiv. Try not to get on their blacklists. Although it is denied officially, some researchers are blocked automatically from publishing there. Ginsparg has fought ferociously [25] with those who put forward papers that are not part of mainstream science, while a mountain of string papers was piling up on ArXiv. He appears to be a cop ticketing nose pickers while sitting on a big heap of shit. ArXiv has liberated itself from its founder, but the spirit of the moderators persists. Many of them are string theorists shifting disagreeable papers to low-rank general categories or even banning their authors.</i></b><i> Cornell is a shame for a country where the freedom of speech has a value.</i><br /><br /><b><i>Needless to say, the dominance of string theory makes it hard to get alternative ideas published in mainstream journals. But everywhere in theoretical physics you have to deal with string theorists. They are still in power, having withstood the storm brought up by Smolin and Woit by waiting it out like politicians in a scandal. Scientists have to get their proposals reviewed, and you have to be careful to formulate the theoretical aspects. Therefore, many researchers kowtow to the string merchants with clenched teeth, in order not to jeopardize their funding. Casually mentioning that some phenomenon under investigation &#39;might be one day solved by string theory&#39; has become the tip of the iceberg in a sick, dishonest game.</i></b><br /><br /><i>From The Spirits That I Called, Sir, Deliver Me!</i><br /><br /><i>&#39;Men, it has been well said, think in herds; it will be seen that they go mad in herds, while they only recover their senses slowly, and one by one.&#39;</i><br />- <i>Charles Mack ay, Scottish poet</i><br /><br /><i>There is no suggestion here that Witten or others went into string theory with any intention of causing harm to science. But after a generation of theoretical physicists employing their talents to advancing string theory, we have arrived at a point where there are 10^500 different versions of string theory, and there is no rule whatsoever that guides us toward a reasonable choice in this chaos. This arbitrary construction is called &#39;landscape,&#39; a perspective Witten found utterly discomforting, so much so that some feared his retreat from string theory altogether. Imagine the pope leaving the church! However, there is a so-called anthropic &#39;solution&#39; to the problem of so many possible string worlds. One of this nearly infinite number of universes (multiverses) must be realized -- because otherwise we humans wouldn&#39;t live here in it!</i><br /><br /><i>David Gross fights against this multiversion absurdity with Winston Churchill&#39;s words: &#39;Never, never, never give up!&#39; Gross has become aware at this point, that the theory has slid into pseudoscience, though he and his &#39;Princeton String Quartet&#39; have contributed a lot to this sad end. Just like a confused mountain guide, Gross is lost in the new &#39;landscape&#39; of prophets such as Leonard Susskind, who don&#39;t look into the gorge but continue to promise the pie in the sky. It seems as if string theory is providing a sort of negative evolution, where the most stupid ideas prevail.</i><br /><br /><i>What next? In YouTube videos [26] you can follow the birth of future string theories. Witten sketches a line on a piece of paper, connecting the five conventional string theories, and that&#39;s about it. Oh yes, a name is invented, &#39;M-theory.&#39; As Witten explains, it refers to magic, mystery, or maybe matrix. Brian Green would presumably call it the mother of all theories, </i><b><i>but there are also mockers such as Sheldon Glashow, who see the &#39;M&#39; as an upside down &#39;W&#39; for Witten. The cheeky Joao Magueijo associated &#39;M&#39; with mathematical masturbation.</i></b><i> I do not want to pursue this idea, but be careful. Theoretical physicists have become frustrated with their eternal fiancee, the standard model. String theory is the hot mistress, whose thrilling sexiness outstrips all imagination. Up to now, however, it&#39;s all in their minds.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"Exploration vs Exploitation","summary":"Optimal Learning Requires that You Sometimes Choose to Avoid the Action You Think Will be Most Rewarding / The Point is to Multiply Your Aha! Moments - the Sense of \"a Sudden Comprehension, Often Reinterpreting a Situation or Combining Known Elements Differently to Solve a Problem or Invent Something New\" / Some News Aggregators Already Use Aha! as a Metric / Social Networks are Next","image":"https://lh6.googleusercontent.com/-TGgmXqzVDT8/V9jjjWIi_VI/AAAAAAAAIss/fN-qbpL54js1HYE1wzRdWb4NHM2EFV_PACL0B/exploration-vs-exploitation-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/W6q76LQaTSA","publishDate":"2016-09-14T05:43:45.914Z","updateDate":"2017-01-03T02:01:39.976Z","text":"<b>Exploration vs Exploitation: Optimal Learning Requires that You Sometimes Choose to Avoid the Action You Think Will be Most Rewarding / The Point is to Multiply Your Aha! Moments - the Sense of &quot;a Sudden Comprehension, Often Reinterpreting a Situation or Combining Known Elements Differently to Solve a Problem or Invent Something New&quot; / Some News Aggregators Already Use Aha! as a Metric / Social Networks are Next</b><br /><br /><a href=\"http://www.oh-i-see.com/blog/2013/10/10/brain-science-behind-the-aha-moment/\" class=\"ot-anchor\">http://www.oh-i-see.com/blog/2013/10/10/brain-science-behind-the-aha-moment/</a><br /><br /><b><i>&quot;In an aha moment, a person has a sudden comprehension, often reinterpreting a situation or combining known elements differently to solve a problem or invent something new.&quot;</i></b><br /><br /><a href=\"https://www.youtube.com/watch?v=0v86Yk14rf8\" class=\"ot-anchor\">https://www.youtube.com/watch?v=0v86Yk14rf8</a><br /><br /><b>The real story behind Archimedes’ Eureka! - Armand D&#39;Angour</b><br />Transcript<br /><br /><i>&quot;When you think of Archimedes&#39; &#39;Eureka!&#39; moment, you probably think of this. [a bath]</i><br /><br /><i>As it turns out, it may have been more like this. [a ship]</i><br /><br /><i>In the third century BC, Hieron, king of the Sicilian city of Syracuse, chose Archimedes to supervise an engineering project of unprecedented scale.</i><br /><br /><i>Hieron commissioned a sailing vessel 50 times bigger than a standard ancient warship, named the Syracusia after his city.</i><br /><br /><i>Hieron wanted to construct the largest ship ever, which was destined to be given as a present for Egypt&#39;s ruler, Ptolemy.</i><br /><br /><i>But could a boat the size of a palace possibly float? In Archimedes&#39;s day, no one had attempted anything like this.</i><br /><br /><i>It was like asking, &#39;Can a mountain fly?&#39; King Hieron had a lot riding on that question.</i><br /><br /><i>Hundreds of workmen were to labor for years on constructing the Syracusia out of beams of pine and fir from Mount Etna, ropes from hemp grown in Spain, and pitch from France.</i><br /><br /><i>The top deck, on which eight watchtowers were to stand, was to be supported not by columns, but by vast wooden images of Atlas holding the world on his shoulders.</i><br /><br /><i>On the ship&#39;s bow, a massive catapult would be able to fire 180 pound stone missiles.</i><br /><br /><i>For the enjoyment of its passengers, the ship was to feature a flower-lined promenade, a sheltered swimming pool, and bathhouse with heated water, a library filled with books and statues, a temple to the goddess Aphrodite, and a gymnasium.</i><br /><br /><i>And just to make things more difficult for Archimedes, Hieron intended to pack the vessel full of cargo: 400 tons of grain, 10,000 jars of pickled fish, 74 tons of drinking water, and 600 tons of wool.</i><br /><br /><i>It would have carried well over a thousand people on board, including 600 soldiers.</i><br /><br /><i>And it housed 20 horses in separate stalls.</i><br /><br /><i>To build something of this scale, only for that to sink on its maiden voyage? Well, let&#39;s just say that failure wouldn&#39;t have been a pleasant option for Archimedes.</i><br /><br /><i>So he took on the problem: will it sink? Perhaps he was sitting in the bathhouse one day, wondering how a heavy bathtub can float, when inspiration came to him.</i><br /><br /><i>An object partially immersed in a fluid is buoyed up by a force equal to the weight of the fluid displaced by the object.</i><br /><br /><i>In other words, if a 2,000 ton Syracusia displaced exactly 2,000 tons of water, it would just barely float.</i><br /><br /><i>If it displaced 4,000 tons of water, it would float with no problem.</i><br /><br /><i>Of course, if it only displaced 1,000 tons of water, well, Hieron wouldn&#39;t be too happy.</i><br /><br /><i>This is the law of buoyancy, and engineers still call it Archimedes&#39; Principle.</i><br /><br /><i>It explains why a steel supertanker can float as easily as a wooden rowboat or a bathtub.</i><br /><br /><i>If the weight of water displaced by the vessel below the keel is equivalent to the vessel&#39;s weight, whatever is above the keel will remain afloat above the waterline.</i><br /><br /><i>This sounds a lot like another story involving Archimedes and a bathtub, and it&#39;s possible that&#39;s because they&#39;re actually the same story, twisted by the vagaries of history.</i><br /><br /><i>The classical story of Archimedes&#39; Eureka! and subsequent streak through the streets centers around a crown, or corona in Latin.</i><br /><br /><i>At the core of the Syracusia story is a keel, or korone in Greek.</i><br /><br /><i>Could one have been mixed up for the other? We may never know.</i><br /><br /><i>On the day the Syracusia arrived in Egypt on its first and only voyage, we can only imagine how residents of Alexandria thronged the harbor to marvel at the arrival of this majestic, floating castle.</i><br /><br /><i>This extraordinary vessel was the Titanic of the ancient world, except without the sinking, thanks to our pal, Archimedes.&quot;</i><br /><br /><a href=\"http://journals.plos.org/plosbiology/article/asset?id=10.1371/journal.pbio.0020097.PDF\" class=\"ot-anchor\">http://journals.plos.org/plosbiology/article/asset?id=10.1371/journal.pbio.0020097.PDF</a><br /><br /><b><i>Neural Activity When People Solve Verbal Problems with Insight</i></b><br /><br /><i>&quot;... According to legend, Archimedes shouted &#39;Eureka!&#39; (&#39;I have found it!&#39;) when he suddenly discovered that water displacement could be used to calculate density. Since then, &#39;Eureka!,&#39; or &#39;Aha!,&#39; has often been used to express the feeling one gets when solving a problem with insight. Insight is pervasive in human (and possibly animal [Epstein et al. 1984]) cognition, occurring in perception, memory retrieval, language comprehension, problem solving, and various forms of practical, artistic, and scientific creativity (Sternberg and Davidson 1995). </i><b><i>The Archimedes legend has persisted over two millennia in part because it illustrates some of the key ways in which insight solutions differ from solutions achieved through more straightforward problem solving.</i></b><i> We examine the neural bases of these different problem-solving methods.</i><br /><br /><i>Although many processes are shared by most types of problem solving, insight solutions appear to differ from noninsight solutions in several important ways. The clearest defining characteristic of insight problem solving is the subjective &#39;Aha!&#39; or &#39;Eureka!&#39; experience that follows insight solutions (Schooler et al. 1993). </i><b><i>This subjective experience can lead to a strong emotional response -- according to legend, Archimedes ran home from the baths shouting &#39;Eureka!&#39; without donning his clothes first. In addition, problem solving with insight is characterized by the following features. (1) Solvers first come to an impasse, no longer progressing toward a solution (Duncker 1945). Archimedes, for example, was stymied by King Hiero’s challenge to determine whether his new crown was pure gold without damaging the crown. (2) Solvers usually cannot report the processing that enables them to reinterpret the problem and overcome the impasse (Maier 1931). Insight often occurs when people are not even aware they are thinking of the problem, as reportedly happened to Archimedes while in the baths. (3) Solvers experience their solutions as arising suddenly (Metcalfe and Wiebe 1987; Smith and Kounios 1996) and immediately recognize the correctness of the solution (or solution path). (4) Performance on insight problems is associated with creative thinking and other cognitive abilities different from those associated with performance on noninsight problems (Schooler and Melcher 1997).</i></b><i> Some researchers have argued that all these characteristics of insight solutions are essentially epiphenomenal, that insight and noninsight solutions vary only in emotional intensity, and that they are attained with precisely the same cognitive (hence neural) mechanisms (Weisberg and Alba 1981; Weisberg 1986; Perkins 2000).</i><br /><br /><i>Persistent questions about insight concern whether unconscious processing precedes reinterpretation and solution, whether distinct cognitive and neural mechanisms beyond a common problem-solving network are involved in insight, and whether the apparent suddenness of insight solutions reflects truly sudden changes in cognitive processing and neural activity.</i><br /><br /><b><i>Recent work suggests that people are thinking -- at an unconscious level -- about the solution prior to solving problems with insight.</i></b><i> Specifically, while working on a verbal problem they have yet to solve, people presented with a potential solution word read the actual solution word faster than they read an unrelated word (Bowden and Beeman 1998). This &#39;solution priming&#39; effect is greater -- and in fact people make solution decisions about presented words more quickly -- when words are presented to the left visual hemifield, which projects directly to the right hemisphere (RH), than when words are presented to the right visual hemifield, which projects to the left hemisphere (LH). This suggests that RH semantic processing is more likely than LH semantic processing to produce lexical or semantic information that leads to the solution. These RH advantages occur only when solvers experience insight—the &#39;Aha!&#39; or &#39;Eureka!&#39; feeling that comes with insight solutions (Bowden and Jung-Beeman 2003a). Moreover, when subjects try to solve classic insight problems, they benefit more from hints presented to the left visual field (i.e., the RH) than from hints presented to the right visual field (i.e., the LH) (Fiore and Schooler 1998).</i><br /><br /><i>Problem solving is a complex behavior that requires a network of cortical areas for all types of solving strategies and solutions, so solving problems with and without insight likely invokes many shared cognitive processes and neural mechanisms. </i><b><i>One critical cognitive process distinguishing insight solutions from noninsight solutions is that solving with insight requires solvers to recognize distant or novel semantic (or associative) relations;</i></b><i> hence, insight-specific neural activity should reflect that process. The most likely area to contribute to this component of insight problem solving is the anterior superior temporal gyrus (aSTG) of the RH. Language comprehension studies demonstrate that the RH is particularly important for recognizing distant semantic relations (Chiarello et al. 1990; Beeman 1998), and bilateral aSTG is involved in semantic integration. For example, sentences and complex discourse increase neural activity in aSTG bilaterally (Mazoyer et al. 1993; Stowe et al. 1999), and discourse that places particular demands on recognizing or computing distant semantic relations specifically increases neural activity in RH temporal areas (St. George et al. 1999; Mason and Just 2004), especially aSTG (Meyer et al. 2000; Kircher et al. 2001). If this prediction of RH aSTG involvement is confirmed, it will help constrain neurocognitive theories of insight. Other cortical areas, such as prefrontal cortex and the anterior cingulate (AC) may also be differentially involved in producing insight and noninsight solutions ...&quot;</i><br /><br /><a href=\"http://cacm.acm.org/magazines/2015/9/191174-new-news-aggregator-apps/abstract\" class=\"ot-anchor\">http://cacm.acm.org/magazines/2015/9/191174-new-news-aggregator-apps/abstract</a><br /><br /><b><i>New News Aggregator Apps: How apps like Inkl and SmartNews are overcoming the challenges of aggregation to win over content publishers and users alike.</i></b><br /><br /><i>&quot;... SmartNews (</i><i><a href=\"http://www.smartnews.com\" class=\"ot-anchor\">www.smartnews.com</a></i><i>) describes itself as an &#39;addictively simple news app.&#39; This free download is ranked No. 1 in both Google Play and Apple’s App Store. Downloads have exceeded 10 million, with more than 1 million active users in the U.S. alone, and it now evaluates more than 10 million articles every day. SmartNews has won numerous &#39;App of The Year&#39; and &#39;Best News App&#39; awards. According to CNET, &#39;SmartNews is an excellent free alternative to other popular news apps.&#39;</i><br /><br /><i>Co-founder and co-CEO Ken Suzuki developed SmartNews to try to entice Tokyo’s millions of subway commuters away from playing mindless games on their smartphones into reading news stories. Suzuki believes SmartNews’ rivals are not &#39;other news apps,&#39; but &#39;mobile games, social media, and the top apps people use on their smartphones every day. We are competing for the overall attention of the user.&#39; The beta international edition (ultimately covering more than 150 countries and a growing number of local cities) was launched earlier this year.</i><br /><br /><i>Despite the decline of newspapers in recent years, Pew Research Center data (</i><i><a href=\"http://pewrsr.ch/1OrUDlA\" class=\"ot-anchor\">http://pewrsr.ch/1OrUDlA</a></i><i>) shows consumers continue to have a large appetite for news, though increasingly in digital form. Indeed, digital media has recently overtaken traditional news sources in terms of trust, according to a global survey by Edelman (</i><i><a href=\"http://bit.ly/1I6recK\" class=\"ot-anchor\">http://bit.ly/1I6recK</a></i><i>). The Pew research showed news is &#39;part of the explosion of social media and mobile devices, and in a way that could offer opportunity to reach more people with news than ever before.”</i><br /><br /><i>According to the International News Media Association, &#39;A quarter of the top 20 apps in the news category ... are aggregator apps. More and more, publishers are understanding the need to have a presence wherever their readers or potential readers are consuming content.&#39;</i> (<a href=\"http://bit.ly/1I6rUij\" class=\"ot-anchor\">http://bit.ly/1I6rUij</a>)<br /><br /><b><i>Traditional news aggregators always introduce a dilemma to publishers: if users read an article in the app, they will not go to the publisher’s site and the publisher will not make money as a result; yet, this procedure is necessary to reach new audiences.</i></b><i> This is an issue highlighted by a report in The New York Times on the substantial decrease of traffic to publishers’ home pages and the subsequent loss of engagement.</i> (<a href=\"http://bit.ly/1CB04qM\" class=\"ot-anchor\">http://bit.ly/1CB04qM</a>)<br /><br /><i>Inkl has an unusual approach to this challenge, as users pay for the articles they read, supporting publishers and creating sustainable news. The payback is an ad-free format, which is its main selling point.</i><br /><br /><i>SmartNews has its own approach: to get the best publisher content, it created a uniquely publisher-friendly model, ensuring the first click always goes to the publisher’s mobile site, so it is able to keep traffic and related revenue. If the user is offline in the readability mode (SmartView), publishers can place an ad at the bottom of that view and maintain the revenue stream.</i><br /><br /><i>These approaches appear to be working, as SmartNews already has over 150 publisher channels (see the full list at </i><i><a href=\"http://bit.ly/1aJxEUa/\" class=\"ot-anchor\">http://bit.ly/1aJxEUa/</a></i><i>) and Inkl’s global publisher list is also growing.</i><br /><br /><i>SmartNews senior communications manager Vincent Chang says, </i><b><i>&#39;Our machine learning algorithm evaluates 10 million articles and signals a day to uncover the top 20 most interesting and important articles in each category -- that matter right now. Our algorithm is unique in that it is optimized for discovery and diversity, instead of targeted personalization. This prevents the &#39;filter bubble&#39; problem (the same topics become boring over time), ensuring you can always find something fresh and interesting each day.&#39;</i></b><br /><br /><b><i>The SmartNews algorithm looks at more than just social signals and what is trending; it also evaluates many other user behaviors, including whether people finish reading an article, how they share it, and which topics are in the article. The algorithm then uses natural language processing to classify each article into one of the appropriate topic channels (Lifestyle, Business, Sports, Tech, World, U.S., and so on), topics that people are actually reading (not just clicking) and that matter most to them. The algorithm also takes into account what is culturally relevant for each country, as what is trending in one country often differs from what is trending in the next.</i></b><br /><br /><i>SmartNews is designed like a newspaper, a design proven for over a century, with skimmable headlines, minimalist design, and high-quality news content, plus an offline reading mode (SmartView) for times when Internet connectivity is unreliable. The design lets the user see many stories at a glance, and easily find one to read. Looking at the app’s reviews, users appear to be impressed with its interface, usability, intuitiveness, and customizability, and to like the efficient browsing and the smooth swiping between tabs.</i><br /><br /><b><i>Unlike most news aggregators that focus on personalization, SmartNews focuses on discovery. The ethos of the company is about helping users expand their interests and encounter new and fascinating things every day, while connecting them to what others are reading to encourage discussion.</i></b><br /><br /><b><i>The discovery of news is analogous to variety-seeking buying behavior in marketing; reading articles outside your existing preferences is like switching brands. Variety-seeking is exploration for users, although the risk is that they will experience low satisfaction by trying novel products.</i></b><i> SmartNews is designed to introduce users to stories they did not know they wanted to read, rewarding them by widening their horizons without too much additional time cost.</i><br /><br /><i>As SmartNews uses state-of-the-art machine learning algorithms instead of human editors to determine which news to capture for its users, the app can evaluate a huge volume of culturally relevant news articles in real time. Chang says, &#39;In the past, this would require building many newsrooms of editors in each country. It allows us to deliver the news much faster and impartially, which is especially important for political news.”</i><br /><br /><b><i>Recommending news through content discovery is a decision-making problem involving uncertainty. In probability theory, this is referred to as the multiarmed bandit problem: a trade-off between the exploration of new possibilities for unknown payoff and the exploitation of known possibilities for an expected payoff. In the context of news aggregator apps, this theory relates to the trade-off between the time invested in reading or searching, and the payoff with quality, relevant, and useful information received.</i></b><br /><br /><b><i>There are ways to quantify the relationship between exploration/exploitation and risk-taking/avoiding for humans.</i></b><i> While at IBM, Ricky Takahashi, one of SmartNews’s data science and machine learning engineers, researched the prediction of consumer choice under uncertainty.</i><br /><br /><b><i>Most news apps use algorithms optimized for personalization -- giving us more and more of what we want, with nothing to challenge our beliefs, enlarge our perception, or give us new ideas -- which leads to a circular narrowing of knowledge, or a &#39;filter bubble,&#39; a phrase coined by Eli Pariser of Upworthy. In a review of Pariser’s book The Filter Bubble (</i></b><b><i><a href=\"http://www.thefilterbubble.com\" class=\"ot-anchor\">http://www.thefilterbubble.com</a></i></b><b><i>), American writer, consultant, and teacher Clay Shirky says, &#39;Internet firms increasingly show us less of the wide world, locating us in the neighborhood of the familiar. The risk, as Eli Pariser shows, is that each of us may unwittingly come to inhabit a ghetto of one.&#39; Indeed, Pew data questions whether the self-selective process, combined with algorithmic feeds, is narrowing the kinds of information to which Americans are exposed.</i></b><br /><br /><i>SmartNews is unique in that to avoid the trap of personalization and the filter bubble, </i><b><i>it optimizes its algorithms for serendipity -- those magical &#39;aha&#39; moments when you find something new and amazing</i></b><i>, not just things you already follow and know. As Jaroslovsky explains, </i><b><i>&#39;what people think they are interested in ahead of time and what they are actually interested in can be completely different.&#39;</i></b><br /><br /><i>In addition to the advanced algorithm, the app uses natural language processing to gain greater understanding of articles by processing the nuance in words. The app lets a user tap into the most interesting and important stories immediately. SmartNews co-founder/co-CEO Kaisei Hamamoto says, &#39;Our machine learning algorithm gets smarter every day, learning from millions of users what stories really matter to humanity—not just what your friends are reading.”</i><br /><br /><i>By helping mobile audiences &#39;discover&#39; new articles, SmartNews helps publishers discover new audiences that may not have otherwise found them ...&quot;</i><br /><br /><a href=\"http://www.tomstafford.staff.shef.ac.uk/?p=48\" class=\"ot-anchor\">http://www.tomstafford.staff.shef.ac.uk/?p=48</a><br /><br /><b><i>Fundamentals of Learning: The Exploration-Exploitation Trade-Off</i></b><br /><br /><i>&quot;The exploration-exploitation trade-off is a fundamental dilemma whenever you learn about the world by trying things out. </i><b><i>The dilemma is between choosing what you know and getting something close to what you expect (‘exploitation’) and choosing something you aren’t sure about and possibly learning more (‘exploration’).</i></b><i> For example, suppose you are in a restaurant and you look at the menu:</i><br /><br /><i>Fish and Chips</i><br /><i>Chole Poori</i><br /><i>Paneer Uttappam</i><br /><i>Khara Dosa</i><br /><br /><i>Assuming for the sake of example that you’re not very good with Sri Lankan food, you’ve now got a choice. You can ‘exploit’ -- go with the fish and chips, which will probably be alright -- or you can ‘explore’ -- try something you haven’t had before and see what you get. Obviously which you decide to do will depend on many things: how hungry you are, how good the restaurant reviews are, how adventurous you are, how often you reckon you’ll be coming back ... etc. </i><b><i>What’s important is that the study of the best way to make these kinds of choices -- called reinforcement learning -- has shown that optimal learning requires that you to sometimes make some bad choices. This means that sometimes you have to choose to avoid the action you think will be most rewarding, and take an action which you think will be less rewarding. The rationale is that these ‘sub-optimal’ actions are necessary for your long term benefit -- you need to go off track sometimes to learn more about the environment. The exploration-exploitation dilemma is really a trade-off: enjoy more now vs learn more now and enjoy later. You can’t avoid it, all you can do is position yourself somewhere along the spectrum.</i></b><br /><br /><i>Because the trade-off is fundamental we would expect to be able to see it in all learning domains, not just restaurant food choices.&quot;</i><br /><br /><a href=\"http://iosband.github.io/2015/07/05/ian-launches-blog.html\" class=\"ot-anchor\">http://iosband.github.io/2015/07/05/ian-launches-blog.html</a><br /><br /><b><i>Exploration vs Exploitation</i></b><br /><br /><i>&quot;One of the fundamental problems for decision making under uncertainty is the so-called exploration vs exploitation problem. This describes </i><b><i>one of the main dilemmas that comes when you are forced to make decisions under uncertainty. You can either try to take the best action you can given what you know (exploitation), or you can try to do something you don’t know a lot about in the hope that you’ll increase your understanding (exploration). Traditional statistical analyis is great for telling you how to make sense of a dataset, which can give you an estimate of how best to exploit. However, if you’re not careful, you could end up doing really badly in the long run if you don’t make an effort to explore.</i></b><br /><br /><i>To make this more concrete I will use a very simple example. Imagine you’re a doctor, a data-driven doctor of course, and your goal is to save as many cancer patients as possible. For simplicity sake we’ll start with the setting where all cancer is the same, all people are the same and there are only two drugs available. We will also assume that once someone is diagnosed they will certainly die that same day without any drugs, if you give them drug 1 they will be cured with probability p1 and if you use drug 2 they will be cured with probability p2. What should you do?</i><br /><br /><i>Of course, if you know the success rates of both drugs p1, p2 then the solution is trivial, pick the better drug and use that one. However, in the real world you never know the exact probabilities ... so what can you do? Well usually people gather lots of experimental data to estimate the probabilities p̂1, p̂2 and then choose the the one with the higher estimate, all good? Not really ... here are some problems:</i><br /><br /><b><i>1. Your estimates might be wrong. We could have got unlucky and measured drug 1 was better than drug 2 in the trial. If we never try drug 2 again we will never learn and for the rest of time people will die needlessly.</i></b><br /><br /><b><i>2. What about all the people we killed gathering the experimental data? Even if we made the right conclusion in the end, if lots of people had to die to test out another suboptimal drug this seems like a bad outcome.</i></b><br /><br /><b><i>3. What do we do when new drugs come on the market?</i></b><br /><br /><b><i>You can see that even in this incredibly simple example, the traditional A/B testing don’t really give satisfactory answers. This simple problem is known in the academic literature as a two-armed bandit</i></b><i> and no it’s not a very descriptive model for how an actual doctor works.&quot;</i><br /><br /><b>Infering Aha! Moments</b><br /><br />The sense of aha! is a fleeting sense that we&#39;ve learned something important.  It&#39;s a special, oftentimes uncommon category of learning that can leave us with a belief that we are on the right track and that the world makes more sense than it did before.<br /><br />The sense of aha! can be an intense motivator for further learning and effort.  In the presence of skill, it can lead to radical innovations.  It can even act as a catalyst for learning the skills required to create innovations.<br /><br />Content which inspires an aha! moment is not just an article that you made it to the end of.  It&#39;s more than that.<br /><br />It&#39;s also more than just something you&#39;ve shared with somebody else.  You may share many things for other reasons.<br /><br />The aha! moment is something that you <b>feel</b>.  You may say something which indicates to others that you&#39;ve experienced it, but trying to discern it from strictly user behavior will likely leave the observer with a noisy signal.<br /><br />It is probably necessary that a person tell a system that they&#39;ve experienced it.  Why not?  It&#39;s probably one of the most important signals that a site dedicated to learning can possibly measure.<br /><br />And why stop there?  Just as (we are told) Archimedes wanted to shout about his experience, it&#39;s probably the case that people <b>want</b> to provide more details about this special experience.  Social networks which don&#39;t record it are throwing away this crucial signal when they should be trying to asking their users to tell them all about it.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>   ﻿","category":"thinking"},{"name":"The Dolomite Problem","summary":"Dolomite is a Common Mineral and Rock with Mysterious Origins Despite an Intense 200-Year Scientific Effort / Geologists Have Tried Every Tool Available to them to Discover How it Forms / Interest Stems from the Fact that 80% of the Oil and Gas Reserves from Carbonate Reservoirs in North America are in Dolomite / Large Masses of Limestone Appear to Have Transformed into Dolomite, But Attempts to Synthesize Dolomite Under Lab Conditions Appear to Only Work at High Temperatures and Pressures","image":"https://lh3.googleusercontent.com/-1qzBBu4Jwuk/V9eYXxOojwI/AAAAAAAAIr4/rRXVkxaG8coUl9H8pHIK-M38jALwiAF2ACJoC/w7142-h9999/the-dolomite-problem-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/csQfo8g3QH8","publishDate":"2016-09-13T06:20:44.551Z","updateDate":"2017-01-03T02:02:00.305Z","text":"<b>The Dolomite Problem: Dolomite is a Common Mineral and Rock with Mysterious Origins Despite an Intense 200-Year Scientific Effort / Geologists Have Tried Every Tool Available to them to Discover How it Forms / Interest Stems from the Fact that 80% of the Oil and Gas Reserves from Carbonate Reservoirs in North America are in Dolomite / Large Masses of Limestone Appear to Have Transformed into Dolomite, But Attempts to Synthesize Dolomite Under Lab Conditions Appear to Only Work at High Temperatures and Pressures</b><br /><br />GRAPHIC REFERENCES:<br /><br />[1] <a href=\"https://www.reddit.com/r/geology/comments/1cy677/can_you_guys_explain_the_dolomite_problem_to_me/\" class=\"ot-anchor\">https://www.reddit.com/r/geology/comments/1cy677/can_you_guys_explain_the_dolomite_problem_to_me/</a><br /><br />[2] <a href=\"https://web.archive.org/web/20100328220045/http://www.scitopics.com/Sedimentary_dolomite_a_reality_check_on_the_Dolomite_Problem.html\" class=\"ot-anchor\">https://web.archive.org/web/20100328220045/http://www.scitopics.com/Sedimentary_dolomite_a_reality_check_on_the_Dolomite_Problem.html</a><br /><br />[3] <i>New Scientist</i>, October 26, 1991.  Paywalled at <a href=\"https://www.newscientist.com/article/mg13217925-000/\" class=\"ot-anchor\">https://www.newscientist.com/article/mg13217925-000/</a>.<br /><br />[4] <b>The Sudden Appearance of Earth&#39;s Sand</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/a2b4eyyame3\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/a2b4eyyame3</a><br /><br />[5] <b>The Questionable Origins of Earth&#39;s Sand</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/86HBUghNYYk\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/86HBUghNYYk</a><br /><br />[6] <b>The Trouble with Sand&#39;s Color</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Lt6SjW4NUXB\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Lt6SjW4NUXB</a><br /><br />[7] <b>Uniformitarianism vs Catastrophism</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/S9iiBv1SucM\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/S9iiBv1SucM</a><br /><br />[8] <a href=\"https://www.amazon.com/Plasma-Chemistry-Alexander-Fridman/dp/1107684935/ref=sr_1_1?ie=UTF8&amp;qid=1473747378&amp;sr=8-1&amp;keywords=plasma+chemistry\" class=\"ot-anchor\">https://www.amazon.com/Plasma-Chemistry-Alexander-Fridman/dp/1107684935/ref=sr_1_1?ie=UTF8&amp;qid=1473747378&amp;sr=8-1&amp;keywords=plasma+chemistry</a><br /><br />[9] <a href=\"http://www.sciencedirect.com/science/article/pii/0167577X84901538\" class=\"ot-anchor\">http://www.sciencedirect.com/science/article/pii/0167577X84901538</a><br /><br />[10] <b>Science - It&#39;s Not Just a Process of Elimination</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/MEBFnq984RD\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/MEBFnq984RD</a><br /><br />[11] <a href=\"https://www.researchgate.net/figure/263027253_fig12_Fig-13-Characteristics-of-the-coarse-crystalline-dolomite-facies-of-the-Hasheer-Member\" class=\"ot-anchor\">https://www.researchgate.net/figure/263027253_fig12_Fig-13-Characteristics-of-the-coarse-crystalline-dolomite-facies-of-the-Hasheer-Member</a><br /><br />[12] <a href=\"https://scihub22266oqcxt.onion.link/http://jsedres.geoscienceworld.org/content/37/2/695.full.pdf+html\" class=\"ot-anchor\">https://scihub22266oqcxt.onion.link/http://jsedres.geoscienceworld.org/content/37/2/695.full.pdf+html</a><br /><br />[13] <a href=\"http://saltworkconsultants.businesscatalyst.com/assets/dolomite_warren_2000.pdf\" class=\"ot-anchor\">http://saltworkconsultants.businesscatalyst.com/assets/dolomite_warren_2000.pdf</a><br /><br />[14] <a href=\"http://www.eu-geology.com/?page_id=107\" class=\"ot-anchor\">http://www.eu-geology.com/?page_id=107</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Science Troll","summary":"Trolling Began as the Art of Saying Something Wrong, But in Such a Way that Everybody Except the Target Could Tell You Were Being Deliberately Obtuse / Once the Internet Went Mainstream, this Original Definition Dramatically Expanded to Provocative Online Speech / Today, it is Also Applied to Critics of Science / And Critical, Thought-Provoking Claims are at Risk of Being Caught in the Dragnet to Eliminate the Vaguely-Defined Threat","image":"https://lh3.googleusercontent.com/-uYCmIFzEVSo/V85gji0qldI/AAAAAAAAIqM/6vQ23zKVWpgFxFwb1Kysvx9Dy5saCtMnQCJoC/w1800-h2520/the-science-troll-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/TNz14ELFkRp","publishDate":"2016-09-06T06:23:01.463Z","updateDate":"2017-01-03T02:02:23.100Z","text":"<b>The Science Troll: Trolling Began as the Art of Saying Something Wrong, But in Such a Way that Everybody Except the Target Could Tell You Were Being Deliberately Obtuse / Once the Internet Went Mainstream, this Original Definition Dramatically Expanded to Provocative Online Speech / Today, it is Also Applied to Critics of Science / And Critical, Thought-Provoking Claims are at Risk of Being Caught in the Dragnet to Eliminate the Vaguely-Defined Threat</b><br /><br /><b>The History of the Internet Troll</b><br /><br /><a href=\"https://encyclopediadramatica.se/Troll\" class=\"ot-anchor\">https://encyclopediadramatica.se/Troll</a><br /><br /><i>&quot;Trolling is a method, a style of rhetoric and action. It can be used for constructive or destructive purposes. Some of the greatest champions of Western philosophy have implemented the troll method.&quot;</i> — Weev<br /><br /><a href=\"http://fishbowl.pastiche.org/2012/09/27/dont_feed_the_trolls_part_one/\" class=\"ot-anchor\">http://fishbowl.pastiche.org/2012/09/27/dont_feed_the_trolls_part_one/</a><br /><br /><i>&quot;The Fishbowl</i><br /><i>tail -f /dev/mind &gt; blog</i><br /><br /><i>Don’t Feed the Trolls</i><br /><br /><i>September 27, 2012 3:43 PM</i><br /><br /><i>Troll: v.i. to fish by trailing a lure or baited hook from a moving boat. - The Merrriam-Webster Dictionary</i><br /><br /><i>Around twenty years ago when I first set foot on Usenet, trolling had a much gentler meaning than it does today. </i><b><i>Trolling was the art of saying something wrong, but in such a way that everybody except the target of your trolling could tell you were being deliberately obtuse.</i></b><br /><br /><i>Trolls ranged from throwaway jokes like the deliberately typo-ridden spelling correction, to elaborate long-term performance art; for example the jokers who completely derailed the Star Trek newsgroups by dragging half the readers into a choreographed argument about whether sound (and, when that got too boring, light) could travel in a vacuum.</i><br /><br /><i>On one hand this kind of trolling was elitist and exclusionary, often a way for forum regulars to one-up newbies who didn&#39;t know the pecking order. </i><b><i>On the other hand it served to discourage the very common nerd trait of wanting to one-up the world by leaping in to correct the most trivial of errors, a defence against the kind of knee-jerk pedantry that can clog otherwise interesting discussion.</i></b><br /><br /><b><i>&#39;Don’t feed the trolls&#39; was a warning as much as anything else. Don’t jump into a newsgroup discussion before you’ve read enough to know who is who; don’t make it your job to correct every trivial, irrelevant [mistake]. Learn the ropes first, and you might just avoid being the butt of everyone’s in-joke.</i></b><br /><br /><i>&#39;Some people claim that the troll (sense 1) is properly a narrower category than flame bait, that </i><b><i>a troll is categorized by containing some assertion that is wrong but not overtly controversial.&#39;</i></b><i> — The Jargon File</i><br /><br /><b><i>As the early 90s drifted on, the definition of trolling broadened to encompass anyone who acts like an asshole on the Internet just to get attention. By the end of the decade, few people even remembered the original definition.</i></b><br /><br /><b><i>The reason for the sudden shift? The rise of the consumer Internet and with it, easy anonymity.</i></b><br /><br /><b><i>Anonymity on the old-school Internet of shell-accounts granted by universities or employers was a rare currency, mostly limited to &#39;anonymous remailers&#39; like </i></b><b><i><a href=\"http://anon.penet.fi\" class=\"ot-anchor\">anon.penet.fi</a></i></b><b><i>, addresses that made it obvious that the author was masking their identity. In the 90s, with the rise of dial-up Internet and subscriber online services, throw-away anonymity became the norm rather than the exception. And with anonymity came the ability for anyone to be an asshole without fear of repercussions.</i></b><br /><br /><i>While anonymity was the catalyst for the ‘rise of the trolls’ in the 90s, the behaviour of these trolls was quickly normalised, and now you see the same toxic garbage signed with real names in Facebook threads that you find next to eggs on Twitter.</i><br /><br /><i>The accepted wisdom was that the best way to react to the influx of assholes was &#39;don’t feed the trolls&#39;. Starve them of attention and they would get bored and go away.</i><br /><br /><i>Looking back from 2012, I can’t see any evidence of that tactic having worked. Ever. What happened was the opposite. By normalising the idea that the only way to deal with assholes is to pretend they aren’t there, we made the Internet a safe space for sociopaths.</i><br /><br /><b><i>&#39;[Trolls’] own online activity tends to be dull and disruptive, but they think they&#39;re entitled to the kind of large audience for their behaviour you can only get by being interesting.</i></b><i> This is why they don’t actually want free speech. All that would give them is the freedom to call the shots on their own websites. </i><b><i>What they really want is someone else’s audience.&#39;</i></b><i> — Teresa Nielsen Hayden</i><br /><br /><i>Trolls get more than enough validation from each other, or from knowing they have successfully hijacked somebody’s audience, that any reaction from their target is secondary. And yet there is always some well-meaning sea-lion ready to blame the victim, and to reinforce the trolls’ own excuses for their behaviour.</i><br /><br /><i>We need to do something about assholes on the Internet, and &#39;Don’t feed the trolls&#39; is not only the wrong thing, it’s not a thing at all.</i><br /><br /><i>This post was originally meant to be a two-parter, but soon after I published it, all the things I wanted to say in the second part had been covered by commentators with far more experience than I. As a follow-up, I would recommend Sarah Jeong&#39;s Internet of Garbage.</i><br /><br /><i>The Fishbowl is the Weblog of Charles Miller. (Archives)</i><br /><i>cmiller@pastiche.org&quot;</i><br /><br /><b>The Shifting Meaning of the Troll Term</b><br /><br /><a href=\"https://encyclopediadramatica.se/Troll\" class=\"ot-anchor\">https://encyclopediadramatica.se/Troll</a><br /><br /><i>&quot;Over the years, cases of amateur failtrolls and users on the losing side of an argument have created a misconception about what trolling really is. Some interwebs users think that now yelling TROLO and UMAD will make them a successful troll. Many 13 year old boys have found the internet an excellent place to get on their soapbox and spout unsolicited opinions, which is the major cause of arguing on the internet. Trolling both sides, though entertaining, is a very easy form of trolling and thus is often used by newer, inexperienced trolls. As it&#39;s become commonplace, it&#39;s also become a common excuse for a user who loses an internet argument. To say that they never intended to win the argument the entire time is a justification for their inadequate argument skills.</i><br /><br /><b><i>Since then, the definition of trolling (through eliciting an argument) has devolved into anyone trying to elicit an argument altogether. Though this technically fits the definition of the troll, the intention behind arguing is what makes the difference. Remember: to be considered a troll, the user must have had the intention of trolling from the beginning. If there isn&#39;t some subtle hint of the intention to troll during the argument, the user is either a failtroll or a fail arguer.&quot;</i></b><br /><br /><b>Trolling as Cause for Censoring or Eliminating Two-Way Online Dialogue</b><br /><br /><a href=\"http://www.nytimes.com/2014/08/15/technology/web-trolls-winning-as-incivility-increases.html?_r=0\" class=\"ot-anchor\">http://www.nytimes.com/2014/08/15/technology/web-trolls-winning-as-incivility-increases.html?_r=0</a><br /><br /><i>&quot;Web Trolls Winning as Incivility Increases</i><br /><i>Farhad Manjoo</i><br /><br /><i>The New York Times</i><br /><i>AUG. 14, 2014</i><br /><br /><i>[...]</i><br /><br /><i>&#39;Troll&#39; is the fuzzy term for agitators who pop up, often anonymously, sometimes in mobs, in comment threads and on social networks like Facebook and Twitter, apparently intent on wreaking havoc. </i><b><i>The term is vague precisely because trolls lurk in darkness; their aims are unclear, their intentions unknown, their affiliations mysterious.</i></b><br /><br /><b><i>In recent years the term has become a catchall label for routine, graphically provocative online speech</i></b><i> ...</i><br /><br /><i>[...]</i><br /><br /><b><i>Gawker Media said it was setting up a system in which only approved commenters will be allowed to post to the main comment section. Comments from the unapproved will be relegated to a &#39;pending&#39; section that readers will be advised to avoid.</i></b><br /><br /><b><i>Others have called for more far-reaching efforts, including reducing the possibility of posting anonymously on many online forums, or of posting at all. Responding to the Jezebel situation this week, Nicholas Jackson, the digital director of Pacific Standard, and Margaret Eby, a writer at Brooklyn Magazine, argued that new sites should eliminate comments.</i></b><br /><br /><b><i>But Dr. Phillips, of Humboldt State, pointed out that many efforts to curb trolling ran into a larger problem: &#39;To what extent do you want to make it harder for people to express themselves on the Internet?&#39; she asked.</i></b><br /><br /><b><i>&#39;This is not the good-faith exchange of ideas,&#39; she said. &#39;It’s just people being nasty</i></b><i>, and if anything, it might encourage marginalized groups to not speak up.&#39; She added, </i><b><i>&#39;On the other hand, by silencing that valve, there’s a lot of other stuff that is important culturally that might also be minimized.&#39;</i></b><br /><br /><i>If there’s one thing the history of the Internet has taught us, it’s that trolls will be difficult to contain because they really reflect base human society in all its ugliness. Trolls find a way.</i><br /><br /><i>[...]</i><br /><br /><b>Are People Who Critique Science Trolls?</b><br /><br />Now that the term has lost its original meaning, it&#39;s increasingly directed towards even legitimate critique.  With such a malleable definition, justification for the censorship of disruptive online ideas is just a short step away.<br /><br /><a href=\"http://phys.org/news/2014-08-debating-science-withtrolls.html\" class=\"ot-anchor\">http://phys.org/news/2014-08-debating-science-withtrolls.html</a><br /><br /><b><i>&quot;What I learned from debating science with trolls</i></b><br /><i>August 20, 2014 by Michael J. I. Brown, The Conversation</i><br /><br /><i>I often like to discuss science online and I&#39;m also rather partial to topics that promote lively discussion, such as climate change, crime statistics and (perhaps surprisingly) the big bang. </i><b><i>This inevitably brings out the trolls.</i></b><br /><br /><i>&#39;Don&#39;t feed the trolls&#39; is sound advice, but I&#39;ve ignored it on occasion – including on The Conversation and Twitter – and I&#39;ve been rewarded. Not that I&#39;ve changed the minds of any trolls, nor have I expected to.</i><br /><br /><i>But I have received an education in the tactics many trolls use. </i><b><i>These tactics are common not just to trolls but to bloggers, journalists and politicians who attack science, from climate to cancer research.</i></b><br /><br /><i>Some techniques are comically simple. Emotionally charged, yet evidence-free, accusations of scams, fraud and cover-ups are common. While they mostly lack credibility, such accusations may be effective at polarising debate and reducing understanding.&quot;</i><br /><br />(Includes criticism of scientism in an article about trolling ...)<br /><br /><b><i>&quot;And I wish I had a dollar each time a scientifically incompetent ideologue claimed science is a religion.</i></b><i> The chairman of the Prime Minister&#39;s Business Advisory Council, Maurice Newman, trotted out that old chestnut in The Australian last week. Australia&#39;s Chief Scientist, Ian Chubb, was less than impressed by Newman&#39;s use of that tactic.</i><br /><br /><i>Unfortunately there are too many tactics to discuss in just one article (sorry Gish Gallop and Strawman), so I will focus on just a few that I&#39;ve encountered online and in the media recently.</i><br /><br /><i>&#39;Experts&#39;</i><br /><br /><b><i>Internet trolls know who their experts are. There are thousands of professors scattered across academia, so it isn&#39;t surprising that a few contrarians can be found. In online discussions I&#39;ve been told of the contrarian views of &#39;respected&#39; professors from Harvard, MIT and Princeton.</i></b><br /><br /><i>Back in The Conversation&#39;s early days I even copped abuse for not being at Princeton by someone who was clearly unfamiliar with both science and my employment history. It was a useful lesson that vitriol is often disconnected from knowledge and expertise.</i><br /><br /><i>At times expert opinion is totally misrepresented, often with remarkable confidence.</i><br /><br /><i>Responding to one of my Conversation articles, the Australian Financial Review&#39;s Mark Lawson distorted the findings of CSIRO&#39;s John Church on sea levels.</i><br /><br /><i>Even after I confirmed with Church that Lawson had the science wrong, Lawson wouldn&#39;t back down.</i><br /><br /><i>Such distortions aren&#39;t limited to online debates. In The Australian, Maurice Newman warned about imminent global cooling and cited Professor Mike Lockwood&#39;s research as evidence.</i><br /><br /><i>But Lockwood himself stated last year that solar variability this century may reduce warming by:</i><br /><br /><i>&#39;between 0.06 and 0.1 degrees Celsius, a very small fraction of the warming we&#39;re due to experience as a result of human activity.&#39;</i><br /><br /><i>Newman&#39;s claims were debunked, by his expert, before he even wrote his article.&quot;</i><br /><br />(Trolls as advocates for minority views in science ...)<br /><br /><b><i>Sometimes experts are quoted correctly, but they happen to disagree with the vast majority of their equally qualified (or more qualified) colleagues.</i></b><i> How do the scientifically illiterate select this minority of experts?</i><br /><br /><i>I&#39;ve asked trolls this question a few times and, funnily enough, they cannot provide good answers. To be blunt, </i><b><i>they are choosing experts based on agreeable conclusions rather than scientific rigour</i></b><i>, and this problem extends well beyond online debates.&quot;</i><br /><br /><i>[...]</i><br /><br />Some select conversations from the article&#39;s comments:<br /><br />The following comment seems to make an assumption that the article&#39;s author is actually aware of <i>troll</i>&#39;s original meaning, but his conclusion may apply regardless ...<br /><br />@Eikka: <i>&quot;There&#39;s an important distiction to be made between trolling.</i><br /><br /><i>Trolls are intentionally wrong. They&#39;re not trying to sell you anything, change your opinions or change the world; </i><b><i>they&#39;re in it to make you angry and then laugh at you. They know they&#39;re misrepresenting the issue at hand and use various rhetorical tactics towards that end to proverbially pull your nose and kick you on the shin.</i></b><br /><br /><b><i>When you&#39;re doing it for any other end, it&#39;s not trolling.</i></b><i> The person may be a crank, a fool, a paid shill, ignorant, stupid, anything - but not a troll.</i><br /><br /><i>So the big flaw in categorically labeling anyone who persistently disagrees and argues with you trolls is that you&#39;re implying they know they&#39;re wrong and you are right. </i><b><i>That is actually a fallacy known as begging the question</i></b><b> [see definition below]</b><b><i>, because by &#39;arguing against trolls&#39; you are in fact defining yourself and your own point of view the correct one by default.&quot;</i></b><br /><br />(Note: <i>&quot;Begging the question is a fallacy in which the premises include the claim that the conclusion is true or (directly or indirectly) assume that the conclusion is true.&quot;</i>, definition from <a href=\"http://www.nizkor.org/features/fallacies/begging-the-question.html\" class=\"ot-anchor\">http://www.nizkor.org/features/fallacies/begging-the-question.html</a>)<br /><br />@Modernmystic: <i>&quot;Actually scientists have said that it would be impossible for rockets to operate in space, impossible to break the sound barrier, impossible for heavier than air machines to fly, people whose &#39;views ultimately DID NOT triumph despite the fact that they were supported by observation....&#39;</i><br /><br /><i>A little history and perspective along with your egocentric lambasting of other people might ACTUALLY teach you something....&quot;</i><br /><br />In order to preserve the point that critics of mainstream science are trolls, the article&#39;s author incorrectly suggests that resistance to space-based travel was not widespread ...<br /><br />@Michael Brown: <i>&quot;Of course some of the tactics used by trolls that I described in the article are appearing in the comments.</i><br /><br /><i>An interesting variation of the selective use of experts is to take quotes of a few famous scientists from the past who were wrong, and then assume their attitudes applied to all scientists of the time.</i><br /><br /><b><i>The claim that scientists believed rockets couldn&#39;t operate in a vacuum would be an example of this. Perhaps some scientists believed this</i></b><i> (references please), but the most famous promotion of this idea was a New York Times editorial from 13 January 1920 which was finally corrected on 17 July 1969. One can compare the style of the 1920 editorial with some of the editorials attacking climate science in 2014.&quot;</i><br /><br />Michael Brown has seemingly failed to research the issue.  A comprehensive review of these failures is available at ...<br /><br /><b>What We Failed to Learn from the Moonshot</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/14GWK5FqhAy\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/14GWK5FqhAy</a><br /><br />The following comment was rated by 28 people for a score of 4.3 of 5:<br /><br />@TheGhostofOtto1923: <i>&quot;Trolling is easy and didn&#39;t start on the internet. It is human nature to use deception in order to further ones position, and also to resist owning up to it when discovered.</i><br /><br /><i>The internet has perhaps made trolling easier but it also offers an eventual solution. </i><b><i>The instant access to facts will make it possible for automation to instantly flag bullshit and expose trolls, or to simply preempt them.</i></b><br /><br /><b><i>This is inevitable because trolls have made it necessary. And AI moderation will signal the ultimate triumph of science over human nature.&quot;</i></b><br /><br /><b>The Thoughtlessness of Eliminating Comments</b><br /><br />Trolling is not some devastating problem which demands that we must somehow reduce the number of people talking to one another.  Disruptive behavior&#39;s visibility is a symptom of the failure of our social networking sites to service the needs of their niche -- generally through a failure to offer systems for rating &amp; visualization which emphasizes the values of a particular community.<br /><br />It&#39;s extraordinary that in this modern world of global, nearly instantaneous communications, we continue to look to the thumbs-up / thumbs down rating system as somehow appropriate for dealing with complex discussions.  This technology is around 2,000 years old.<br /><br />What exactly does a thumbs down mean when the conversation pertains to a clash of worldviews?<br /><br />Does it mean that the person disagrees with the technical point being made?<br /><br />Does it mean that there is a mistake in the logic?<br /><br />Does it mean that the reader simply disagrees with the speaker&#39;s worldview?<br /><br />Who knows?  But, I am told by fans of Slashdot that it is cutting edge.<br /><br />Why do we pretend that this is better than mob mentality?  Why do we think that this is sufficient to talk about complex subjects -- like science &amp; technology?<br /><br />Is the goal to create a homogenous user base?<br /><br />Perhaps.<br /><br />But, what happens to the quality of discourse when you cut off its long tail?<br /><br />The problem is not the existence of people who disagree with you; when we seek to surround ourselves with agreeable people, we risk insulating ourselves from worthwhile feedback.  That is not an environment from which you can effectively learn or create.<br /><br />The idea that we must eliminate comments in order to defeat trolls ignores the significant ambiguity that exists in our social networking ratings systems.  We should be experimenting with new systems for rating and visualizing comments, and then testing those systems in real-world conditions.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The Collapse of the Roman Civilization as a Lesson for the Scientific Community","summary":"Roman Culture Thought of Science as a Slave to Technology / This Set them Up to Grow Beyond their Means / The Romans thought they Knew it All Already / That Arrogance Killed an Openness to Intellectual Discovery, which We Today Acknowledge as a Necessary Condition to Support a Growing Population","image":"https://lh3.googleusercontent.com/-PbBXUBm6bCA/V7XRBBWkhZI/AAAAAAAAImI/B209EoWAGKAnEXlXqNQv3t3J3NlCta4TACJoC/w3600-h5040/the-collapse-of-roman-civilization-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/KYeTKvKY9PV","publishDate":"2016-08-18T15:15:52.159Z","updateDate":"2017-01-03T02:02:40.954Z","text":"<b>The Collapse of the Roman Civilization as a Lesson for the Scientific Community: Roman Culture Thought of Science as a Slave to Technology / This Set them Up to Grow Beyond their Means / The Romans thought they Knew it All Already / That Arrogance Killed an Openness to Intellectual Discovery, which We Today Acknowledge as a Necessary Condition to Support a Growing Population</b><br /><br /><a href=\"http://www.rome.info/history/empire/fall/\" class=\"ot-anchor\">http://www.rome.info/history/empire/fall/</a><br /><br /><i>&quot;Another factor that had contributed to decline and fall of the Roman empire was that during the last 400 years of the empire, </i><b><i>the scientific achievements of the Romans were limited almost entirely to engineering and the organization of public services.</i></b><i> They built marvelous roads, bridges, and aqueducts. They established the first system of medicine for the benefit of the poor. But since the Romans relied so much on human and animal labor, </i><b><i>they failed to invent many new machines or find new technology to produce goods more efficiently. They could not provide enough goods for their growing population.</i></b><i> They were no longer conquering other civilizations and adapting their technology, they were actually losing territory they could [no] longer maintain with their legions.&quot;</i><br /><br /><i>The Death of $cience: A Companion Study to Martin Lopez Corredoira&#39;s The Twilight of the Scientific Age</i><br />by Andrew Holster<br />p.79-80<br /><br /><i>&quot;Science as technology is of course the stereotypical image in our culture.  Think of all those TV ads for toothpaste, cars, tyres, breakfast cereals, painkillers ... with people in white laboratory coats representing Scientific Authority. Science is often proclaimed as the modern cornucopia, giving us all sorts of consumer goodies, medical technologies, weapon technologies, etc. That is the function that makes sense of science to bureaucrats and executives and marketers. After all, if science doesn&#39;t give us new technology to increase production, new power to dominate nature, new ways to increase consumption, then how it going to make money or enhance power? And if it doesn&#39;t make money or power, what value could it possibly have? &#39;What is it going to do for me?&#39; is what people in our modern ego-centered culture instinctively ask. Few people are really interested in knowledge for its own sake: most are satisfied they already understand the nature of the world perfectly well without reference to science. For most, it is not science but social ideologies that define their meaning and values and importance. The rich and powerful also resent the potential of science to question their beliefs, which are already fixed to their satisfaction, and see it primarily as a technology factory, enhancing their material wealth or power. But of course that is a purely exploitative materialistic view of science. It ignores the real transformational power of knowledge.</i><br /><br /><i>This modern corruption of </i><b><i>science to a technology slave</i></b><i> already sounds alarm-bells. Roman civilization gives us a powerful image of the dead-end of technology when it is isolated from &#39;pure science&#39;. The Romans inherited science and natural philosophy from the Greeks (and other cultures). They established a technological civilization of great power from this, with an Empire based on bureaucracy, law and order, material comfort and military security. </i><b><i>But they failed to develop pure science -- or natural philosophy -- any further, and that fundamentally limited their technology. They were on the cusp of developing modern science, with its advanced technology of complex machinery, steam power, metallurgy, electricity, crop science, medicines, etc., but they failed to take the opportunity. Their materialistic, hedonistic, militaristic, technocratic culture killed openness to intellectual discovery, as exemplified by the Classical Greeks. The Romans thought they knew it all already. They came to exemplify an extreme and fatal arrogance. Our own materialistic culture, the global syndrome of the modern West, resembles the Roman arrogance.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"Why Outsider Mavericks Matter in Science","summary":"Professional Opposition to Outsiders Crossing Boundaries of Specializations is one of the Defining Features of Modern Science, and One of the Most Powerful Forces Against Heterodox Thinkers / But Combining Insights from Multiple Fields is Often the Essential Ingredient for Making Progress / This Entrenched Opposition to Interdisciplinary Synthesis Ironically Ensures that the Next Big Revolution Will Come from an Outsider","image":"https://lh3.googleusercontent.com/-urSbvsGoY70/V7U4fUvDvmI/AAAAAAAAIko/X6Sxf2x-7sI8HK2h91ieaM_i_pyPFnhcwCJoC/w1800-h2520/why-outsider-mavericks-matter-in-science-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/UoR73bKjt4F","publishDate":"2016-08-18T04:25:55.412Z","updateDate":"2017-01-03T02:03:06.051Z","text":"<b>Why Outsider Mavericks Matter in Science: Professional Opposition to Outsiders Crossing Boundaries of Specializations is one of the Defining Features of Modern Science, and One of the Most Powerful Forces Against Heterodox Thinkers / But Combining Insights from Multiple Fields is Often the Essential Ingredient for Making Progress / This Entrenched Opposition to Interdisciplinary Synthesis Ironically Ensures that the Next Big Revolution Will Come from an Outsider</b><br /><br /><i>The Death of $cience: A Companion Study to Martin Lopez Corredoira&#39;s The Twilight of the Scientific Age</i><br />by Andrew Holster<br /><br />p.73:<br /><br /><b><i>&quot;Professional opposition to outsiders crossing boundaries of specialisations is one of the defining features of modern science, and one of the most powerful forces against heterodox thinkers.  But combining insights from multiple fields is often the essential ingredient for making progress.&quot;</i></b><br /><br />p.77:<br /><br /><b><i>&quot;Because of the number of severe and unexplained anomalies in modern physics and cosmology, foundational physics is open to revolutionary change.  Indeed, I believe it would have gone through such a revolution over the last few decades, were it not for the severe repression of ideas in modern physics.  And any such revolution will deeply affect other natural sciences, including theories of mind. Yet while I believe a revolution is immanent, I also think it is unlikely to be made through our present scientific institutions.  It will be made by outsiders.  The new approach required to advance physics will be attacked from within conventional physics, because it has to revolutionize certain foundational concepts of quantum theory and relativity theory, but this goes profoundly against the interests of professional physicists.</i></b><br /><br /><b><i>Having painted themselves into a corner, prominent scientists cannot emotionally afford to allow new ideas they have ridiculed to be recognised.  They have too much to lose personally.</i></b><i> [54]  And this brings us to the second crux of the problem.  Scientists typically complain of the bureacratisation of their subject, and of attacks on science by forces of &#39;pseudo-science&#39; and anti-scientific culture from outside.  </i><b><i>But the greatest enemy of science is from within: the bigotry and mediocrity in the professional culture of science.</i></b><br /><br /><i>[54] Put yourself into the position of an established string theorist, who has spent all his professional life in this specialty, and still has a decade or two or three left before retirement.  Any new foundational theory will immediately wipe string theory off the map.  What future career prospects will string theorists have?  Their research careers will be over.  The same goes for all the other big theoretical research projects.  Whether they admit it or not, for practically all theoretical physicists over about 30, by which time they have committed themselves to some program, the prospect of a successful new foundational theory represents a personal disaster.  What do you think it is like for a middle-aging academic with only one specialist talent to be suddenly cast on the scrap-heap?  </i><b><i>This is really why scientists fear the suggestion of heterodox theories so much.  Since the scientific institutions are so powerful, and have such huge self-interest at stake, they simply cannot let this happen.&quot;</i></b><br /><br /><i>Against the Tide: A Critical Review by Scientists of How Physics and Astronomy Get Done</i><br />Martín López Corredoira &amp; Carlos Castro Perelman (Eds.)<br /><br />p.13-14: &quot;Challenging Dominant Physics Paradigms&quot;<br />by J. M. Campanario and B. Martin<br /><br /><i>&quot;The usual image of the scientist is of a cool, calm, detached, objective observer, but the reality is quite different (Mahoney 1976; Mitroff 1974), as anyone who knows scientists is aware. The classic study of the psychology of scientists is Ian Mitroff’s book The Subjective Side of Science, in which he revealed that Apollo moon scientists were strikingly committed to their ideas, so much so that contrary evidence seemed to have little influence on their views. As well, scientists express strong views, often quite derogatory, about other scientists. To expect every scientist to react coolly and objectively to a competitor’s idea is wishful thinking, though there are some scientists who approach the ideal. Intriguingly, </i><b><i>Mitroff found that it was often the top scientists who were the most strongly committed to their ideas.</i></b><br /><br /><i>Tom Van Flandern commented to us:</i><br /><br /><b><i>&#39;I have taken aside several colleagues whose pet theories are now mainstream doctrine, and asked quizzically what it would mean to them personally if an alternative idea ultimately prevailed. To my initial shock (I was naïve enough that I did not see this coming), to a person, the individuals I asked said they would leave the field and do something else for a living.</i></b><i> Their egos, the adulation they enjoy, and the satisfaction that they were doing something important with their lives, would be threatened by such a development. As I pondered this, it struck me that their vested interests ran even deeper than if they just had a financial stake in the outcome (which, of course, they do because of grants and promotions). So a challenger with a replacement idea would be naïve to see the process as anything less than threatening the careers of some now-very-important people, who cannot be expected to welcome that development regardless of its merit.&#39; (1 August 2002)</i><br /><br /><i>[...]</i><br /><br /><i>What happens when some challengers, who have spent years or decades developing their ideas, show up and ask a busy career scientist for an assessment? Even for an open-minded or sympathetic scientist, it is a real sacrifice to spend days or even just hours examining alternative ideas, since that means correspondingly less time available for their own pressing work. </i><b><i>The more eminent scientists serve as editors and referees for prestigious journals where they typically are focused on rejecting work that fails to meet the standards of orthodox science, making it even more difficult for them to accept work that challenges those standards.</i></b><br /><br /><i>Most challengers believe their ideas have value, otherwise they would not bother with them. </i><b><i>What they desire from mainstream scientists is not acceptance (though that would be nice!) but a fair-minded examination of their ideas. There is a certain irony here: challengers confront academic power and what some of them see as corruption, but what they really desire is the attention of mainstream scientists. The practical problem facing challengers is a scarcity of attention: there are not enough scientists who have both the time and inclination to scrutinize their unorthodox ideas.</i></b><br /><br /><b><i>The way that science is organized exacerbates the problem of shortage of attention for paradigm challengers. Most scientists work as part of a small network, local and/or international, members of which address the same topic, share common interests and goals, exchange information and reprints, and attend the same conferences (Crane 1972). Scientists are more likely to devote attention to work by others in their network than they are to the work of outsiders. Dissidents who go to the roots of a paradigm do not specialize sufficiently to be part of such a network: they are outsiders in the field in the sense that they do not focus on a small portion of a paradigm. As a result, few scientists will be willing to give them any attention.</i></b><br /><br /><i>[...]</i><br /><br /><i>Choosing research problems can be likened to an investment process (Bourdieu 1975, 1988). Scientists have available a certain amount of &#39;capital&#39; — knowledge, experience, time and effort — that they can invest in different ways. A conservative investment strategy is to pursue small, incremental innovations, with a high likelihood of success and a modest return on investment, following Peter Medawar’s dictum that science is &#39;the art of the soluble&#39; (Medawar 1967). A risky strategy is to pursue a speculative idea: the likelihood of success may be low but the returns, if the idea pans out, can be huge.</i><br /><br /><i>[...]</i><br /><br /><i>In a sense, paradigm challengers are ambitious investors, in that they commonly criticize entire theories, such as relativity and quantum theory, rather than just a part of such theories. They seek to change theories at the level of university textbooks.</i><br /><br /><b><i>A different investment calculation comes into play, though, when it comes to someone else’s ideas. To examine or even promote someone else’s challenge to orthodoxy requires significant time and energy, yet the major returns go to the other person, if they are recognized as the innovator.</i></b><i> If the idea is a promising one, the temptation is to grab credit, for example by domesticating the radical idea and publishing in orthodox journals. It is no surprise that many innovators are afraid of having their ideas stolen.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The College Experience","summary":"College Leaders have Long Excused Decades of Relentlessly Rising Prices, Exploding Student-Loan Debt, and Alarmingly High Dropout Rates with the Assumption that Students are Learning / A Recent Study Challenges that Notion / The Data Instead Suggests that a Full 45% of College Experiences are Fundamentally Social Experiences / If the Researchers are Right, the Faculty Must Share Some Responsibility","image":"https://lh3.googleusercontent.com/-8n_Z6iuQpX0/V7PJbCFzS6I/AAAAAAAAIj8/RY-D9Sad8McAZqCN-90RPKnliBL7EX5nQCJoC/w1800-h2520/the-college-experience-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/LHiQnz7caYV","publishDate":"2016-08-17T02:18:55.126Z","updateDate":"2017-01-03T02:03:23.247Z","text":"<b>The College Experience: College Leaders have Long Excused Decades of Relentlessly Rising Prices, Exploding Student-Loan Debt, and Alarmingly High Dropout Rates with the Assumption that Students are Learning / A Recent Study Challenges that Notion / The Data Instead Suggests that a Full 45% of College Experiences are Fundamentally Social Experiences / If the Researchers are Right, the Faculty Must Share Some Responsibility</b><br /><br /><i>Academically Adrift: Limited Learning on College Campuses</i><br />by Richard Arum and Josipa Roksa<br />(first 5 pages of book describing the study)<br /><br /><b><i>&quot;College Cultures and Student Learning</i></b><br /><br /><b><i>&#39;Colleges and universities, for all the benefits they bring, accomplish far less for their students than they should,&#39; the former president of Harvard University, Derek Bok, recently lamented.</i></b><i> Many students graduate college today, according to Bok, &#39;without being able to write well enough to satisfy their employers … reason clearly or perform competently in analyzing complex, nontechnical problems.&#39; While concern over undergraduate learning in this country has longstanding roots, in recent years increased attention has been focused on this issue not only by former Ivy League presidents, but also by policy makers, practitioners, and the public. Stakeholders in the higher education system have increasingly come to raise questions about the state of collegiate learning for a diverse set of reasons. Legislators -- and privately, middle-class parents as well -- increasingly have expressed worry over the value and returns to their investments in higher education. Business leaders have begun to ask whether graduates have acquired the necessary skills to ensure economic competitiveness. And increasingly, </i><b><i>educators within the system itself have begun to raise their voices questioning whether organizational changes to colleges and universities in recent decades have undermined the core educational functions of these institutions.</i></b><br /><br /><i>These diverse concerns about the state of undergraduate education have served to draw attention to measuring whether students are actually developing the capacity for critical thinking and complex reasoning at college. In a rapidly changing economy and society, there is widespread agreement that these individual capacities are the foundation for effective democratic citizenship and economic productivity. </i><b><i>&#39;With all the controversy over the college curriculum,&#39; Derek Bok has commented, &#39;it is impressive to find faculty members agreeing almost unanimously that teaching students to think critically is the principal aim of undergraduate education.&#39; Institutional mission statements also echo this widespread commitment to developing students’ critical thinking. They typically include a pledge, for example, that schools will work to challenge students to &#39;think critically and intuitively,&#39; and to ensure that graduates will become adept at &#39;critical, analytical, and logical thinking.&#39;</i></b><i> These mission statements align with the idea that educational institutions serve to enhance students’ human capital-knowledge, skills, and capacities that will be rewarded in the labor market. Economists Claudia Goldin and Lawrence Katz, for example, have recently argued that increased investment in U.S. higher education attainment is required for both economic growth and reduced economic inequality. </i><b><i>Goldin and Katz’s recommendations rest on the assumption that increased college graduation rates will likely have such desirable economic outcomes</i></b><i> because the labor market values &#39;the highly analytical individual who can think abstractly.&#39; But </i><b><i>what if increased educational attainment is not equivalent to enhanced individual capacity for critical thinking and complex reasoning?</i></b><br /><br /><i>While there has been a dearth of systematic longitudinal research on the topic, there are ample reasons to worry about the state of undergraduate learning in higher education. Policy makers and practitioners have increasingly become apprehensive about undergraduate education as there is growing evidence that individual and institutional interests and incentives are not closely aligned with a focus on undergraduate academic learning per se. While as social scientists we want to avoid the pitfalls of either propagating historically inaccurate sentimental accounts of a romantic collegiate past followed by a tragic &#39;fall from grace&#39; or, alternatively, scapegoating students, faculty, and colleges for the current state of affairs, it is imperative to provide a brief description of the historical, social, and institutional context in which the phenomenon under investigation manifests itself to illuminate its multifaceted dimensions.</i><br /><br /><b><i>Higher Education Context: Continuity and Change</i></b><br /><br /><i>Historians have noted that from the inception of U.S. colleges, many students often embraced a collegiate culture that had little to do with academic learning. While some students who used colleges to prepare for the ministry &#39;avoided the hedonism and violence of their rowdy classmates&#39; and focused on academic pursuits rather than extracurricular activities, the majority of students chose another path. For many students in past decades, college was a time when one &#39;forged a peer consciousness sharply at odds with that of the faculty and of serious students.&#39; Undergraduates as a whole historically embraced a college life -- complete with fraternities, clubs, and social activities -- that was produced, shaped, and defined by a peer culture oriented to nonacademic endeavors.</i><br /><br /><i>Sociologists have long cautioned about the detrimental effects of peer cultures on an individual’s commitment to academic pursuits in general and student learning in particular. Many students come to college not only poorly prepared by prior schooling for highly demanding academic tasks that ideally lie in front of them, but -- more troubling still -- they enter college with attitudes, norms, values, and behaviors that are often at odds with academic commitment. In recent cohorts of students, Barbara Schneider and David Stevenson have described the prevalence of &#39;drifting dreamers&#39; with &#39;high ambitions, but no clear life plans for reaching them.&#39; These students &#39;have limited knowledge about their chosen occupations, about educational requirements, or about future demand for these occupations.&#39; They enter college, we believe, largely academically adrift.</i><br /><br /><i>While prior historical scholarship reminds us that U.S. undergraduates have long been devoted to pursuing social interests at college, </i><b><i>there is emerging empirical evidence that suggests that college students’ academic effort has dramatically declined in recent decades.</i></b><i> Labor economists Philip Babcock and Mindy Marks, for example, have recently conducted critically important empirical work that meticulously examines data from twelve individual-level surveys of student time use from the 1920s to today. They have found that </i><b><i>full-time college students through the early 1960s spent roughly forty hours per week on academic pursuits (i.e., combined studying and class time); at which point a steady decline ensued throughout the following decades. Today, full-time college students on average report spending only twenty-seven hours per week on academic activities -- that is, less time than a typical high school student spends at school. Average time studying fell from twenty-five hours per week in 1961 to twenty hours per week in 1981 and thirteen hours per week in 2003. The trends are even more pronounced when Babcock and Marks identify the percentage of students who report studying more than twenty hours per week: in 1961, 67 percent of full-time college students reported this level of effort; by 1981, the percentage had dropped to 44 percent; today, only one in five full-time college students report devoting more than twenty hours per week on studying.</i></b><i> Babcock and Marks carefully explored the extent to which changes in student effort simply reflect the fact that different types of individuals currently attend college and course taking patterns have changed.</i><br /><br /><i>They found that such compositional explanations were inadequate: </i><b><i>&#39;Study time fell for students from all demographic subgroups</i></b><i>, within race, gender, ability and family background, overall and within major, for students who worked in college and for those who did not, and at four-year colleges of every type, size, degree structure and level of selectivity.&#39; </i><b><i>Students’ lack of academic focus at today’s colleges, however, has had little impact on their grade point averages and often only relatively modest effects on their progress towards degree completion as they have developed and acquired &#39;the art of college management,&#39; in which success is achieved primarily not through hard work but through &#39;controlling college by shaping schedules, taming professors and limiting workload.&#39;</i></b><i> Biostatistician Valen Johnson has taken advantage of unique data from Duke University on student course evaluations, grades, and enrollment decisions to demonstrate that </i><b><i>students &#39;preferentially enroll in classes (and subject areas) with instructors who grade leniently.&#39;</i></b><i> For example, an undergraduate in Mary Grigsby’s recent study of collegiate culture at a Midwestern public university commented:</i><br /><br /><b><i>&#39;I hate classes with a lot of reading that is tested on. Any class where a teacher is just gonna give us notes and a worksheet or something like that is better. Something that I can study and just learn from in five [minutes] I’ll usually do pretty good in. Whereas, if I’m expected to read, you know, a hundred-and-fifty-page book and then write a three-page essay on it, you know, on a test let’s say, I’ll probably do worse on that test because I probably wouldn’t have read the book. Maybe ask the kids, what’s in this book? And I can draw my own conclusions, but I rarely actually do reading assignments or stuff like that, which is a mistake I’m sure, but it saves me a lot of time.&#39;</i></b><br /><br /><i>Grigsby’s student not only saved a great deal of time with his approach to classes -- hours that could be reapportioned to leisure pursuits -- but also was able to do well by conventional standards of his grade point average and progress towards degree. The student observed: </i><b><i>&#39;You know I can get out of here with a 3.5 but it doesn’t really matter if I don’t remember anything … . It’s one thing to get the grade in a class and it’s another to actually take something from it, you know.&#39;</i></b><br /><br /><b><i>Students’ ability to navigate academic course requirements with such modest levels of individual investment and cognitive effort points to a second set of social actors responsible for growing concern over undergraduate learning on today’s campuses: the college professoriate. If one is to cast aspersions on student cultures that exist on college campuses today, one would do well to focus equal attention on the faculty cultures and orientations that have flourished in U.S. higher education. Learning at college, after all, is an activity that ideally emerges from an interaction between faculty and students. &#39;What students and teachers mean by ‘taking’ and ‘teaching’ courses is determined not by subject or levels alone, but also by the intentions of the participants,&#39;</i></b><i> Arthur Powell and his colleagues observed two decades ago about U.S. high schools. In these settings, formal and informal &#39;treaties&#39; often emerged: where teaching was &#39;perceived as an art of capturing audiences and entertaining them,&#39; </i><b><i>and teachers and students &#39;arrange deals or treaties that promote mutual goals or that keep the peace.&#39;</i></b><i> Higher education researcher George Kuh has extended this insight to colleges and universities, arguing that a &#39;disengagement compact&#39; has been struck on many contemporary campuses between faculty and students.</i><br /><br /><i>This compact is described by Kuh as</i><br /><br /><b><i>&#39;I’ll leave you alone if you leave me alone.&#39; That is, I won’t make you work too hard (read a lot, write a lot) so that I won’t have to grade as many papers or explain why you are not performing well. The existence of this bargain is suggested by the fact that at a relatively low level of effort, many students get decent grades -- B’s and sometimes better. There seems to be a breakdown of shared responsibility for learning -- on the part of faculty members who allow students to get by with far less than maximum effort, and on the part of students who are not taking full advantage of the resources institutions provide.</i></b><br /><br /><b><i>If students are able to receive high marks and make steady progress towards their college degrees with such limited academic effort, must not faculty bare some responsibility for the low standards that exist in these settings?&quot;</i></b><br /><br /><i>[...]</i><br /><br /><a href=\"https://www.insidehighered.com/news/2013/05/20/studies-challenge-findings-academically-adrift\" class=\"ot-anchor\">https://www.insidehighered.com/news/2013/05/20/studies-challenge-findings-academically-adrift</a><br /><br /><b><i>&quot;Less Academically Adrift?</i></b><br /><br /><i>Challenging findings of landmark 2011 study, new data suggest that college students make significantly bigger gains in critical thinking. But differences in methodology may contribute to the differing conclusions.</i><br /><br /><i>May 20, 2013</i><br /><i>by Doug Lederman</i><br /><br /><b><i>It&#39;s hard to think of a study in the last decade that has had a bigger impact on public discourse about higher education and the internal workings of colleges and universities alike than has Academically Adrift.</i></b><br /><br /><b><i>The 2011 book, among the most extensive analyses of the extent and quality of college-level learning in many years, found that many students showed no meaningful gains on key measures of learning during their college years.</i></b><i> The findings fed a burgeoning critique of higher education -- adding doubts about student learning to existing qualms about rising costs and degree completion -- and led many campuses to take a closer look at their students&#39; performance and at the rigor and depth of their curriculums.</i><br /><br /><i>Two recent reports by a prominent researcher purport to challenge Academically Adrift&#39;s underlying conclusions about students&#39; critical thinking gains in college, and especially the extent to which others have seized on those findings to suggest that too little learning takes place in college. The studies by the Council for Aid to Education show that students taking the Collegiate Learning Assessment made an average gain of 0.73 of a standard deviation in their critical thinking scores, significantly more than that found by the authors of Academically Adrift.</i><br /><br /><i>&#39;The notion that college doesn&#39;t matter is inaccurate,&#39; Roger Benjamin, president of the Council for Aid to Education, said in an interview. (The council produces the CLA.) In the paper and a recent presentation of its data, Benjamin said that CAE&#39;s findings contrast with Academically Adrift&#39;s, though in the interview he sought to play down the extent to which his findings undermine those of the book.</i><br /><br /><i>Not much at all, one of the authors of Academically Adrift said in an e-mail message upon reviewing the CAE studies. Richard Arum, professor of sociology and education at New York University, noted methodological differences in how the two sets of data were drawn -- </i><b><i>most significantly that the CAE study does not follow the same group of students over time</i></b><i> -- and questioned the figures from Academically Adrift that Benjamin used to draw his conclusions.</i><br /><br /><i>He also took issue with the suggestion that his study, with the University of Virginia&#39;s Josipa Roksa, questioned the contribution that college makes to student learning (&#39;Neither Roksa nor I have made such a claim,&#39; he wrote) -- although certainly some of the book&#39;s champions have done so.</i><br /><br /><i>First of Its Kind</i><br /><br /><i>The 2011 publication of Academically Adrift was noteworthy for several reasons. First, it was among the first to make use of results from the CLA or another of a relatively new set of assessment tools </i><b><i>that specifically seek to measure the &#39;added value&#39; that colleges impart to their students&#39; learning, by allowing for the comparison of the performance of students over time.</i></b><br /><br /><i>Second, it was injected into the public discourse at a time of intensifying political and public criticism of higher education on a range of fronts, especially tied to rising tuition and student debt levels and apparent declining international performance of the American college system in the postsecondary attainment of 25- to 34-year-olds.</i><br /><br /><b><i>Making use of transcript data from 25 selective colleges that had given the same cohorts of students the Collegiate Learning Assessment at various points over the course of their college careers, the study found that significant minorities of students &#39;did not demonstrate any significant improvement in learning&#39; over two or four years of college, and that students on average showed only very limited gains in learning.</i></b><br /><br /><i>The findings punched some data-driven holes in what historically has been the seemingly unassailable strength of American higher education -- the quality and rigor of the learning required by professors and gained by students.</i><br /><br /><b><i>&#39;How much are students actually learning in contemporary higher education? The answer for many undergraduates, we have concluded, is not much,&#39; Arum and Roksa wrote.</i></b><br /><br /><i>The study came under some criticism, mostly for depending so heavily on the Collegiate Learning Assessment as evidence of whether students have learned. But the findings by the well-regarded researchers prompted significant soul searching on the part of many college administrators and professors, providing evidence to confirm their worst fears. And as pointed as the authors&#39; assertions were, they emboldened much more sweeping condemnations by those whose philosophical world views (namely, that higher education is broken) were reinforced by actual data.</i><br /><br /><b><i>&#39;Academically Adrift ... documents the ugly underbelly of American higher education -- a culture that is anti-intellectual and that often produces students who have neither the skills or knowledge they will need to succeed after graduation,&#39; Anne Neal, president of the American Council of Trustees and Alumni, wrote in early 2011.</i></b><br /><br /><b><i>&#39;College leaders have long excused decades of relentlessly rising prices, exploding student-loan debt, and alarmingly high dropout rates with the assumption that students are learning,&#39; Kevin Carey, who is now director of the education policy program at the New America Foundation, wrote in 2012. &#39;The prices are reasonable and the loans repayable, they say, because of the skills and knowledge that students acquire in exchange. And while dropouts are regrettable, we are told, that&#39;s an unavoidable -- nay, admirable -- consequence of maintaining high academic standards.</i></b><br /><br /><b><i>&#39;Academically Adrift exposed the bankruptcy of those assertions.&#39;</i></b><br /><br /><i>It was in response to commentaries like those -- in many ways more than to the more measured findings of Academically Adrift itself -- that CAE aimed its new reports, Benjamin said.</i><br /><br /><i>The council&#39;s study examines a much broader group of students, drawn from more than 1,300 colleges and universities that had used the CLA to test the critical thinking skills of groups of freshmen and seniors between 2005 and 2012. While there were significant differences among institutions, the &#39;more robust&#39; analysis found average gains of 0.73 of a standard deviation over several test administrations, with colleges showing &#39;similar levels of growth regardless of sector (public v. private), institution size, Carnegie Classification, selectivity, or minority-serving status,&#39; the authors write.</i><br /><br /><i>&#39;This stands in contrast to the findings of Academically Adrift,&#39; the authors add. &#39;They suggest that there is little growth in critical thinking as measured by CLA. </i><b><i>They report an effect size of 0.18, or less than 20 percent of a standard deviation,&#39;</i></b><i> though the CAE paper notes that Arum and Roksa use different methods of estimating the growth, including tracking it over two years (rather than four).</i><br /><br /><i>Those differences are not to be minimized, Arum said in response. </i><b><i>He said he was perplexed why Benjamin and his colleagues used the two-year figure in their analysis rather than the four-year figure that was also included in Academically Adrift, which showed a gain of 0.47 standard deviation</i></b><i> -- still significantly smaller than, but closer to, the CAE finding.</i><br /><br /><b><i>Arum, in his e-mail, also noted that rather than follow the same cohort of students throughout their collegiate careers -- allowing for a direct comparison of how the same students performed on the CLA over time -- the CAE report &#39;relies on cross-sectional data comparing a random sample of freshmen and seniors at a college. Given the high rates of attrition in higher education, such a methodology can result in upward bias of estimates. Roksa and I strongly prefer identifying growth from observing individual gains using longitudinal data.&#39;</i></b><br /><br /><b><i>Alexander Astin, the Allan M. Cartter Professor Emeritus at the University of California at Los Angeles, seconded Arum&#39;s view that the use of a cross-sectional rather than longitudinal comparison is significant, because at most institutions significant numbers of the entering freshmen will have dropped out. &#39;[L]iterally hundreds of research studies over the years have clearly demonstrated that dropouts are not comparable to degree completers in several crucial respects: they are less well prepared academically (i.e., their school grades and test scores are lower), less motivated, have poorer study habits, more likely to be commuters rather than residents, more likely to be underrepresented minorities, and more likely to come from lower SES families,&#39; Astin said via e-mail. &#39;Since each of these qualities, in turn, is likely to be associated with lower CLA scores, the study&#39;s findings could be entirely attributable to this methodological flaw.&#39;</i></b><br /><br /><i>Benjamin said that studies by CAE and others had found the use of cross-sectional rather than longitudinal data to be immaterial.</i><br /><br /><i>But in an e-mail response, he sought to play down his studies&#39; differences from those of Academically Adrift. &#39;I decided to make these data public because they suggest, overall, student learning growth in colleges is significant.</i><br /><br /><i>&#39;However, until we carry out numerous studies building upon Academically Adrift, we will not have a complete enough evidence-based picture of student learning in the American collegiate landscape to permit both finer grained generalizations about the quality of student learning and also what actions we can take to continue to improve it.</i><br /><br /><i>&#39;I only singled out Academically Adrift because it was a seminal attempt to examine the question of student learning on our campuses. We need many more such serious efforts.&#39;&quot;</i><br /><br />Learning During Unsettled Times - A Talk by Richard Arum - Author of Academically Adrift - The New School - YouTube<br /><br /><a href=\"https://www.youtube.com/watch?v=zGjzmP2rpGo\" class=\"ot-anchor\">https://www.youtube.com/watch?v=zGjzmP2rpGo</a><br /><br />09:20 - 10:08<br /><i>&quot;To address these questions, we could not rely on federal data that has existed in K-12 education for decades.  Since at least 1980, the federal government has been collecting national probability surveys of students in K-12 education following them over time with repeated measures of student performance, and to allow one to assess the determinants of learning -- How learning outcomes are related to academic experiences, school climates, social background.  </i><b><i>That type of data has not existed in higher education, and that -- we think -- is a national shame, a national disgrace.&quot;</i></b><br /><br />10:08 - 11:06<br /><i>&quot;Because of that shortcoming, we were forced to create our own dataset, and to do it, if you will, out of our back pocket, going cap-in-hand to foundations and saying, &#39;Can we have some money to cobble together this resource in order to do this research.  So, we&#39;ve collected data.  First of all, it&#39;s critically got a longitudinal design.  We&#39;re following the same students as they entered college and university in Fall 2005.  They&#39;re given an assessment -- the CLA assessment … They&#39;re retested again in Spring 2007 at the end of their sophomore year.  They&#39;re retested again in 2009.  If they&#39;re on track, that&#39;s the end of their 4th year.  And we&#39;re also following them up last spring.  And this next month, we&#39;re gonna be surveying them again to see how they&#39;re faring after graduation.&quot;</i><br /><br />11:17 - 11:21<br /><i>&quot;The book [Academically Adrift] is based upon 24 diverse four-year institutions&quot;</i><br /><br />13:26 - 13:51<br /><i>&quot;There&#39;s also, if you note, a change in the number of students, and of course that&#39;s because many college students drop out over the four-year period.  We had no capacity, because we were dependent upon the collaborative institutions, to follow those kids up like a national probability survey would.  And we also did not have the possibility to follow them up if they transferred to another institution.&quot;</i><br /><br />13:51 - 14:35<br /><i>&quot;Now, what our data set does have is a breadth of information.  It has detailed questions on family background.  It has detailed information on the high schools that individuals attended, including -- we asked for -- the name of the high school, and went to federal government data, and matched it with characteristics.  Of those high schools, detailed questions about students&#39; college experiences and college context, as well as data from college transcripts.  It also critically has this assessment -- the collegiate learning assessment -- and so, that&#39;s the objective measure of student performance over time.&quot;</i><br /><br />14:40 - 15:44<br /><b><i>&quot;So, let me tell you about what the CLA.  First of all, the dimensions assessed are critical thinking, complex reasoning and written communication.  The distinguishing characteristics of the CLA.  First of all, it&#39;s a direct measure of student performance.  Much of the research in higher education before this book has been based on student-self reports of learning.  So, students graduate, and they&#39;re asked how much did you learn when you were in college?  And these kids are American, so what do they say?  &#39;I learned a lot!&#39;  So, great.  Of course, students don&#39;t know what they don&#39;t know, and they have no way to compare their learning to anything other than the student down the hall.  And it&#39;s probably pretty clear that the student down the hall isn&#39;t learning very much, so they think that they&#39;re learning more.  And they also have the grades.  They&#39;re getting very good grades, as we&#39;ll see shortly.  So, based on that, they&#39;d also believe that they had been learning a lot.&quot;</i></b><br /><br />16:00 - 16:28<br /><b><i>&quot;The CLA is not a multiple choice test.  Rather, it&#39;s a holistic assessment that&#39;s based upon open-ended prompts representing real-world scenarios.  That is, students are given a task to do that they might be confronted with after they&#39;ve graduated, by an employer.  And so, it&#39;s a complex, complicated task that asks them to make sense of a set of documents, and to do something with those documents.&quot;</i></b><br /><br />17:29 - 17:52<br /><i>&quot;All measures of student performance, all measures of these concepts, are by definition imperfect and limited.  The CLA certainly has those drawbacks.  It is not a perfect measure of these underlying concepts, and it is limited.  </i><b><i>It is not a measure of subject-specific skills.&quot;</i></b><br /><br />18:12 - 18:45<br /><b><i>&quot;However, the generic higher order skills are the skills that transfer from job to job, occupation to occupation, industry to industry.  They are the skills that help people navigate the increasingly unstable economic environment that they&#39;re exposed to.  They are also, of course, the higher-order skills that are the foundations for peoples&#39; ability to demonstrate Democratic citizenship -- to make sense of the world and to make decisions based upon it.&quot;</i></b><br /><br />19:05 - 20:58<br /><i>&quot;So, the test is administered online.  You&#39;re given a computer, and you&#39;re given a task like this: </i><b><i>You&#39;re asked to imagine that you&#39;re an assistant to Pat Williams, the president of DynaTech, a company that makes precision electronic instruments and navigational equipment.  There&#39;s a recommendation that you are to buy a private plane for your corporation to help with sales.  There&#39;s an accident on one of these planes, and you&#39;re asked to make a recommendation: Based on now that there&#39;s been an accident, what should we do?  Should we go ahead with the purchase or not?  You&#39;re given a set of documents.  You&#39;re given newspaper articles, a federal accident report, email exchanges between your boss and the people trying to sell the plane.  You&#39;re given technical manuals, and you&#39;re asked -- based on this information -- to write a memo with your recommendation, and the basis for your recommendation.</i></b> <br /><i>Now, that&#39;s not a simple task.  That&#39;s a complicated task.  </i><b><i>It&#39;s one that asks you to, first of all, to take the documents that are put in front of you and think critically about them -- to cross-examine them, to interrogate them.  Some of them -- the validity of them is likely higher than others.  You&#39;re asked to synthesize across these documents (complex reasoning).  And you&#39;re asked to write something, based on it, that requires you to produce a logical argument -- a logical argument where the evidence supports your position.&quot;</i></b><br /><br />21:46 - 22:34<br /><i>&quot;We asked students -- We went in as social scientists and we asked students -- this is in the spring of their sophomore year -- in the previous semester, we were naive social scientists, we asked, </i><b><i>&#39;How many courses did you take last semester -- how many -- where you wrote more than 20 pages over the course of the semester?  Not a twenty page paper, over the course of the semester more than 20 pages.  How many courses?  Naive social scientists.  The data came back so skewed to zero … Did you take any course, any single course where you wrote more than 20 pages over the course of the semester?  And half of the students say no.  We did not take a single course where we wrote more than 20 pages over the course of the semester.&quot;</i></b><br /><br />23:02 - 23:12<br /><i>&quot;Large numbers of students are figuring out ways to navigate through college and university with very little asked of them.&quot;</i><br /><br />24:36 - 24:58<br /><i>&quot;And when you go and interview freshman on colleges and universities today, they&#39;ll tell you something a little surprising.  They&#39;ll say, &#39;I thought college and university was going to be harder than high school.  And I&#39;m surprised.  It&#39;s not.  It&#39;s easier.  That&#39;s what the qualitative research suggests.  And it is in terms of their hours devoted to the task.&quot;</i><br /><br />25:02 - 25:15<br /><i>&quot;Maybe it was never any different.  Maybe college and university was always primarily a social activity.  Well, it&#39;s always been a social activity in part, but it&#39;s changed.  It&#39;s changed dramatically over time.&quot;</i><br /><br />25:55 - 26:27<br /><b><i>&quot;From the 1920&#39;s to the 1960&#39;s, it was relatively constant.  A full-time college student spent about 35 - 40 hours per week in class and studying.  That&#39;s what full-time meant.  Beginning in 1960, things started to change.  If you&#39;re just looking at studying and preparing for class, you go from 25 hours per week studying and preparing for class to today, again, 12 or 13 hours studying per week.  It&#39;s dropped in half.&quot;</i></b><br /><br />26:33 - 28:09<br /><i>&quot;Now, if we&#39;re gonna put students&#39; grades in there, the grades have gone in the other direction.  So, college freshman were asked in the UCLA Heary (sp?) study -- incoming college freshmen are asked -- do you think you&#39;re going to get a B-average or better in college and university?  And in 1971, the first time they asked this … 27% of the brave students said, &#39;Yes, I&#39;m gonna get a B-average or better in college and university.&#39;  </i><b><i>Today, it&#39;s close to 70%.  And you know what?  We don&#39;t disappoint them … In our study, we had 36% of our students say they studied alone five or fewer hours per week … We have their transcripts.  Their grade point averages were 3.2.  So, again, college and university students have figured out ways to navigate through the system with little asked of them, little applied, but with the grades … that are able to give them a credential that they believe is what college and university is all about -- a credential that can be traded for labor market success.&quot;</i></b><br /><br />28:36 - 29:36<br /><i>&quot;Well, given these low demands, we can go to the CLA test.  But, is there any surprise what the results are gonna be here?  The results are very low.  With a treatment this weak, one would not expect great gains on any measure.  And the CLA -- like this multiple-choice measure, CAP, which is being used by the Wabbash study … they are finding the same thing as we are.  Identical.  A different instrument, different colleges and universities, the same finding.  And, of course it&#39;s the same findings, because again, if students are being asked to do so little, you&#39;re not gonna see them moving on any indicator.&quot;</i><br /><br />30:21 - 31:16<br /><i>&quot;For 45% of the sample over the first two years, there are no statistically significant gains on this measure … The CLA measure ranges by over a thousand points.  We only want … 9 points is significant!  If it&#39;s not significant, they&#39;re not in our measure.  We&#39;re not counting them.  </i><b><i>So, if you had a test that went from zero to a hundred, if they moved up one point on a 0-100 test, they&#39;re not in that 45%.  The 45% … they&#39;re showing little-to-no gains on the first two years.  And by 2009, it&#39;s only 36%.&quot;</i></b><br /><br />35:56 - 36:09<br />Educational Measures Associated with Learning: <i>&quot;So again, to highlight, faculty expectations matter, course requirements matter [reading and writing requirements], hours studying alone positively tracks with outcomes, and again, the traditional arts and sciences majors [core subjects]&quot;</i><br /><br />42:30 - 42:56<br /><i>&quot;These students are highly indebted.  60% report having loans.  And, if they did report having loans, they reported having $27k in debt on their student loans.  And our research suggests that they also have something like $4000 on their credit cards, where they&#39;ve often used the credit card for academic expenses.&quot;</i><br /><br />44:02 - 44:10<br /><b><i>&quot;31% of the students reported that they were back living at home.  They&#39;re living with parents and family members.&quot;</i></b><br /><br />44:42 - 45:18<br /><i>&quot;We, again, believe that college and university is not just about preparing students for labor market success, but also for preparing them to function as Democratic citizens in society.  And so we asked them questions about this.  </i><b><i>First of all, we asked them about their news: How often do you read the newspaper, in print or online?  And 30% of them came back and said monthly or never.</i></b><i>  These are college graduates, a year out, facing these very, very difficult situations.&quot;</i><br /><br />45:34 - 46:12<br /><i>&quot;Colleges and universities are about not just promoting higher-order skills -- critical thinking and complex reasoning, and the like.  They are about training students in attitudes and dispositions about being productive, successful adults that are engaged in society in constructive ways.  If they&#39;re not being taught and trained in those attitudes and dispositions, you&#39;ll get this outcome.  So, if you ask students -- students reporting five or fewer hours studying … This is studying with friends, or any studying -- 52% say, &#39;I read the paper monthly or never.&#39;&quot;</i><br /><br />46:36 - 47:10<br /><i>&quot;We asked a very broad question: </i><b><i>How often do you discuss politics or public affairs with friends, family members or colleagues in person, on the telephone or online?</i></b><i>  As broad a definition as we could get, right?  We were just: Please tell us you&#39;re gonna do this … We could not construct a survey item which was, if you will, fishing for positive responses than this: </i><b><i>39% say monthly or never.&quot;</i></b><br /><br />48:29 - 49:10<br /><i>&quot;However, we do believe that accountability should be built in at lower levels in the system.  So, being against federally-imposed accountability doesn&#39;t mean that we think students, faculty and institutions, administrators …  shouldn&#39;t be held accountable.  We do, but we think that there&#39;s mechanisms within the system already that can accomplish that effectively if they fulfill the functions they were set out to do.  So, private institutions, of course, have boards of trustees.  Public institutions have state regents and overseers.  They should be asking their administrators, &#39;What are you doing to improve learning?  </i><b><i>How are you measuring it?</i></b><i>  How are you focusing upon accomplishing these goals?&#39;&quot;</i><br /><br />49:22 - 50:13<br /><i>&quot;We do believe the federal government, however, has a role to play.  First of all, the federal government today spends billions and billions of dollars promoting research and encouraging … colleges and universities to focus on research.  We, of course, think that&#39;s a worthy end.  We believe in research, and colleges and universities can play a large role in improving society through generating knowledge … However, the incentives are so much in that direction that we believe it is likely they&#39;ve distracted colleges and universities from focusing on their core missions of undergraduate education.&quot;</i><br /><br />1:29:22 - 1:31:20<br /><b><i>&quot;We definitely have a problem today where the students are not engaged in their academic instruction.  But, I think why they&#39;re not engaged in their academic instruction is much more complicated than simply that … our pedagogy is not technologically sophisticated to meet them halfway … The technology is used by students, in part, to navigate the system.  They use the technology to find out where the easy programs are, where the easy courses are</i></b><i> … You don&#39;t believe that?  Well, go look at &#39;Rate My Professor&#39;.  And not just Rate My Professor, but lots of colleges have community spaces where they talk about, &#39;Go take this course.  It&#39;s an easy A.&#39;  And so, they are using the technology effectively.  They&#39;re using the technology also often to get around the hard work of writing papers.  The self-reported plagiarism has increased dramatically.  We don&#39;t hold students accountable for plagiarism.  And so, again, I think … it&#39;s absolutely true that faculty need to think about how we deliver instruction, and we could do better than that.  But, again, </i><b><i>I think it&#39;s a lot more complicated than that.</i></b><i>  And, a lot of these old-fashioned ideas like academic rigor -- you gotta apply yourself, you have to read and you have to write if you want to see these gains.  That&#39;s an old-school idea, but it turns out to be true.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Unlearning of Creativity","summary":"It is a Myth that Creativity is a Gift that a Select Few are Born with / Young Children are Generally More Creative Because they Look through Unpolluted and Unsullied Fresh Eyes / To Reverse the Years of Filtered Thinking, We Must Start Connecting Experiences and Synthesizing New ideas","image":"https://lh3.googleusercontent.com/-EPdPmEIuO_Y/V7KqZHDiP-I/AAAAAAAAIi4/2ceygnHrxzIKbr6pGmquzOckTCmBHZwLA/w1800-h2520/the-unlearning-of-creativity-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/FpsEoynk6cH","publishDate":"2016-08-16T05:54:43.766Z","updateDate":"2017-01-03T02:03:42.100Z","text":"<b>The Unlearning of Creativity: It is a Myth that Creativity is a Gift that a Select Few are Born with / Young Children are Generally More Creative Because they Look through Unpolluted and Unsullied Fresh Eyes / To Reverse the Years of Filtered Thinking, We Must Start Connecting Experiences and Synthesizing New ideas</b><br /><br />&quot;Fresh Thinking Drives Creativity &amp; Innovation&quot;<br />by Larry Arno Vint<br /><br /><a href=\"http://www98.griffith.edu.au/dspace/bitstream/handle/10072/7880/33187_1.pdf?sequence=1\" class=\"ot-anchor\">http://www98.griffith.edu.au/dspace/bitstream/handle/10072/7880/33187_1.pdf?sequence=1</a><br /><br /><b><i>&quot;Creativity is Not Learned, But Rather Unlearned</i></b><br /><br /><i>... </i><b><i>It is a myth that creativity is a gift that a few select people are born with. As children, we were all more creative than we are today. Perhaps not to the same degree, but we all do have innate creative abilities. All of us have the potential to be creative and innovative.</i></b><i> Through research study findings, this premise has been tested, documented, and published many times (George Land &amp; Beth Jarman, 1992; James Higgins, 1996; Unda Naiman, 2000). </i><b><i>As young children we are more creative because we are looking through &#39;unpolluted&#39; and &#39;unsullied&#39; fresh eyes. As teenagers and adults, we start to filter everything we see, just like a polarised lens that lets in only light that is aligned one way. To reverse the years of filtered thinking, we need to start connecting experiences and synthesising new ideas.</i></b><i> We need to teach our students the creative steps to bring out new, innovative and imaginative ideas. Ideally, creative ideas that the students themselves thought they could never have previously conceived.</i><br /><br /><b><i>The reason children are so creative (Stephen Shapiro, 2001) is that they look at the world with fresh eyes. They are always collecting information, data and particulars that they eventually compile together. Everything is a new experience. And rarely do kids jump to quick solutions. However, once they start going to school and socialise with other children, they are forced to fit in. Peer pressure drives conformity. Education focuses on the regurgitation of facts rather than on gathering new experiences. Within a school and at university, students choose major areas of study and become proficient, and sometimes expert, in that area. As we get older we find things in life that we like, to the exclusion of all else. We read the same sections of the newspaper. Read the same magazines. We watch the same style of movies. Narrow down the foods we eat. Socialise with the same friends. We tend to find ways of functioning within our lives that work for us. We use these modes continually without trying anything new, such as our thinking patterns, communication style, our view of the world, and our political thoughts. As we get older, instead of collecting &#39;ideas&#39;, we begin a process of information elimination. We continue to narrow down our relevance, concerns, curiosity, concentration and awareness as we get older.</i></b><br /><br /><i>In 1968, Gearge Land (George Land &amp; Beth Jarman, 1992) gave 1,600 five-year-olds a creativity test to see how highly creative they scored. </i><b><i>This was the same test used by NASA to select innovative engineers and scientists.</i></b><i> He re-tested the same children when they were of 10 years of age (1978), and again at 15 years of age (1983). He later tested 280,000 adults to see how highly creative they scored. The test results were:</i><br /><br /><i>Age group</i>  /  <i>Number tested</i>  /  <i>Year</i>  /  <i>% who scored &#39;highly creative&#39;</i><br /><i>5 year olds</i>  /  <i>1,600 children</i>  /  <i>1968</i>  /  <i>98%</i><br /><i>10 year olds</i>  /  <i>1,600 children</i>  /  <i>1978</i>  /  <i>30%</i><br /><i>15 year olds</i>  /  <i>1,600 children</i>  /  <i>1983</i>  /  <i>12%</i><br /><i>25+ year olds</i>  /  <i>280,000 adults</i>  /  <i>1985</i>  /  <i>2%</i><br /><br /><i>In Land&#39;s research of 1,600 five-year-olds tested, 98 percent of the children scored in the &#39;highly creative&#39; range. When re-testing these same children five years later, only 30 percent of these 10-year-olds were still rated &#39;highly creative&#39;. By the age of 15, just 12 percent of them were ranked in this category; while a mere 2 percent of 280,000 adults over the age of 25 who had taken the same tests were still on this level. </i><b><i>&#39;What we have concluded,&#39; wrote Land, &#39;is that non-creative behavior is learned.&#39; From this and similar research we can conclude that creativity is theretore not learned, but rather unlearned.</i></b><i> We as teachers need to reinforce and encourage student&#39;s fresh thinking and promote high levels of creative behavior, enriching their thinking skills, critical thinking, problem solving and decision making.</i><br /><br /><i>Through well thought out projects and classroom activities, with well designed briefs, enriched thought provoking content, encouraging guidance and frequent feedback, in conjunction with external influences such as media input, guest speakers and excursions, we can begin to reverse these figures.</i><br /><br /><b><i>Why Aren&#39;t Adults as Creative as Children?</i></b><br /><br /><i>For most, creativity has been buried by rules and regulations. Our educational system was designed during the Industrial Revolution over 200 years ago to train us to be good workers and to follow instructions.</i><br /><br /><b><i>Creativity is the collecting and connecting of ideas, disciplines, ways of looking at problems, and experiences.</i></b><i> In order to tap into this innovative potential, we cannot revert back to a second childhood or hire five year olds. Albert Einstein once said, &#39;Imagination is more important than knowledge. Knowledge is limited. Imagination encircles the world.&#39; (Linda Naiman, 2000) </i><b><i>Knowledge in many ways is the enemy of creativity, for once your brain finds what it thinks is the best solution, it stops looking. We look for solutions in our memory banks of what has worked in the past. Finding an answer quickly is often not the best solution and often undersells our ability and intelligence. Unfortunately, these solutions might not be new, innovative or even good. What we need to do is train our brain to keep looking, even when we have found an answer.</i></b><br /><br /><b><i>Creating a Culture of Innovation &amp; Creativity</i></b><br /><br /><i>World-wide we have a talent shortage. Educational institutes and industries that understand the relationship between creativity, innovation and performance, and actively promote creativity in their students and staff, will be the winners in the marketplace. The root of invention and innovation is creativity.</i><br /><br /><i>Listed below are techniques to get you started in creating a culture of innovation and creativity within your classroom.</i><br /><br /><b><i>1. CONNECT AND SYNTHESIZE NEW EXPERIENCES. Creativity is having the ability to connect and synthesize ideas, disciplines, ways of looking at problems, and experiences. By encouraging students to collect and hoard every experience for later use, these stored random experiences can be used as the catalyst for breakthrough thinking. Experiencing more and analysing these experiences is one reason some students are more creative. Like through the eyes of a child, having fresh approaches by looking at the world in a new way, with simplicity and imagination, students will then begin to see what they have never seen before, and be able to expand what they focus on. Students will see, interpret, and sense new things through these new filters.</i></b><br /><br /><b><i>2. BREAK OLD PATTERNS.</i></b><i> During class, students tend to do the same things over and over because our teaching is repetitious. Break this pattern. </i><b><i>Give students unusual, controversial, and critical resources that they have never seen before. Test and expand their ideas on things they previously did not like. Introduce them to new people and influences. Try different angles of analyses of theories. The more you do this, the more new experiences they will gain and the more ideas they will be able to draw from in the future.</i></b><br /><br /><b><i>3. REINTERPRET NEEDS WHICH REFLECT THE NOW. As adults, when we try to solve a problem, we often try to pull the answer from our knowledge bank, just like finding the solution in a textbook. [But] solving the problem the way it has previously been solved provides a limited set of possibilities; one of replication and regurgitation. An alternative and more insightful way of looking at problems is to make connections and find analogies, metaphors, and associations that fit the problem you are looking to solve. Recombine ideas in new ways.</i></b><i> If a student is redesigning a cultural centre, look at hospitals, airports, subways, hotel foyers and cinema designs to see how they deal with human traffic. Also look at ant colonies and bee hives, anything with a flow. One needs to take the design a step further and look to non-educational connections, analogies, metaphors and associations. Students have limitless ways of recombining their ideas to create something new and creative. This is not about invention, which is pulling something out of the thin air. </i><b><i>Innovation is about reinterpreting situations, creating scenarios. Students should not always go for the obvious solution.</i></b><i> Some of the best ideas come from some of the most unlikely combinations and abstract references.</i><br /><br /><b><i>4. LOOK AT NATURE.</i></b><i> Look at nature, the evolution process, the ecosystem, micro-organisms, astronomy, music, or any other discipline of interest to be inspired by to find solutions. These have unique attributes that provide an endless source of inspiration. The careful observation and analysis of one or more of nature&#39;s forms can yield a wealth of practical and useful attributes in terms of structure, form, geometry, unique mechanisms, use of colour, surface language, pattern and practical strategies. For example, a student wishing to design a chest of drawers may be inspired by nature, through analysing the giraffe. The student looks at nature&#39;s characteristics, instead of reinventing the design. The chest of drawers could be inspired by the structure, form and geometry of the giraffe&#39;s neck. The texture, pattern and varnish colour may be inspired by the giraffe&#39;s surface language and colour. In another design, a computer mouse could be designed, inspired by the man-o-war jelly fish; the design being comfortable, flexible and therapeutic to hold, having nodules and being transparent in appearance.</i><br /><br /><b><i>5. FORCE ILLOGICAL ASSUMPTION COMBINATIONS.</i></b><i> Turn everything upside down. </i><b><i>Bring out the hidden assumptions we take for granted</i></b><i> by asking the questions &#39;who, what, where, when, how and why.&#39; Challenge the models of assumption. This may include forcing illogical assumption combinations. Ask students to come up with various answers to some or all of the listed assumptions to solve a problem. Then randomly mix and match these various combinations. For example, in the process of redesigning a car security system, they might look at &#39;who, &#39;where&#39;, and &#39;why&#39;. The typical combination for the car security system is that it is used by car owners (who), when parked and left unattended (where), for the purpose of theft prevention (why). Let&#39;s now bring into the equation two other combinations such as a satellite navigation system and a mobile telephone. Students can now begin to think creatively and design a handheld device functioning as a mobile phone which comes standard with a vehicle immobiliser security navigation system. If the car is broken into, driven away or moved, the owner is alerted, allowing them to track and immobilise the vehicle through their mobile phone. Most often creative ideas come from the most illogical combinations.</i><br /><br /><b><i>6. BUILD ON THE IDEAS OF OTHERS.</i></b><i> Students love to interact, where they fully use their imagination. This interaction can continue for hours. Interestingly, when adults congregate, such as in meetings or socially, instead of contributing, they often either critisise the previous idea, pass judgment, just agree, or do not add to the discussion or ideas presented. With little contribution and/or negativity, the process ends quickly. This is often the case in the classroom. We see all of the reasons why things will not work. In solving this, the next time students have a problem to resolve, such as the design of a good web site. Have one student cast out the first idea, and let the class continue with, &#39;Yes, and ...&#39;, building on the previous ideas. </i><b><i>Do not allow innovation to be crushed at the beginning. Do not accept the response of &#39;Yeah, but ...&quot;.</i></b><i> The key is to answer quickly and avoid thinking too much. Top-of-head answers tend to tap into a part of the brain we don&#39;t use during our normal thinking process. And be sure that the students&#39; answers are a contribution, which builds on what the previous student said rather than invalidates it.</i><br /><br /><b><i>Many of the new ideas will be ineffective, but don&#39;t be concerned. Work with it, as you never know when an excellent solution to an idea will be found. Over time, this will become a normal mode of operating. Your students will become the masters of breakthrough thinking on a regular basis by building on the ideas of others.</i></b><br /><br /><b><i>7. WELCOME CREATIVE TENSION. It is human nature to surround ourselves with people we get along with. Unfortunately our students choose who they work with through focusing on this human chemistry and students with similar competencies. This perpetuates a limited set of creative possibilities.</i></b><i> Instead, place students in teams with different analytical, creative, and personality styles. Welcome the creative tension. As long as the students are open to new ideas, they are bound to emerge. These team combinations can create new ideas never previously conceived.&quot;</i><br /><br />[...]<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"Reacting vs Creating","summary":"We have been Educated to Problem-Solve, to be Normal and to React / But Following the Crowd or Tradition to Pre-Determined Answers (Problem-Solving) will Oftentimes Take Us in the Wrong Direction / This Non-Creative Behavior is Learned / Part of the Process of Becoming Successful in Life is Learning and Gaining Experience with the Creative Process","image":"https://lh3.googleusercontent.com/-6By_uoxDcZo/V7EmCuDattI/AAAAAAAAIhs/4aP9ZYLg1OwcMR77Hi1T4NaVeMVWERDaw/w7142-h9999/reacting-vs-creating-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/RSX6d2kiSUR","publishDate":"2016-08-15T02:20:52.447Z","updateDate":"2017-01-03T02:04:02.753Z","text":"<b>Reacting vs Creating: We have been Educated to Problem-Solve, to be Normal and to React / But Following the Crowd or Tradition to Pre-Determined Answers (Problem-Solving) will Oftentimes Take Us in the Wrong Direction / This Non-Creative Behavior is Learned / Part of the Process of Becoming Successful in Life is Learning and Gaining Experience with the Creative Process</b><br /><br />This graphic is entirely based upon a webinar titled &quot;Creating the Practice you Want&quot; by a dentist, Dr. Michael Schuster.<br /><br /><a href=\"https://attendee.gotowebinar.com/recording/83354353083173377\" class=\"ot-anchor\">https://attendee.gotowebinar.com/recording/83354353083173377</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"The Pre-Scientific Judgment of New Ideas","summary":"The Process of Finding Ideas in Science is Not Part of the Scientific Method / And the Scientific Method Offers No Way to Assign Value to New Ideas / It's Essentially Lawless / These Oversights Suggest that There is Progress to be Made from Knowledge and Communication Tools Designed to Service these Needs","image":"https://lh3.googleusercontent.com/-_EUfzXsqeGQ/V60tf5IU3II/AAAAAAAAIgk/1hdyv-7flv0kon8aMlvzfzeYqTt2aMy5Q/w7142-h9999/the-pre-scientific-judgment-of-new-ideas-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/9zhxMNRDha3","publishDate":"2016-08-12T01:59:52.408Z","updateDate":"2017-01-03T02:05:02.133Z","text":"<b>The Pre-Scientific Judgment of New Ideas: The Process of Finding Ideas in Science is Not Part of the Scientific Method / And the Scientific Method Offers No Way to Assign Value to New Ideas / It&#39;s Essentially Lawless / These Oversights Suggest that There is Progress to be Made from Knowledge and Communication Tools Designed to Service these Needs</b><br /><br /><i>The Death of Science: A Companion Study to Martin Lopez Corredoira&#39;s the Twilight of the Scientific Age</i><br />by Andrew Holster<br />p23-24<br /><br /><i>&quot;[T]he key, traditional goal of scientific research is the exploration of new knowledge, and the key problem lies in making judgments of the quality of novel ideas, in fields with apparently diminishing returns.  What new research should be funded and supported?  What new ideas should be investigated?  The research enterprise itself depends on the potential for new or revolutionary ideas to be found. </i><b><i>But as time goes on, the most obvious veins of research are explored, and new ideas are harder to come by. It is a bit like a gold mine. The first prospectors strike it rich and find nuggets in easy reach; but when the first strikes have been panned, you need to switch to industrialised methods to extract what is left. And when there is no gold in the ground left to find at all, it doesn&#39;t matter how big you make your industrial gold mine, you will only find microscopic gains. The spirit of gold mining changes too.  The first prospectors are individualists, exploring new frontiers. The first gold miners are adventurers, prepared to undertake hazardous journeys with serious risks and endure harsh conditions. They are individualists, and entrepreneurs, working for themselves. Later the mine becomes an industrialised factory, run by production-line wage-workers, requiring expensive heavy equipment, needing large capital investment, with the enterprise and the profits controlled by executives and accountants from comfortable offices.</i></b><br /><br /><i>Gold miners are always keen to determine if there is likely to be gold in a certain piece of ground.  But scientists have no way of telling if there is any scientific gold to find in a certain area or not. </i><b><i>In this respect, scientific discovery -- the process of finding ideas, theories, concepts, possibilities -- is not a part of the &#39;scientific method&#39;. Orthodox philosophy of science (positivism, empiricism, Popperism, etc), which proclaims the triumph of the &#39;empirical method&#39;, says nothing about the creation of scientific ideas -- or the pre-scientific judgment of the quality of new ideas. The conventional account of the &#39;scientific method&#39; is an arm-chair rationalisation, designed to justify scientific knowledge. It is primarily a banal account of a method to test a given empirical hypothesis.  But it assumes hypotheses, theories, ideas, are given: plucked out of thin air. This is where the &#39;scientific method&#39; has a yawning chasm: it has no method for either the creation of ideas, or for the judgment of value.</i></b><br /><br /><b><i>Given that science has the task of extending knowledge (rather than just accumulating more facts to support present theories), then science must explore new ideas.  Genuinely new ideas are notoriously difficult to judge -- and impossible to judge for the vast majority of &#39;scientific technicians&#39; churned out of our academic institutions. For new ideas, by their very nature, are discovered by scientists with rare talents and by rare accidents. They are ideas hiding behind the facades of conventional paradigms, hiding in the gaps of the textbook explanations and exam exercises that define &#39;scientific reality&#39; for academics. They require novel thoughts, concepts, models, formalisms and solutions. Ordinary science students struggle to understand and recite even the conventional text-book problems that have been solved for them. If they cannot verify even these by themselves without following and trusting a text-book, they have no capacity at all to make judgments of new solutions at the forefront of their field.</i></b><i> How many scientists mature from the state of science students to develop any kind of mature scientific judgment, when confronted with novelty?  The answer is obvious enough from history and experience: very few.  The present scientific ideology and mediocrity that dominates scientific institutions often reduces this to: </i><b><i>none.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Narrative of Scientific Discovery","summary":"Many People Today Still Believe that When a Revolutionary Discovery is Made, the Scientist is Immediately Hailed as a Genius / The Doors Swing Open and Shouts of Eureka Fill the Hallways / Colleagues Approach and Ask, 'Have You Made a Revolutionary Discovery?  Come in!  Come in!!  We Were Waiting for You.' / This Was in Fact Never the Way in Which Novel Discoveries Were Accepted","image":"https://lh3.googleusercontent.com/-DQm5vt9a0Lo/V6q_9cQXaXI/AAAAAAAAIfE/XEw_OmyTGXwYIej4ZfKuE4yaSLakTYWHwCL0B/the-narrative-of-scientific-discovery-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/ARg3vVaoKfk","publishDate":"2016-08-10T05:50:19.857Z","updateDate":"2017-01-03T02:05:45.350Z","text":"<b>The Narrative of Scientific Discovery: Many People Today Still Believe that When a Revolutionary Discovery is Made, the Scientist is Immediately Hailed as a Genius / The Doors Swing Open and Shouts of Eureka Fill the Hallways / Colleagues Approach and Ask, &#39;Have You Made a Revolutionary Discovery?  Come in!  Come in!!  We Were Waiting for You.&#39; / This Was in Fact Never the Way in Which Novel Discoveries Were Accepted</b><br /><br />[1] The narrative comes from a recent G+ conversation<br /><br /><i>The Twilight of the Scientific Age</i><br />by Martín López Corredoira<br /><br /><i>&quot;There are many naïve persons, scientists or non-scientists, around the world who still believe that science is an open process in which the best ideas are quickly recognized and accepted, while the wrong ideas are immediately discarded. This kind of individual thinks that achievement in science nowadays depends on intelligence, on genius. </i><b><i>They think that someone could be working hard in a laboratory, or developing some theoretical idea and, if they were to make a revolutionary discovery, they would open the door of the room in which they was cloistered and shout along the corridors &#39;Eureka! Eureka!&#39;; then, colleagues would approach and say: &#39;Have you made a revolutionary discovery? Come in! Come in! We were waiting for you ...&#39; and the genius would have the chance to show their new discoveries and their colleagues would open their mouths, surprised by the new idea, recognizing its merit, and carrying the genius on their shoulders while shouting &#39;Torero, torero, ... !&#39; (&#39;Bullfighter&#39; in Spanish). This was never the way in which general ideas were accepted.&quot;</i></b> (p108)<br /><br /><b><i>&quot;The reality is that nobody is waiting for revolutionary ideas, they are not welcome, now less than ever, and the difficulties that professional researchers have when they want to challenge dominant ideas (e.g., Campanario &amp; Martin) are enough to dissuade them in their enterprise or cause [them] to be rejected as outsiders by the system.&quot;</i></b> (p111)<br /><br />[2] Gerald Pollack<br />University of Washington<br />Founder of the Institure for Venture Science<br /><br /><a href=\"https://youtu.be/_1FYjZ77eew?t=853\" class=\"ot-anchor\">https://youtu.be/_1FYjZ77eew?t=853</a><br /><br /><i>&quot;I think what I alluded to before is it&#39;s the culture of science today that is the culprit ... The emphasis on productivity, producing lots of papers, competing to get into the prestigious journals and such.  And so, it&#39;s a culture where power is very important.  Power is given to the experts and power is given to the vested interests.</i><br /><br /><i>We need money to do research.  The experts are the ones who decide whether the revolutionairies get the money, or don&#39;t get the money.  This is not a system to encourage radical thinking and fresh ideas.</i><br /><br /><i>So, you come up with a fresh idea, and the typical response from the establishment is something like this ... You&#39;re proposing what?  Yuck!  So, this is the kind of, you know ... many of you have been there ... this is the kind of response.&quot;</i><br /><br />[3] Cartoon is from <a href=\"http://www.twisteddoodles.com/\" class=\"ot-anchor\">http://www.twisteddoodles.com/</a><br /><br />[4] Gerald Pollack<br />University of Washington<br />Founder of the Institure for Venture Science<br /><br /><a href=\"https://youtu.be/_1FYjZ77eew?t=591\" class=\"ot-anchor\">https://youtu.be/_1FYjZ77eew?t=591</a><br /><br /><i>&quot;Most scientists understand that if you threaten the stakeholders, you have to be careful -- particularly in the institutions that I deal with all of the time, the National Institute of Health and the National Science Foundation. </i><b><i>Most scientists with revolutionary ideas keep it secret. They don&#39;t apply. Once they apply, they know they won&#39;t get the money. And also, they develop a reputation as being the kind of person who you stay away from. So, revolutionary change is suppressed -- As a result, I think, largely of the institutions we have that fund science.&quot;</i></b><br /><br />[5] <i>The Twilight of the Scientific Age</i><br />by Martín López Corredoira<br />Cosmologist / Astrophysicist / Philosopher / Published 50 Academic Papers, Often as Lead<br /><br /><b><i>&quot;Scientists are educated nowadays in a habit of self-censorship. The system promotes self-repression in the spread of ideas, so most scientists, when writing a paper, think something like &#39;I think this and that, but I cannot say so in my paper because this will not pass the referee&#39;s control, so I will not say it&#39;. This causes serious harm to creativity among people who dare to think new things ... scientists have to choose between developing their own ideas freely or being constrained by subjects which allow academic success.&quot;</i></b><br /><br />[6] <a href=\"http://faculty.washington.edu/ghp/images/stories/pubPDF/2005-Pollack-Revitalizing_Science.pdf\" class=\"ot-anchor\">http://faculty.washington.edu/ghp/images/stories/pubPDF/2005-Pollack-Revitalizing_Science.pdf</a><br /><br />In other words, there are scientists today who want to make the world a better place.  But our narratives stand in the way of our own realization that we need to support them in order to make it happen.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"Tourists vs Explorers","summary":"Tourists Visit Safe Places that are Novel and Exciting to them Personally / Explorers, by Contrast, are Driven by an Excitement of Discovering New and Unexplored Frontiers / When Science is Seen as Having Already Explored Most of the Dangerous and Exciting Places, its Spirit Likewise Changes in the Process, in that it Becomes Unattractive to People Wired for Novelty and Discovery","image":"https://lh3.googleusercontent.com/-0NqBBZ4JAmw/V6nW8E115GI/AAAAAAAAIek/CpHbS8Ve9KAK7iqhSQZq5SKwEZ81SNDoA/w1800-h2520/tourists-vs-explorers-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Rauu21NEors","publishDate":"2016-08-09T13:14:15.638Z","updateDate":"2017-01-03T02:06:04.362Z","text":"<b>Tourists vs Explorers: Tourists Visit Safe Places that are Novel and Exciting to them Personally / Explorers, by Contrast, are Driven by an Excitement of Discovering New and Unexplored Frontiers / When Science is Seen as Having Already Explored Most of the Dangerous and Exciting Places, its Spirit Likewise Changes in the Process, in that it Becomes Unattractive to People Wired for Novelty and Discovery</b><br /><br />From <i>The Death of $cience: A Companion Study to Martín López Corredoira&#39;s The Twilight of the Scientific Age</i>, p27-28 ...<br /><br /><i>&quot;There is [a] ... kind of personality trait related to the difference between science as an exploratory activity of discovery, and science as an academic activity of mastering a domain of pre-existing knowledge. It is parallel to the difference between explorers and tourists. Both are travellers, and there are lots of people today for whom ‘travel’ is a major hobby. </i><b><i>But most of us travel as tourists, visiting places and cultures that are novel and exciting to us personally, but are actually well-known, thoroughly explored, and essentially safe. Travelling to experience other cultures is a good thing to do, but it is quite different to the traditional role of explorers: visiting unknown and dangerous lands, driven by an excitement of discovering new and unexplored frontiers.</i></b><i> The real explorers in history typically endured great hardship and real danger. But the age of the intrepid explorer is essentially over on the Earth’s surface, because it has been systematically explored. Nowadays, travelling has become the popular middle-class hobby of tourism. And few modern tourists would have been explorers in the old days. Conversely, few genuine explorers are interested in tourism. They seek other challenges. There is really no role left for true explorers, in the geographical sense. Their age has gone.</i><br /><br /><i>I think the scenario painted in the Twilight of Science is analogous to this. </i><b><i>Science is seen as having already explored most of the dangerous and exciting places. Scope for new discoveries is increasingly limited for modern scientists, and most are now professional ‘scientific experts’, tourist guides through textbooks who take their pride in mastering existing knowledge. The spirit of science has changed in the process, and this makes science unattractive for the explorers: the people wired for novelty and discovery, and typically some kind of heroic individuality. Thus science changes its nature when it has been fully explored, and scientists of the present age are now quite unlike the explorers of the past in spirit.</i></b><br /><br /><b><i>The scientific culture changes its fundamental values when it becomes dominated by this different type of personality, just as frontier outposts change their personality when they become settlements, and then towns, and then urban environments.</i></b><i> In the first phase of ‘civilisation’, the original jungle or prairie is rendered into farms by pioneers – those able to tame nature from its raw and untamed state. In the next phase, the farms are taken off the pioneers by carpetbaggers, lawyers and bankers – those able to dominate the system of legal ownership. And then the whole environment is subsequently incorporated into the estates of the wealthy and powerful: who have learned how to dominate the entire social jungle. Colonial towns often retain a sentimental nostalgia celebrating the deeds of the pioneers: </i><b><i>but this is a false image, designed for a sense of social solidarity, and themes for tourist attractions.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"Groupthink and the Snowball Effect","summary":"The More Popular a Line of Research is, the More Money and Scientists are Dedicated to Working On It, and the Greater the Number of Experiments and Observations that Can be Explained Ad Hoc / This Leads to the Sense that the Theory Being Considered is Popular Because it is Successful / But, Once an Idea Becomes Popular, We Must Trace the Arguments for Evidence of Groupthink Social Patterns","image":"https://lh3.googleusercontent.com/-nxd-Aou8f8A/V6kv3Oz5lKI/AAAAAAAAIdw/dnj5j54r-UoGIgeuk2QVqIdBGtjWlHI6Q/w1800-h2520/groupthink-and-the-snowball-effect-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/YsXw1xmdWyv","publishDate":"2016-08-09T01:20:57.424Z","updateDate":"2017-01-03T02:06:46.475Z","text":"<b>Groupthink and the Snowball Effect: The More Popular a Line of Research is, the More Money and Scientists are Dedicated to Working On It, and the Greater the Number of Experiments and Observations that Can be Explained Ad Hoc / This Leads to the Sense that the Theory Being Considered is Popular Because it is Successful / But, Once an Idea Becomes Popular, We Must Trace the Arguments for Evidence of Groupthink Social Patterns</b><br /><br />From <i>The Twilight of the Scientific Age</i> by Martín López Corredoira, p85-86 ...<br /><br /><i>&quot;The snowball effect ... is a feedback loop: the more successful a line of research is, the more money and scientists are dedicated to working on it, and the greater the number of experiments [and] observations that can be explained ad hoc, such as in Ptolemaic goecentric astronomy; this leads to the theory being considered more successful.  In some cases, the system supports conservative views, but there are also cases of speculative lines of research that have been converted into large enterprises.  For instance, in theoretical physics, string theory has absorbed a lot of people and funds, as well as marginalizing and deprecating other approaches ...&quot;</i><br /><br />From Janis, I. L. (1972). Groupthink: psychological studies of policy decision and fiascos. Boston (MA, USA): Houghton Mifflin ...<br /><br /><i>&quot;In a sociological analysis, Janis categorizes the symptoms of groupthink as:</i><br /><br /><i>1) An illusion of invulnerability, shared by most or all the members, which creates excessive optimum and encourages the taking of extreme risks.</i><br /><br /><i>2) An unquestioned belief in the group&#39;s inherent morality, allowing the members to ignore the ethical or moral consequences of their decisions.</i><br /><br /><i>3) Collective efforts at rationalization in order to discount warnings or other information that might lead the members to reconsider their assumptions before they recommit themselves to their past policy decisions.</i><br /><br /><i>4) Stereotyped views of enemy leaders as too deviant to warrant genuine attempts to negotiate, or as too weak and stupid to counter risky attempts made at defeating their purposes.</i><br /><br /><i>5) Self-censorship of deviations from the apparent group consensus, reflecting each member&#39;s inclination to minimize to himself the importance of his doubts and counterarguments.</i><br /><br /><i>6) A shared illusion of unanimity concerning judgments conforming to the majority view (partly resulting from self-censorship of deviations, augmented by the false assumption that silence means consent).</i><br /><br /><i>7) Direct pressure on any member who expresses strong arguments against any of the group&#39;s stereotypes, illusions, or commitments, making clear that this type of dissent is contrary to what is expected of all loyal members.</i><br /><br /><i>8) The emergence of self-appointed mindguards -- members who protect the group from adverse information that might shatter their shared complacency about the effectiveness and morality of their decisions.&quot;</i><br /><br />(Editorial note: I&#39;ve corrected what appears to be a mistake in the bullet numbers)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"The Third Story","summary":"Several Thousand Years Ago, At Around the Same Time, Humans Across the Entire Globe Started Telling Similar Stories / Comparative Mythologists Study the Global Patterns in these First Stories / David Talbott's Third Story Refers to a Cultural Transformation which Could Follow if Scholars were to Decode Mythology as Conveying a Shared Cultural History We Knew Nothing About / \"The Stakes Are High\"","image":"https://lh3.googleusercontent.com/-XY6FMDhTQVs/V6fg4-WPy4I/AAAAAAAAIcc/HHlyeH6U79A8kClncVTImdRE31lstYR_w/w1800-h2520/the-third-story-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/YckpG9A67S5","publishDate":"2016-08-08T01:32:05.108Z","updateDate":"2017-01-03T02:06:23.446Z","text":"<b>The Third Story: Several Thousand Years Ago, At Around the Same Time, Humans Across the Entire Globe Started Telling Similar Stories / Comparative Mythologists Study the Global Patterns in these First Stories / David Talbott&#39;s Third Story Refers to a Cultural Transformation which Could Follow if Scholars were to Decode Mythology as Conveying a Shared Cultural History We Knew Nothing About / &quot;The Stakes Are High&quot;</b><br /><br />Comparative mythologist, David Talbott, has proposed that all of human history can be broken down into two stories.  The second one corresponds to the current era, but in contradiction to widespread cultural notions, we can already see that it is not the final story.  When Talbott invokes the possibility of a third human story, which seeks to unify and make sense of the first two, he is likely channeling the former work of Claude Levi-Strauss.<br /><br />So, to understand the third story, we must start with Strauss&#39; explanation of the first two human stories, which we will then follow with David Talbott&#39;s timeline of the progression.<br /><br />Part 3 will introduce the comparative mythology approach with one of the better, more compelling examples that I&#39;ve seen.<br /><br />I&#39;ll then conclude with an open letter to the scientific community which proposes that the third story was set in motion by the first, many thousands of years ago ...<br /><br /><b>Part 1: Excerpts from </b><b><i>Myth and Meaning</i></b><br />by Claude Levi-Strauss<br /><br /><a href=\"https://www.phil.muni.cz/~vndrzl/indians/culture/Strauss_MandM.pdf\" class=\"ot-anchor\">https://www.phil.muni.cz/~vndrzl/indians/culture/Strauss_MandM.pdf</a><br /><br /><b><i>&quot;The Meeting of Myth and Science</i></b><br /><br /><i>Let me start with a personal confession. There is a magazine which I read faithfully each month from the first line to the last, even though I don&#39;t understand all of it; it is the Scientific American. I am extremely eager to be as informed as possible of everything that takes place in modern science and its new developments. My position in relation to science is thus not a negative one.</i><br /><br /><b><i>Secondly, I think there are some things we have lost, and we should try perhaps to regain them, because I am not sure that in the kind of world in which we are living and with the kind of scientific thinking we are bound to follow, we can regain these things exactly as if they had never been lost; but we can try to become aware of their existence and their importance.</i></b><br /><br /><i>In the third place, my feeling is that </i><b><i>modern science is not at all moving away from these lost things, but that more and more it is attempting to reintegrate them in the field of scientific explanation.</i></b><i> The real gap, the real separation between science and what we might as well call mythical thought for the sake of finding a convenient name, although it is not exactly that - </i><b><i>the real separation occurred in the seventeenth and the eighteenth century. At that time, with Bacon, Descartes, Newton, and the others, it was necessary for science to build itself up against the old generations of mythical and mystical thought, and it was thought that science could only exist by turning its back upon the world of the senses, the world we see, smell, taste, and perceive; the sensory was a delusive world, whereas the real world was a world of mathematical properties which could only be grasped by the intellect and which was entirely at odds with the false testimony of the senses. This was probably a necessary move, for experience shows us that thanks to this separation - this schism if you like - scientific thought was able to constitute itself.</i></b><br /><br /><i>Now, my impression (and, of course, I do not talk as a scientist - I am not a physicist, I am not a biologist, I am not a chemist) is that </i><b><i>contemporary science is tending to overcome this gap, and that more and more the sense data are being reintegrated into scientific explanation as something which has a meaning, which has a truth, and which can be explained.</i></b><br /><br /><i>[...]</i><br /><br /><b><i>Mythical stories are, or seem, arbitrary, meaningless, absurd, yet nevertheless they seem to reappear all over the world. A &#39;fanciful&#39; creation of the mind in one place would be unique -- you would not find the same creation in a completely different place.</i></b><i> My problem was trying to find out if there was some kind of order behind this apparent disorder -- that&#39;s all. And I do not claim that there are conclusions to be drawn.</i><br /><br /><i>[...]</i><br /><br /><b><i>What I have been trying to say here is that there has been a divorce -- a necessary divorce -- between scientific thought and what I have called the logic of the concrete, that is, the respect for and the use of the data of the senses, as opposed to images and symbols and the like. We are witnessing the moment when this divorce will perhaps be overcome or reversed, because modern science seems to be able to make progress not only in its own traditional line -- pushing forward and forward but still within the same narrow channel -- but also at the same time to widen the channel and to reincorporate a great many problems previously left outside.</i></b><br /><br /><i>[...]</i><br /><br /><b><i>&#39;Primitive&#39; Thinking and the &#39;Civilized&#39; Mind</i></b><br /><br /><i>The way of thinking among people we call, usually and wrongly, &#39;primitive&#39; - </i><b><i>let&#39;s describe them rather as &#39;without writing,&#39; because I think this is really the discriminatory factor between them and us</i></b><i> - has been interpreted in two different fashions, both of which in my opinion were equally wrong. The first way was to consider such thinking as of a somewhat coarser quality, and in contemporary anthropology the example which comes to mind immediately is the work of Malinowski. I must say immediately that I have the greatest respect for him and consider him a very great anthropologist, and I&#39;m not at all deriding his contribution. But nevertheless </i><b><i>the feeling in Malinowski was that the thought of the people he was studying was, and generally speaking the thought of all the populations without writing which are the subject matter of anthropology was entirely, or is, determined by the basic needs of life. If you know that a people, whoever they are, is determined by the bare necessities of living - finding subsistence, satisfying the sexual drives, and so on - then you can explain their social institutions, their beliefs, their mythology, and the like. This very widespread conception in anthropology generally goes under the name of functionalism.</i></b><br /><br /><b><i>The other fashion is not so much that theirs is an inferior kind of thought, but a fundamentally different kind of thought.</i></b><i> This approach is exemplified by the work of Levy-Bruhl, who considered that the basic difference between &#39;primitive&#39; thought -- I always put the word &#39;primitive&#39; within quotes -- and modern thought is that the first is entirely determined by emotion and mystic representations. </i><b><i>Whereas Malinowski&#39;s is a utilitarian conception, the other is an emotional or affective conception; and what I have tried to emphasize is that actually the thought of people without writing is, or can be in many instances, on the one hand, disinterested</i></b><i> - and this is a difference in relation to Malinowski - and, on the other hand, intellectual - a difference in relation to Levy-Bruhl. What I tried to show in Totemism and in The Savage Mind, for instance, is that </i><b><i>these people whom we usually consider as completely subservient to the need of not starving, of continuing able just to subsist in very harsh material conditions, are perfectly capable of disinterested thinking; that is, they are moved by a need or a desire to understand the world around them, its nature and their society. On the other hand, to achieve that end, they proceed by intellectual means, exactly as a philosopher, or even to some extent a scientist, can and would do.</i></b><br /><br /><i>This is my basic hypothesis.</i><br /><br /><i>I would like to dispel a misunderstanding right away. To say that a way of thinking is disinterested and that it is an intellectual way of thinking does not mean at all that it is equal to scientific thinking. Of course, it remains different in a way, and inferior in another way. </i><b><i>It remains different because its aim is to reach by the shortest possible means a general understanding of the universe - and not only a general but a total understanding. That is, it is a way of thinking which must imply that if you don&#39;t understand everything, you don&#39;t explain anything. This is entirely in contradiction to what scientific thinking does, which is to proceed step by step, trying to give explanations for very limited phenomena, and then going on to other kinds of phenomena, and so on. As Descartes had already said, scientific thinking aimed to divide the difficulty into as many parts as were necessary in order to solve it.</i></b><br /><br /><i>So this totalitarian ambition of the savage mind is quite different from the procedures of scientific thinking. Of course, the great difference is that this ambition does not succeed. We are able, through scientific thinking, to achieve mastery over nature - I don&#39;t need to elaborate that point, it is obvious enough - while, of course, myth is unsuccessful in giving man more material power over the environment. </i><b><i>However, it gives man, very importantly, the illusion that he can understand the universe and that he does understand the universe. It is, of course, only an illusion.</i></b><br /><br /><b><i>We should note, however, that as scientific thinkers we use a very limited amount of our mental power. We use what is needed by our profession, our trade, or the particular situation in which we are involved at the moment. So, if somebody gets involved for twenty years and even more in the way myths or kinship systems operate, then he uses this part of his mental power. But we cannot request that each of us be interested in exactly the same things; so each of us uses a certain amount of our mental power for what is needed or for what interests us.</i></b><br /><br /><i>Today we use less and we use more of our mental capacity than we did in the past; And it is not exactly the same kind of mental capacity as it was either. For example, </i><b><i>we use considerably less of our sensory perceptions.</i></b><i> When I was writing the first version of Mythologiques (Introduction to a Science of Mythology), I was confronted with a problem which to me was extremely mysterious. It seems that there was a particular tribe which was able to see the planet Venus in full daylight, something which to me would be utterly impossible and incredible. I put the question to professional astronomers; they told me, of course, that we don&#39;t but, nevertheless, when we know the amount of light emitted by the planet Venus in full daylight, it was not absolutely inconceivable that some people could. Later on I looked into old treatises on navigation belonging to our own civilization and it seems that sailors of old were perfectly able to see the planet in full daylight. Probably we could still do so if we had a trained eye.</i><br /><br /><i>It is exactly the same with our knowledge about plants or animals. People who are without writing have a fantastically precise knowledge of their environment and all their resources. All these things we have lost, but we did not lose them for nothing; we are now able to drive an automobile without being crushed at each moment, for example, or in the evening to turn on our television or radio. This implies a training of mental capacities which &#39;primitive&#39; peoples don&#39;t have because they don&#39;t need them. I feel that, with the potential they have, they could have changed the quality of their mind, but it would not be needed for the kind of life and relationship to nature that they have. You cannot develop all the mental capacities belonging to mankind all at once. You can only use a small sector, and this sector is not the same according to the culture. That is all.</i><br /><br /><i>[...]</i><br /><br /><i>Now, I would not like you to think that I am putting scientific explanation and mythical explanation on an equal footing. </i><b><i>What I would say is that the greatness and the superiority of scientific explanation lies not only in the practical and intellectual achievement of science, but in the fact, which we are witnessing more and more, that science is becoming able to explain not only its own validity but also what was to some extent valid in mythological thinking. What is important is that we are becoming more and more interested in this qualitative aspect, and that science, which had a purely quantitative outlook in the seventeenth to nineteenth centuries, is beginning to integrate the qualitative aspects of reality as well. This undoubtedly will enable us to understand a great many things present in mythological thinking which we were in the past prone to dismiss as meaningless and absurd.</i></b><br /><br /><i>[...]</i><br /><br /><b><i>Harelips and Twins: The Splitting of a Myth</i></b><br /><br /><i>[...]</i><br /><br /><i>Many people have reproached me for this kind of procedure, claiming that myths of a given population can only be interpreted and understood in the framework of the culture of that given population. There are several things which I can say by way of an answer to that objection.</i><br /><br /><i>In the first place, it seems to me pretty obvious that, as was ascertained during recent years by the so-called Berkeley school, the population of the Americas before Columbus was much larger than it had been supposed to be. And since it was much larger, it is obvious that these large populations were to some extent in contact with one another, and that beliefs, practices, and customs were, if I may say so, seeping through. Any neighbouring population was always, to some extent, aware of what was going on in the other population. The second point in the case that we are considering here is that </i><b><i>these myths do not exist isolated in Peru on the one hand and in Canada on the other, but that in between we find them over and over again. Really, they are pan-American myths, rather than scattered myths in different parts of the continent.</i></b><br /><br /><i>[...]</i><br /><br /><b><i>When Myth Becomes History</i></b><br /><br /><i>This topic presents two problems for the mythologist. One is a theoretical problem of great importance because, </i><b><i>when we look at the published material both in North and South America and elsewhere in the world, it appears that the mythic material is of two different kinds. Sometimes, anthropologists have collected myths which look more or less like shreds and patches, if I may say so; disconnected stories are put one after the other without any clear relationship between them. In other instances, as in the Vaupes area of Colombia we have very coherent mythological stories, all divided into chapters following each other in a quite logical order.</i></b><br /><br /><i>And then we have the question: what does a collection mean? It could mean two different things. It could mean, for instance, that the coherent order, like a kind of saga, is the primitive condition, and that whenever we find myths as disconnected elements, this is the result of a process of deterioration and disorganization; we can only find scattered elements of what was, earlier, a meaningful whole. Or we could hypothesize that the disconnected state was the archaic one, and that the myths were put together in an order by native wise men and philosophers who do not exist everywhere, but only in some societies of a given type. We have exactly the same problem, for instance, with the Bible, because it seems that its raw material was disconnected elements and that learned philosophers put them together in order to make a continuous story.</i><br /><br /><i>[...]</i><br /><br /><i>[T]he simple opposition between mythology and history which we are accustomed to make -- is not at all a clear-cut one, and that there is an intermediary level. </i><b><i>Mythology is static, we find the same mythical elements combined over and over again, but they are in a closed system, let us say, in contradistinction with history, which is, of course, an open system.</i></b><br /><br /><i>The open character of history is secured by the innumerable ways according to which mythical cells, or explanatory cells which were originally mythical, can be arranged and rearranged. It shows us that by using the same material, because it is a kind of common inheritance or common patrimony of all groups, of all clans, or of all lineages, one can nevertheless succeed in building up an original account for each of them. What is misleading in the old anthropological accounts is that a kind of hodge-podge was made up of tradition and beliefs belonging to a great many different social groups. This makes us lose sight of a fundamental character of the material -- that each type of story belongs to a given group, a given family, a given lineage, or to a given clan, and is trying to explain its fate, which can be a successful one or a disastrous one, or be intended to account for rights and privileges as they exist in the present, or be attempting to validate claims for rights which have since disappeared.</i><br /><br /><i>When we try to do scientific history, do we really do something scientific, or do we too remain astride our own mythology in what we are trying to make as pure history? </i><b><i>It is very interesting to look at the way both in North and South America, and indeed everywhere in the world, in which an individual, who has by right and by inheritance a certain account of the mythology or the legendary tradition of his own group, reacts when he listens to a different version given by somebody belonging to a different family or to a different clan or lineage, which to some extent is similar but to some extent too is extremely different. Now, we would think that it is impossible that two accounts which are not the same can be true at the same time, but nevertheless, they seem to be accepted as true in some cases, the only difference made is that one account is considered better or more accurate than the other. In other cases, the two accounts can be considered equally valid because the differences between them are not perceived as such.</i></b><i> We are not at all aware in our daily life that we are exactly in the same situation in relation to different historical accounts written by different historians. We pay attention only to what is basically similar, and we neglect the differences due to the fact that the way historians carve the data and the way they interpret them are not exactly the same. So if you take two accounts by historians, with different intellectual traditions and different political leanings, of such events as the American Revolution, of the French-English war in Canada, or the French Revolution, we are not really so shocked that they don&#39;t tell us exactly the same thing.</i><br /><br /><i>Thus my impression is that </i><b><i>by studying carefully this history, in the general sense of the word, which contemporary Indian authors try to give us of their own past, by not considering this history as a fanciful account, but by trying extremely carefully, with the help of a type of salvage archeology -- excavating village sites referred to in the histories -- and by trying to establish correspondences, inasmuch as this is possible, between different accounts, and by trying to find what really corresponds and what does not correspond, we may in the end reach a better understanding of what historical science really is.</i></b><br /><br /><i>I am not far from believing that, in our own societies, history has replaced mythology and fulfills the same function, that </i><b><i>for societies without writing and without archives the aim of mythology is to ensure that as closely as possible - complete closeness is obviously impossible - the future will remain faithful to the present and to the past. For us, however, the future should be always different, and ever more different, from the present</i></b><i>, some difference depending, of course, on our political preferences. But nevertheless the gap which exists in our mind to some extent between mythology and history can probably be breached by studying histories which are conceived as not at all separated from but a continuation of mythology.</i><br /><br /><b><i>Myth and Music</i></b><br /><br /><b><i>[I]t is impossible to understand a myth as a continuous sequence. This is why we should be aware that if we try to read a myth as we read a novel or a newspaper article, that is line after line, reading from left to right, we don&#39;t understand the myth, because we have to apprehend it as a totality and discover that the basic meaning of the myth is not conveyed by the sequence of events but - if I may say so - by bundles of events even although these events appear at different moments in the story. Therefore, we have to read the myth more or less as we would read an orchestral score, not stave after stave, but understanding that we should apprehend the whole page and understand that something which was written on the first stave at the top of the page acquires meaning only if one considers that it is part and parcel of what is written below on the second stave, the third stave, and so on. That is, we have to read not only from left to right, but at the same time vertically, from top to bottom. We have to understand that each page is a totality. And it is only by treating the myth as if it were an orchestral score, written stave after stave, that we can understand it as a totality, that we can extract the meaning out of the myth.</i></b><br /><br /><i>Why and how does this happen? My feeling is that it is the second aspect, the aspect of contiguity, which gives us the significant clue. As a matter of fact, </i><b><i>it was about the time when mythical thought - I would not say vanished or disappeared - but passed to the background in western thought during the Renaissance and the seventeenth century, that the first novels began to appear instead of stories still built on the model of mythology. And it was exactly at that time that we witnessed the appearance of the great musical styles characteristic of the seventeenth and, mostly, the eighteenth and nineteenth centuries.</i></b><br /><br /><b><i>It is exactly as if music had completely changed its traditional shape in order to take over the function - the intellectual as well as emotive function - which mythical thought was giving up more or less at the same period.</i></b><i> When I speak here of music, I should, of course, qualify the term. The music that took over the traditional function of mythology is not any kind of music, but music as it appeared in western civilization in the early seventeenth century with Frescobaldi and in the early eighteenth century with Bach, music which reached its full development with Mozart, Beethoven, and Wagner in the eighteenth and nineteenth centuries.</i><br /><br /><i>What I would like to do in order to clarify this statement is to offer a concrete example, which I shall take from Wagner&#39;s tetralogy, The Ring. One of the most important musical themes in the tetralogy is the one which we call in French &#39;le theme de la renunciation a l&#39;amour&#39; - the renunciation of love. As is well known, this theme appears first of all in the Rhinegold at the moment when Alberich is told by the Rhine maidens that he can conquer the gold only if he renounces all kind of human love. This very startling musical motif is a sign to Alberich, given at the very moment when he says that he takes the gold but he renounces love once and for all. All this is very clear and simple; it is the literal sense of the theme: Alberich is renouncing love.</i><br /><br /><i>Now the second striking and important moment when the theme reappears is in the Valkyrie in a circumstance which makes it extremely difficult to understand why. At the moment when Siegmund has just discovered that Sieglinde is his sister and has fallen in love with her, and just when they are going to initiate an incestuous relationship, thanks to the sword which is buried in the tree and which Siegmund is going to tear away from the tree - at that moment, the theme of the renunciation of love reappears. This is some kind of a mystery, because at that moment Siegmund is not at all renouncing love - he&#39;s doing quite the opposite and knowing love for the first time of his life with his sister Sieglinde.</i><br /><br /><i>The third appearance of the theme is also in the Valkyrie, in the last act when Wotan, the king of the gods, is condemning his daughter Brunhilde to a very long magical sleep and surrounding her with fire. We could think that Wotan is also renouncing love because he is renouncing his love for his daughter; but this is not very convincing.</i><br /><br /><b><i>Thus you see that we have exactly the same problem as in mythology; that is, we have a theme - here a musical theme instead of a mythological theme - which appears at three different moments in a very long story</i></b><i>: once at the beginning, once in the middle, and once at the end, if for the sake of the argument we limit ourselves to the first two operas of The Ring. What I would like to show is that </i><b><i>the only way of understanding this mysterious reappearance of the theme is, although they seem very different, to put the three events together, to pile them up one over the other, and to try to discover if they cannot be treated as one and the same event.</i></b><br /><br /><i>We can then notice that, on the three different occasions, there is a treasure which has to be pulled away or torn away from what it is bound to. There is the gold, which is stuck in the depths of the Rhine; there is the sword, which is stuck in a tree, which is a symbolic tree, the tree of life or the tree of the universe; and there is the woman Brunhilde, who will have to be pulled out of the fire. The recurrence of the theme then suggests to us that, as a matter of fact, the gold, the sword, and Brunhilde are one and the same: the gold as a means to conquer power, the sword as a means to conquer love, if I may say so. And the fact that we have a kind of coalescence between the gold, the sword, and the woman is, as a matter of fact, the best explanation we have of the reason why, at the end of the Twilight of the Gods, it is through Brunhilde that the gold will return to the Rhine; </i><b><i>they have been one and the same, but looked at through different angles.</i></b><br /><br /><i>[...]</i><br /><br /><b>Part 2: David Talbott, Seeking the Third Story, Electric Universe 2012 Conference</b><br /><br /><a href=\"https://youtu.be/gf7hTSWw0sc?t=160\" class=\"ot-anchor\">https://youtu.be/gf7hTSWw0sc?t=160</a><br /><br /><b><i>&quot;[T]here really is something wrong with our cultural and scientific environment today.  And there is a growing yearning for a deeper understanding -- deeper and more clear, and more coherent, and unified, and non-contradictory and non-fragmented -- way of seeing the universe ...&quot;</i></b><br /><br /><i>&quot;... How did human history begin?</i><br /><br /><i>The evidence may seem too complex for generalization.  Yet, all of history can be stated as just two stories.</i><br /><br /><i>First came the story of mythology, filled with cosmic dramas and celestial wonders.  We lived in the presence of the gods.  It was the gods who determined the fate of the world, demanding allegiance even as they called humans to sacrifice.  The gods were unpredictable.  When angry, they went to war.  Or they destroyed their own creation.</i><br /><br /><b><i>Over time, however, man&#39;s quest for deeper meaning in the myths ignited sparks of spiritual awakening.  And it was from these sparks that great religions arose.</i></b><br /><br /><b><i>But, all of the world&#39;s religions shared a tap root in mythology.  And not one freed itself entirely from the momentum of the first story.</i></b><br /><br /><b><i>The second story arose from doubt, as philosophers, poets and naturalists began to lose faith in the myths.  And some stopped believing in the gods altogether.</i></b><br /><br /><b><i>That was because of direct experience under a familiar and highly predictable sky gave no support to the archaic myths.</i></b><br /><br /><b><i>Skepticism inspired closer attention to nature -- the essential requirement for the rise of science and technology that would change the world; thus, the heroic completion of the second story: rational science rising to vanquish myth and magic.</i></b><br /><br /><b><i>And so the full sweep in human history comes down to just two stories in competition -- a cliche: science vs myth and religion.  Well into the 20th century, it seemed that we were forced to choose.</i></b><br /><br /><b><i>But a few have wondered: Could the myths harbor a truth we have missed?  Something yet to be discovered?  A message to science hidden in the myths and symbols of deepest antiquity? ...&quot;</i></b><br /><br /><b><i>&quot;... You don&#39;t start with mathematics.  You get the interdisciplinary picture by seeing patterns and understanding what is going on at a practical, common sense level.  And you direct the mathematicians to begin working with these patterns which are telling us something is fundamentally wrong.</i></b><br /><br /><b><i>Don&#39;t just keep elaborating a previous idea mathematically to make things work.</i></b><br /><br /><b><i>And that means we need a broader field of view ...&quot;</i></b><br /><br /><b><i>&quot;... If you&#39;re always looking an inch wide and a mile deep, how well are you able to see the evidence shouting at you from the broader horizon of your science?</i></b><br /><br /><b><i>It&#39;s telling you that your focus is too narrow, and you&#39;re missing the things that actually falsify your assumption.&quot;</i></b><br /><br />An explanation of comparative mythology by virtue of an example ...<br /><br /><b>Part 3: Mythology at Stake</b><br />Rens van der Sluijs (<a href=\"http://mythopedia.info\" class=\"ot-anchor\">http://mythopedia.info</a>)<br /><br /><a href=\"https://www.thunderbolts.info/wp/2011/09/12/mythology-at-stake/\" class=\"ot-anchor\">https://www.thunderbolts.info/wp/2011/09/12/mythology-at-stake/</a><br /><br />Sep 13, 2011<br /><br /><b><i>&quot;The central challenge in comparative mythology is to account for the many similarities between traditions from different cultures and ages.</i></b><br /><br /><b><i>This task is all the more daunting in cases where the pertinent cultures are not known to have been in touch with each other, even indirectly, and in those where the content of the shared themes is not immediately obvious from the perspective of the local culture or environment.</i></b><br /><br /><i>A vivid example of the striking degree of convergence encountered so often in this field concerns the following two traditions from respectively ancient Sumer, recorded on clay tablets some 4,000 years ago, and New South Wales, Australia, delivered orally sometime prior to 1925. Both belong to the prominent class of &#39;origin myths&#39; and describe how some mythical being lifted up the sky from the earth by means of a golden rod-like implement.</i><br /><br /><i>In the Sumerian tale, the god Enlil, also styled Gibil, separates heaven and earth with his al or &#39;hoe&#39;:</i><br /><br /><i>&#39;Then Enlil praised his hoe (al), his hoe (al) wrought in gold, its top inlaid with lapis lazuli, his hoe (al) whose blade was tied on with a cord, which was adorned with silver and gold, his hoe (al), the edge of whose point (?) was a plough of lapis lazuli, whose blade was like a battering ram {standing up to a great (gal) wall} … Gibil made his hoe (al) raise its head towards the heavens – he caused the hoe (al), sacred indeed, to be refined with fire.&#39;</i><br /><br /><i>The deity allegedly did so at a sacred place called uzu-è-a, &#39;where flesh came forth,&#39; raising or suspending the bulug, the &#39;axis of the world,&#39; at Dur-an-ki in the ancient Sumerian city of Nippur. The idea may have been that the hoe itself transformed into this column.</i><br /><br /><i>A remarkably close parallel to this tradition is furnished by a myth from the people from the headwaters of the Murrumbidgee River, a branch of the Wiradjuri, in central New South Wales. In this, a &#39;rod made of gold&#39; once enabled an anonymous chief to lift up the sky by its magic protrusion:</i><br /><br /><i>&#39;It surely was a rod made of gold, for the blacks say that it was very bright and of the colour of gold. … He lifted it up; and, behold! As it touched the sky, the sky went on and up before it. And the rod grew. … So the sky went on up as far as the rod grew, and for as long as the chief pushed it.&#39;</i><br /><br /><i>The elevation of the sky, the prodigious expansion of a sky pillar and the golden stanchion of the sky are archetypal motifs that recur and combine worldwide in a seemingly endless kaleidoscope of narrative variants. </i><b><i>Is the occasional occurrence of near-identical versions amid this dazzling phantasmagoria a mere result of coincidence? Or does the precise embedding of myths such as the above in a refined structural edifice of cross-cultural &#39;creation mythology&#39; suggest that something more fundamental is at stake?</i></b><br /><br /><b><i>Just as in other disciplines that handle vast amounts of &#39;raw data,&#39; such as comparative linguistics and paleontology, the recognition of patterns functions as an indispensable pillar of methodology.</i></b><i> Yet whereas meanings lie beyond words and the bodies of living animals beyond dead bones, the mythologist may at times feel &#39;poleaxed&#39; by the bafflingly complex world of elusive forces and agents evoked in this intricate network of corresponding stories and beliefs. Just what is the subject matter of mythology in the real world preceding the myths?</i><br /><br /><i>In his monumental The Golden Bough, the famed Scottish anthropologist and belletrist, Sir James George Frazer (1854-1941), staked his claim that the mind everywhere evolves along similar lines, thus translating similar experiences in nature into similar superstitions and institutions. For all its elegance, however, this hypothesis does not stand up to the light of day.</i><br /><br /><i>It may account for some elementary traits of archaic folklore, such as that the sun journeys beneath the surface of the earth at night or is accosted by a monster during eclipses, </i><b><i>but it fails spectacularly when it comes to a clarification of the hundreds of interconnected themes constituting the global nexus of origin myths.</i></b><i> Few would stake their lives on the accuracy of the many tortuous &#39;naturalistic&#39; explanations that have been advanced for universal archetypes by those that followed in Frazer’s footsteps.</i><br /><br /><b><i>A far more attractive proposition is that such myths find their original inspiration in the collective experience of a series of conspicuous transient events occurring in the earth’s atmosphere and above. With a nod to Frazer, the perplexing resemblance many of these myths bear to each other springs from the tendency of the human mind to interpret a similar display of forms, colours and motions using similar metaphors.</i></b><i> Meteorites, bolides, comets, eclipses, aurorae – all such phenomena were universally mythologised in terms of similar symbols. Likewise, a far more dramatic &#39;celestial pageant&#39; will have suggested to Sumerians and proto-Wiradjuri alike that someone lifted up the sky by means of a golden staff. </i><b><i>On that solid foundation, interdisciplinarians can subsequently debate what physical conditions could have provoked such imagery.</i></b><i> Does a meteorological-optical effect, such as a &#39;sun pillar&#39; occasionally seen at sunrise, do the trick? Or does the evidence point to the erstwhile formation and collapse of a glowing plasma z-pinch in the earth’s atmosphere and beyond? </i><b><i>The stakes are high.&quot;</i></b><br /><br />And now my own thoughts on the &quot;third story&quot; ...<br /><br /><b>Part 4: An Open Letter to the Scientific Community on Mythology</b><br /><br />Dear Scientists,<br /><br />When the first people who told stories, the Pagans, all at once began telling similar stories across the globe, there of course was no writing because it hadn&#39;t been invented yet.  This was around 5,000 years ago.<br /><br />If something happened to them which was sufficiently important that they should be struck by the need to inform future generations -- I&#39;m talking about us -- it would have to be conveyed orally.<br /><br />The popular idea today that there is no meaning within these first stories is defied by the mere existence of the oral tradition which kept them alive for 5,000 years: Each person who memorized and conveyed the story weighed in on their own perception of the story&#39;s value through their actions.<br /><br />Great civilizations have risen and fallen since the first stories were told.  But, those first stories -- the original myths -- were the inspiration for most of today&#39;s modern entertainment -- not just music, but also the modern novel and the modern movie.  They also, many would argue unfortunately, saddled us with numerous religions.  Their legacy carries on to this day at the core of our own culture.<br /><br />Yet, few people in today&#39;s modern society can trace all of these activities back to their formative inspiration.  The origin has largely been forgotten.<br /><br />But, what I am here to propose is that if comparative mythologists are correct, then the Pagans in fact succeeded at what they set out to do.  Starting from the premise that all of those stories refer to a single event, the stories of mythology are collectively brilliant because they survived.<br /><br />Their brilliance becomes obvious once a person simply asks why these stories have survived.<br /><br />The point of passing down a bazillion different stories that are in fact analogies of just one single event (or event category) is that the event, and the lessons and numerous details pertaining to it, can become permanently recorded in the minds of the people.  No writing tools are necessary, because the stories themselves act as mnemonic devices.  By anthropomorphizing the event in a bunch of ways, the mind&#39;s tendency to recall on the basis of emotional content is taken advantage of.<br /><br />Put another way, our minds developed to record things that are pertinent to our daily survival; the fact that this same system could be dual-purposed to record some event which former people might have witnessed in the sky is a stunning hypothesis.  The fact that modern scholars can pick apart these stories and come to any specific conclusions at all about what that event must have been like is so incredible that many simply reject it before even hearing of the evidence.<br /><br />But it seems that the Pagans did it.<br /><br />Realize that they may have also simply explained what occurred in plain terms -- without dressing the story up in human drama -- but the mnemonics would not work in that case.  There would be no emotional content to that form, and those stories would necessarily be less likely to survive.<br /><br />The popular assumption today that the original storytellers were confused or ignorant is not just arrogance; it is also not in the spirit one bit of acting as a steward of the human race to assume it (for, after all, the stories commonly convey catastrophe).  Do not forget that scholars have been documenting, discussing and debating the astronomical nature of mythology since the era when meteors were considered impossible.<br /><br />Science itself has progressed in dramatic fashion during this same time period, and even as of recently ...<br /><br />- The hypothesis that the dinosaurs and ammonites may have suffered from an extinction event is now widely accepted.<br /><br />- The observation of a comet striking into Jupiter had a big impact upon the scientific community&#39;s rejection of global catastrophe.<br /><br />- The recent find of 8 mammoth tusks peppered with small meteorites and a mammoth carcass partially draped by a black radioactive mat (see Firestone) possibly places the most recent event in human-historical times (for it is agreed that mammoths only recently became extinct).<br /><br />- The relatively recent recognition that some of the meteorites that fall to Earth are from Mars was unexpected because it was earlier assumed that Martian rocks could never achieve escape velocity without becoming completely vaporized; but today, it is acknowledged through scientific analysis of both the rocks and the planet.<br /><br />Science has dramatically changed, yet scholars have been reporting back on the astronomical nature of mythology the whole time.<br /><br />Their message has been surprisingly consistent.  But at various stages of the progression of scientific theory, the scientists tell us that the mythology scholars must be wrong. Truthfully, these denials have been a moving target.<br /><br />Although the argument is never put in this manner, the Electric Universe idea -- which seeks to inspire a more thoughtful approach to mythology -- conjectures a Herbig-Haro configuration of planets that ends in a capture event.  They call it a conjunction (from the perspective of Earth, a polar conjunction).<br /><br />Many people have tried to claim that such a coaxial configuration is impossible, but Herbig-Haro objects are astronomically observed. Nature has already provided the proof that it can be.<br /><br />A second premise of the EU approach is that mythology can possibly be interpreted as a story about a stellar capture event.  And that analysis long preceded the recent observations of wandering &#39;rogue&#39; red/brown dwarf stars.  Today, scientists understand that capture events must accordingly be more common than we used to believe.<br /><br />Third, the Herschel telescope has -- quite recently -- shown protostars scattering like buckshot at the end of the star formation process.  Such an observation can, of course, explain why the dwarf stars are observed flying across space, to begin with.<br /><br />That&#39;s three separate changes to scientific theory.<br /><br />Each were unexpected.<br /><br />That&#39;s three unrelated astronomical observations.<br /><br />Yet, the combination of all three is the physical basis for the EU interpretation of mythology -- the result of simply taking the mythological stories at face value.<br /><br />It should be clear that there&#39;s nothing at all unscientific about such an approach in the least.  It&#39;s innovative theory-making.  It suffers, in part, from the the cultural delay of astronomical observations trickling through society.  People still don&#39;t know what a Herbig-Haro even is.<br /><br />The fact is that, despite tremendous effort, the scientific community failed to completely do away with the mythological approach to cosmology in the 70&#39;s.  The approach never actually died.<br /><br />And in fact the reason why people continue to pay it heed is because this approach suggests a path towards explaining the oldest mythological archetypes through science.  It works well enough to act as a mnemonic for those stories -- in reverse, essentially, of what the Pagans created.<br /><br />Put simply, the reason why the Pagans were brilliant is because the message has arrived to YOU.<br /><br />Now, having received the message, you may simply choose to ignore it.  But, the point was not to force you to listen; if comparative mythologists are correct, the point was to simply <b>deliver</b> the message.<br /><br />Whether or not it matters that most people today, scientists included, ignore the message seems left to chance.<br /><br />What is particularly remarkable about the scientific community&#39;s claim that the stories largely lack literal-historical content is their refusal to cultivate an expertise in comparative mythology. As is often the case in science debates, the critics of mythology frequently do not actually stake any claim of expertise in that which they nevertheless critique.<br /><br />It&#39;s really the ultimate expression of disrespect for our own ancestors to throw away a message which traveled for 5,000 years to get here.<br /><br />The message in the bottle arrived, and you smashed it on the ground like a spoiled child because it challenged your own (largely inherited) notions of what the universe is -- notions which, remarkably, had you lived just 30 years earlier, would have been different, no less.<br /><br />The fact is that the stories of mythology will persist for consideration until the message is thoughtfully received.<br /><br />The Pagans&#39; brilliance is that they figured out how to get their message past this current scientific community thousands of years before any of it even existed: They ensured that despite your own advocacy against people seeking to understand their own ancestors&#39; message, people will nevertheless continue to tell the first stories that man told.<br /><br />The irony is that few people will remember specific critiques of mythology in even a couple of days from hearing of them.  The problem is that these critiques are just not as memorable as the original stories.<br /><br />People will continue to tell the stories of the archetypes a thousand years from today.  And those stories will continue on until they are understood.<br /><br />Their mission is already, in a sense, destined to be accomplished.<br /><br />When the scientific community ridicules our first stories without actually gaining an expertise in them, what this means for history is that this community has made itself irrelevant to the larger historical process.<br /><br />The third story of human history will begin once most people, including scientists, realize that there exists much unconsidered information within these first human stories.<br /><br />People will one day take notice of the scientific support for claims of catastrophe in human-historical times.  Eventually, that will become accepted.<br /><br />That is when the real time of humans in space will begin.  Although we call the current era the Space Age, true colonization of space will require a far deeper understanding of cosmic plasmas and space phenomena than we possess today.  And that level of knowledge will require an understanding not just of what is happening today, but also an accurate understanding of what has happened in the recent past ... In other words, the full range of cosmic plasma phenomena, as described by human testimony from thousands of years ago and as encoded in the myths.<br /><br />We&#39;re not quite there yet.  We remain in the Second Story.<br /><br />(Dedicated to the memory of Dwardu Cardona, who has perhaps done more to preserve the message of the Pagans than any other modern human.)<br /><br />Sincerely,<br /><br />Chris Reeve<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Trouble with Sand's Color","summary":"Sand's Color Comes from Iron / But Iron is Not Involved in the Process of Creating or Transporting the Sand / This Thin Varnish of Iron-Oxide is Possibly a Game-Changer for the Theory of Sand / One Theorist, Gary Gilligan, is Convinced it's Evidence that Earth's Sand Comes from Mars / Is He Right?!","image":"https://lh3.googleusercontent.com/-u3T1tKB-FZ0/V6H5lYqfltI/AAAAAAAAIbA/USwokJkw4ocmFyQrUwiSOK8VxemMtUvWw/w7142-h9999/the-trouble-with-sands-color-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/ecYwtQL4GKV","publishDate":"2016-08-03T14:25:38.983Z","updateDate":"2016-08-03T14:25:38.983Z","text":"<b>The Trouble with Sand&#39;s Color: Sand&#39;s Color Comes from Iron / But Iron is Not Involved in the Process of Creating or Transporting the Sand / This Thin Varnish of Iron-Oxide is Possibly a Game-Changer for the Theory of Sand / One Theorist, Gary Gilligan, is Convinced it&#39;s Evidence that Earth&#39;s Sand Comes from Mars / Is He Right?!</b> <br /><br />This graphic is based upon the theory presented in ...<br /><br /><i>Extraterrestrial Sands</i><br />by Gary Gilligan<br />p.150-177<br /><br />REFERENCES:<br /><br />[1] <a href=\"https://books.google.com/books?id=i_YEDAAAQBAJ&amp;pg=PA152&amp;lpg=PA152&amp;dq=extraterrestrial+sands+all+agree+the+colour+derives+from+the+staining+of+individual+grains&amp;source=bl&amp;ots=bnle6OOfDy&amp;sig=XywcvXhQcOsEhFYcim76j7884Yc&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjm3oGItaXOAhUN42MKHUiUCrsQ6AEIHDAA#v=onepage&amp;q=extraterrestrial%20sands%20all%20agree%20the%20colour%20derives%20from%20the%20staining%20of%20individual%20grains&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=i_YEDAAAQBAJ&amp;pg=PA152&amp;lpg=PA152&amp;dq=extraterrestrial+sands+all+agree+the+colour+derives+from+the+staining+of+individual+grains&amp;source=bl&amp;ots=bnle6OOfDy&amp;sig=XywcvXhQcOsEhFYcim76j7884Yc&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjm3oGItaXOAhUN42MKHUiUCrsQ6AEIHDAA#v=onepage&amp;q=extraterrestrial%20sands%20all%20agree%20the%20colour%20derives%20from%20the%20staining%20of%20individual%20grains&amp;f=false</a><br /><br />[2] <i>Desert Geomorphology</i>. Cooke, Warren, Goudie, 1993 p316<br /><br />[3] <a href=\"http://journals.fcla.edu/jcr/article/view/80685/77867\" class=\"ot-anchor\">http://journals.fcla.edu/jcr/article/view/80685/77867</a><br /><br />[4] <i>Desert Geomorphology</i>. Cooke, Warren, Goudie, 1993 p314<br /><br />[5] <a href=\"http://indiana.edu/~hydrogeo/Penn,%20Zhu%20et%20al-2001-Geology.pdf\" class=\"ot-anchor\">http://indiana.edu/~hydrogeo/Penn,%20Zhu%20et%20al-2001-Geology.pdf</a><br /><br />[6] <a href=\"https://books.google.com/books?id=yrY-DAAAQBAJ&amp;pg=PA159&amp;lpg=PA159&amp;dq=Since+geologists+are+short+of+answers,+a+somewhat+vague,+sweeping+statement+extraterrestrial+sands&amp;source=bl&amp;ots=SMtmShnOCy&amp;sig=P0c-Yx0RTLrxY736qxHNwakqqXk&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiYtOuJjp_OAhVW6WMKHWI6D_YQ6AEIHDAA#v=onepage&amp;q=since%20geologists%20are%20short%20of%20answers%2C%20a%20somewhat%20vague&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=yrY-DAAAQBAJ&amp;pg=PA159&amp;lpg=PA159&amp;dq=Since+geologists+are+short+of+answers,+a+somewhat+vague,+sweeping+statement+extraterrestrial+sands&amp;source=bl&amp;ots=SMtmShnOCy&amp;sig=P0c-Yx0RTLrxY736qxHNwakqqXk&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiYtOuJjp_OAhVW6WMKHWI6D_YQ6AEIHDAA#v=onepage&amp;q=since%20geologists%20are%20short%20of%20answers%2C%20a%20somewhat%20vague&amp;f=false</a><br /><br />[7] <a href=\"http://www.bu.edu/remotesensing/files/pdf/174.pdf\" class=\"ot-anchor\">http://www.bu.edu/remotesensing/files/pdf/174.pdf</a><br /><br />[8] [9] <a href=\"https://malagabay.wordpress.com/2016/04/26/guest-post-by-gary-gilligan/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2016/04/26/guest-post-by-gary-gilligan/</a><br /><br />[10] <a href=\"https://books.google.com/books?id=i_YEDAAAQBAJ&amp;pg=PA164&amp;dq=the+first+question+that+jumps+out+is+what+happened+to+its+colour?&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiExIvioZ_OAhVPw2MKHQZKDFIQ6AEIHjAA#v=onepage&amp;q=the%20first%20question%20that%20jumps%20out%20is%20what%20happened%20to%20its%20colour%3F&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=i_YEDAAAQBAJ&amp;pg=PA164&amp;dq=the+first+question+that+jumps+out+is+what+happened+to+its+colour?&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiExIvioZ_OAhVPw2MKHQZKDFIQ6AEIHjAA#v=onepage&amp;q=the%20first%20question%20that%20jumps%20out%20is%20what%20happened%20to%20its%20colour%3F&amp;f=false</a><br /><br />[11] <a href=\"http://www.livescience.com/8889-sand-grains-african-desert-1-million-years.html\" class=\"ot-anchor\">http://www.livescience.com/8889-sand-grains-african-desert-1-million-years.html</a><br /><br />[12] <a href=\"http://written-in-stone-seen-through-my-lens.blogspot.com/2013/02/the-great-sand-dunes-of-colorado-part-i.html\" class=\"ot-anchor\">http://written-in-stone-seen-through-my-lens.blogspot.com/2013/02/the-great-sand-dunes-of-colorado-part-i.html</a><br /><br />[13] [14] <a href=\"https://books.google.com/books?id=i_YEDAAAQBAJ&amp;pg=PA151&amp;dq=most+sand+grains+are+coated+with+a+thin+glaze+%28varnish%29&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjHwIbotqXOAhVQwGMKHeHOAVIQ6AEIJzAA#v=onepage&amp;q=most%20sand%20grains%20are%20coated%20with%20a%20thin%20glaze%20%28varnish%29&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=i_YEDAAAQBAJ&amp;pg=PA151&amp;dq=most+sand+grains+are+coated+with+a+thin+glaze+(varnish)&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjHwIbotqXOAhVQwGMKHeHOAVIQ6AEIJzAA#v=onepage&amp;q=most%20sand%20grains%20are%20coated%20with%20a%20thin%20glaze%20(varnish)&amp;f=false</a><br /><br />[15] <a href=\"http://indiana.edu/~hydrogeo/Penn,%20Zhu%20et%20al-2001-Geology.pdf\" class=\"ot-anchor\">http://indiana.edu/~hydrogeo/Penn,%20Zhu%20et%20al-2001-Geology.pdf</a><br /><br />[16] <a href=\"http://www.space.com/16999-mars-red-planet.html\" class=\"ot-anchor\">http://www.space.com/16999-mars-red-planet.html</a><br /><br />[17] <a href=\"https://books.google.com/books?id=yrY-DAAAQBAJ&amp;pg=PA167&amp;lpg=PA167&amp;dq=%22every+object+has+a+net+charge.+that+net+charge+can+be+positive,+negative+or+zero.+quartz+grains+are+considered+as+having+a+negative%22+extraterrestrial+sands&amp;source=bl&amp;ots=SMtmTkoQDx&amp;sig=YXltKSedvSsUqWe4dCOHxLENFjE&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiH5Jbv9qLOAhXJKGMKHQlqApAQ6AEIHDAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=yrY-DAAAQBAJ&amp;pg=PA167&amp;lpg=PA167&amp;dq=%22every+object+has+a+net+charge.+that+net+charge+can+be+positive,+negative+or+zero.+quartz+grains+are+considered+as+having+a+negative%22+extraterrestrial+sands&amp;source=bl&amp;ots=SMtmTkoQDx&amp;sig=YXltKSedvSsUqWe4dCOHxLENFjE&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiH5Jbv9qLOAhXJKGMKHQlqApAQ6AEIHDAA#v=onepage&amp;q&amp;f=false</a><br /><br />[18] [19] <a href=\"https://malagabay.wordpress.com/2016/04/28/gary-gilligan-extraterrestrial-sands/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2016/04/28/gary-gilligan-extraterrestrial-sands/</a><br /><br />[20] - [22] <a href=\"https://malagabay.wordpress.com/2016/04/28/gary-gilligan-extraterrestrial-sands/\" class=\"ot-anchor\">https://malagabay.wordpress.com/2016/04/28/gary-gilligan-extraterrestrial-sands/</a><br /><br />[23] <a href=\"http://sed.utah.edu/Navajo.htm\" class=\"ot-anchor\">http://sed.utah.edu/Navajo.htm</a><br /><br />[24] [25] <a href=\"http://www.whoi.edu/news-release/dissolved-iron\" class=\"ot-anchor\">http://www.whoi.edu/news-release/dissolved-iron</a><br /><br />[26] The Origins of the Latin God Mars by Ev Cochrane<br /><br />[27] See some more discussion at <a href=\"http://www.space.com/16105-asteroid-belt.html\" class=\"ot-anchor\">http://www.space.com/16105-asteroid-belt.html</a><br /><br />[28] There are more image comparisons in an article at <a href=\"http://www.businessinsider.com/photos-of-mars-landscapes-by-nasas-curiosity-rover-2015-6\" class=\"ot-anchor\">http://www.businessinsider.com/photos-of-mars-landscapes-by-nasas-curiosity-rover-2015-6</a><br /><br />[29] <a href=\"http://www.webexhibits.org/causesofcolor/5D.html\" class=\"ot-anchor\">http://www.webexhibits.org/causesofcolor/5D.html</a><br /><br />[30] <a href=\"http://www.smithsonianjourneys.org/blog/how-the-red-sea-got-its-name-180950850/\" class=\"ot-anchor\">http://www.smithsonianjourneys.org/blog/how-the-red-sea-got-its-name-180950850/</a><br /><br />[31] <a href=\"http://phys.org/news/2016-06-dutch-crops-grown-mars-soil.html\" class=\"ot-anchor\">http://phys.org/news/2016-06-dutch-crops-grown-mars-soil.html</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23mars\">#mars</a>   <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23geology\">#geology</a>   <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23sand\">#sand</a>  ﻿","category":"unknown"},{"name":"The Questionable Origins of Earth's Sand","summary":"Geologists Tell Us that the Vast 100,000 Cubic km Navajo Sandstone Bed Started Out as Sand that was Transported Across the Entire United States Mainland from the Appalachians by a Massive River / Yet, Somehow it Managed to End Up As 90% Pure Quartz / Evidence for this Hypothetical River Has Never Emerged / And the Algebra Suggests that the Appalachians Must have Reached 28 km (17 mi) in Height to Create this Amount of Sand","image":"https://lh3.googleusercontent.com/-Re_vzvhtCFg/V5z62g6t1rI/AAAAAAAAIZo/4xGtYkmU9L42D9g_iw9G643CTCj8C_oIACL0B/the-questionable-origins-of-earths-sand-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/86HBUghNYYk","publishDate":"2016-07-30T19:07:53.502Z","updateDate":"2017-01-03T02:07:10.974Z","text":"<b>The Questionable Origins of Earth&#39;s Sand: Geologists Tell Us that the Vast 100,000 Cubic km Navajo Sandstone Bed Started Out as Sand that was Transported Across the Entire United States Mainland from the Appalachians by a Massive River / Yet, Somehow it Managed to End Up As 90% Pure Quartz / Evidence for this Hypothetical River Has Never Emerged / And the Algebra Suggests that the Appalachians Must have Reached 28 km (17 mi) in Height to Create this Amount of Sand</b><br /><br /><i>Extraterrestrial Sands</i><br />Gary Gilligan<br />p.66-79<br /><br /><i>&quot;The Appalachian Mountains are a chain of mountains located in the eastern United States and Canada, running across the North American continent from Alabama in the south to Newfoundland in the north. The mountain range measures roughly 100 to 300 miles (160.9 to 482.7 kilometres) wide and covers an area of about 1,500 miles (2,413.5 kilometres) from north to south (image below). In 2003, geologist Jeffrey Rahl and his colleagues analysed tiny grains of the mineral zircon embedded in the Navajo Sandstone. By &#39;dating&#39; these zircon grains using the uranium-lead (UPb) radioactive method, it has been postulated that the sand grains in the Navajo Sandstone came from the Appalachians far to the east. If this is true, it means the sand grains were transported at least 1,250 miles (2,012 kilometres) across North America.</i><br /><br /><i>I refer the reader to the image on page 54 showing the Navajo deposit with the respect to the proposed source, the Appalachians in the east. How does 100,000 cubic kilometres of quartz sand become transported over such a vast distance? In a very vague and feeble attempt to explain, geologists here propose an immense transcontinental &#39;river system&#39; delivered the sand to the west where the wind took over and swept it into a fantastic sand pile</i> (<a href=\"http://www.agiweb.org/geotimes/nov03/NN_navajo.html\" class=\"ot-anchor\">http://www.agiweb.org/geotimes/nov03/NN_navajo.html</a>).<br /><br /><i>Considering that Navajo Sandstone is believed to be some 180 million years old and was built layer by layer over 15 million years, you would need more than a leap of faith to accept this. It is not only impossible for such a river to have persisted for millions of years but also inconceivable that it would create such a virtually pure deposit of quartz sand (90 per cent) hundreds of feet thick and covering thousands of square miles. Modern rivers such as the Mississippi, Nile and Amazon do not do this.</i><br /><br /><i>If such a river did exist, where was it? Certainly nowhere near the surface as there are no geological features to suggest a great transcontinental river once flowed across the U.S. Perhaps the evidence lies hidden below ground? We recall the network of paleo-rivers and tributaries hidden beneath the Sahara and Arabian deserts, which are thought to be around 20 to 30 million years old. Indeed, even more so in that, to transport so much sand over such a long period of time and over such a great distance, this river must have been truly colossal -- is there any evidence for a great &#39;radar&#39; river system? The author is unaware of any ground-penetrating radar evidence for a continent wide paleo-river and would suggest with a great deal of confidence that none will ever be found. It would seem unlikely that the Navajo sand sea and its proposed parent source, the river system, has magically disappeared off the face of the Earth.</i><br /><br /><i>It&#39;s not just the sand. We also have to take into account the clay-forming feldspars which, having eroded from the Appalachians, would equate to roughly four times the volume of sand (approximately 500,000 cubic kilometres). Added to this the majority of Colorado Plateau sandstones (and corresponding four times silt), which some also believe originated from the Appalachians, and we have mind-boggling quantities of material that was apparently transported across the US.</i><br /><br /><i>Few rivers in the world are over 2,000 kilometres long. Even if they were, it would be highly unlikely these rivers could move such vast quantities of sediment that distance. Modem rivers do carry heavy sediment loads, but they do not move quartz-bearing sediments over such distances in the suggested quantities.</i><br /><br /><i>The proposed &#39;river system&#39; must have been an interior one, that is to say, it didn&#39;t flow out to sea. A logical assumption since the Navajo Sandstone wouldn&#39;t be where it is today if it was initially washed out to the ocean. This raises the question as to where the many millions of tons of eroded sediment forming silt ended up (the by-product of any quartz sand production). At a rough estimate, this would be some ten times the volume of Navajo sand, enough perhaps to cover the whole U.S. at some considerable depth. Where did this settle?</i><br /><br /><i>One would naturally assume it was deposited in the same rough location as the Navajo Sandstone to form multiple sedimentary deposits of shale (or mudstone and suchlike) the most common of sedimentary rock -- is this what happened? Unfortunately, there&#39;s no mention of this by geologists; as discussed previously, the awkward question as to the whereabouts of the weathered material (clays) from granite is rarely considered or simply dismissed as transported &#39;in solution&#39; somewhere.</i><br /><br /><i>But it must have settled somewhere and by all accounts it should be a more prominent geological feature than all the sandstone deposits put together! Perhaps it underwent a few recycling events and was scattered throughout the region to form some of the sedimentary layers we see outcropping in locations such as the Grand Canyon? There are plenty of shale deposits in the region. However, as we are dealing with gargantuan volumes on a continent-wide scale, the evidence here should be unequivocal with large-scale deposits consistent with that of being transported across the U.S. and laid down in one location. After all, this is the same sediment that apparently produced the Navajo (and many other sandstone deposits such as the Coconino), which not only managed to remain as virtually &#39;pure&#39; quartz but was also deposited in one basic location, despite supposedly being laid down over many millions of years.</i><br /><br /><i>No matter how you look this, evidence for Appalachian silt the other side of the continent should be glaringly obvious, as clear and as distinctive as its derivative, the Navajo sand bed and other quartzose deposits. Geologists should be confidently identifying such deposits and explaining how they got there. But they cannot. A question for geologists: how is it that only the quartz ended up being deposited as the Navajo and Coconino Sandstones? What happened to all the feldspars and mica?</i><br /><br /><i>The Appalachian Mountains are rolling, green mountains and look nothing like the Rockies or the Tetons. When compared with the snow-capped peaks of the Himalayas, Andes or Alps they are seldom mentioned among the world&#39;s great mountain ranges, mainly because they consist mostly of low gentle ridges, almost like the keel of a boat. The Appalachians are a complex mix of mountains and have a long and complex geologic history spanning some 480 million years.</i><br /><br /><i>A look at rocks exposed in today&#39;s Appalachian Mountains reveals elongated belts of folded and thrust-faulted marine sedimentary rocks, volcanic rocks and slivers of ancient ocean floor. So here we see they are largely of the wrong composition.</i><br /><br /><i>Thought to have formed as a result of a series of continental collisions, granitic outcrops occur in several areas, especially in the Piedmont region. They vary tremendously in size, shape and position in the landscape. Some consist of small, flat-lying exposures (or flatrocks) only a few square feet in area while others span between a few hundred square kilometres to approximately 2,000 square kilometres, as is the case with the Rolesville granite batholith located in North Carolina. One way of visualising the granitic intrusions would be to describe the Appalachians as an elongated bowl of porridge with prunes protruding the surface -- as the granitic outcrops are just that, rock intrusions.</i><br /><br /><i>Although there is little information on this (no one, it seems is too interested in tracking down the primary source of sand), a conservative estimate of the total area covered by granitic outcrops found in the Appalachians would be about 3 per cent. This would equate to roughly 36,000 square kilometres of granitic outcrops, if that. Was this ever enough to produce the huge sandstone deposits in the westem U.S.? I strongly doubt this for several reasons.</i><br /><br /><i>Using our height formula and bearing in mind the weathered material, some absolutely impossible heights would have to be realised. Indeed, today&#39;s granitic outcrops would once have had to stand at an incredible 28 kilometres to create the Navajo and Coconino deposits alone. This is some three times the height of the Himalayas and sounds highly implausible -- especially when taking into account the fact that granite is an intrusive rock not a sedimentary rock. It forms under Earth&#39;s surface (mainly sedimentary rock overlain with soil) which having eroded away exposes the granitic rock. This deems the whole of the Appalachian complex must have once stood even higher than its granitic intrusions. Although the Appalachians are vast in length, peaking at a mere 6,684 feet (2,037 metres) they are low lying, and while there is ample evidence to suggest they were higher in the past, I doubt any geologist would ever suggest they reached the dizzying heights of 28 kilometres. This would give unprecedented heights as well as unfathomable quantities of sediments, bordering on biblical proportions.</i><br /><br /><i>Some may postulate that the Appalachians underwent several phases of tectonic uplift (mountain forming) and erosion simultaneously, that is to say several kilometres of rock were eroded away at the same time as the mountains were slowly thrust upwards, thus preventing them from ever attaining such astronomical heights. This is a nonstarter, inasmuch as the granite outcrops are said to have formed around 300 million years ago while the Navajo Sandstone is reputedly about 180 million years old. What this means is the granitic intrusions seen today are the only possible source for the sand. But this makes little sense, for it is impossible for them to have stood at 28 kilometres high -- as rock intrusions that began life just below Earth&#39;s surface they cannot be too far off from where they stand today. This is proven by the fact that sedimentary rocks weather much faster than granitic rock. So, if the granites were at any considerable elevation, the surrounding sedimentary material would have weathered away first, leaving behind some truly imposing granite prunes! Like skyscrapers piercing the skies of the world&#39;s major cities, they would have dwarfed the surrounding landscape. Nowhere on Earth has this occurred on such a scale and it certainly isn&#39;t a feature of the Appalachians.</i><br /><br /><i>If we assume for a moment that the sand did somehow originate from the Appalachians, then exactly how would this work? To understand this further let us briefly run through the likely sequence of events before analysing the main points a little more in depth. As will become evident, the whole scenario is impossible. I would add, what follows here could also apply to the process of quartz production worldwide.</i><br /><br /><i>The process would go something like this; rains fall on the mountains and over eons of time erosion removes thousands of feet of overlying mainly sedimentary rock, exposing the more resistant bodies of granite. The surrounding rocks along with the bulk (80 per cent) of the granitic rocks weather away into solution to form silt and clay minerals (mud). These would normally be washed out to sea by rivers, but as we are dealing with an interior river system, they are transported and deposited at some, yet to be found, location in the west. We can only assume a basin containing a lake here. The tough quartz crystals on the other hand are not suspended in solution; they are transported along river beds by a process called saltation (jumping along the bed of a river). Regardless, they combine to form unfathomable volumes of clay, silt and sand (mud) which would have been deposited in and around said lake, in particular around the mouth of the river feeding it.</i><br /><br /><i>Incessant rain?</i><br /><br /><i>It is without doubt that the erosion, transportation and deposition of millions of tons of sediment thousands of miles west across the continent via a river system involved rain, and lots of it. In fact, rainwater must have been continuous and totally unremitting for literally billions of years in order to create so much sand. If there were any significant respite then the sand/silt/mud would have been more randomly scattered, i.e. deposited further upstream during dryer periods, thus more prone to wind transportation and deposition. In essence, it wouldn&#39;t have ended up in one basic place.</i><br /><br /><i>In any case, there is no evidence whatsoever for a great eastwest continental river system emanating from the direction of the Appalachians and depositing sediments en route. This raises the question of: what changed if the Appalachians were responsible for the majority of the sedimentary layers in the west? What great cataclysm befell the U.S. causing a giant east-west river system to disappear off the face of the earth?</i><br /><br /><i>Where were the Milankovitch cycles (MCs) during all this time?</i><br /><br /><i>As we have seen, many believe the MCs were the driving force in shaping the climate of North America (Arabia etc.), i.e. causing the region to experience cyclical wet and dry periods over many millennia -- are we to believe the same variations in Earth&#39;s axial tilt played no recognisable part in the U.S.? Where were the cycles during the erosion, transportation and deposition of unfathomable quantities of sedimentary material apparently laid down in the west? Are the MCs recorded in the Navajo or Coconino sandstone layers? Are any of them recorded in the strata of the Grand Canyon? If not, why not? Were these regions exempt from global changes? If the MCs were responsible for the catastrophic transformation of North Mrica, the Middle East and the Asian continent then why can&#39;t scholars correlate these events with events in the U.S.? It&#39;s all very vague and riddled with inconsistencies -- yet more reasons to dismiss the Milankovitch cycles as they clearly do not work.</i><br /><br /><i>Magical filtration system?</i><br /><br /><i>As we can see, the deposition of several million tons of sediment in the west also involved mud (clay, silt and sand), and lots of it. In fact, it&#39;s difficult to conceive the quantities of mud involved -- probably enough to turn Earth into one gigantic mud ball. It&#39;s a wonder this alone didn&#39;t silt up the whole system. Mixed in with this mud we have the relatively small amount of quartz grains, at a guess probably less than 2 per cent (if that, when taking into account the additional mix of eroded material from the Appalachians, i.e. soil etc.). Laid down in a continental basin or lake somewhere to form multiple beds of sediment, how on Earth was this filtered out to form the vast pure sandstone deposits found in the Colorado Plateau? The Navajo sand sea comprises of more than 90 per cent pure quartz sand, and the Coconino an even higher concentration. Both deposits stretch for thousands of miles -- what magical filtration system manages to sort hundreds of thousands of cubic kilometres of quartz grains from masses upon masses of silt and clay? Just how did the quartz sand end up so well sorted? If of the same origin, why the variants in the purity of the sand? Are similar processes at work today? Are the Appalachian granites still churning out first-phase quartz grains creating new, virtually pure sand seas at some location? If so, where? If not, why not?</i><br /><br /><i>We may ask the same questions in regard to the world&#39;s granitic outcrops: are they still giving up their 20 per cent quartz? Mainstream says yes but since we are dealing with erosional processes that take millions upon millions of years, it is imperceptible on such a small scale. Indeed!</i><br /><br /><i>While it isn&#39;t doubted that quartz grains are being eroded from granitic rocks, I would suggest this to be on such a negligible scale as to be virtually ineffective. Certainly not even coming close to ever providing a source for the world&#39;s vast deposits of quartz sand (be it beach, dune, sandstone or otherwise) no matter how much time you ascribe to it. Time just doesn&#39;t favour the laying down of vast pure deposits of sand (first phase or recycled). It would require the supposition that there was no environmental or geological change during the multimillion years of deposition -- given the dynamicity of Earth, highly unlikely. Where were the Milankovitch cycles during such times? Why aren&#39;t glacialinterglacial periods clearly represented in the strata?</i><br /><br /><i>Rivers do of course carry sediments; some such as the Amazon, Mississippi, Nile and Indus carry very heavy loads. This is deposited at the mouth of the rivers to form new land called deltas. A prime example would be the Mississippi, which drains all or part of thirty-one U.S. states and two Canadian provinces between the Rocky and Appalachian Mountains, dumping the sediment in the Gulf of Mexico, forming a large soil-rich delta. As we have seen, sediment typically consists of sand, silt and clay. However, proportions can vary considerably between rivers. For instance, there is ample evidence to show that the Mississippi has not only deposited large amounts of sand in the past but this is an ongoing process -- so much so it has to be dredged on a regular basis to keep waterways open. But how much of river sand is quartz grains disintegrated from granitic outcrops? How much of this is primary source sand?</i><br /><br /><i>Using the standard model of granite taking many millions of years to erode it is logical to conclude this to be negligible -- virtually undetectable. But if this so, then where does all the sand in our rivers come from if not source rock? It is thought that the majority of sand in rivers is derived from secondary sources (recycled) such as sandstone or unconsolidated sand washed down off the land -- very little sand originates directly from granitic sources. This is easily understood when considering the slow erosion of granite rock versus the more rapid erosion of sedimentary rocks. And yet, it is from this &#39;trickle&#39; that the world&#39;s beaches were apparently formed; it is from this trickle that great oceans of dune sands are believed to have been created; it is from this trickle that enough sand to cover the continents to a height of 1,160 metres (approx. 1.16 kilometres) was created (eroded from rock that would stand at a height of 6 kilometres). Hard to believe given the fact that all the water, in all of the streams and rivers of the world represents only 0.005 per cent of all the water in the world outside the ocean basin.</i><br /><br /><i>When taking into account the above, it is plain to see there is something seriously wrong with the idea that vast swathes of sand (and sediment) were transported across the U.S. and dumped in the west via a river system. As demonstrated, there is no geological evidence whatsoever to support this. The only thing linking the two is an apparent match between zircon crystals -- nothing else! What does this tell us? It tells us that the dating of zircon grains is seriously flawed and cannot be relied on. Moreover, we are left with even more vast quartz sand seas of unknown origin.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Sudden Appearance of Earth's Sand","summary":"The Sahara and Arabian Deserts Both Lack a Confirmed Origin / The Sahara Sand is Thought to be Millions of Years Old / Yet, Excavated Bones are From Water-Adapted Creatures and Human Settlements / People Just a Few Thousand Years Ago Thrived in these Regions, and they Were Wet / The Textbook Theories Struggle to Make Sense of It","image":"https://lh3.googleusercontent.com/-JRUKb8Oq104/V5rdvx2bdFI/AAAAAAAAIYg/s5TBqljSE3sPFv5z-plyuPFff9R3r_GVA/w7142-h9999/the-sudden-appearance-of-the-sands-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/UqmXkiohCKQ","publishDate":"2016-07-29T19:55:54.977Z","updateDate":"2016-07-29T19:55:54.977Z","text":"<b>The Sudden Appearance of Earth&#39;s Sand: The Sahara and Arabian Deserts Both Lack a Confirmed Origin / The Sahara Sand is Thought to be Millions of Years Old / Yet, Excavated Bones are From Water-Adapted Creatures and Human Settlements / People Just a Few Thousand Years Ago Thrived in these Regions, and they Were Wet / The Textbook Theories Struggle to Make Sense of It</b><br /><br />The analysis comes from <i>Extraterrestrial Sands</i>, by Gary Gilligan.  My personal opinion is that the book is a sh*t-show of unsupported claims wrapped like a bun around a very interesting hot dog.  I&#39;ve left out the unsupported claims and focused on the hot dog.<br /><br />Gary Gilligan seems to have figured out one very important thing, and we might as well focus on that.<br /><br />I think you will agree.<br /><br />There is a bit more coming up on this fascinating subject of sand!<br /><br />REFERENCES:<br /><br />[1] [2] <a href=\"http://www.thunderbolts.info/tpod/2010/arch10/100408sahara.htm\" class=\"ot-anchor\">http://www.thunderbolts.info/tpod/2010/arch10/100408sahara.htm</a><br /><br />[3] <a href=\"https://books.google.com/books?id=i_YEDAAAQBAJ&amp;pg=PA9&amp;lpg=PA9&amp;dq=it+is+perplexing+how+so+much+emphasis+is+placed+on+the+%27drying+out%27+of+the+sahara+with+little+or+no+consideration+as+to+the+implications&amp;source=bl&amp;ots=bnkn9UPkzr&amp;sig=XpgD4wtPb-FlBjeVYIYm2CGP2kg&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwixjI-l6pLOAhWB34MKHUeJAuEQ6AEIHDAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=i_YEDAAAQBAJ&amp;pg=PA9&amp;lpg=PA9&amp;dq=it+is+perplexing+how+so+much+emphasis+is+placed+on+the+%27drying+out%27+of+the+sahara+with+little+or+no+consideration+as+to+the+implications&amp;source=bl&amp;ots=bnkn9UPkzr&amp;sig=XpgD4wtPb-FlBjeVYIYm2CGP2kg&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwixjI-l6pLOAhWB34MKHUeJAuEQ6AEIHDAA#v=onepage&amp;q&amp;f=false</a><br /><br />[4] [5] <a href=\"http://phys.org/news/2013-11-sahara-sand.html\" class=\"ot-anchor\">http://phys.org/news/2013-11-sahara-sand.html</a><br /><br />[6] <a href=\"http://www.ldeo.columbia.edu/~peter/site/Research/Entries/2013/10/29_East_African_Hydroclimate_files/Tierney.deMenocal.2013.pdf\" class=\"ot-anchor\">http://www.ldeo.columbia.edu/~peter/site/Research/Entries/2013/10/29_East_African_Hydroclimate_files/Tierney.deMenocal.2013.pdf</a><br /><br />[7] <a href=\"https://books.google.com/books?id=i_YEDAAAQBAJ&amp;pg=PA9&amp;lpg=PA9&amp;dq=it+is+perplexing+how+so+much+emphasis+is+placed+on+the+%27drying+out%27+of+the+sahara+with+little+or+no+consideration+as+to+the+implications&amp;source=bl&amp;ots=bnkn9UPkzr&amp;sig=XpgD4wtPb-FlBjeVYIYm2CGP2kg&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwixjI-l6pLOAhWB34MKHUeJAuEQ6AEIHDAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=i_YEDAAAQBAJ&amp;pg=PA9&amp;lpg=PA9&amp;dq=it+is+perplexing+how+so+much+emphasis+is+placed+on+the+%27drying+out%27+of+the+sahara+with+little+or+no+consideration+as+to+the+implications&amp;source=bl&amp;ots=bnkn9UPkzr&amp;sig=XpgD4wtPb-FlBjeVYIYm2CGP2kg&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwixjI-l6pLOAhWB34MKHUeJAuEQ6AEIHDAA#v=onepage&amp;q&amp;f=false</a><br /><br />[8] <a href=\"https://books.google.com/books?id=i_YEDAAAQBAJ&amp;pg=PA9&amp;lpg=PA9&amp;dq=it+is+perplexing+how+so+much+emphasis+is+placed+on+the+%27drying+out%27+of+the+sahara+with+little+or+no+consideration+as+to+the+implications&amp;source=bl&amp;ots=bnkn9UPkzr&amp;sig=XpgD4wtPb-FlBjeVYIYm2CGP2kg&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwixjI-l6pLOAhWB34MKHUeJAuEQ6AEIHDAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=i_YEDAAAQBAJ&amp;pg=PA9&amp;lpg=PA9&amp;dq=it+is+perplexing+how+so+much+emphasis+is+placed+on+the+%27drying+out%27+of+the+sahara+with+little+or+no+consideration+as+to+the+implications&amp;source=bl&amp;ots=bnkn9UPkzr&amp;sig=XpgD4wtPb-FlBjeVYIYm2CGP2kg&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwixjI-l6pLOAhWB34MKHUeJAuEQ6AEIHDAA#v=onepage&amp;q&amp;f=false</a><br /><br />[9] <a href=\"http://phys.org/news/2013-11-sahara-sand.html\" class=\"ot-anchor\">http://phys.org/news/2013-11-sahara-sand.html</a><br /><br />[10] <a href=\"http://anthropology.msu.edu/anp363-ss13/2013/02/06/the-lost-tribes-of-the-green-sahara/\" class=\"ot-anchor\">http://anthropology.msu.edu/anp363-ss13/2013/02/06/the-lost-tribes-of-the-green-sahara/</a><br /><br />[11] <a href=\"http://blogs.discovermagazine.com/80beats/2008/08/14/stone-age-graveyard-in-the-sahara-recalls-an-era-of-lakes-and-wetlands/#.V5i59mQrL1t\" class=\"ot-anchor\">http://blogs.discovermagazine.com/80beats/2008/08/14/stone-age-graveyard-in-the-sahara-recalls-an-era-of-lakes-and-wetlands/#.V5i59mQrL1t</a><br /><br />[12] <a href=\"http://www.ancient-atlantis.com/middle-east-green-before-flood/\" class=\"ot-anchor\">http://www.ancient-atlantis.com/middle-east-green-before-flood/</a><br /><br />[13] <a href=\"http://www.khammash.com/research/jawa-dawn-water-engineering\" class=\"ot-anchor\">http://www.khammash.com/research/jawa-dawn-water-engineering</a><br /><br />[14] <a href=\"http://www.bbc.com/news/world-middle-east-26841410\" class=\"ot-anchor\">http://www.bbc.com/news/world-middle-east-26841410</a><br /><br />[15] <a href=\"http://www.bbc.com/news/world-middle-east-26841410\" class=\"ot-anchor\">http://www.bbc.com/news/world-middle-east-26841410</a><br /><br />[16] - [20] (approximate) <a href=\"https://books.google.com/books?id=i_YEDAAAQBAJ&amp;pg=PA9&amp;lpg=PA9&amp;dq=it+is+perplexing+how+so+much+emphasis+is+placed+on+the+%27drying+out%27+of+the+sahara+with+little+or+no+consideration+as+to+the+implications&amp;source=bl&amp;ots=bnkn9UPkzr&amp;sig=XpgD4wtPb-FlBjeVYIYm2CGP2kg&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwixjI-l6pLOAhWB34MKHUeJAuEQ6AEIHDAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=i_YEDAAAQBAJ&amp;pg=PA9&amp;lpg=PA9&amp;dq=it+is+perplexing+how+so+much+emphasis+is+placed+on+the+%27drying+out%27+of+the+sahara+with+little+or+no+consideration+as+to+the+implications&amp;source=bl&amp;ots=bnkn9UPkzr&amp;sig=XpgD4wtPb-FlBjeVYIYm2CGP2kg&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwixjI-l6pLOAhWB34MKHUeJAuEQ6AEIHDAA#v=onepage&amp;q&amp;f=false</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23sand\">#sand</a>   <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23desert\">#desert</a>  ﻿","category":"unknown"},{"name":"The Misbehaving Fundamental Constants","summary":"The Trouble with Some Constants is that they Don't Appear all that Constant / The Gravitational Constant has Proven the Most Elusive / Scientists Commonly Justify these Difficulties by Reminding Us that the Universe's Dominant Force is \"Far Weaker than Other Forces Such as Electromagnetism\" / Rather than Embracing the Data as an Opportunity for New Science, they See it as Embarrassing","image":"https://lh5.googleusercontent.com/-p7VpvMgbwMM/V43L7J1Lc7I/AAAAAAAAIWw/r5R41a9U_yw64AaGHIZpU0Ek7KY_absPACL0B/the-misbehaving-fundamental-constants-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Tf2wFCo9rbz","publishDate":"2016-07-19T06:56:04.028Z","updateDate":"2017-01-03T02:07:41.822Z","text":"<b>The Misbehaving Fundamental Constants: The Trouble with Some Constants is that they Don&#39;t Appear all that Constant / The Gravitational Constant has Proven the Most Elusive / Scientists Commonly Justify these Difficulties by Reminding Us that the Universe&#39;s Dominant Force is &quot;Far Weaker than Other Forces Such as Electromagnetism&quot; / Rather than Embracing the Data as an Opportunity for New Science, they See it as Embarrassing</b><br /><br /><b>Rupert Sheldrak: Science Set Free, Part 1</b><br />23:20 mins<br /><a href=\"https://youtu.be/0waMBY3qEA4?t=1400\" class=\"ot-anchor\">https://youtu.be/0waMBY3qEA4?t=1400</a> (jump to timestamp)<br /><br /><i>[...]</i><br /><br /><i>&quot;It&#39;s assumed that the constants of nature are constant -- the fundamental constants like Newton&#39;s Gravitational Constant (big G) or the speed of light (c).</i><br /><br /><i>Well, I began to wonder whether they really were constant when I got into the habit viewing nature.  And so I tried to find out what the actual values were.  I started off by getting handbooks of physical constants and looking at old editions.  Most people only look at the latest edition, and they usually throw the old ones away.  But, in the patent office library in London, I found that they kept them all.  And so, I got them all out of the reserve stock at 10-year intervals.  They wheeled in a trolly of handbooks of physical constants, dusting them off.  And I looked through these things to see how they&#39;ve changed.</i><br /><br /><b><i>To my amazement, I found that the speed of light dropped by 20 km/s between 1928 and 1945. I then looked up the data in more detail, and found that all over the world, people had been getting this much lower figure with very small error bars. The original figure was up there with little error bars ... It wasn&#39;t as if the error bars were 20 km/s. No, they were point decimal places of km/s.</i></b><br /><br /><i>I checked in the primary literature and found this indeed seemed to be the case, </i><b><i>and then they went up again after 1945.</i></b><i> I couldn&#39;t understand what was going on, so I asked the head of the metrology department (metrologists are the people who measure constants) at the British national physical laboratory if I could go and see him.  And I went to visit him.  He was very friendly.</i><br /><br /><i>And I said to him, &#39;Dr. Petley, I&#39;d like to know how you explain this drop in the speed of light between 1928 and 1945.&#39;</i><br /><br /><i>And he said, &#39;Oh dear.&#39;</i><br /><br /><i>And I said, &#39;What?&#39;</i><br /><br /><i>&#39;You&#39;ve uncovered one of the most embarrassing incidents in the history of our subject.&#39;&quot;</i><br /><br />(audience laughs)<br /><br /><i>&quot;And, so, I said, &#39;Well, could it mean that the speed of light really did drop -- at least as measured on Earth during that period?&#39;</i><br /><br /><i>He said, &#39;Of course not!&#39;</i><br /><br /><i>And I said, &#39;Why not?&#39;</i><br /><br /><i>He said, </i><b><i>&#39;Because it&#39;s a constant!&#39;</i></b><br /><br /><i>So, I said, &#39;Well, then, I can&#39;t see any other explanation than that people all around the world were sort of fudging their results to get what they thought everybody else would expect them to get -- and then discarding outliers and stuff, and coming up with these very narrow arrow bars that agree with everyone else.  And so, it must have been produced by some kind of fudging process.&#39;</i><br /><br /><i>He said, &#39;We don&#39;t like to use the word &#39;fudge.&#39;&quot;</i><br /><br />(audience laughs)<br /><br /><i>&quot;So, I said, &#39;What do you prefer?&#39;</i><br /><br /><i>He said, </i><b><i>&#39;We prefer to call it intellectual phase locking.&#39;&quot;</i></b><br /><br />(audience laughs)<br /><br /><i>&quot;So, I said to him, </i><b><i>&#39;If it was happening then, how do we know it&#39;s not happening now?&#39;</i></b><br /><br /><i>He said, &#39;Oh, we know it&#39;s not happening now.&#39;</i><br /><br /><i>And I said, &#39;Why?&#39;</i><br /><br /><i>He said, </i><b><i>&#39;Because we fixed the speed of light by definition in 1972.&#39;&quot;</i></b><br /><br />(audience laughs)<br /><br /><i>&quot;So I said, &#39;Well, it might still vary&#39;</i><br /><br /><i>And he leaned back, looking very smug, and said, </i><b><i>&#39;Well, if it did, no one would ever find out because we&#39;ve defined the meter in terms of the speed of light, so the units would vary with it.&#39;</i></b><br /><br /><i>So, I said, &#39;Okay, well you fixed that one, but what about the gravitational constant?&#39;  I said, &#39;That&#39;s been varying wildly. </i><b><i>And even in the last three of our years, it&#39;s varied by more than 1.3%, as measured in different laboratories.&#39;</i></b><br /><br /><i>The usual assumption is that this is just error.  It&#39;s experimental error.  It&#39;s hard to measure.  It&#39;s error.</i><br /><br /><b><i>So, labs all over the world get quite different results. And the International Committee on metrology fixes the results every few years by averaging ones from different labs, weighting ones they think are more reliable, discarding ones they think are not.</i></b><br /><br /><i>Indeed, when I left Dr. Petley, thanking him for his time, he reached down to a cardboard box besides his desk full of pamphlets.  He said, &#39;By the way, these have just come from the printers.  You might like one.&#39;</i><br /><br /><i>He handed me this pamphlet: </i><b><i>The latest values of the physical constants.&quot;</i></b><br /><br />(audience laughs)<br /><br /><i>&quot;And so, I looked at these data from different labs on G (big G), and the question that I was wondering is ... </i><b><i>all of these big errors, could it be that they&#39;re actually changing together in different labs as the Earth rotates around the Sun, as it rotates during the day, and as the whole solar system moves through different astronomical environments?</i></b><br /><br /><i>To find that out, one would look at the day-by-day measurements from different labs, and see if these so-called errors are correlated.</i><br /><br /><b><i>I&#39;ve spent more than ten years trying to persuade metrologists to do this.  And they simply will not, cause they say it&#39;s a constant.  So, there&#39;s no point looking for variations.</i></b><br /><br /><i>And I say, &#39;You&#39;ve got these huge differences!&#39;</i><br /><br /><i>And they say, &#39;Oh, they&#39;re just errors.  It&#39;s hard to measure.&#39;</i><br /><br /><i>But, they simply won&#39;t do it.  An exercise in open science would be if they put their raw data with the dates online, and anyone can try and look for patterns.  There could be a website where they&#39;re discussed.  It would cost nothing, and we might find something out.</i><br /><br /><b><i>We&#39;ll find out nothing by pretending it&#39;s fixed.</i></b><br /><br /><b><i>There are in fact already papers which suggest diurnal variations in accordance with the sidereal day. A group at MIT recently found a daily variation, and they&#39;re some evidence of annual variations.</i></b><i> But, there may be other wilder fluctuations that happen in concert.</i><br /><br /><i>I, myself, think the so-called physical constants may vary from time-to-time -- and possibly even chaotically, within certain limits. </i><b><i>I think the day may come when in scientific periodicals like Nature there&#39;d be a page like the stock market reports, you know: This week&#39;s value of the constants.</i></b><br /><br /><b><i>&#39;This week, the G was slightly up.  The charge on the electron held steady.  There was a drop in the fine structure constant.&#39;</i></b><br /><br /><i>And if that was the case, then they would give varying qualities of time when things could happen differently.</i><br /><br /><i>The idea they&#39;re all rigidly fixed is a hangover from an old Platonic way of thinking.</i><br /><br /><i>So, there, right in the heart of physics, is, I think, a really open and interesting question.</i><br /><br /><i>[...]</i><br /><br /><b><i>The Science Delusion: Freeing the Spirit of Enquiry</i></b><br /><br />How constant are the ‘fundamental constants’?<br />by Rupert Sheldrake<br /><br />[Note: I&#39;m editing out Sheldrake&#39;s discussion of the fine-structure constant, as he has himself in many online versions of this essay, because it assumes the validity of contested claims on quasars]<br /><br /><i>&quot;... What about the data? All of the published values of constants vary with time, [9] but here I will discuss only three of them: the Universal Gravitational Constant, the fine-structure constant and the speed of light.</i><br /><br /><i>The oldest of the constants, Newton’s Universal Gravitational Constant, Big G, is also the one that shows the largest variations. Towards the end of the twentieth century, as methods of measurement became more precise, the disparity in measurements of G by different laboratories increased, rather than decreased. [10] Between 1973 and 2010, the lowest value of G was 6.6659, and the highest 6.734, a 1.1 per cent difference (Figure 3.1). These published values are given to at least three places of decimals, and sometimes to five, with estimated errors of a few parts per million. Either this appearance of precision is illusory, or G really does change. The difference between recent high and low values is more than forty times greater than the estimated errors (expressed as standard deviations). [11]</i><br /><br /><i>What if G really does change? Maybe it does so because measurements are affected by changes in the earth’s astronomical environment, as the earth moves around the sun and as the solar system moves within the galaxy. Or maybe there are inherent fluctuations in G. Such changes would never be noticed as long as measurements are averaged over time and across laboratories.</i><br /><br /><i>In 1998, the US National Institute of Standards and Technology published values of G taken on different days, rather than averaging them to iron out variations, revealing that there was a remarkable range: for example, on one day the value was 6.73, a few months later it was 6.64, 1.3 per cent lower. [13]</i><br /><br /><i>In 2002, a team led by Mikhail Gershteyn, of the Massachusetts Institute of Technology, published the first systematic attempt to study changes in G at different times of day and night. G was measured around the clock for seven months, using two independent methods. </i><b><i>They found a clear daily rhythm, with maximum values of G 23.93 hours apart, correlating with the length of the sidereal day, the period of the earth’s rotation in relation to the stars.</i></b><br /><br /><i>Gershteyn’s team looked only for daily fluctuations, but G may well vary over longer time periods as well; there is already some evidence of an annual variation. [14] By comparing measurements from different locations, it should be possible to find more evidence of underlying patterns. Such measurements already exist, buried in the files of metrological laboratories. The simplest and cheapest starting point for this enquiry would be to collect the measurements of G at different times from laboratories all over the world. Then these measurements could be compared to see if the fluctuations are correlated. [15] If they are, we will discover something new.&quot;</i><br /><br />[... fine structure constant edit ...]<br /><br /><i>&quot;Finally, what about the speed of light, c? According to Einstein’s theory of relativity, the speed of light in a vacuum is an absolute constant, and modern physics is based on this assumption.</i><br /><br /><i>Not surprisingly, early measurements of the speed of light varied considerably, but by 1927, the measured values had converged to 299,796 kilometres per second. At the time, the leading authority on the subject concluded, ‘The present value of c is entirely satisfactory and can be considered more or less permanently established.’ [20] However, all around the world from about 1928 to 1945, the speed of light dropped by about 20 kilometres per second.[21] The ‘best’ values found by leading investigators were in impressively close agreement with each other. Some scientists suggested that the data pointed to cyclic variations in the velocity of light. [22]</i><br /><br /><i>In the late 1940s the speed of light went up again by about 20 kilometres per second and a new consensus developed around the higher value. </i><b><i>In 1972, the embarrassing possibility of variations in c was eliminated when the speed of light was fixed by definition. In addition, in 1983 the unit of distance, the metre, was redefined in terms of light. Therefore if any further changes in the speed of light happen, we will be blind to them because the length of the metre will change with the speed of light.</i></b><i> (The metre is now defined as the length of the path travelled by light in a vacuum in 1/299,792,458 of a second.) </i><b><i>The second is also defined in terms of light</i></b><i>: it is the duration of 9,192,631,770 periods of vibration of the light given off by cesium 133 atoms in a particular state of excitation (technically defined as the transition between the two hyperfine levels of the ground state).&quot;</i><br /><br />[10] Mohr, P. J., and Taylor, B. N. (2001), ‘Adjusting the values of the fundamental constants’, <i>Physics Today</i>, 54, 29.<br /><br />[11] [13] Schwarz, J. P., Robertson, D. S., Niebauer, T. M., and Fuller, J. E. (1998), ‘A free-fall determination of the Newtonian constant of gravity’, <i>Science</i>, Captant 282, 2230–34.<br /><br />[14] Stephenson, L. M. (1967), ‘A possible annual variation of the gravitational constant’, <i>Proceedings of the Physical Society</i>, 90, 601–4.<br /><br />[15] For a discussion, see Sheldrake, R. (1994), <i>Seven Experiments That Could Change the World: A Do-It-Yourself Guide to Revolutionary Science</i>, Fourth Estate, London, Chapter 6.<br /><br />[20] Birge, W. T. (1929), ‘Probable valves of the general physical constants’ <i>Reviews of Modern Physics</i>, 33, 233–9, p.68.<br /><br />[21] For data and references, see Sheldrake, R. (1994), <i>Seven Experiments That Could Change the World: A Do-It-Yourself Guide to Revolutionary Science</i>, Fourth Estate, London, Chapter 6.<br /><br />[22] Petley, B. W. (1985), <i>The Fundamental Physical Constants and the Frontiers of Metrology</i>, Adam Hilger, Bristol, p.294.<br /><br />For a detailed review of the machines used to accurately measure G, see article at ...<br /><br /><a href=\"http://scienceblogs.com/principles/2010/08/26/measuring-gravity-aint-nothin/\" class=\"ot-anchor\">http://scienceblogs.com/principles/2010/08/26/measuring-gravity-aint-nothin/</a><br /><br />REFERENCES FOR GRAPHIC:<br /><br />[1] <a href=\"http://scienceblogs.com/principles/2010/08/26/measuring-gravity-aint-nothin/\" class=\"ot-anchor\">http://scienceblogs.com/principles/2010/08/26/measuring-gravity-aint-nothin/</a><br /><br />[2] Measuring the gravitational constant - Science News - May 18, 1996<br /><br />[3] <a href=\"http://parispolter.com/problems-with-the-gravitational-constant/\" class=\"ot-anchor\">http://parispolter.com/problems-with-the-gravitational-constant/</a><br /><br />[4] <a href=\"http://www.scientificamerican.com/article/puzzling-measurement-of-big-g-gravitational-constant-ignites-debate-slide-show/\" class=\"ot-anchor\">http://www.scientificamerican.com/article/puzzling-measurement-of-big-g-gravitational-constant-ignites-debate-slide-show/</a><br /><br />[5] <a href=\"http://www.scientificamerican.com/article/puzzling-measurement-of-big-g-gravitational-constant-ignites-debate-slide-show/\" class=\"ot-anchor\">http://www.scientificamerican.com/article/puzzling-measurement-of-big-g-gravitational-constant-ignites-debate-slide-show/</a><br /><br />[6] <a href=\"https://www.newscientist.com/article/dn24180-strength-of-gravity-shifts-and-this-time-its-serious/\" class=\"ot-anchor\">https://www.newscientist.com/article/dn24180-strength-of-gravity-shifts-and-this-time-its-serious/</a><br /><br />[7] <a href=\"http://scienceblogs.com/principles/2010/08/26/measuring-gravity-aint-nothin/\" class=\"ot-anchor\">http://scienceblogs.com/principles/2010/08/26/measuring-gravity-aint-nothin/</a><br /><br />[8] <a href=\"http://www.nature.com/news/2010/100823/full/4661030a.html\" class=\"ot-anchor\">http://www.nature.com/news/2010/100823/full/4661030a.html</a><br /><br />[9] <a href=\"http://parispolter.com/problems-with-the-gravitational-constant/\" class=\"ot-anchor\">http://parispolter.com/problems-with-the-gravitational-constant/</a><br /><br />[10] - [14] <a href=\"http://www.scientificamerican.com/article/puzzling-measurement-of-big-g-gravitational-constant-ignites-debate-slide-show/\" class=\"ot-anchor\">http://www.scientificamerican.com/article/puzzling-measurement-of-big-g-gravitational-constant-ignites-debate-slide-show/</a><br /><br />[15] <a href=\"http://phys.org/news/2013-09-gravitational-constant-higher.html\" class=\"ot-anchor\">http://phys.org/news/2013-09-gravitational-constant-higher.html</a><br /><br />(unnumbered quote, oops: <a href=\"http://www.scientificamerican.com/article/puzzling-measurement-of-big-g-gravitational-constant-ignites-debate-slide-show/\" class=\"ot-anchor\">http://www.scientificamerican.com/article/puzzling-measurement-of-big-g-gravitational-constant-ignites-debate-slide-show/</a>)<br /><br />[16] <a href=\"http://parispolter.com/problems-with-the-gravitational-constant/\" class=\"ot-anchor\">http://parispolter.com/problems-with-the-gravitational-constant/</a><br /><br />[17] [18] <a href=\"http://phys.org/news/2015-04-gravitational-constant-vary.html\" class=\"ot-anchor\">http://phys.org/news/2015-04-gravitational-constant-vary.html</a><br /><br />[19] <a href=\"http://www.nature.com/news/2010/100823/full/4661030a.html\" class=\"ot-anchor\">http://www.nature.com/news/2010/100823/full/4661030a.html</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Please Stop Saying that the 1919 Eclipse Expedition Verified Relativity","summary":"Science Historians John Earman and Clark Glymour have Shown that the Evidence Presented was Unquestionably Inadequate / It was Principally the Triple-Pronged Public Relations of the Astronomer Royal, the President of the Royal Society and Arthur Eddington that Lent General Relativity its 1919 Victory","image":"https://lh3.googleusercontent.com/-f_jUMeJWZFA/V4w7wyDBjwI/AAAAAAAAIWQ/NgVQSk3Alhc6lYibVUvCj79PP1DUs2HhQ/w5357-h7500/1919-eclipse-expedition-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/gaug8MFMJqy","publishDate":"2016-07-18T02:17:07.422Z","updateDate":"2017-01-03T02:08:42.720Z","text":"<b>Please Stop Saying that the 1919 Eclipse Expedition Verified Relativity: Science Historians John Earman and Clark Glymour have Shown that the Evidence Presented was Unquestionably Inadequate / It was Principally the Triple-Pronged Public Relations of the Astronomer Royal, the President of the Royal Society and Arthur Eddington that Lent General Relativity its 1919 Victory</b><br /><br /><b><i>Fabulous Science Fact and Fiction in the History of Scientific Discovery</i></b><br />by John Waller<br /><br />p17:<br /><br /><b><i>&quot;The experimental phase must rigorously test the theory; it must not itself be shaped or modified by an imperative to prove the theory correct.</i></b><i> To accuse somebody of being tyrannized by ‘preconceived ideas’ is to suggest that just such a process of shaping and modification has occurred. </i><b><i>Implicit in this accusation is the claim that the experimenter has stood between his or her findings and the rest of the scientific community, filtering out any results and eliminating any methods that seem likely to discomfit their own view</i></b><i> ...&quot;</i><br /><br />Eddington’s story is attractive because <i>&quot;the ‘duel in the sun’ he managed to set up refined some very complex physics down to a seemingly simple matter of the degree of deflection. Throw in the exotic locations, the struggle to reach them, and, in counterpoint, the extreme savagery of the First World War, and you have the scientific Odyssey par excellence.</i><br /><br />p52-53:<br /><br /><i>&quot;Delve a little deeper, however, and one begins to see that the solar eclipse expeditions of 1918–19 were no more successful than thousands of lesser experiments — past and present — in satisfying these model criteria. The chief reason that these studies retain their popularity is that Einstein’s ideas ultimately triumphed. Looking back on the solar-eclipse expeditions our presentist sensibilities incline us to think that the researchers of 1919 must have produced accurate and compelling data. But this, as we have seen in the past two chapters, need not be true at all. Indeed, here again it’s clear that the scientists involved were very lucky to be accepted by their posterity as having proved their point. For at the time, </i><b><i>as the science historians John Earman and Clark Glymour have shown, the evidence they presented was unquestionably inadequate. This leads on to the further question of why the scientific community embraced with such alacrity an experimental ‘proof’ that was really nothing of the sort.&quot;</i></b><br /><br />p53:<br /><br /><i>&quot;Both 1918–19 eclipse expeditions comprised British physicists. The first team, which observed the eclipse from Sobral in Brazil, was led by A. Crommelin and C. Davidson. The other, headed by Arthur Eddington and his assistant E. Cottingham, made its observations from the island of Principe, which lies off the coast of West Africa. </i><b><i>Eddington, born in the English Lake District, was already an eminent Cambridge physicist and it was his interpretation of both teams’ data-sets that would serve to vindicate Einstein. For this reason it is noteworthy that even before departing for Principe he was well known for his Einsteinian sympathies. As the most important expositor of general relativity within Britain, most of his colleagues knew that he was undertaking the eclipse expedition in the fervent hope of confirming his radical intuition that Einstein was right.&quot;</i></b><br /><br />p54-55:<br /><br /><i>&quot;Quite apart from the smallness of the measurements to be made, the technical difficulties facing the two teams were simply immense. </i><b><i>The most fundamental problems stemmed from the fact that a comparison was being made between the apparent locations of stars photographed in different parts of the sky in different seasons. Unavoidably, therefore, ambient temperatures were going to differ from one occasion to the other. This is important because the disparity in focal length between a warm and a cold telescope can easily produce a distortion equivalent to that which the experimenters were expecting to observe. A similar effect may be produced by the fact that the solar-eclipse photographs were to be taken during the day and the remaining photographs during the night. Aside from ambient temperature, both studies were also hampered by different degrees of ‘atmospheric turbulence’.</i></b><i> (This is the distortion to background images, mainly caused by convection currents, that can be seen when looking across the top of a hot barbecue; in tropical locations atmospheric turbulence would have been a very serious problem.) </i><b><i>On top of this, both parties faced the unavoidable problem of inclement weather. In the event, clouds were partially to obscure exposures taken by both groups.</i></b><br /><br /><b><i>Add to these hazards the possible mechanical changes to the telescopes caused by their having to be transported to sites so far from England, when even the slightest damage affecting the angle of the photographic plates would have had disastrous results. Exacerbating this problem, the eclipses had to be observed in remote areas where large state-of-the-art equipment could not be transported. Both teams had to rely on smaller models that required a long exposure time. As such, their telescopes had constantly to be counter-rotated so that the Earth’s rotation did not alter the point in the sky at which they were aimed. The mechanisms for rotation that the two teams constructed introduced yet another potential source of error.</i></b><br /><br /><i>Some of these difficulties could be controlled for and taken into account at the calculation stage. This generally involved determining the displacement of stars whose altered position could only have been caused by mechanical changes with the telescopes and photographic equipment. The measure of their displacement could serve as a reliable index to the amount of experimental distortion involved. Once these effects had been quantified, the behaviour of the target star beams could be isolated. </i><b><i>But making these adjustments accurately required a minimum of six undisplaced stars in each photographic frame; otherwise there was insufficient data for the statistical procedures to be performed. Additionally, neither team could deny that their experimental method was likely to involve errors that had not been identified and would therefore pass unrecognized.</i></b><br /><br /><b><i>To give a sense of just how serious these difficulties were, it’s worth mentioning that in 1962 a much-better equipped British party tried to reproduce Eddington’s findings. At the end of a frustrating attempt to do so they concluded that the method was much too difficult and could not be implemented successfully.&quot;</i></b><br /><br />p55-56:<br /><br /><i>&quot;On the long-awaited night of the eclipse, the Sobral team managed to obtain 19 plates from their astrographic telescope and 8 plates from their 4-inch telescope. </i><b><i>Eddington’s Principe team was hampered by cloud cover and took away just 16 plates, but only two of these, each showing only five stars, were actually usable.</i></b><i> The Sobral team managed to take the clearest photographs with its 4-inch telescope. These suggested a deflection of star beams grazing the Sun at between 1.86 and 2.1 seconds of arc, averaging out at 1.98 seconds. (Note that Einstein’s prediction was 1.7 seconds.) The Sobral team’s astrograph shots were of a lower quality, but 18 of them were used to calculate an average of 0.86 seconds. </i><b><i>In other words, one set of photographs was close to Einstein’s prediction, the other was very close to the Newtonian value of 0.8. Unfortunately the first score was too high to be strictly compatible with general relativity and the score in the second set was based on low-quality exposures. In addition, each set of photographs involved very large standard errors. This should have immediately prompted doubts as to the reliability of the averages themselves.&quot;</i></b><br /><br />p56:<br /><br /><b><i>&quot;From the usable Principe plates, Eddington calculated a star-beam displacement of between 1.31 and 1.91 seconds. But even these plates were of embarrassingly poor quality and it has been suggested that the mathematical formula he used to reach these figures was in itself biased.&quot;</i></b><br /><br />p56:<br /><br /><i>&quot;Self-evidently, with such poor and contradictory evidence, attempting a resolution of the controversy on the basis of these figures was an extremely risky affair. Take just one of the hazards mentioned above: atmospheric turbulence. In the hot environments in which both teams were working it was likely that all but the largest displacements would be cancelled out by this phenomenon. Had the teams been measuring star beams just clipping the Sun’s edge, their displacement might have been large enough to eliminate atmospheric turbulence as the sole cause. </i><b><i>In 1919, however, with the star beams closest to the Sun obliterated by the corona, those that could be observed were some way from the Sun’s rim. Consequently the displacements were so small that the entire effect could quite easily have been caused by atmospheric turbulence alone. At some level, the teams were aware of this. Thus, in discussions after the announcement of the eclipse results, Eddington and his assistants admitted that calculations of small displacements were unreliable. Yet, they refused to let this effect their presentation of the measurements. As we have seen, within a few months Einstein’s ideas were being adjudged victorious from the pulpit of the Astronomer Royal.&quot;</i></b><br /><br />p57:<br /><br /><b><i>&quot;The Sobral and Principe expeditions most certainly did not produce measurements that could definitively confirm either Newtonian or Einsteinian theory. In his book The Physical Foundations of General Relativity (1972), the British astronomer Dennis Sciama explained that eclipse observations are notoriously ‘hard to assess . . . since other astronomers have derived different results from a re-discussion of the same material’. In this case, there can be no doubt at all that both theories could potentially have been declared victorious, although it may have appeared to the Sobral team that the most likely verdict was a tie. But this is not what happened. Under Eddington’s hand, the eclipse results were subjected to extensive cosmetic surgery until they matched Einstein’s prediction. Without this treatment Einstein could not have been vindicated in 1919.&quot;</i></b><br /><br />p57:<br /><br /><i>&quot;Eddington began by casting doubt on the scores obtained by the Sobral team. He claimed that their astrographic results were not randomly distributed around the mean score as one would expect with normal data points. Instead, they were mostly beneath it, suggesting that a ‘systematic error’ had occurred that had artificially lowered the mean score itself. Without this error, he implied, their results would also have approximated to the higher Einsteinian prediction. This was a reasonable argument. </i><b><i>The problem was Eddington’s abject inability to show that the same error had not occurred in the other data-sets. When challenged, he produced not a single piece of unambiguous evidence to demonstrate that the measurements he accepted were unaffected by the same error. Even more seriously, Eddington conveniently ignored the fact that the Sobral team’s astrographic photographs were visually far superior to his own two hazy plates.</i></b><i> There may have been valid concerns about the reliability of Crommelin and Davidson’s photographs. But one thing should have been clear: Eddington’s were very much worse. As the American commentator W. Campbell wrote in 1923:</i><br /><br /><b><i>&#39;Professor Eddington was inclined to assign considerable weight to the African determination, but, as the few images on his small number of astrographic plates were not so good as those on the astrographic plates secured in Brazil, and the results from the latter were given almost negligible weight, the logic of the situation does not seem entirely clear.&#39;</i></b><br /><br /><i>This was an understatement of which any Briton would have been proud. Note also that Eddington’s two plates contained an insufficient number of undisplaced stars from which to make the necessary adjustments for error (five rather than six). Then factor in the large standard deviation in the results that he accepted (rendering most of the results either too high or too low), and one can understand why Earman and Glymour concluded in their 1980 article, </i><b><i>‘the eclipse expeditions confirmed the theory [of Einstein] only if part of the observations were thrown out and the discrepancies in the remainder ignored’.</i></b><i> In short, they didn’t.&quot;</i><br /><br />p58-59:<br /><br /><i>&quot;A core principle of the standard model of the scientific method is that theoretical predictions should not be allowed to influence which results are used and which are discarded. In Eddington’s approach, however ... predictions and data interpretation became mutually confirming. </i><b><i>Eddington evaluated his results according to how they conformed to his preferred theoretical predictions.</i></b><i> On one hand, inordinate value was attached to photographs that approximated Einstein’s 1.7 seconds of arc deflection; on the other, dubious ad hoc reasons were invented for jettisoning any that disagreed. </i><b><i>‘Einstein’s prediction had not been verified as decisively as was once believed’, Sciama gently pointed out in 1972. Reflecting on eclipse expeditions in general, he added, ‘one might suspect that if the observers did not know what value they were &#39;supposed&#39; to obtain, their published results might vary over a greater range than they actually do’. Or, as the Polish-American physicist Ludwik Silberstein said at a meeting of the Royal Astronomical Society in 1919, ‘If we had not the prejudice of Einstein’s theory we should not say that the figures strongly indicated a radial law of displacement’.</i></b><i> So serious were Eddington’s manipulations that one strongly suspects that had the predictions of the rival theories been the reverse -- Newton high, Einstein low -- Eddington would have discarded his own photographs as too hazy and accepted with alacrity the Sobral party’s astrographic pictures.</i><br /><br /><i>Most of Eddington’s contemporaries were either less incisive or less cynical than Silberstein and Sciama. As a result, after careful massaging, Eddington’s judiciously selected data-set could be presented as unequivocally supporting his candidate’s theoretical predictions. </i><b><i>Having discarded a full 18 plates on very specious grounds, he set about writing the official accounts of the expeditions. In these he routinely referred to only two sets of prints: the four 4-inch telescope photographs obtained by the Sobral team and his own very poor two photographs. As these images gave mean scores of 1.98 and 1.671 respectively, few scientific readers could avoid concluding that Newton had been decisively beaten:</i></b><i> the reigning champion for over 200 years had fallen at last.&quot;</i><br /><br />p59:<br /><br /><i>&quot;Once the eighteen astrographic plates had been rejected and forgotten, concerns about the quality of the Principe photographs quickly evaporated. The complexities of the issue receded from view, and the controversy between Einstein and the Newtonians suddenly -- but falsely -- appeared to be a one-horse race. This is clear from the account of the eclipse expeditions in James A. Coleman’s best-selling Relativity for the Layman (1969):</i><br /><br /><i>&#39;The Sobral group found that their stars had moved an average of 1.98 seconds of arc, and the Principe group’s had moved 1.6 seconds of arc. This nearness to the 1.74 seconds of arc predicted by Einstein was sufficient to verify the effect.&#39;</i><br /><br /><b><i>But in many cases unwittingly, Coleman and dozens of other scientific commentators skate over the fact that among astronomers Eddington’s account did not win immediate assent. Already, in 1918, an American expedition had travelled to Washington state to observe an eclipse. They had reported that the 1.7-second light deflection was ‘non-existent’. Ten further eclipse observations were made between 1922 and 1952. Only one of these produced seemingly high-quality data, and that suggested a displacement arc of 2.24 seconds -- substantially higher than predicted by Einstein. In fact, virtually every eclipse observation was either unreliable or, in most cases, both unreliable and higher than the Eddington scores. In light of these results, many of those at the cutting-edge of research into general relativity sensibly deferred judgement for rather longer than the accepted view implies. Some embraced general relativity only when evidence of an entirely different type became available.&quot;</i></b><br /><br />p59-61:<br /><br /><i>&quot;In overwhelming his critics, Eddington used the Royal Society of London to great effect. This body was set up in the late seventeenth century amidst a nation recoiling from a regicide and years of civil war.  Against such a background, the peaceful and mannerly resolution of controversies was given a very high priority. Scientists were no exception. The focal point of the Royal Society was a large lecture theatre in which the cream of the scientific establishment could gather to watch experiments being performed. The idea was that members would reserve judgement on any given topic until the relevant experiments had been carried out in front of them. Then, having personally seen the unvarnished facts, the scientific community could democratically arrive at a consensus and thereby avoid protracted conflict. In many ways, the same principles are alive and well today. Journal articles require the inclusion of detailed methodologies that should allow experiments to be repeated in other laboratories. Witnessing and consensus-forming no longer take place within one location on one occasion, but they can nevertheless be achieved.</i><br /><br /><i>But there have always been problems with achieving agreement on what an experiment does or does not prove. Today, science takes place on such a vast scale that it is not always convenient to replicate every important experiment performed. Further, as the British sociologist Harry Collins and his American collaborator Trevor Pinch have shown, some experiments require specialized training, highly recondite knowledge, and technical expertise that may take months or years for another laboratory to acquire. This means that scientists sometimes just have to take their colleagues’ word for it. In the case of the Sobral and Principe expeditions, quite apart from the tremendous difficulty in understanding general relativity and performing the appropriate calculations, </i><b><i>the experiments themselves were exceptionally difficult to perform, extremely expensive, and totally reliant on eclipses of the Sun. Thus, few astronomers were inclined to try to replicate Eddington’s results. In these circumstances, most astronomers were more than happy to accept his interpretations without demur.</i></b><i> Whatever else it may be, this case is a powerful demonstration of the role of trust in the advancement of science.</i><br /><br /><i>Yet, however high Eddington’s personal reputation stood in 1919, there were still major challenges facing him. Success required that the scientific community sin by omission by colluding, first, with his suppression of well over two-thirds of the photographs from the Sobral and Principe expeditions, and, second, with his ignoring the much more equivocal evidence advanced by other eclipse expeditions.&quot;</i><br /><br />p61:<br /><br /><i>&quot;In understanding why the scientific rank and file placed so much confidence in Eddington it has first to be appreciated that, to many, Einstein was already the greatest modern physicist. In addition, Eddington was not only an extremely accomplished astronomer in his own right, but he was British at a time when this counted for a great deal. Taking full advantage of his esteemed status, Eddington had the clout to secure the ascendancy of his own interpretation by enshrining it within a series of seminal papers and books that he himself authored. By 1919, Eddington had also acquired enormous credibility because he was such a fine expositor of general relativity. He grasped its implications with a flair that could not but inspire confidence. Such was his standing in this new scientific area that the following apocryphal story had wide currency. Eddington’s fellow physicist Ludwig Silberstein remarks, ‘Professor Eddington, you must be one of three persons in the world who understands general relativity’. After a longish pause, he continues, ‘Don’t be modest Eddington’, to which the latter replies, ‘On the contrary, I am trying to think who the third person is!’ The story is entirely mythical, but it is as illuminating as it is amusing.&quot;</i><br /><br />p61:<br /><br /><i>&quot;Arthur Eddington’s apparent vindication of Einstein’s ideas also gained rapid credence because of the status of many of its earliest converts. On 6 November 1919, Sir Joseph J. Thomson, the President of the Royal Society, announced to the assembled ranks of the scientific elite, ‘It is difficult for the audience to weigh fully the meaning of the figures that have been put before us, but the Astronomer Royal and Professor Eddington have studied the material carefully, and they regard the evidence as decisively in favour of the larger value for the displacement.’ With the weight of the President of the Royal Society and the Astronomer Royal on his side, Eddington could hardly have been surprised to read the following banner headlines in The Times the following morning:</i><br /><br /><i>Revolution in Science</i><br /><i>New Theory of the Universe</i><br /><i>Newtonian Ideas Overthrown&quot;</i><br /><br />p61-62:<br /><br /><i>&quot;‘It was generally accepted’, The Times report went on, ‘that the observations [of the eclipse] were decisive in the verifying of the prediction of the famous physicist Einstein.’ Over the next few weeks The Times carried several letters from respected scientists in support of relativity and even one from Einstein himself on the 28 November. The contributions of detractors, in contrast, were invariably scorned. Indeed, if we return once more to J. J. Thomson’s announcement, we see that he was as determined to browbeat the scientific community as was The Times the general reader. His concluding remarks included the observation that, ‘It is difficult for the audience to weigh fully the meaning of the figures that have been put before us’. It seems not unreasonable to paraphrase this as ‘It’s beyond your competence to judge in this matter so take our word for it’. Thus, if anybody present had challenged Eddington’s conclusions, the challenger would have been up against more than the weight of the evidence. With the three-line whip imposed by a seemingly holy alliance of the Astronomer Royal, the President of the Royal Society, and Eddington himself, none saw serious merit in disagreeing.</i><br /><br /><b><i>Once Thomson’s decree had been issued, the scientific community accepted the party line virtually en masse. And for the most part they did so despite lacking a proper understanding of the expeditionary data. Clearly, then, in this case much of the scientific community was prepared to endorse interpretations without being able to justify their decision on empirical grounds. Furthermore, most scientists subsequently stood by this position irrespective of the later publication of eclipse data that did not corroborate Eddington’s figures.</i></b><i> It is extraordinary how little these later critics managed to influence the debate after 1919. Cutting-edge researchers were the only scientists prepared to dispute the Eddington figures, but even though their results were published they did not have the strength to overturn the interpretations of 6 November 1919. After that date, they were battling against what can fairly be called a cultural consensus. Quite rationally, where non-astronomers reached the limits of their knowledge of astronomical science, they followed their instincts and backed their most accomplished and highly regarded colleagues. At least in the short run, what is perceived to be scientific truth is usually to be found on the side of the big battalions.&quot;</i><br /><br />p63:<br /><br /><i>&quot;Eddington fell far short of the canonical rules of the scientific method ... </i><b><i>In 1919, general relativity won the debate because it had the best public relations available.</i></b><i> But this was not a new phenomenon. Indeed, there is a certain poetic justice in Sir Isaac Newton having been eclipsed in this way. After all, several recent biographies have shown that it was partly Newton’s power-play tactics as President of the Royal Society that managed to win unusually rapid assent for his own ideas two centuries earlier.&quot;</i><br /><br />p99:<br /><br /><i>&quot;As of yet, the history of science is too young a discipline to have built the kind of database necessary for us to form a balanced view as to how typical such behaviour really is. All we can say now is that a not-inconsiderable proportion of the scientific greats examined by historians have had a real world existence at considerable variance with the near-perfect characters attributed to them by myth.</i><br /><br /><i>Nevertheless, although this may be a discrepancy from which only the great suffer, intuitively, this seems unlikely. Common experience tells us that feet of clay are not unique to heroes. </i><b><i>In all probability, manipulation of experimental data is not just the sin of a few great men who have somehow managed to slip through the net. Their particular ‘warts’ have been exposed for no other reason than that their fame has attracted historians to them.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"The Illusion of the Scientific Hero","summary":"Science Historian John Waller Warns in His Book to Take Nothing at Face Value in Science / He Argues that the Pristine Scientific Hero is All Too Often an Elaborate Fiction / And Failing to Take into Account the Larger Context for the Rise of Various Scientific Theories will Favor Narratives Imbued with Presentism - the False Sense that the Victors' Hypotheses were Brilliant Anticipations of Modern Knowledge","image":"https://lh3.googleusercontent.com/-Q4F4dfUZHgU/V4vo9UiQ7vI/AAAAAAAAIVo/4kBARA0uKzIsdXeQ192RvJfC0zOkOC51w/w1800-h2520/the-illusion-of-the-scientific-hero-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/eNKarADhoeN","publishDate":"2016-07-17T20:24:04.216Z","updateDate":"2017-01-03T02:08:14.010Z","text":"<b>The Illusion of the Scientific Hero: Science Historian John Waller Warns in His Book to Take Nothing at Face Value in Science / He Argues that the Pristine Scientific Hero is All Too Often an Elaborate Fiction / And Failing to Take into Account the Larger Context for the Rise of Various Scientific Theories will Favor Narratives Imbued with Presentism - the False Sense that the Victors&#39; Hypotheses were Brilliant Anticipations of Modern Knowledge</b><br /><br /><b><i>Fabulous Science Fact and Fiction in the History of Scientific Discovery</i></b><br />by John Waller<br /><br />p5:<br /><br /><b><i>&quot;[E]stablishing precisely where the truth lies is rarely easy. Personal biases of which the researcher may be unaware, a lack of unambiguous data, and the difficulty of seeing the world as our ancestors saw it can sometimes combine to make the task virtually impossible.&quot;</i></b><br /><br />p6:<br /><br /><i>&quot;When first thought about, it might seem that science is a field in which propaganda ... would have little or no scope. After all, science itself is all about the search for truth and the adversarial nature of the scientific method should work relentlessly to ensure that only valid ideas supported by well-designed experiments survive. I think that after reading this book, the most likely response to such notions will be </i><b><i>‘but that they were so’</i></b><i>. Because they have been sought out to demonstrate the gap that can exist between myth and reality, the various case studies I include are not offered as representative of all science or all scientists. </i><b><i>But one general message of the utmost importance can be drawn from them. Even in the realms of science, take nothing at face value.&quot;</i></b><br /><br />p6-7:<br /><br /><b><i>&quot;Until recent decades, the history of science was largely written by those who wished to place their chosen subject in as favourable a light as possible.</i></b><i> Their motivations were various. Sometimes they worked at the behest of individual scientists who wanted to make sure that their part in the great drama of discovery did not go unsung. In other cases, the key requirement was a good story. More laudably generations of teachers of scientific subjects have wanted heroes for much the same reason that Livy gave the Romans Horatio: to inspire by example. The chosen ones entered the Pantheon of scientific heroes. Great laboratories and institutes were named in their honour; each new generation of students was given accounts of their travails and ultimate triumphs; and assorted statuary serves as a perpetual memorial to their greatness.</i><br /><br /><b><i>In the last few decades, however, this approach has been rightfully impugned. A new generation of scholars has shown that in many cases what actually happened simply cannot sustain the enormous edifice subsequently built on it. Many of the great luminaries of the past were neither as heroic nor selfless as has been supposed. Seemingly crucial experiments are sometimes found to have been fatally flawed; results were often modified to suit the case being argued; and many were happy to use political influence to advance their cause. Indeed, ample evidence is now available to show that scientific merit is only one of many factors influencing the acceptance of new ideas. Many pre-eminent scientific heroes fell far short of proving the theories for which they are now famous.</i></b><i> Men such as Louis Pasteur, Joseph Lister, and Alexander Fleming were neither as surefooted nor as scrupulous as they are now thought to have been. Charles Darwin was right at least partly for the wrong reasons. Others, such as Gregor Mendel, have had greatness thrust on them by a highly manipulative posterity. </i><b><i>And, not infrequently, individuals now cast as scientific villains prove on closer examination to have been able scientists who just happened, often for very good reasons, to have backed the wrong horse.&quot;</i></b><br /><br />p7:<br /><br /><b><i>&quot;Above all, what this new research shows is that the conduct of scientific enquiry is often a lot more haphazard than we tend to think. Although the eventual outcome of a research programme may be a fabulously rich collection of well-attested and highly predictive ideas, the route to this happy state is often far more convoluted than subsequent accounts will allow.</i></b><i> Revealing what actually happened in some very high-profile cases may help bring our conception of the scientific enterprise into much closer alignment with the actuality. None of this undercuts the status I believe modern science deservedly enjoys as the best way of increasing our understanding of the physical world. </i><b><i>But our expectations will be more realistically grounded if we come to appreciate that science is as subject to extraneous influences -- including the human ego -- as is any other field of human endeavour, past or present.</i></b><br /><br />p7-8:<br /><br /><i>&quot;There is another important service that historians of science can render. As in all other branches of history, ‘great man’ approaches massively underplay the contributions made by the myriad individuals who did not achieve this honoured status. Thousands upon thousands of now largely forgotten researchers have contributed to scientific progress. And with very few exceptions, great men or women are cumulatively far less important than these forgotten legions of unsung heroes about whom little is popularly known. Indeed, some mute inglorious scientists were just as insightful and technically ingenious as those whose names have lived on. In many such cases, the differences in historical treatment are best explained in terms of a general preference for attaching major ideas to a limited number of names, coupled with skills, or the want of them, in the arts of self-promotion. </i><b><i>The pristine hero, exemplified by brave Horatio, is all too often an elaborate fiction. If we go back and look at the primary sources, few reputations escape entirely unscathed.&quot;</i></b><br /><br />p8:<br /><br /><i>&quot;In the context of these broader considerations, I have tried to use these case studies of nineteenth- and twentieth-century science to make three basic points. </i><b><i>First, that we need to treat received accounts of scientific genius with the utmost circumspection.</i></b><i> Thus we will find that Louis Pasteur, Charles Darwin, Gregor Mendel, Thomas Huxley, Joseph Lister, John Snow, Alexander Fleming, Frederick Winslow Taylor, James Young Simpson, Charles Best, Arthur Eddington, the Nobel Prize-winning Robert Millikan, and the authors of the famous ‘Hawthorne Study’ (Fritz J. Roethlisberger and William J. Dickson) have all been squeezed, or have squeezed themselves, into romantic schemas strongly redolent of the Horatio myth. In many cases, these men were competing for laurels in a highly competitive world in which the Queensbury rules of the scientific method were routinely dropped in favour of the more-permissive code of bare-knuckle fighting. The chapters I have devoted to Pasteur, Lister, Taylor, Millikan, Eddington, Best, and Roethlisberger and Dickson illustrate this particularly strongly. Indubitably, each of these scientific greats carried their share of human frailties.</i><br /><br /><b><i>The second point I seek to make is the critical importance of contextualization. Science is about much more than disembodied ideas.</i></b><i> In each chapter I set the events described in the broader context essential to a full appreciation of the complexities involved in the process of scientific discovery. </i><b><i>This book stresses the role of the prevailing scientific paradigm, the social and political context, and the vagaries of chance, all of which powerfully influence the rate and direction of scientific progress. Traditional approaches rarely accorded such factors their full weight.</i></b><i> My hope is that these cases will demonstrate the critical importance of remedying this.</i><br /><br /><i>This allusion to context brings me to the final theme of the book. </i><b><i>Failure to take full account of context leads to an error that modern historians call ‘presentism’.</i></b><i> Individual chapters dealing with Lister, Mendel, Darwin, Snow, Huxley, Simpson, and Fleming serve to elucidate this problem. </i><b><i>A vague affinity between a currently accepted theory and a much earlier set of ideas is often enough to elevate the ancestor into the Pantheon of scientific heroes.</i></b><i> Just as Macaulay, Margaret Thatcher, and very many others, have mistakenly read Magna Carta as an early flowering of English democratic values, </i><b><i>those recounting the history of science have often wrenched older ideas entirely out of context and interpreted them as brilliant anticipations of modern knowledge.</i></b><br /><br /><i>Some of the greatest icons of science have acquired hero status in precisely this way. </i><b><i>Put back into the context in which the originators lived out their lives, many ideas are found to be much less clearly aligned with what we now believe to be true.</i></b><i> But we are taught to demand much of our founding fathers. Their having been there at the beginning, pointing the way forward, does not seem to be enough. There is also a tendency to expect them, long after they have entered the grave, to remain in the van of progress, their ideas at least broadly anticipating each new development. </i><b><i>What we need to bear in mind is that the past really is another country and most certainly not one of which the present was an inevitable culmination.</i></b><i> Therefore my third aim is to encourage contextualization not only in its own right but also as a sovereign remedy to presentism. </i><b><i>We need to be committed to understanding the past on its own terms without any reference to ‘what happened next’.&quot;</i></b><br /><br />p12:<br /><br /><i>&quot;Naturally showing how scientific debates can be distorted by historical context and the human ego does detract from science’s reputation for unalloyed objectivity. Likewise, </i><b><i>there is no avoiding the conclusion that some of the greats of the history of science sometimes let ambition get in the way of integrity and good science.&quot;</i></b><br /><br />p12-13:<br /><br /><i>&quot;The twentieth-century philosopher of science Karl Popper made a useful distinction between discovery and verification in the development of scientific knowledge. A committed and eloquent believer in the ability of scientists to make sense of the world, </i><b><i>he nonetheless saw that the discovery stage may be much less rigorous and disciplined than the point at which other scientists become involved and begin the process of verification by trying to ‘falsify’ the original researcher’s ideas. ‘The question how it happens that a new idea occurs to a man -- whether it is a musical theme, or a dramatic conflict, or a scientific theory -- may be of great interest to empirical psychology; but it is irrelevant to the logical analysis of scientific knowledge’ is how Popper put it in 1959. Science only becomes reliable knowledge, he argued, after its validity has been extensively tested over the course of many years. Indeed, he was ‘inclined to think that scientific discovery is impossible without faith in ideas which are of a purely speculative kind, and sometimes even quite hazy’ … [T]he true complexities of Popper’s discovery stage are not widely appreciated&quot;</i></b><br /><br />p13:<br /><br /><b><i>&quot;The cases looked at here are important, therefore, in that they indicate just how tricky, uncertain, and byzantine a business scientific discovery can actually be. Contrary to the traditional view, this critical stage is mediated by a wide range of social and psychological factors that all too easily tempt researchers from the path of righteousness laid down by the rules of the scientific method as conventionally defined. During the verification stage, the prognosis for bad ideas supported by good PR is extremely poor. But when new territory is being opened up, there is far more scope for tactical skills and sheer force of personality to play decisive parts.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"Kristian Birkeland's Terrella","summary":"Kristian Birkeland is Today Recognized as the World's First Laboratory Astrophysicist / He was the First to Decode the Cause for the Aurora, and He Did So, in Part, by Constructing a Terrella (that's Latin for \"Little Earth\") / But, Birkeland's Work was Opposed in Britain / His Biggest Critic, Sydney Chapman, Refused to Even Look at the Terrella Experiment","image":"https://lh6.googleusercontent.com/-Z2SLX-0Tqnc/V4rxX7xXTAI/AAAAAAAAITw/71XQZzJm8l8tBXfCVA22aVPW2s7rg7hPACL0B/kristian-birkelands-controversial-little-earth-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/jnKCxcGEvuo","publishDate":"2016-07-17T02:46:52.075Z","updateDate":"2017-01-03T02:25:25.571Z","text":"<b>Kristian Birkeland&#39;s Terrella: Kristian Birkeland is Today Recognized as the World&#39;s First Laboratory Astrophysicist / He was the First to Decode the Cause for the Aurora, and He Did So, in Part, by Constructing a Terrella (that&#39;s Latin for &quot;Little Earth&quot;) / But, Birkeland&#39;s Work was Opposed in Britain / His Biggest Critic, Sydney Chapman, Refused to Even Look at the Terrella Experiment</b><br /><br /><i>The Electric Sky</i><br />by Don Scott<br /><b><i>&quot;Chapter 8: The Plasma Pioneers</i></b><br /><br /><i>Just before the dawn of the Twentieth Century, the magnificent aurora borealis displays in the northern night sky captivated the attention of a highly intelligent scientist, a man with an excellent understanding of the then new science of electricity.  He realized that to determine the cause of the auroras he had to leave his laboratory and gather actual data out in the frozen Land of the Midnight Sun.  That man was Kristian Birkeland.  He was the first physicist to look up at the night sky and seriously consider the idea that electricity might be actively at work out in space.  No one else did.  The development of what has come to be called Plasma Cosmology is a tale of both physical and intellectual heroism.  Birkeland started it all.</i><br /><br /><i>In the late 1800s and early part of the Twentieth Century, things were somewhat different in science from the way they are now.  Individual scientists were pretty much free to investigate what they wanted, to announce their ideas and to publish their results.  Birkeland decided to dedicate his career to solving the mystery of the aurora.  He did not have to get anyone&#39;s permission to do it.  He just did it.  This was before the advent of Big Science.  The peer review system had not yet gained control over what research areas were to be allowed to thrive.</i><br /><br /><i>Today most science is done within the confines of laboratories and universities.  Government and private funding agencies exert firm control over the direction and acceptability of scientific investigation.  If a lone investigator has an idea that runs counter to the scientific establishment, that idea is not likely to be funded and thus not likely to be investigated.  This has been the fate of electric plasma cosmology, especially recently.</i><br /><br /><i>But let us begin at the beginning.</i><br /><br /><b><i>Kristian Birkeland</i></b><br /><br /><i>In 1898 Birkeland was a professor at the University of Christiana (now Oslo).  He knew how electricity behaves.  He was familiar with Maxwell&#39;s Treatise on Electricity and Magnetism.  But most important, Birkeland was a willing data gatherer.  He went out from his lab and took his measurements of geomagnetic activity underneath those aurora displays -- risking both his own life and those of his men in that bleak, twenty-four-hour winter darkness.  He built remote observing stations in northern Norway, where he and his assistants almost froze to death several times in their treks across those vast windswept snowfields.</i><br /><br /><i>He was the first person to propose that electric currents come from the Sun, flow into Earth&#39;s upper atmosphere, and cause the auroras.  These currents pass through and excite the plasma that exists high above Earth&#39;s surface to such a degree that it becomes visible: it begins to glow.  In those days the word </i><b><i>plasma</i></b><i> had not yet been applied to the phenomenon that Birkeland was investigating.  He talked simply about &#39;electric effects&#39; in the upper atmosphere.</i><br /><br /><i>Maxwell had demonstrated that electric currents and magnetic fields are inextricably interrelated.  But Birkeland stood alone in proposing that electric currents from the Sun cause the auroras.  Only he (since Franklin) said that electricity is important in the cosmos -- that our sky is an electric sky.</i><br /><br /><b><i>Birkeland was roundly criticized by mathematical theoretical physicists who never left their desks to do the kind of experimental data-gathering that he had done. They claimed that his theory was impossible because mutual repulsion of like charges within any current stream coming from the Sun would dissipate that stream before it reached Earth. The theorists did not yet know that the plasma surrounding the Sun, and in which all the planets reside, is a good electrical conductor for exactly that kind of stream of charges.  Birkeland had stood out under the aurora and measured it.</i></b><br /><br /><i>Although Birkeland is best known for his investigations of the auroras, his vacuum chamber experiments with magnetized metal spheres, which he called &#39;terrellas,&#39; are equally significant.  An electromagnet inside a metal sphere was slightly tilted to mimic the angle by which Earth&#39;s magnetic axis is tilted away from its rotational axis.  The vacuum chamber into which he placed the sphere mimicked the space that surrounds the Earth.  A cathode within the chamber supplied an electric current to the sphere just as Birkeland thought the Sun does to Earth.</i><br /><br /><i>Lucy Jago, in her biography of Birkeland, The Northern Lights, describes the first demonstration of his terrella:</i><br /><br /><i>&#39;Once Birkeland was satisfied that electrons were streaming from the cathode, he flicked the switch beside the chamber and powered the electromagnet in the terrella.  Within seconds, a purple glow could be seen encircling the Earth at the equator. </i><b><i>As Birkeland increased the strength of the magnetic field around the Earth, the circle divided, and two circles began to move toward the poles. The audience fell silent as the two spiral rings of glowing phosphorescent light hovered around the poles of the Earth, eerie and magical.  After a few minutes Birkeland turned off the magnet and the cathode in the terrella; the glow disappeared, and the audience took a collective breath.&#39;</i></b><br /><br /><i>Copies of Birkeland&#39;s 300-page book, The Norwegian Aurora Polaris Expedition 1902-1903, published in 1908, were sent to eminent scientists and heads of state all over Europe. </i><b><i>It was very well received in all countries -- except in Britain. Why this happened is unclear. Was it because Newton was an Englishman, and British astronomers sensed a threat to the &#39;gravity only&#39; cosmological mindset even at that early time? Was it because Kelvin had already made clear his antipathy toward electromagnetism? Whatever the reason, there was a decidedly anti-electrical attitude within English scientific circles. Hostility toward electrical effects in astronomical (and auroral) phenomena continued to predominate in British and American science.</i></b><br /><br /><b><i>A particularly tenacious English mathematician, Sydney Chapman, who was interested in geomagnetism, continually denigrated Birkeland and criticized his work for half a century. Chapman&#39;s ideas about auroras involved the kinetic theory of neutral gases and a &#39;dynamo&#39; which he said was driven by tidal flows in Earth&#39;s ionosphere. Fifteen years after Birkeland&#39;s death, Chapman admitted that plasma from the Sun caused the auroras.  But he continued to belittle Birkeland&#39;s work.</i></b><br /><br /><b><i>As late as 1967, Chapman said that Birkeland&#39;s &#39;direct observational contributions to auroral knowledge were slight.&#39; An American scientist and electrical engineer, Alex Dressler, a former editor of the prestigious journal, Geophysical Research Letters, questioned Chapman about Birkeland. &#39;I asked him whether Birkeland&#39;s work had any influence on him at all. He glanced at me and said, &#39;How could it?  It was all wrong.&#39;&#39;</i></b><br /><br /><i>With the advent of high altitude rockets and satellite technology, Birkeland&#39;s explanation of the aurora was found to be correct.  Today Birkeland is acknowledged as having been the first scientist to accurately determine, through heroic observation in the field, laboratory experimentation, and theoretical description, the correct cause of the auroral displays -- electric current from the Sun flowing in plasma and causing that plasma to emit light.</i><br /><br /><i>Birkeland also discovered the twisted corkscrew-shaped paths taken by high-intensity electric currents in plasmas.  Sometimes those twisted shapes are visible and sometimes now -- depending on the strength of the current.</i><br /><br /><i>A somewhat counterintuitive result of Birkeland&#39;s work was the discovery that electric currents in space tend to </i><b><i>follow</i></b><i> magnetic fields, not cut across them.  Today, because of this discovery and in his honor, these spiraling streams of ions and electrons that we observe in space are called Birkeland Currents.  The strange &#39;sprites,&#39; &#39;ELVES,&#39; and &#39;blue jets&#39; associated with electrical storms on Earth are examples of Birkeland currents within the plasma of our upper atmosphere.  The Helix Nebula is another example at a stellar scale.</i><br /><br /><b><i>When Birkeland died, the Nobel awards committee was considering his name.</i></b><i> His portrait is on the 200 Kroner Norwegian currency to this day.</i><br /><br /><i>Lucy Jago says of this pioneer, &#39;Birkeland now has a crater on the Moon named for him, which, together with Birkeland Currents and the wider acceptance of his work, should prevent his memory from fading, but </i><b><i>rejection of his theories probably slowed the advance of geomagnetic and auroral physics for nearly half a century.&#39;</i></b><br /><br /><i>Another writer said of him, &#39;Misunderstood in his lifetime, some of Birkeland&#39;s ideas about our universe are now considered brilliantly prophetic.&#39;  But Birkeland was not misunderstood. </i><b><i>He was just tenaciously opposed by lesser men.&quot;</i></b><br /><br /><a href=\"https://www.plasma-universe.com/Birkeland_current\" class=\"ot-anchor\">https://www.plasma-universe.com/Birkeland_current</a><br />(Ian Tresman&#39;s site)<br /><br /><i>&quot;The history of Birekland Currents appears to mired in politics. [17]</i><br /><br /><i>After Kristian Birkeland suggested &#39;currents there are imagined as having come into existence mainly as a secondary effect of the electric corpuscles from the sun drawn in out of space,&#39; (1908), </i><b><i>his ideas were generally ignored in favour of an alternative theory from British mathematician Sydney Chapman.</i></b><br /><br /><i>In 1939, the Swedish Engineer and plasma physicist Hannes Alfvén promoted Birkeland&#39;s ideas in a paper published on the generation of the current from the Solar Wind. [18] One of Alfvén&#39;s colleagues, Rolf Boström, also used field-aligned currents in a new model of auroral electrojets (1964). [19]</i><br /><br /><i>In 1966 Alfred Zmuda, J.H. Martin, and F.T.Heuring reported their findings of magnetic disturbance in the aurora, using a satellite magnetometer, </i><b><i>but did not mention Alfvén, Birkeland, or field-aligned currents, even after it was brought to their attention by editor of the space physics section of the journal, Alex Dressler.</i></b><i> [20]</i><br /><br /><i>In 1967 Alex Dessler and one of his graduates students, David Cummings, wrote an article arguing that Zmuda et al had indeed detected field align-currents. [21] Even Alfvén subsequently credited (1986) that Dessler &quot;discovered the currents that Birkeland had predicted&quot; and should be called Birkeland-Dessler currents. [22]</i><br /><br /><i>In 1969 Milo Schield, Alex Dessler and John Freeman, used the name &quot;Birkeland currents&quot; for the first time. [23] In 1970, Zmuda, Armstrong and Heuring wrote another paper agreeing that their observations were compatible with field-aligned currents as suggested by Cummings and Dessler, and by Bostrom, </i><b><i>but again made no mention of Alfvén and Birkeland.</i></b><i> [24]</i><br /><br /><i>In 1970, a group from Rice University also suggested that the results of an earlier rocket experiment was consistent with field-aligned currents, and credited the idea to Boström, and Dessler and his colleagues, </i><b><i>rather than Alfvén and Birkeland.</i></b><i> [25] In the same year, Zmudu and Amstrong did credit Alfvén and Birkeland, but felt that they &#39;...cannot definitely identify the particles constituting the field-aligned currents.&#39; [26]</i><br /><br /><i>It wasn&#39;t until 1973 that the navy satellite Triad, carrying equipment from Zmuda and James Armstrong, detected the magnetic signatures of two large sheets of electric current. Their papers (1973, 1974) reported &quot;more conclusive evidence&quot; of field-aligned currents, citing Cummings and Dessler </i><b><i>but not mentioning Birkeland or Alfven.</i></b><i> [27]</i><br /><br /><i>It had taken 65 years to confirm Birkeland&#39;s original predictions.&quot;</i><br /><br />[17] Brush, Stephen G., &quot;Alfven&#39;s programme in solar system physics&quot;, IEEE Transactions on Plasma Science (ISSN 0093-3813), vol. 20, no. 6, p. 577-589.<br /><br />[18] Alfvén, Hannes, &quot;Theory of Magnetic Storms and of the Aurorae&quot;, K. Sven. Vetenskapsakad. Handl., ser. 3, vol. 18, no. 3, p. 1, 1939. Reprinted in part, with comments by A. J. Dessler and J. Wilcox, in Eos, Trans. Am. Geophys. Un., vol. 51, p. 180, 1970.<br /><br />[19] Bostrom, R., &quot;A model of the auroral electrojets,&quot; J. Geophys. Res., vol. 69, p. 4983, 1964.<br /><br />[20] Zmuda, A. J.; Martin, J. H.; Heuring, F. T. &quot;Transverse magnetic disturbances at 1100 kilometers in the auroral region,&quot; J . Geophys. Res., vol. 71, p. 5033, 1966.<br /><br />[21] Cummings, W. D.; Dessler, A. J.; &quot;Field-aligned currents in the magnetosphere,&quot; J. Geophys. Res., vol. 72, p. 1007, 1967.<br /><br />[22] Alfvén, Hannes, &quot;Double layers and circuits in astrophysics,&quot; IEEE Trans. Plasma Sci., vol. 14, p. 779, 1986 (on p. 787). PEER REVIEWED. Reproduced from &quot;Keynote Address&quot; in Double Layers in Astrophysics, Proceedings of a Workshop held in Huntsville, Ala., 17-19 Mar. 1986. Proceedings FULL TEXT PDF (page 12)<br /><br />[23] Schield, M. A.; Freeman, J. W.; Dessler, A. J., (1969) &quot;A Source for Field-Aligned Currents at Auroral Latitudes&quot;, Journal of Geophysical Research, Vol. 74, p.247.<br /><br />[24] Zmuda, A. J.; Armstrong, J. C.; Heuring, F. T. &quot;Characteristics of transverse magnetic disturbances observed at 1100 km in the auroral oval&quot;, J. Geophys. Res., vol. 75, p. 4757, 1970.<br /><br />[25] P. A. Cloutier, H. R. Anderson, R. J. Park, R. R. Vondrak, R. J. Spiger and B. R. Sandel, &quot;Detection of geomagnetically aligned currents associated with an auroral arc&quot;, J. Geophys. Res., vol. 75, p. 2595, 1970.<br /><br />[26] Armstrong J. C.; Zmuda, A. J.; &quot;Field-aligned current at 1100km in the auroral region measured by satellite,&quot; J. Geophys. Res., vol. 75, p. 7122, 1970.<br /><br />[27] Armstrong J. C.; Zmuda, A. J.; &quot;Triaxial magnetic measurements of field-aligned currents at 800 kilometers in the auroral region: Initial results,&quot; J. Geophys. Res., vol. 78, p. 6802, 1973. PEER REVIEWED; Zmuda A. J.; Armstrong J. C.; &quot;The diurnal flow pattern of field-aligned currents,&quot; J. Geophys. Res., vol. 79, p. 4611, 1974.<br /><br /><a href=\"https://www.thunderbolts.info/tpod/2007/arch07/070529parallelauroras.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2007/arch07/070529parallelauroras.htm</a><br /><br />May 29, 2007<br /><b>Parallel Auroras</b><br /><br /><i>&quot;The nature of auroras was the subject of a debate that began early in the 20th century. But later in the same century, our space probes settled the debate in favor of the electrical theorists Kristian Birkeland and Hannes Alfvén.</i><br /><br /><i>One of the conflicts in early 20th Century astronomy was between Sydney Chapman and Hannés Alfvén. Alfvén, following Birkeland&#39;s lead, believed the auroras to be powered by charged particles from the Sun. Chapman developed a mathematically elegant theory showing that the auroras were generated entirely in the Earth&#39;s magnetosphere by buffeting of the solar wind. Chapman refused to give Alfvén&#39;s ideas a hearing. At conferences, rather than address particular points of the theory, Chapman would state that he and his colleagues disagreed with Alfvén and that a paper explaining it all was in process. </i><b><i>On one occasion, when Chapman was a guest of Alfvén&#39;s in Sweden, Alfvén built a replica of Birkeland&#39;s terrella experiment, which produced auroras on a magnetized sphere suspended in a vacuum. Alfvén hoped that if Chapman could see how plasma behaves in the laboratory, he would be more amenable to discussing it. Chapman refused to look at the experiment.</i></b><br /><br /><i>But eventually, in the late 60&#39;s and early 70&#39;s, space probes did show that Birkeland and Alfvén were right -- the auroras are caused by charged particles flowing from the Sun. Solar plasma does enter the Earth&#39;s magnetospheric bubble. And today we have websites that monitor solar observations and predict when and where to look for auroras. During a geomagnetic storm in 2001, the Polar Mission took the above photos (see link to view the video version) of the north and south auroras at the same time, here projected on a globe. This project showed that the two phenomena brighten and dim together as a near-mirror image.</i><br /><br /><i>This shows that the Earth is no longer an isolated body in space. It is connected to the circuitry of the Sun, and from there to the circuitry of the galaxy and beyond. It provides us with a preview of the discoveries of the future.&quot;</i><br /><br />Link to press release and animation:<br /><a href=\"http://www.gsfc.nasa.gov/topstory/20011025aurora.html\" class=\"ot-anchor\">http://www.gsfc.nasa.gov/topstory/20011025aurora.html</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"Uniformitarianism vs Catastrophism","summary":"The Uniformitarian Assumption is Possibly the Most Important Assumption in Modern Science / It Dictates that Our Explanations Need Not Consider Processes which are Not Currently at Work / The Idea Was Originally Designed as a Clever Political Weapon to Undermine the Religious Authority of the Monarch / Today's Catastrophism has Little Need for Religion, Yet the Assumption is Applied Just the Same","image":"https://lh3.googleusercontent.com/-tHDjOittE1o/V4j3vsekY-I/AAAAAAAAITM/w3hwcNzXkgUylgYLBoGdh5i36KelQVhIg/w7142-h9999/uniformitarianism-vs-catastrophism-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/S9iiBv1SucM","publishDate":"2016-07-15T14:48:56.328Z","updateDate":"2017-01-03T02:09:17.474Z","text":"<b>Uniformitarianism vs Catastrophism: The Uniformitarian Assumption is Possibly the Most Important Assumption in Modern Science / It Dictates that Our Explanations Need Not Consider Processes which are Not Currently at Work / The Idea Was Originally Designed as a Clever Political Weapon to Undermine the Religious Authority of the Monarch / Today&#39;s Catastrophism has Little Need for Religion, Yet the Assumption is Applied Just the Same</b><br /><br />(the graphic&#39;s references are listed at the end of this text)<br /><br />A collection of the best essays on the debate.  They are ordered so that they become more nuanced and complex the further you read along ...<br /><br />Formerly available online at <a href=\"http://courses.washington.edu/ess408/Gould1965.pdf\" class=\"ot-anchor\">http://courses.washington.edu/ess408/Gould1965.pdf</a>﻿<br /><br /><i>&quot;[N]ineteenth century catastrophists postulated a succession of great upheavals culminating in a &#39;general flood which swept away the quadropeds from the continents, tore up the solid strata, and reduced the surface to a state of ruin&#39; (Buckland, 1823, p224). </i><b><i>Their worldview extended beyond a simple theory of geologic change; it encompassed a methodological procedure, excluded from the modern definition of science, which permitted direct providential control of earth history</i></b><i> ...</i><br /><br /><i>... </i><b><i>Lyell and the uniformitarian school could not agree. To become a science, they affirmed, geology needed not only an empirical theory unencumbered by Biblical preconception, but also a methodology which affirmed the potential natural explanation of terrestrial development and relegated intrinsic mystery to its proper theological realm ...</i></b><br /><br /><i>... But Lyell was even more interested in controverting the catastrophist methodology which affirmed that geology &#39;could never rise to the rank of an exact science (1833, v.3, p.3); for it could lead only to &#39;hypotheses, in which we see the ancient spirit of speculation revived, and a desire manifested to cut, rather than patiently untie, the Gordian knot&#39; [an extremely difficult or involved problem] (p.6). </i><b><i>He thus postulated another, very different, type of uniformity that asserted the invariability of natural laws in space and time as a necessary condition to his contention that reference need only be made to observable processes in explaining past changes.&quot;</i></b><br /><br />From: Catastrophism and Ancient History III:2 (July 1981)<br /><b>The Great Debate</b><br /><br />Dudley Bromley<br /><br /><i>&quot;Here lies the core of the problem: </i><b><i>Did the earth change in a slow process</i></b><i>, a year added to a year and a million added to a million, the peaceful ground of nature being the broad arena of the contest of throngs, in which only the fittest survived? </i><b><i>Or did it happen, too, that the very arena itself, infuriated, rose up against the contestants and made an end to their battles?</i></b><br /><br /><b><i>Thirty years ago when Immanuel Velikovsky asked these questions, he challenged one of the fundamental principles of science. He suggested that a large part of scientific thinking was based on a doctrine that had evolved into dogma, a theory -- unproven -- masquerading as law.</i></b><br /><br /><i>Not surprisingly, Velikovsky&#39;s ideas set off a storm of controversy in 1950. </i><b><i>What is surprising, especially to those who so vehemently dismissed Velikovsky as a crank, is that the controversy continues to this day.</i></b><i> What&#39;s more, there is every indication that the debate may soon culminate with surprising -- and shocking -- results.</i><br /><br /><i>The disagreement revolves around two doctrines of scientific thought and is indicative of a sharp division in the world of science, a division that is as emotional as it is intellectual. The controversy has made as well as ruined careers and has slashed across the entire spectrum of sciences, although </i><b><i>arguments tend to become the most heated in the fields of astronomy, biology, and geology. It is in these areas that the war of ideas is now being waged.</i></b><br /><br /><i>The past few years especially have seen startling evidence presented which indicates that the scientific community as a whole may be on the verge of a dramatic turnaround in its most fundamental position, a reversal that could see a long accepted scientific doctrine rejected in favor of one that has been in disdain for well over 100 years.</i><br /><br /><i>The two doctrines are known as &#39;catastrophism&#39; and &#39;uniformitarianism.&#39; </i><b><i>Proponents of catastrophism believe that the world is not the safe place we have been led to believe, that holocausts on a global scale are recurrent events in earth&#39;s natural history and that these cataclysmic episodes are the primary sculptors of the planet&#39;s surface. Uniformitarianism, on the other hand, demands that all the evidence be explained by processes which are observed to be currently at work; in other words, nothing has happened in the past that is not observed to be happening right now.</i></b><br /><br /><i>That the earth has undergone tremendous change since its formation 3.5 billion years ago is not the issue. The debate is over the time involved in the changes (slow and gradual vs. almost overnight) and in the types of changes (constant, uniform, small-scale events vs. sudden, rare, large-scale events).</i><br /><br /><i>The debate seemed to have been settled once and for all in the middle of the last century. At that time astronomers, adhering to the time-honored mathematical assurances of such giants as Newton and Kepler, had long espoused the idea of a smooth and orderly system of planets revolving around the sun in eternal harmony. The only opposition to the idea of uniformity came from among those naturalists who in the course of their work had made a study of the earth&#39;s surface.</i><br /><br /><i>The belief that various catastrophes had wracked the earth in the past was shared by a number of naturalists in the early part of the 19th century, including the brilliant French paleontologist, </i><b><i>Georges Cuvier</i></b><i>, who proposed that fossils were the remains of organisms that had been annihilated by catastrophes and that new organisms -- or modified forms of old organisms -- resulted after each such holocaust. But the majority of experts lost little time embracing the doctrine of uniformitarianism. Interestingly, the reason for their acceptance of uniformitarianism over catastrophism was not very scientific.</i><br /><br /><b><i>Geology as a science was in its infancy in the 19th century. In their attempt to find acceptance in the scientific community, most geologists sought to separate themselves from Bible literalists who taught that the world was created in six days. They tried at all costs to avoid the rare and mysterious; thus uniformity was encouraged and those who preferred catastrophic explanations drifted off into other fields, such as paleontology or glacial geology. The result was an almost unanimous agreement among geologists that uniformitarianism was valid.</i></b><br /><br /><b><i>The beginning of the end of the doctrine of catastrophism came in 1830, when English attorney Charles Lyell published a theory which declared that all natural forces had been going on in the same basic manner throughout history. The theory became very popular, and uniformitarianism was elevated to the position of scientific law.</i></b><br /><br /><b><i>As it happened, a close friend and disciple of Lyell&#39;s was responsible for carrying the doctrine of uniformity into the field of biology. The friend, who dedicated his Journal to Lyell, is more responsible for the widespread acceptance of the doctrine of uniformitarianism in the scientific community than any other person in history. His name was Charles Darwin.</i></b><br /><br /><i>Herbert Spencer and Alfred R. Wallace each independently came to the same conclusion as Darwin (</i><b><i>the phrase &#39;survival of the fittest&#39; belongs to Spencer</i></b><i>), but it was Darwin who became the most influential. In 1859 Darwin published The Origin of the Species, in which he proposed his idea of organic evolution through a slow and meticulous process called &#39;natural selection.&#39; The main tenet of his theory was that heavy, delicate, or poorly defended species gradually disappear, eliminated (out-adapted, actually) by lighter, stronger, better defended species. Furthermore, said Darwin, natural selection is the prime process at work in evolution.</i><br /><br /><b><i>Darwin admitted in Origin that &#39;scarcely any paleontological discovery is more striking than the fact that the forms of life change almost simultaneously throughout the world.&#39; But because it went against his training, he rejected a catastrophic explanation, saying that in order to wreak the havoc described by the evidence it would be necessary to &#39;shake the entire framework of the globe,&#39; which was exactly what the catastrophists maintained.</i></b><br /><br /><i>Darwin explained the apparent conflict between his theory and the observable evidence by postulating that enormous gaps existed within the fossil records, &#39;blank intervals&#39; during which the slow evolutionary process could be observed. He then asserted that although the fossil record was incomplete it nevertheless revealed many examples of organismic variation and modification in the past. Fossils, he concluded, are evidence that the forms of life have changed throughout history and that some species have become extinct while others have survived. </i><b><i>Of course, no sensible person would question Darwin&#39;s conclusion; the controversy arises over his contention that the main or only agent responsible for evolutionary process is natural selection.</i></b><br /><br /><b><i>Darwin wrote The Origin of Species primarily to put to rest the doctrine of catastrophism. He fully expected the biggest objections to his book to come from that sector of science. But the few remaining catastrophists in the scientific community were geologists, and, as remarked above, they were well on the wane by the time Origin was published. Darwin&#39;s major critic turned out to be the Church, which steadfastly maintained that all animals had been created in their present forms less than 6,000 years ago, a position so untenable that natural selection won by default. As a result, Darwin&#39;s theory was embraced by science, and the doctrine of catastrophism would lay buried and forgotten for the next 100 years.</i></b><br /><br /><i>Then in 1950 a remarkable book resurrected catastrophism and set off a storm of controversy unprecedented in the world of science. The book was Worlds in Collision, and its author, Immanuel Velikovsky, defiantly challenged astronomers, biologists, geologists -- anyone and everyone who adhered to the doctrine of uniformitarianism. He produced a consummate body of evidence, including historical accounts contained in the literature of ancient peoples all over the world, that cataclysms have occurred within the memory of man. </i><b><i>&#39;We are the descendants of survivors,&#39; was his message and the implication was clear: &#39;survival of the fittest&#39; should read &#39;survival of the lucky.&#39;</i></b><br /><br /><i>Although scientists from every discipline vehemently attacked Velikovsky&#39;s work, his ideas would not go away. In part this was due to the irrefutable nature of much of his evidence, but even more important, scientists began to realize there are many aspects of natural history not satisfactorily explained by slow, uniform processes. Consequently, more and more scientists began to reexamine the evidence, which when combined with new technological breakthroughs of the past few years has brought about a change in attitude among many experts, a change that may signal the beginning of a complete turnaround in scientific thinking.</i><br /><br /><i>As in the past, geologists (and their close associates, the paleontologists, who study fossils) lead the scientific world in the active consideration of catastrophism as a legitimate doctrine. Perhaps this is because they are almost everywhere confronted by what is otherwise inexplicable. As they study the surface of our planet, geologists find the earth&#39;s crust crumpled and shattered on a scale so enormous as to be incomprehensible in fashionable terms. The physical state of the globe cannot be satisfactorily explained in terms of the slow, meticulous, natural forces that uniformity demands.</i><br /><br /><i>For example: many geologists agree that the earth&#39;s greatest mountain range, the Himalayas, was formed relatively recently and relatively rapidly. Some geologists believe the Himalayas reached their present height within the past 10,000 years, and at least two estimates put the date within recorded times.</i><br /><br /><i>There is convincing evidence that numerous climatic changes have taken place on earth, most occurring quite suddenly. Related to this is the fact that recurring Ice Ages have swept over the planet, leaving glacial scars all over the earth&#39;s surface, including the equator, where glaciation would not normally be expected to occur.</i><br /><br /><i>Even more intriguing is evidence suggesting that the magnetic poles have changed their places on the earth&#39;s surface hundreds of times since geological history began and that they have exchanged places, i.e., the earth has reversed polarity 171 times in the past. (The alignment of magnetic particles found in some ancient pottery suggests that the most recent pole reversal occurred since the dawn of civilization.)</i><br /><br /><i>Nowhere is the evidence more overwhelming or more awesome that in the field of paleontology. The fossil record is strewn with the debris of mass annihilation of life forms, of abrupt extinction on a global scale, and of similarly abrupt emergence of new species. Was Cuvier right after all? Some paleontologists think so.</i><br /><br /><i>Perhaps it would be prudent here to discuss the origin of fossils. One of the central reasons for the widespread acceptance of catastrophism among paleontologists has always been the failure of uniformitarianism to explain fossilization. In fact, the explanation offered by the proponents of uniformity is contradictory to the fundamental principle of that doctrine.</i><br /><br /><b><i>According to the long-accepted theory, dead plants and animals become buried in sediments of ocean bottoms and lake beds, where they eventually become fossilized. But in order for a plant or animal to become entombed before its remains disintegrate or before being devoured by scavengers, it must be buried rapidly, which is diametrically opposed to the slow process defined by uniformity.</i></b><br /><br /><i>Furthermore, in normal conditions the rate of sedimentation in deep seas varies from a few millimeters to a few centimeters per millennium; in shallow water this rate may be as high as ten centimeters per century, or one millimeter per year. </i><b><i>In either case the rate of sedimentation is not nearly enough to cause fossilization. This strongly implies that no fossils are currently being formed, thus tolling the death knell on uniformity, whose disciples religiously maintain that nothing took place in the past that is not taking place in the present.</i></b><br /><br /><i>Catastrophism, on the other hand, readily accounts for fossilization. If ocean floors are suddenly lifted in some places and dropped in others, sediments will shift dramatically and overwhelmingly; the contents of the sea depths will spill onto land; plants and animals will be engulfed by enormous tides and buried by avalanches of sand and volcanic ash and thus become entombed, undevoured and undecayed.</i><br /><br /><i>That mass extinctions have recurred in the past is no longer even debated among paleontologists. The most recent extinction came at the close of the last Ice Age </i><b><i>(which, incidentally, was once believed to have occurred 3.5 million years ago, but gradually the figure has been trimmed until it now reads 10,000 years ago -- and still questioned)</i></b><i>. Among its victims were the woolly mammoth, the mastodon, the sabre-tooth tiger, the super-bison, and many other large land mammals.</i><br /><br /><i>Thousands of tons of torn and broken carcasses of these animals have been found above the Arctic Circle in Alaska and Siberia, heaped together with millions of shattered trees. In Alaska the conglomeration is known as &#39;muck,&#39; and it is deposited across the entire northern part of the state, forming a frozen mass longer than the Atlantic seaboard from Newfoundland to Florida.</i><br /><br /><i>Perhaps the most famous mass extinction occurred 65 million years ago. At that time, the dinosaurs, which were by far the most successful animals in the history of the world, having dominated the planet, we hear, for over 150 million years, died out completely and suddenly (estimates range from a million years -- barely a second on the geological clock -- to a single day, with neither extreme being more or less likely than the other).</i><br /><br /><i>Along with the dinosaurs went their air-borne cousins (the pterosaurs) and their relatives in the sea (the ichthyosaurs, plesiosaurs, and morasaurs). In addition, virtually all oceanic plankton disappeared, as did nearly all of the higher mollusks called ammonites and the entire population of marine reptiles, except turtles. Many land plants, mammals, birds and reptiles were also wiped out. This extinction, like all the others, occurred throughout the world and in all environments -- land, sea, and air -- simultaneously.</i><br /><br /><i>One possible explanation for this particular extinction has received very strong support of late. Impressive evidence has been uncovered in various parts of the world by Luis and Walter Alvarez of Berkeley, California, which suggests that a giant asteroid hit the earth 65 million years ago, an asteroid so big that it threw up a dust cloud thick enough to block out the sun (thus bringing photosynthesis to a halt) for more than a decade.</i><br /><br /><i>One of the most recent and far-reaching studies of fossils was done by Niles Eldredge, curator of the Department of Invertebrates at the American Museum of Natural History. His conclusions are indicative of the new emerging attitude of many scientists.</i><br /><br /><i>Eldredge studied thousands of fossilized trilobites (extinct relatives of crabs and shrimps) which flourished on earth 380 million years ago. The fossils he studied came primarily from South America. They were exceptionally well-defined specimens and they ranged over a vast expanse of geological time.</i><br /><br /><i>Uniformity depicts anatomical changes through time as caused by natural selection, which theoretically preserves the best of the designs available for coping with the environment. By definition this must be a generation-by-generation process, working on small amounts of variation to slowly change the genetic and anatomical makeup of a species.</i><br /><br /><b><i>In his study of trilobite fossils, however, Eldredge found no evidence of such smooth progression. Instead, he noted that species persisted for long periods of time without change, followed by the sudden appearance of new species, with no transitional form in evidence. He found that in almost every case the sudden appearance of new species followed immediately after the abrupt disappearance of earlier species.</i></b><br /><br /><i>Eldredge&#39;s evidence conflicts with the theoretical predictions of natural selection. Therefore, he suggests, something must be wrong with the theory. </i><b><i>The anatomical and genetic changes he observed in the fossil record occur too rapidly for natural selection to be valid.</i></b><i> &#39;Natural selection per se does not work to create new species ... [T]he process of creating a new, descendant reproductive community from an ancestral one -- in a way other than that proposed by natural selection -- must be a crucial step in the evolutionary process.&#39;</i><br /><br /><i>Although Eldredge stops short of embracing catastrophism, his conclusion concurs exactly with what catastrophists have been pointing out for years: evolution may be widely accepted, but its basic premise -- that all forms of life evolved gradually from the most primitive, one-celled creatures -- has yet to be proven; in fact, in all the recorded history of science, no truly new animal species has come into being ...</i><br /><br /><i>Unlike their counterparts, catastrophists do not reject opposing views out of hand. They readily admit that slow, gradual changes have gone on in the past and continue in the present. They do maintain, however, that recurrent, violent upheavals have contributed far more to the present physical state of the world -- and to the life forms which live on it -- than all but an eternity of uniformity can account for.</i><br /><br /><i>Today the majority of scientists still subscribes to the century-old principles of uniformitarianism, but a new generation of experts has begun the tedious task of reexamining and reevaluating the evidence. Meanwhile, the great debate continues, and only time will tell which side will eventually win.</i><br /><br /><i>But perhaps one day in the not-too-distant future the scientific community as a whole will admit that cataclysms have indeed wracked the earth in the past. The experts may even apologize for misleading everyone all these years. </i><b><i>Hopefully, their apology will come before the next catastrophe strikes, while enough of humanity is left to hear it.&quot;</i></b><br /><br />Dudley Bromley<br /><br />Selected Bibliography<br /><br />Isaac Asimov, A Choice of Catastrophes. Simon &amp; Schuster, 1979.<br />William B. N. Berry, Growth of Prehistoric Time Scale. W. H. Freeman, 1968.<br />A. J. Desmond, The Hot-Blooded Dinosaurs. Dial, 1976.<br />Niles Eldredge, &quot;An Extravagance of Species.&quot; Natural History, July, 1980.<br />René L. C. Gallant, Bombarded Earth. John Baker, 1964.<br />Jeffrey Goodman, The Earthquake Generation. Seaview Books, 1978.<br />Stephen Jay Gould, &quot;The Belt of an Asteroid.&quot; Natural History, June, 1980.<br />Charles H. Hapgood, The Path of the Poles. Chilton, 1970.<br />John C. McLaughlin, Archosauria. Viking, 1979.<br />Brian O&#39;Leary, &quot;Cataclysm.&quot; Omni, July, 1980.<br />Arthur N. Strahler, The Earth Sciences. Harper &amp; Row, 1963.<br />Immanuel Velikovsky, Earth in Upheaval. Doubleday, 1955.<br />Immanuel Velikovsky, Worlds in Collision. Doubleday, 1950.<br /><br /><a href=\"http://www.catastrophism.com/online/pubs/journals/horus/v0102/horus12.htm\" class=\"ot-anchor\">http://www.catastrophism.com/online/pubs/journals/horus/v0102/horus12.htm</a><br /><br />From: Horus Vol. 1 No. 2 (Summer 1985)<br /><b>What is Uniformitarianism and how did it get here?</b><br /><br />Alex Marton<br /><br /><i>&quot;When Charles Darwin published his now classic On the Origin of Species by Means of Natural Selection in 1859, he was riding the crest of a long wave of scientific speculation regarding the history of the earth and its inhabitants ... I have called attention to the word scientific not to demean its value, but to highlight the fact that that was just one side of the on-going controversy about the Creation and the level of interest that the Almighty might have in the affairs of men. This last point was one that preoccupied many in the nineteenth century; in addition to scientific curiosity and the drive to discover the real nature of things, </i><b><i>the truth of the Biblical story was directly connected to a highly political issue: the legitimacy of the Monarchy.</i></b><br /><br /><i>Almost thirty years earlier, Charles Lyell had paved the way with the publication of his Principles of Geology, in three volumes, between 1830 and 1833. Geology was in its infancy, but Lyell and others had labored diligently to lay the foundations of the new science and, to the extent that success is the approval of later generations, they succeeded with honors.</i><br /><br /><i>In 1807, a small group of amateurs had formed the London Geological Society. In the words of one of its founders, they were starting &#39;a little talking geological dinner club.&#39; Of the original group of thirteen, four were doctors, two booksellers, one an ex-minister, two amateur chemists who were also independently wealthy, and so on. Only one member had training in geology, but did not pursue it as a livelihood. In fact, </i><b><i>an amazing aspect of the London Geological Society is that none of its founders were geologists experienced in or prepared to do field work, but gentlemen inclined to meet for dinner and talk.</i></b><br /><br /><i>Even so, in its second year, the London Geological Society was joined by two dozen Fellows of the Royal Society. Its growth accelerated so much that within ten years, its membership was in excess of 400; in 1825, the year of its incorporation, it was up to more than 630. </i><b><i>Though England was going through a busy period of canal building and mine exploration (so that there was plenty of digging going on), the number of active geologists who were members of the London Geological Society was very close to zero. The amateurs who were members were interested in geology not so much because of its practical applications or even for the theoretical speculations of a new science, but because of the religious and political consequences it might have.</i></b><br /><br /><b><i>In the 18th century, the winds of democracy from America and the attacks of thinkers like Locke and Rousseau, among others, questioned the Monarchy as the natural form of government. Liberalism was moving, and its method was to go after Biblical Geology (specifically the Flood) in order to disarm the Monarchists. The social context in which these political-religious-scientific battles came to a head in England was the popular restlessness of the early nineteenth century. After defeating Napoleon, England fell into a severe depression. The army was demobilized, throwing almost half a million men into unemployment; the overseas market for British exports dried up; the government&#39;s need for war supplies evaporated. A set of laws (the Corn Laws) passed to protect farmers against cheap imported grain resulted in prices so high that workers were unable to buy it. The effects rippled through Britain&#39;s farms and industry alike, creating starving workers and bankrupt businesses.</i></b><br /><br /><b><i>Incidents of popular unrest led the (monarchist) government to enact laws curtailing certain rights. Free speech was one of them. There were those who smelled revolution in the air, but the liberal middle class could still remember the ravages of the French Revolution. That&#39;s not what they wanted. What they wanted was reform in Parliament, but traditional theological doctrine stood in the way. Paley&#39;s Natural Theology claimed that sovereignty descended from God to the King; if he was satisfied with it, there was no need to reform it.</i></b><br /><br /><b><i>Paley&#39;s doctrine was required study in the universities, and was the received wisdom in society. There was only one way to reform Parliament, and that was to destroy Paley&#39;s Natural Theology - and the only way to do that was to discredit the catastrophist notions of its religious defenders who sought to reconcile the geological evidence with the story of Genesis.</i></b><br /><br /><b><i>Scientists in Britain and on the Continent had been making discoveries in the geological record that strained the literal interpretation of the biblical story, while others tried to save it by reinterpreting the words (six days were really six eras, and so on). Many scientists who were also religious tried to find solutions acceptable to the churches, to the people who were increasingly confused, and to themselves as honest individuals laboring to establish the truth. Others wanted nothing less than to destroy once and for all the connection between science and religion. And those who were politically motivated wanted to bury forever the notion of the divine right of kings. If the scientific evidence denied the truth of the Bible, then it also denied any connection between God and the Monarchy, thus freeing Parliament and the people to redefine the political equations.</i></b><br /><br /><b><i>So science, its methods and its scope, in the formative beginnings, was very much a creature of the times, unabashedly enlisted in the service of political causes by those who sought to affect political and social developments in their own favor. It is in this context that we must view the formation and growth of the London Geological Society and its vast influence on the parameters within which geology was to develop into a respectable science. The society succeeded in recruiting influential members - doctors, lawyers, members of Parliament, and eventually even geologists. One member, a young whig lawyer named Charles Lyell, decided to take a novel approach: in his Principles of Geology, he argued against the catastrophists by saying that the diluvial theory was, in effect, mythological, and that it stood in the way of progress in geology. He concentrated on the gradual effects of erosion and volcanic uplift to rationalize the geological observations, completely ignoring all evidence of catastrophism. The liberals were delighted, and they elected him secretary, and later president of the Geological Society.</i></b><br /><br /><i>Catastrophism was dead, and the principle of Uniformitarianism was established: geological changes took place slowly, over extremely long period of time, free of widespread catastrophic changes. </i><b><i>The Society grew powerful: it was able to prevent publication of material favorable to catastrophism, and to arrange evidence so as to satisfy a uniformitarian view. Similarly, the political battle was won by the liberals, and the power flow between the King, the Parliament, and the People changed direction.</i></b><br /><br /><i>It was in this atmosphere that Darwin made the headlines, and the rest is history. After some early skirmishes, Darwin&#39;s &#39;theory of evolution&#39; won the day - a mechanistic theory of evolution subservient to and dependent upon geological uniformitarianism: gradual change over eons, with no violent jumps. Discontinuities in the fossil record, evidence of mass extinctions - all this was swept under the carpet. The new picture was one of long-term stability and imperceptible change. There was no evidence of and, indeed, no need for a divine presence.</i><br /><br /><b><i>It is unfortunate that these crusty notions have shaped the present dilemma of geology. The political issues were settled long ago, but geology is still committed to a paradigm established primarily as part of a political front that is no longer relevant. It is also unfortunate that catastrophism is linked to biblical fundamentalism because that association has inhibited the evolution of catastrophism as a legitimate avenue of scientific inquiry, without reference to religion or politics.</i></b><i> What has happened in geology, and is happening in evolutionary biology, is that the 44 &#39;classic&#39; principles are still being taught and accepted, but the mounting body of evidence for events that don&#39;t fit the theory is swelling up from the museum basements onto the main floors, where they cannot be ignored anymore.</i><br /><br /><i>Political Geology? Is this really how we got here from there intellectually? Yes. And catastrophism is still viewed as an enemy, rather than as a potential partner. This will change. In addition, catastrophism doesn&#39;t have to represent a religious viewpoint: </i><b><i>Religion may have needed, or may still need catastrophes, but catastrophism doesn&#39;t Religion. Where there is evidence of catastrophe, that information should be described and studied, as the Scientific Method counsels us to do. If overwhelming evidence argues in favor of new theories, so be it. If overwhelming evidence argues in favor of catastrophic occurrences within the memory of humankind, well, there is a test for openmindedness. Those who defend a posture established one hundred and fifty years ago against today&#39;s evidence are only ironic reminders of the victims of Uniformitarianism itself.</i></b><br /><br /><b><i>The arguments against the occurrence of global catastrophe as the source of ancient stories about such events have carried the day, at least in part, because of the historical influences on our interpretation of knowledge. After generations of formal education, the bedrock status of this view -- that ancient testimony of world disaster is not to be taken literally -- is so firmly engrained in the sciences that the alternate possibility is almost completely ignored.</i></b><br /><br /><b><i>Yet we are faced with an ancient history in which the principal memories of the race seem to involve the occurrence of world-shattering, natural disaster. This is the real issue at hand here. How can we reconcile the traditional scientific conception of uniformitarian evolution with the obsession of the ancient mind that celestial catastrophe was the agent of change? ...&quot;</i></b><br /><br /><a href=\"http://www.catastrophist.org/home/catastrophist-geology-magazine/pamphlet/\" class=\"ot-anchor\">http://www.catastrophist.org/home/catastrophist-geology-magazine/pamphlet/</a><br /><br />Circulated among the participants of the Charles Lyell centenary symposium, London.<br /><br /><b>Catastrophist Geology 1975</b><br /><i>&quot;A magazine to be dedicated to the study of discontinuities in Earth history</i><br /><br /><i>Uniformitarianism holds that the processes governing the Earth&#39;s organic and inorganic past were the same as those apparent today, and that they operated then at the same intensity and rate as now. When they consider this definition thoughtfully, many geologists realize that they do not really agree with it. Too many events in the Earth&#39;s history do not fit a uniformitarian system - </i><b><i>enormous calderas</i></b><i>, </i><b><i>plateau basalts</i></b><i>, </i><b><i>ice ages</i></b><i>, </i><b><i>alpine nappes</i></b><i>, </i><b><i>bone breccias</i></b><i>, </i><b><i>the sudden appearance of diversified life at the close of the Precambrian</i></b><i>, </i><b><i>the abrupt extinction of dinosaurs and ammonites</i></b><i>, and so on. </i><b><i>In a uniformitarian system the sedimentological and paleontological records are contradictory; if we assume uninterrupted sedimentation, we have to accept catastrophes in evolution; if we do not accept catastrophes in evolution we have to postulate major gaps in the sedimentary record.</i></b><br /><br /><i>We can of course, by retrograde extrapolation over millions of years, relate geological features to the cumulative effects of now active small-scale agents such as the raindrop and the sandgrain. Without this method of research geology cannot exist. </i><b><i>But to state that it is the only one we are allowed to use without becoming &#39;unscientific&#39; is clearly reductionism, a nineteenth-century spook haunting geology just as it haunts history or psychology. Uniformitarian thinking has led to the assumption that discontinuities always require &#39;a few million years&#39; of gradual change between two more stable states.</i></b><i> This assumption overlooks two facts. First, since we are using a time-scale a million times greater than Bishop Usher&#39;s, a catastrophe might last a million years. Second, weak and uniform causes can lead to sudden discontinuities, as anybody can observe by placing a pan full of milk on a stove. </i><b><i>We should investigate the geological evidence for or against the relative suddenness of discontinuities, and not simply assume for them a time-scale which satisfies our preferences or assuages our hidden fears</i></b><i>.</i><br /><br /><i>In non-English speaking countries, the term actualism has been preferred over uniformitarianism. </i><b><i>It implies that &#39;actual&#39; (present-day) causes are sufficient to explain what happened in the past, but not necessarily that they have operated then with the same intensity as now.</i></b><br /><br /><i>But what are actual causes? Two centuries ago, when scientists did not accept the existence of meteorites, the astrobleme [impact crater, etymology: &quot;star wound&quot;] hypothesis would have been unactualistic. [... read slowly ...] During the last century Lord Kelvin, not acquainted with radioactivity, was right - actualistically speaking - when he maintained that the Earth could not be more than 100.000 years old. In this century, geophysicists were also right - from an actualistic standpoint - when they said that Wegener&#39;s hypothesis was an impossibility: seafloor spreading was only measured after its existence had been suggested by the magnetic zebra-striping of the oceanfloors. We even see here an inversion of the actualistic method: investigation of the geologic record has led to the discovery of plates and plate movement. The past as the key to the present.</i><br /><br /><i>These examples illustrate that actual geological causes must be discovered and divulged, meet the consensus of geologists, and not be forgotten again. At a given moment in the history of geological investigation, actual causes are divided into at least two groups: the known and accepted, and the unknown. Actualism as a method is a function of our ignorance and our prejudices, and can only be applied to a restricted number of phenomena. Applied uncritically, it has done extensive harm.</i><br /><br /><b><i>The examples also illustrate that geologists are overly afraid of physicists; meanwhile they continue to adhere to a natural philosophy which physics has discarded since Einstein&#39;s youth. If there is an inconsistency between geological features and accepted physical laws, we should not wait for the physicist&#39;s approval, but should notify them that there is something missing in their system. Both geology and physics could benefit thereby.</i></b><br /><br /><i>Catastrophism admits the occurrence of discontinuities in Earth history - because we observe them now and because we are forced to infer them from the geological record. Even such Lyellian agents as the raindrop and the sandgrain often do their work in discontinuous manners: the catastrophic erosion after a lake-spill for example, or sedimentation by turbidity current. In spite of our proclaimed uniformitarianism, catastrophist hypotheses abound - the capture of the Moon, astroblemes, bursts of cosmic rays, natural nuclear reactors, the breaking up and the collision of continents. </i><b><i>When proposed by geologists of non-catastrophist persuasion, such hypotheses are taken seriously, but when similar ideas are forwarded by less conditioned outsiders, they are regarded as evidence of lunacy simply because they violate uniformitarian dogma. Mainstream geologists often do not even try to formulate clearly their own ideas; while they are cheating, somebody relegated to the lunatic fringe may be exposing the fraud.</i></b><br /><br /><i>Catastrophes do occur. The dinosaurs did die out - </i><b><i>whether it took a million years or a day - either through the cumulative effect of continuous causes, actualistic or not, or through a unique, sudden, terrestrialor extraterrestrial event. Should such riddles ever be solved, the solutions will come from an inspired search for clues and not through application of the methods of medieval scholastics or nineteenth-century rationalists.&quot;</i></b><br /><br /><a href=\"http://www.catastrophist.org/home/catastrophist-geology-2000/\" class=\"ot-anchor\">http://www.catastrophist.org/home/catastrophist-geology-2000/</a><br /><br />[Note, wikipedia: <b>A priori</b> knowledge or justification is independent of experience, as with mathematics (3+2=5), tautologies (&quot;All bachelors are unmarried&quot;), and deduction from pure reason (e.g., ontological proofs).<br /><br /><b>A posteriori</b> knowledge or justification is dependent on experience or empirical evidence, as with most aspects of science and personal knowledge.<br /><br /><b>Ex cathedra:</b> With authority based on one&#39;s rank or office.]<br /><br /><b>Catastrophist Geology 2000</b><br /><br /><b><i>&quot;Uniformitarianism as an a-priori statement about what happened in Earth history is utterly idiotic: in science no a-priori statements should be made about subjects of investigation.</i></b><i> Though it is difficult not to harbour implicit assumptions about the world we live in, it is exactly by making explicit one of those assumptions and then dropping it that scientific breakthroughs are made. By making an assumption explicit and then formulating it ex cathedra we construct a prison for our mind ...</i><br /><br /><i>As a hypothesis defended by James Hutton during the last decades of the 18th century uniformitarianism may sound original and daring, in the historical context of the neptunist-vulcanist controversy, and perhaps even Lamarck could be condoned for adhering to it as late as 1809. </i><b><i>If after 1800 field evidence had been forthcoming substantiating the hypothesis, it could have been transformed, very cautiously, into a theory - though always subject to falsification by evidence to the contrary. But no such evidence came forth. Quite to the contrary, the hypothesis was rapidly falsified by the work of William Buckland, George Cuvier, Hugh Miller and many others, who presented a surabundance of field evidence from all over Europe and from outside Europe, from Devonian sandstone to Pleistocene sand, proving beyond doubt the importance of sudden and unusual events in Earth history.</i></b><i> That Charles Lyell defended uniformitarianism in the 1830s, and that it was adopted, after Romanticism, as the leading doctrine by most academic geologists, meant a setback for science of more than a century. New discoveries between 1860 and 1980 were made in spite of it, and came mostly from outside geology: radioactive dating, continental drift, palaeomagnetism, astroblemes. A half-exception forms the discovery of Alpine nappes, generally accepted by geologists about a century ago. But then, there were always some geologists less dogmatic than others. Joseph Prestwich was an outspoken catastrophist, Eduard Suess less openly so, just like Marcel Bertrand, Louis de Launay, Pierre Termier (De Launay and Termier were not only leading geologists but also Atlantologists). Even so, the fact that palaeosols were found under nappes was passed under silence, except for the protests of Oulianoff (Nicolai, not Ivan Ilyits) and Tazieff, the only ones as far as I know who advocated the possibility of sudden emplacement.</i><br /><br /><i>Actualism, the backward extrapolation of presently known and accepted processes, is a method we use, one amongst several. </i><b><i>It is the easiest, and as such a temptation and a pitfall. To transform one restricted method into a holy virtue and let it, like the cuckoo&#39;s young, push the other chicks out of the nest, is no less idiotic than parroting Hutton&#39;s hypothesis over and over again, in the face of all evidence to the contrary. Actualism as a method should be applied with much caution. Actualism as a principle should never have existed.</i></b><br /><br /><i>Catastrophist geology, the special study of discontinuities in Earth history, will be needed yet for some time to come, in order to restore a balance. </i><b><i>Later it can be dropped, and our descendants will be able to study just Earth history, a succession of more or less quiet periods and sudden disruptions.</i></b><br /><br /><b><i>After 1980 I see appearing, in titles of symposia, books and articles, nonsensical concepts such as catastrophist uniformitarianism. For such meaningless juggling with words I have no words, at least no printable ones. Derek Ager, a native English-speaker, coined &#39;catastrophic uniformitarianism&#39;, and with that I heartily agree - but he didn&#39;t mean it like that.</i></b><br /><br /><i>To prefer the simplest-sounding hypothesis over the best-fitting has had just as pernicious an effect in geology as in other fields of enquiry. Together with the statement to the contrary, that reality is always more complicated than we imagine, or can imagine, it gives a knife with two cutting edges in the hands of academic authorities too lazy to think.</i><br /><br /><i>One authority, Leonard Krishtalka in his 1989 book Dinosaur Plots, shows himself to be full of spite after the K-T discoveries, very much like Ager. He sneers at the &#39;impact buffs&#39; and the &#39;asteroid hoopla&#39; (p. 26). But unlike Ager, who tried to sell us his opinions as scientific obtainments with Occam&#39;s razor in his hand, Krishtalka tries to force upon us these same opinions menacing us with Haldane&#39;s razor (p. 27): &#39;I am wary of simple theories. They may have appeal, but one difference between nature and its interpreters is that nature is not simple.&#39;</i><br /><br /><i>People, readers, scientists, researchers, throw your bloody razors on your kitchen middens, and don&#39;t trust anybody who brandishes them unparcimoniously or otherwise. And do not forget that even the &quot;best-fitting&quot; hypothesis is most paradigm-dependent.&quot;</i><br /><br /><a href=\"http://www.catastrophist.org/home/catastrophist-manifesto-2007/\" class=\"ot-anchor\">http://www.catastrophist.org/home/catastrophist-manifesto-2007/</a><br /><br /><b>The War of the Worldviews</b><br /><br /><i>&quot;More often than not, the controversy between catastrophism and uniformitarianism is represented as a conflict within the geological sciences. But considering that in catastrophism a dynamic picture is given of Earth history, and thus of the Earth, whereas uniformitarians try to play down as much as they think feasible the more global, rapid and life-threatening processes in favour of local, slow and innocuous ones (however preposterous the hypotheses they have to invent and defend), we must conclude that we are dealing with a conflict between two antagonistic, incompatible worldviews, vastly transcending the field of one particular discipline.</i><br /><br /><b><i>The fight has indeed been raging for thousands of years. Plato was a catastrophist, Aristoteles tried to play down what Plato considered to be historical discontinuities.</i></b><i> Two millennia later, Leibniz sounds as Aristoteles redivivus.</i><br /><br /><i>After a wave of catastrophist theories during Romanticism, the uniformitarians proclaimed their view to be the only scientific one, and triumphantly declared the battle won. </i><b><i>When during the 1970s the Chryse Flood was discovered on Mars, an oceanless planet, geology professors didn’t resign en masse and go back to school, but continued pontificating that such a flood was impossible on Earth. Doubts about uniformitarian dogma arose only in 1980, after the K-T discoveries, which caused however but a minor crack in the wall of Academe. The uniformitarians stood their ground and tried to encapsulate the new findings into their system. They turned up with ‘catastrophist uniformitarianism’ – a contradictio in terminis</i></b><i>, and worse, a metaphysical confidence trick: the appropriation of empirical findings by a magical formula. More cracks and holes appeared, but now, after 2005, they have to try corking up a really big one.</i><br /><br /><i>But as before, uniformitarianism will find its staunch defenders – not because they can produce arguments of any validity, but because they are well-conditioned: Natura Non Facit Saltus. The war of the worldviews may go on unabated for a long time to come.&quot;</i><br /><br />Was formerly online at <a href=\"http://courses.washington.edu/ess408/Gould1965.pdf\" class=\"ot-anchor\">http://courses.washington.edu/ess408/Gould1965.pdf</a>﻿<br /><br /><b><i>&quot;As a special term, methodological uniformitarianism was useful only when science was debating the status of the supernatural in its realm; for if God intervenes, then laws are not invariant and induction becomes invalid.  It was useful for those who, as Lyell, needed a guide to combat what we now consider unscientific notions of divine intervention and the resultant discordance of past and present modes of change.  Their battle has been won, and the weapon which secured the victory deserves to be honorably retired, lest it appear like the crossbow in a nuclear age.  The term today is an anachronism; for we need no longer take special pains to affirm the scientific nature of our discipline.&quot;</i></b><br /><br />REFERENCES FOR THE GRAPHIC:<br /><br />[1] - [3] <a href=\"https://www.thunderbolts.info/tpod/2006/arch06/060130crater.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2006/arch06/060130crater.htm</a><br /><br />[4] <a href=\"https://books.google.com/books?id=LuKiCgAAQBAJ&amp;pg=PA189&amp;lpg=PA189&amp;dq=Left+to+itself,+it+is+hard+to+see+how+anything+internal+to+the+Earth+could+ever+break+the+stable+grip+of+an+ice-age.+Thus+to+understand+the+cause+of+interglacials+we+must+look+to+catastrophic+events.&amp;source=bl&amp;ots=EwIfbW2Cqu&amp;sig=XzN3Mn-wS53suCXDMd6-egsffRo&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiOsqvxnvTNAhULzWMKHWfQAk4Q6AEIHDAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=LuKiCgAAQBAJ&amp;pg=PA189&amp;lpg=PA189&amp;dq=Left+to+itself,+it+is+hard+to+see+how+anything+internal+to+the+Earth+could+ever+break+the+stable+grip+of+an+ice-age.+Thus+to+understand+the+cause+of+interglacials+we+must+look+to+catastrophic+events.&amp;source=bl&amp;ots=EwIfbW2Cqu&amp;sig=XzN3Mn-wS53suCXDMd6-egsffRo&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiOsqvxnvTNAhULzWMKHWfQAk4Q6AEIHDAA#v=onepage&amp;q&amp;f=false</a><br /><br />[5] From: Catastrophist Geology Year 1 No. 2 (Dec 1976), The Martian Deluge, by Johan B. Kloosterman<br /><br />[6] <a href=\"http://www.discovery.org/a/22971\" class=\"ot-anchor\">http://www.discovery.org/a/22971</a><br /><br />[7] <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/bies.200900033/epdf\" class=\"ot-anchor\">http://onlinelibrary.wiley.com/doi/10.1002/bies.200900033/epdf</a><br /><br />[8] <a href=\"http://www.evolutionnews.org/2013/07/darwin_defender_1074791.html\" class=\"ot-anchor\">http://www.evolutionnews.org/2013/07/darwin_defender_1074791.html</a><br /><br />[9] &quot;The Great Debate&quot;, Dudley Bromley, Catastrophism and Ancient History III:2 (July 1981)<br /><br />[10] [11] <a href=\"https://www.thunderbolts.info/wp/2014/03/05/the-jurassic-coast-a-transmutation-extinction/\" class=\"ot-anchor\">https://www.thunderbolts.info/wp/2014/03/05/the-jurassic-coast-a-transmutation-extinction/</a><br /><br />[12] <a href=\"http://news.bbc.co.uk/2/hi/science/nature/7130014.stm\" class=\"ot-anchor\">http://news.bbc.co.uk/2/hi/science/nature/7130014.stm</a> and the book, <i>The Cycle of Cosmic Catastrophes - Flood, Fire and Famine in the History of Civilization</i><br /><br />[13] <a href=\"https://www.sciencedaily.com/releases/2015/11/151104151534.htm\" class=\"ot-anchor\">https://www.sciencedaily.com/releases/2015/11/151104151534.htm</a><br /><br />[14] <a href=\"https://archive.org/stream/textbookofgeolog01unse/textbookofgeolog01unse_djvu.txt\" class=\"ot-anchor\">https://archive.org/stream/textbookofgeolog01unse/textbookofgeolog01unse_djvu.txt</a><br /><br />[15] <a href=\"http://earth.geology.yale.edu/~ajs/1960/ajs_258A_11.pdf/115.pdf\" class=\"ot-anchor\">http://earth.geology.yale.edu/~ajs/1960/ajs_258A_11.pdf/115.pdf</a><br /><br />[16] <a href=\"https://www.depts.ttu.edu/gesc/Fac_pages/Yoshinobu/4361_5361_Folder/2013-readings/Hubbert%20and%20Rubey,%201959%20I.pdf\" class=\"ot-anchor\">https://www.depts.ttu.edu/gesc/Fac_pages/Yoshinobu/4361_5361_Folder/2013-readings/Hubbert%20and%20Rubey,%201959%20I.pdf</a><br /><br />[17] <a href=\"https://books.google.com/books?id=f0QRAAAAIAAJ&amp;pg=PA424&amp;lpg=PA424&amp;dq=Such+a+slab+moving+over+ground,+as+is+now+believed+to+have+existed&amp;source=bl&amp;ots=4CbtqIUIeA&amp;sig=4hiNLM3CYzle-Y_hiECB5iKLU3U&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiYn_u71_TNAhVI8mMKHcwkAIEQ6AEIHDAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=f0QRAAAAIAAJ&amp;pg=PA424&amp;lpg=PA424&amp;dq=Such+a+slab+moving+over+ground,+as+is+now+believed+to+have+existed&amp;source=bl&amp;ots=4CbtqIUIeA&amp;sig=4hiNLM3CYzle-Y_hiECB5iKLU3U&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiYn_u71_TNAhVI8mMKHcwkAIEQ6AEIHDAA#v=onepage&amp;q&amp;f=false</a><br /><br />[18] <a href=\"https://books.google.com/books?id=8MxTAAAAcAAJ&amp;pg=PR2-IA9&amp;lpg=PR2-IA9&amp;dq=What+power+must+have+been+required+to+shatter+and+to+sweep+away+the+missing+parts+of+this+pyramid&amp;source=bl&amp;ots=FhMKcV-LeT&amp;sig=k9VGtaEoBzXDb2eTP9RLd1pJEw0&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjxzNiiz_XNAhUH-mMKHdOtBsMQ6AEIHDAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=8MxTAAAAcAAJ&amp;pg=PR2-IA9&amp;lpg=PR2-IA9&amp;dq=What+power+must+have+been+required+to+shatter+and+to+sweep+away+the+missing+parts+of+this+pyramid&amp;source=bl&amp;ots=FhMKcV-LeT&amp;sig=k9VGtaEoBzXDb2eTP9RLd1pJEw0&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjxzNiiz_XNAhUH-mMKHdOtBsMQ6AEIHDAA#v=onepage&amp;q&amp;f=false</a><br /><br />[19] <a href=\"https://books.google.com/books?id=dnwvTXEMFM0C&amp;pg=PA915&amp;lpg=PA915&amp;dq=The+Appalachians,+which+run+from+Newfoundland+to+Alabama,+were+probably+formed+not+by+upward+thrusting,+as+previously+believed&amp;source=bl&amp;ots=zBqdmCommC&amp;sig=XpqvJtoF8UYo_tDL_Vl28r-KJeY&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjjs_bg3vTNAhUEymMKHRyxAvcQ6AEIKDAC#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=dnwvTXEMFM0C&amp;pg=PA915&amp;lpg=PA915&amp;dq=The+Appalachians,+which+run+from+Newfoundland+to+Alabama,+were+probably+formed+not+by+upward+thrusting,+as+previously+believed&amp;source=bl&amp;ots=zBqdmCommC&amp;sig=XpqvJtoF8UYo_tDL_Vl28r-KJeY&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjjs_bg3vTNAhUEymMKHRyxAvcQ6AEIKDAC#v=onepage&amp;q&amp;f=false</a><br /><br />[20] [21] <a href=\"https://en.wikipedia.org/wiki/North_Atlantic_Igneous_Province\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/North_Atlantic_Igneous_Province</a><br /><br />[22] <a href=\"https://en.wikipedia.org/wiki/Deccan_Traps\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Deccan_Traps</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"What Gives Venus Its Feminine Attribute?","summary":"A Feminine Attribute for Venus is Almost Universal Amongst Cultural Traditions / But, it is Very Difficult to Discern Anything in the Present Appearance of the Planet which would Lend it Any Gender at All / So, Why was Venus Consistently Deemed the Female Planet Rather than, Say, Mercury or Jupiter?","image":"https://lh3.googleusercontent.com/-gXyHd_Nt6mw/V4R8YFCT_gI/AAAAAAAAISg/BmO3AAAfY6Qcs0U11wsPMlmaJ27jxpbJw/w1800-h2520/what-gives-venus-its-feminine-attribute-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/eGtuAftZALi","publishDate":"2016-07-12T05:14:39.807Z","updateDate":"2017-01-03T02:26:11.266Z","text":"<b>What Gives Venus Its Feminine Attribute?: A Feminine Attribute for Venus is Almost Universal Amongst Cultural Traditions / But, it is Very Difficult to Discern Anything in the Present Appearance of the Planet which would Lend it Any Gender at All / So, Why was Venus Consistently Deemed the Female Planet Rather than, Say, Mercury or Jupiter?</b><br /><br /><i>The Many Faces of Venus: The Planet Venus in Ancient Myth and Religion</i><br />by Ev Cochrane<br />(2001)<br /><br /><b><i>&quot;Chapter 2: The Female Star</i></b><br /><br /><i>[...]</i><br /><br /><i>A survey of Venus-lore reveals a diverse set of intriguing and endlessly recurring themes: </i><b><i>the planet as female or goddess; the planet as agent of war, death, and destruction; the planet as witch-like hag; the planet as &#39;eye&#39; of heaven; the planet as paramour of Mars; and numerous others to be discussed in this and future volumes.</i></b><i> A systematic analysis of these various mythological themes, in turn, allows for the reconstruction of an archetypal Venus myth which, in a very real sense, represents mankind&#39;s collective memory of our neighbor&#39;s tumultuous recent history.</i><br /><br /><i>While many of our claims with respect to the ancient conceptions surrounding Venus will sound fantastic when enumerated in advance of proper documentation, the planet&#39;s consistent association with </i><b><i>the female sex is commonly acknowledged and readily demonstratable.</i></b><i> Thus, in his recent book on Venus, Peter Cattermole noted that </i><b><i>&#39;a female association is almost universal.&#39;</i></b><i> [2] And so it is, despite occasional statements to the contrary. [3]</i><br /><br /><i>Among the Arabic peoples of Northern Syria and the Mesopotamian desert, Venus was known as al- &#39;Uzza and envisaged as </i><b><i>a powerful female warrior.</i></b><i> [4] This planetary goddess is well-attested in ancient sources: &#39;In sources from the fifth century AD she is identified with </i><b><i>Aphrodite</i></b><i> by an anonymous Syrian historian; with Kaukabta, </i><b><i>&#39;the female star;&#39;</i></b><i> with Balthi, by Isaac of Antioch; and finally with Lucifer, the morning star, by Jerome. [5]</i><br /><br /><i>The Hebrews knew Venus as Kokebet, a name which translates as </i><b><i>&#39;she-star.&#39;</i></b><i> [6] This name finds a cognate in the Syrian name for Venus: Kawkabta.</i><br /><br /><i>In ancient Persia, Venus was identified with </i><b><i>the voluptuous goddess Anahita</i></b><i>, the latter being viewed as a warrior and agent of fertility. [7] A vestige of these ancient traditions is preserved in the Koran, one verse describing </i><b><i>the transfiguration of a young woman into the beautiful star Zohra, an Arabic name for Venus.</i></b><i> [8] Yet variant traditions name this young woman Anahid. [9]</i><br /><br /><b><i>In Africa, as well Venus was commonly thought of as female.</i></b><i> Among the Rotse, for example, Venus was regarded as </i><b><i>the wife of the first man.</i></b><i> [10] Analogous ideas prevailed among the Karanga, where Venus was known as Nehanda </i><b><i>and accorded the strange ability to transform herself into a serpent.</i></b><i> [11]</i><br /><br /><i>Numerous peoples in the New World also viewed Venus as a female star.  The Iroquois of New York knew Venus as </i><b><i>&#39;Star Woman&#39;</i></b><i> and held that </i><b><i>the sun lit his fire by means of her brilliant torch.</i></b><i> [12] A similar conception prevailed among the Chilcotin of British Columbia; they viewed Venus as </i><b><i>an old woman holding a torch.</i></b><i> [13] The Eastern Pomo Indians of California called Venus Xa&#39;a da, </i><b><i>&#39;Day Woman.&#39;</i></b><i> [14] Interestinaly, the Pomo held that Venus served as a guide to the departing soul as it made its way to the celestial hereafter. [15]</i><br /><br /><i>Venus was accorded a feminine nature by various peoples of Central and South America as well.  Among the Zinacenteco Indians of Mesoamerica, Venus was envisaged as </i><b><i>a girl sweeping the path of the sun.</i></b><i> [16] The Chamacoco of Paraguay perceived in the planet </i><b><i>a beautiful woman named Johle.</i></b><i> [17] The Warraus tribe of the Pomeroon knew Venus as Okona-kura.  Of this goddess they told the following story: </i><b><i>&#39;She</i></b><i> it was who stuck in the hole when her people first came down from above the skies to populate the earth.&#39; [18]</i><br /><br /><i>The Inca knew Venus as </i><b><i>a lovely woman by the name of Chasca</i></b><i> [19], </i><b><i>renowned for her long-flowing hair.</i></b><i> An anonymous Jesuit of the seventeenth century described the planet-goddess as especially devoted to women and princesses:</i><br /><br /><i>&#39;[Of Venus] they said that she was </i><b><i>a goddess of young maidens and princesses</i></b><i>, and originator of the flowers of the fields, and </i><b><i>mistress of dawn and twilight</i></b><i>; and </i><b><i>it was she who drew onto the earth when she shook her hair</i></b><i>, and they thus called her Chasca.&#39;</i><br /><br /><i>The ancient Balts knew Venus as Ausrine, represented as </i><b><i>a maiden of stunning beauty. She was famous for her long hair and deemed responsible for preserving the sun&#39;s fire.</i></b><i> [20]</i><br /><br /><i>The Yakuts, an aboriginal people in Siberia, knew the planet by the name Solbon, envisaged as </i><b><i>a beautiful girl.</i></b><i> [21] A legend first recorded in the last century assigns the planet an ominous aspect:</i><br /><br /><i>&#39;She is </i><b><i>the bride and sweetheart of Satan&#39;s son</i></b><i> -- urgel ... When these two stars come close to one another, it is a bad omen; their eager quivering, their discontinuous panting cause great disasters: storms, blizzards, gales.  When they unite, fathomdeep snow will fall even in the summer, and all living beings, animals and trees will perish ...&#39; [22]</i><br /><br /><i>A related Yakut tradition makes the disaster-bringing star </i><b><i>the daughter of the Devil</i></b><i>:</i><br /><br /><b><i>&#39;It is said to be &#39;the daughter of the Devil and to have had a tail in the early days.&#39; If it approaches the earth, it means destruction, storm, and frost, even in the summer; &#39;Saint Leontius, however, blessed her and thus her tail disappeared.&#39;&#39;</i></b><i> [23]</i><br /><br /><i>Viewed in isolation, such traditions can only seem the stuff of fiction.  Yet, when viewed with a critical eye in the light of comparable traditions from around the globe, the suspicion arises that something more than pure fantasy is at work here.</i><br /><br /><i>The malevolent nature accorded the Cytherean planet will prove to be a recurring theme throughout the course of this book.  Among the Samoan Islanders, for example, it was said that Venus (Tapuitea) was </i><b><i>a primeval Fijin queen who, upon becoming wild, sprouted horns from her head and engaged in cannibalistic practices. [24] Shortly thereafter the queen was translated to the heavens where, as Venus, she continues to ported the death of kings and nobles.</i></b><br /><br /><i>Among the Kedang of Indonesia, Venus was known as Peni Uno and </i><b><i>represented as a woman. [25] Indeed, she was said to be the wife of the male &#39;Morning Star.&#39;</i></b><br /><br /><i>The aboriginal peoples native to Australia are renowned for their interest in the stars, the various planets playing a conspicuous role in their sacred traditions, rituals, and artwork. [26] There, too, the planet Venus appears to have been viewed as feminine in nature:</i><br /><br /><i>&#39;The Morning Star was also an important sign to the Aborigines who arose at early dawn to begin their hunting.  It, too, was personified and frequently associated with death.  Arnhem Land legends identify the home of the morning star, Barnumbir, as Bralgu, the Island of the Dead. </i><b><i>Barnumbir was so afraid of drowning that she could be persuaded to light her friends across the sea at night only if she were held on a long string by two old women, who at dawn would pull her back to shore and keep her during the day in a basket.</i></b><i> In Arnhem Land, because of this connection, the morning star ceremony is an important part of the ritual for the Dead.  Barnumbir is represented by a totem stick to the top of which is bound a cluster of white feathers or down, denoting the star, and long strings ending in smaller bunches of feathers to suggest the rays.  When a person dies, his/her spirit is believed to be conducted by the star to Bralgu, its last resting place.&#39; [27]</i><br /><br /><i>The intimate connection of Venus with the celestial hereafter in general and the transport of souls in particular, apparent here and in the aforementioned Pomo tradition, is archetypal in nature and attested around the world. [28]</i><br /><br /><b><i>A widespread conception viewed the planet Venus as the daughter of the ancient sun-god.</i></b><i> The following account, from the Tsimshian Indians of North America, offers a representative example of this theme:</i><br /><br /><i>The sky is a beautiful open country.  It is reached through the hole in the sky, which opens and closes ... The sky may also be reached by means of a ladder which extends from the mountains to the sky ... After reaching the sky, the visitor finds himself on a trail which leads to the house of the Sun chief. </i><b><i>In this house the Sun lives with his daughter ... The Sun&#39;s daughter is the Evening Star.</i></b><i> [29]</i><br /><br /><i>The Desana Indians of the Amazonian rain forest also referred to Venus as </i><b><i>&#39;Daughter of the Sun.&#39;</i></b><i> [30] According to this tribe, </i><b><i>Venus suffered incestuous advances at the hands of her father. Strangely enough, however, it is reported that she continued to live with him as his wife.</i></b><br /><br /><i>Among various Slavic peoples, Venus was known by the name of Danica, &#39;day-star.&#39; [31] This same planet-goddess was elsewhere recalled as </i><b><i>the sister of the sun or as a great king&#39;s daughter.</i></b><i> [32]</i><br /><br /><i>Having now documented the prevalence of the Venus-as-female theme, how are we to explain it?  The astronomer Patrick Moore offered the following suggestion: &#39;A female association is in fact general, except in India; this is natural enough, since to the unaided eye Venus is the loveliest of the planets.&#39; [33] Here&#39;s a male chauvinist answer if ever there was one. Why should a beautiful planet be viewed as female rather than male?</i><br /><br /><i>Others have sought to explain Venus&#39; traditional femininity by reference to the commensuration between the planet&#39;s periods of visibility and human gestation.  This was the view favored by the astronomer David Grinspoon:</i><br /><br /><i>&#39;Another reason to worship Venus and find significance in her movements is that there are numerous connections between the timing of aspects of her motions and timescales of natural interest to humans.  Perhaps most strikingly, the approximate 260-day length of a Venus appearance in the morning or evening coincides closely with the average length of the human gestation period ... A knowledge of the close coincidence between the cycles of Venus and human pregnancy may have contributed to the persistent, but nonexclusive, Western attribution of female characteristics to Venus.&#39; [34]</i><br /><br /><i>This explanation, while more logically consistent than that of Moore, can hardly be given credence, for it implies that primitive cultures around the world were aware of the 260-day appearance interval of Venus and noticed its near-synchronism with the human gestation period.  There is little evidence for this claim and much against it.  While the 260-day period of Venus was certainly known to the Maya and likely known to the ancient Babylonians of the first millenium BCE, it can hardly be believed that the same was true of the Australian Aborigines or the Siberian Yakuts, both of whom regarded Venus as a female.</i><br /><br /><i>The logical problems apparent in the explanations offered by Moore and Grinspoon underscore the fact that </i><b><i>it is very difficult to discern anything in the present appearance of the Cytherean planet which would justify its traditional femininity. Why was Venus deemed the female planet par excellence, rather than Mercury or Jupiter?</i></b><i> ...&quot;</i><br /><br />REFERENCES:<br /><br />[2] P Cattermole, <i>Venus: The Geological Story</i> (Baltimore, 1994), p.1<br /><br />[3] Following the lead of Anthony Aveni, Grinspoon <i>op. cit.</i> p.24, writes that: <i>&quot;It is simply not true that a female association is general.&quot;</i> In support of this statement, Grinspoon points to Quetzalcoatl and Tlahuizcalpanteuctli as classic male Venus deities.  Here it can be shown that most of the male deities which various scholars have hitherto identified with Venus are actually Martian in origin.  See the discussion in E. Cochrane, &quot;Mars Gods of the New World,&quot; <i>Aeon</i> 4:1 (1995), pp.47-63.<br /><br />[4] M. Hofner, &quot;al-&#39;Uzza,&quot; in H. Haussig ed., <i>Gotter und Mythen im Vorderen Orient</i> (Stutgart 1976), p.475.<br /><br />[5] W. Heimpel, &quot;A Catalog of Near Eastern Venus Deities,&quot; <i>Syro-Mesopotamian Studies</i> 4:3 (1982), p.19.<br /><br />[6] R. Stieglitz, &quot;The Hebrew Names of the Seven Planets,&quot; <i>JNES</i> 40:2 (1981), pp.135-136.  See also L. Bobrova &amp; A. Militarev, &quot;From Mesopotamia to Greece: On the Origin of Semitic and Greek Star Names,&quot; in H. Galter ed., <i>Die Rolle der Astronomie in den Kulturen Mesopotamiens</i> (Graz, 1993), p.315.<br /><br />[7] See <i>Yast</i> 5.85 for Anahita&#39;s astral aspect.  See also W. Eilers, <i>Sinn und Herkunft der Planetennamen</i> (Munchen, 1976), pp.55; A. Carnoy, &quot;Iranian Mythology,&quot; in L. Gray ed., <i>The Mythology of All Races</i> (Boston, 1917), pp.279-280.<br /><br />[8] <i>Koran</i> 2.96.  See also W. Eilers, <i>op. cit.</i> p.55.<br /><br />[9] J. Puhvel, <i>Comparative Mythology</i> (Baltimore, 1989), p.104.<br /><br />[10] P. Breutz, &quot;Sotho-Tswana Celestial Concepts,&quot; in <i>Ethnological and Linguistic Studies in Honour of N. J. van Warmelo</i> (Pretoria, 1969), p.206.<br /><br />[11] H. von Sicard, &quot;Karanga Stars,&quot; <i>NADA</i> 19 (1943), p.48.<br /><br />[12] D. Miller, <i>Stars of the First People</i> (Boulder, 1997), p.52.<br /><br />[13] L. Farand, &quot;Traditions of the Chilcotin Indians,&quot; <i>Memoirs of the American Museum of Natural History</i> 4 (1900), p.31.<br /><br />[14] D. Miller, <i>op. cit.</i> p.144.<br /><br />[15] <i>Ibid.</i><br /><br />[16] See E. Vogt, <i>Zinacantan</i> (Cambridge, 1969), pp.316-318.  See also the tale quoted by J. Sosa, &quot;Maya Concepts of Astronomical Order,&quot; in G. Gossen ed, <i>Symbol and Meaning Beyond the Closed Community</i> (Albany, 1986), p.189.<br /><br />[17] J. Wilbert &amp; K. Simoneau, <i>Folk Literature of the Chamacoco Indians</i> (Los Angeles, 1987), p.94.<br /><br />[18] W. Roth, &quot;An Inquiry into the Animism and Folklore of Guiana Indians,&quot; <i>Bureau of American Ethnology</i> 30 (1915), p.260.<br /><br />[19] <i>De las costumbres antiguas de los naturales del Piru</i> (Madrid, 1879), as translated by Jan Sammer, &quot;The Cosmology of Tawantinsuyu,&quot; <i>Kronos</i> 9:2 (1984), p.25.  See also B.C. Brundage, <i>Empire of the Inca</i> (Norman, 1963), p.50.<br /><br />[20] V Straizys &amp; L. Klimka, &quot;Cosmology of the Ancient Balts,&quot; <i>Journal for the History of Astronomy</i> 28 (1997), p.73.<br /><br />[21] L. Mandoki, &quot;Two Asiatic Sidereal Names,&quot; in V. Dioszegi ed., <i>Popular Beliefs and Folklore Traditions in Siberia</i> (Bloomington, 1968), p.489.  See also U. Holmberg, &quot;Finno-Ugric and Siberian Mythology,&quot; in L. Gray ed., <i>The Mythology of All Races</i> (Boston, 1927), p.431.<br /><br />[22] <i>Ibid.</i>, p.489.<br /><br />[23] <i>Ibid.</i><br /><br />[24] R. Williamson, <i>Religious Beliefs and Cosmic Beliefs of Central Polynesia</i>, Vol. 1 (Cambridge, 1933), p.128.<br /><br />[25] R. Barnes, <i>Kedang</i> (Oxford, 1974), p.122.<br /><br />[26] H. Cairns, &quot;Aboriginal sky-mapping,&quot; in C. Ruggles ed., <i>Archaeoastronomy in the 1990&#39;s</i> (Loughborough, 1993), p.136.<br /><br />[27] R. Haynes, &quot;Aboriginal Astronomy,&quot; <i>Aust. J. Astr.</i> 4:3 (1992), p.134.<br /><br />[28] A. Aveni, <i>Conversing With the Planets</i> (New York, 1992), pp. 49ff.  E. Krupp, <i>Beyond the Blue Horizon</i> (New York, 1991), pp.197ff.<br /><br />[29] F. Boas, &quot;Tsimshian Mythology,&quot; <i>ARBAE</i> 31 (1916), pp.453-454.<br /><br />[30] G. Reichel-Dolmatoff, <i>Amazonian Cosmos</i> (Chicago, 1971), pp.28-29,71.<br /><br />[31] William Gibbon, <i>Popular Star Names Among the Slavic Speaking Peoples</i>, a Ph.D dissertation presented to Penn University in 1960, pp.170-174.<br /><br />[32] <i>Ibid.</i>, p.180.<br /><br />[33] D. Grinspoon, <i>op. cit.</i>, p.24, quoting Moore&#39;s <i>The Planet Venus</i> (1959).<br /><br />[34] <i>Ibid.</i>, p.18.  A similar explanation was offered by E. Krupp, <i>Beyond the Blue Horizon</i> (New York, 1991), p.18.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Astronomical Nature of Mythology","summary":"The Casual Reader of Popular Mythology May Not Realize to What Extent Scholars Have Studied Mythology's Astronomical Content / The Scientific Community Will Only Consider the Parts which can be Force-Fitted into Confirmation for their Own Theories / And What is Remarkable is that they Do So Even as those Theories Undergo Significant Changes","image":"https://lh3.googleusercontent.com/-s-BppXy5Sq8/V4MgilLteSI/AAAAAAAAIRY/HB4TWD19sKAbocIXmforTih48Bn240dSQ/w1800-h2520/the-astronomical-nature-of-mythology-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/KkPKfctZp8s","publishDate":"2016-07-11T04:29:49.095Z","updateDate":"2017-01-03T02:25:50.810Z","text":"<b>The Astronomical Nature of Mythology: The Casual Reader of Popular Mythology May Not Realize to What Extent Scholars Have Studied Mythology&#39;s Astronomical Content / The Scientific Community Will Only Consider the Parts which can be Force-Fitted into Confirmation for their Own Theories / And What is Remarkable is that they Do So Even as those Theories Undergo Significant Changes</b><br /><br /><b>A Cliff&#39;s Notes Timeline of Researchers Proposing Planetary Catastrophes as an Explanation for the Origin of Mythology</b><br /><br />This controversial subject has historically been inaccessible to those who do not dedicate a significant amount of time to its study.  I find C.J. Ransom&#39;s historical-timeline approach, which he delivered at the 2016 Electric Universe conference in Phoenix, AZ., to be very helpful for establishing context.  But, his presentation skips over many scholarly works.<br /><br />What I generally seek to do when explaining a controversy is to see how fast I can get a person to understanding it; in this case, I feel a very different approach is required: Since part of the problem of mythology is that people are simply unaware of the enormous volume of scholarship that has to-date gone into the connection of catastrophe with mythology, I&#39;ve decided to abandon my typical attempts to be concise for this timeline.<br /><br />This is not a list of original sources; this list is mostly limited to a review of works of scholarship which relate to the catastrophe-mythology connection, with some context added that I feel can help to understand the bigger picture.<br /><br /><b>~400 BC:</b> <i>Plato&#39;s Dialogues</i><br /><br />- See <a href=\"https://books.google.com/books?id=6IJEAAAAIAAJ&amp;pg=PA367&amp;lpg=PA367#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=6IJEAAAAIAAJ&amp;pg=PA367&amp;lpg=PA367#v=onepage&amp;q&amp;f=false</a><br />- <i>&quot;Phaethon, the son of Helios, having yoked the steeds in his father&#39;s chariot, because he was not able to drive them in the path of his father, </i><b><i>burned up all that was upon the earth, and was himself destroyed by a thunderbolt. Now, this has the form of a myth, but really signifies a declination of the bodies moving around the earth and in the heavens, and a great conflagration of things upon the earth recurring at long intervals of time ... All of these stories, and ten thousand others which are still more wonderful, have a common origin; many of them have been lost in the lapse of ages, or exist only as fragments; but the origin of them is what no one has told ...&quot;</i></b><br /><br />... then further on ...<br /><br /><b><i>&quot;... All of these stories, and ten thousand others which are still more wonderful, have a common origin; many of them have been lost in the lapse of ages, or exist only as fragments; but the origin of them is what no one has told&quot;</i></b><br /><br /><b>First Century:</b> The works of Titus Flavius Josephus<br /><br /><b>6th or 7th Century:</b> <i>The Sibylline Oracles</i><br /><br />- Described as an odd pastiche of Hellenstic and Roman mythology interspersed with Jewish, Gnostic and early Christian legend<br /><br /><b>1681-1690:</b> <i>Sacred Theory of the Earth</i> (various versions in this timeframe), by Thomas Burnet<br /><br />- wikipedia: <i>&quot;a speculative cosmogony, in which Burnet suggested a hollow earth with most of the water inside until Noah&#39;s Flood, at which time mountains and oceans appeared.&quot;</i><br /><br /><b>1696:</b> <i>A New Theory of Earth</i>, by William Whiston<br /><br />- Claimed some biblical stories could be explained scientifically.<br />- Claimed evidence also indicated the biblical flood was due to a comet hitting the Earth.<br />- Believed the doctrine of the Trinity was incorrect, opposed the Nicene Creed, and thought of hell as cruel, as well as an insult to God.<br />- Ideas about catastrophes in history may have been considered had he not mentioned religious views.<br />- Queen Anne died in 1714 so the court dropped the proceedings against him.<br />- However, these events did indirectly seem to contribute to discouraging scientific consideration of ancient stories for many years.<br /><br /><b>1800&#39;s:</b> The rise and fall of the catastrophist worldview<br /><br />- <i>&quot;[N]ineteenth century catastrophists postulated a succession of great upheavals culminating in a &#39;general flood which swept away the quadropeds from the continents, tore up the solid strata, and reduced the surface to a state of ruin&#39; (Buckland, 1823, p224). </i><b><i>Their worldview extended beyond a simple theory of geologic change; it encompassed a methodological procedure, excluded from the modern definition of science, which permitted direct providential control of earth history</i></b><i> ...&quot;</i><br />- <i>&quot;... </i><b><i>Lyell and the uniformitarian school could not agree. To become a science, they affirmed, geology needed not only an empirical theory unencumbered by Biblical preconception, but also a methodology which affirmed the potential natural explanation of terrestrial development and relegated intrinsic mystery to its proper theological realm.&quot;</i></b><br />- <i>&quot;... But Lyell was even more interested in controverting the catastrophist methodology which affirmed that geology &#39;could never rise to the rank of an exact science (1833, v.3, p.3); for it could lead only to &#39;hypotheses, in which we see the ancient spirit of speculation revived, and a desire manifested to cut, rather than patiently untie, the Gordian knot&#39; [an extremely difficult or involved problem] (p.6). </i><b><i>He thus postulated another, very different, type of uniformity that asserted the invariability of natural laws in space and time as a necessary condition to his contention that reference need only be made to observable processes in explaining past changes.&quot;</i></b> (was formerly at <a href=\"http://courses.washington.edu/ess408/Gould1965.pdf\" class=\"ot-anchor\">http://courses.washington.edu/ess408/Gould1965.pdf</a>)<br /><br /><b>1813:</b> <i>Essay on the Theory of the Earth</i>, by Georges Cuvier<br /><br />- Interpreted to have proposed that new species were created after periodic catastrophic floods.<br />- Cuvier became the most influential proponent of catastrophism in geology in the early 19th century.<br />- Cuvier believed there was no evidence for evolution, but rather evidence for cyclical creations and destructions of lifeforms by global extinction events such as deluges.<br /><br /><b>1830-1833:</b> <i>Principles of Geology</i>, by Sir Charles Lyell<br /><br />- <b><i>&quot;In the 18th century, the winds of democracy from America and the attacks of thinkers like Locke and Rousseau, among others, questioned the Monarchy as the natural form of government. Liberalism was moving, and its method was to go after Biblical Geology (specifically the Flood) in order to disarm the Monarchists</i></b><i> ... Scientists in Britain and on the Continent had been making discoveries in the geological record that strained the literal interpretation of the biblical story, while others tried to save it by reinterpreting the words (six days were really six eras, and so on). Many scientists who were also religious tried to find solutions acceptable to the churches, to the people who were increasingly confused, and to themselves as honest individuals laboring to establish the truth. Others wanted nothing less than to destroy once and for all the connection between science and religion. And </i><b><i>those who were politically motivated wanted to bury forever the notion of the divine right of kings. If the scientific evidence denied the truth of the Bible, then it also denied any connection between God and the Monarchy, thus freeing Parliament and the people to redefine the political equations.&quot;</i></b> (from article &quot;What is Uniformitarianism and how did it get here?&quot; by Alex Marton)<br />- <b><i>&quot;So science, its methods and its scope, in the formative beginnings, was very much a creature of the times, unabashedly enlisted in the service of political causes by those who sought to affect political and social developments in their own favor. It is in this context that we must view the formation and growth of the London Geological Society and its vast influence on the parameters within which geology was to develop into a respectable science. The society succeeded in recruiting influential members - doctors, lawyers, members of Parliament, and eventually even geologists. One member, a young whig lawyer named Charles Lyell, decided to take a novel approach: in his Principles of Geology, he argued against the catastrophists by saying that the diluvial theory was, in effect, mythological, and that it stood in the way of progress in geology. He concentrated on the gradual effects of erosion and volcanic uplift to rationalize the geological observations, completely ignoring all evidence of catastrophism. The liberals were delighted, and they elected him secretary, and later president of the Geological Society ... Catastrophism was dead, and the principle of Uniformitarianism was established: geological changes took place slowly, over extremely long period of time, free of widespread catastrophic changes. The Society grew powerful: it was able to prevent publication of material favorable to catastrophism, and to arrange evidence so as to satisfy a uniformitarian view. Similarly, the political battle was won by the liberals, and the power flow between the King, the Parliament, and the People changed direction ... It is unfortunate that these crusty notions have shaped the present dilemma of geology. The political issues were settled long ago, but geology is still committed to a paradigm established primarily as part of a political front that is no longer relevant. It is also unfortunate that catastrophism is linked to biblical fundamentalism because that association has inhibited the evolution of catastrophism as a legitimate avenue of scientific inquiry, without reference to religion or politics.&quot;</i></b> (from article &quot;What is Uniformitarianism and how did it get here?&quot; by Alex Marton)<br /><br /><b>distant past -&gt; well into 2nd half of 19th century:</b> Astronomers insist that since no stones fall from the sky today, no stones have fallen in the distant past.<br /><br />- A heavy fall of 3,000 meteorites in L&#39;Aigle, France in 1803 would eventually convince the astronomers to accept ancient records of falling stones; but the observation was initially rejected by the French scientists from the Academy of Sciences of Paris, who claimed that the observers were <i>&quot;a lot of superstitious peasants who had been fooled by a stroke of lightning hitting a pyritiferous stone just below the sod -- and that the supposed stones fallen from heaven were fakes that should be thrown away; and they were thrown away, only one gramme having miraculously escaped at Paris.&quot;</i> (<a href=\"https://books.google.com/books?id=H1MJAAAAIAAJ&amp;pg=PA32&amp;lpg=PA32&amp;dq=meteorites+france&amp;source=bl&amp;ots=f8fXZaT_4r&amp;sig=eM7_rm3h7C66KlbYCX_19xOX_SI&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjsycOxlt_NAhXCLB4KHfzkCK84ChDoAQg5MAQ#v=onepage&amp;q=meteorites%20france&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=H1MJAAAAIAAJ&amp;pg=PA32&amp;lpg=PA32&amp;dq=meteorites+france&amp;source=bl&amp;ots=f8fXZaT_4r&amp;sig=eM7_rm3h7C66KlbYCX_19xOX_SI&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjsycOxlt_NAhXCLB4KHfzkCK84ChDoAQg5MAQ#v=onepage&amp;q=meteorites%20france&amp;f=false</a>)<br /><br /><b>2nd half of the 19th century:</b> Max Muller<br /><br />- Argued that myth was primarily concerned with recording early man&#39;s perception of the most prominent celestial bodies.<br />- His &quot;solar&quot; theory had a profound influence on scholarship at the turn of the century.<br />- Levi-Strauss: <i>&quot;Max Muller and his school must be given great credit for having discovered, and to some extent deciphered, the astronomical code so often used by the myths.&quot;</i><br />- Most scholars today agree that Muller went too far in tracing so much in ancient myth back to the behavior of the Sun.<br /><br /><b>Late 19th Century - Early 20th Century:</b> Panbabylonism, mainly Hugo Winckler and Alfred Jeremias<br /><br />- Emphasized the astral content of ancient myth and religion.<br />- Held that ancient ideas of cosmogony were based primarily upon a sophisticated knowledge of astronomy, which existed already at 4000 BCE.<br />- They attempt to account for the striking resemblances between the myths and astral traditions of different cultures by supposing that this gave enough time for the ideas to diffuse around the globe.<br />- Jonathan Smith: <i>&quot;Of course, the Pan-Babylonian school was wrong.  At the factual level, its exponents placed too great a reliance on the high antiquity of Near Eastern astrological texts, dating them almost two thousand years too early.  On the theoretical level, they placed too great a reliance on diffusion.  Yet, in many ways they were right. </i><b><i>They saw clearly the need to ground comparison and patterns in a historical process, saw clearly the need to develop a complex model of tradition and the mechanisms for its transmission, saw clearly the need to balance generalities and particularities in a structure which integrated both, saw clearly the priority of comparative systematics over the continued cataloging of isolated comparative exempla, saw clearly the power of pattern (and hence, of comparison) as a device for interpretation. They bequeathed to us this rich heritage of possibilities -- and they bequeathed to us the problems as well.</i></b><i> The two chief options followed by students of religion since then have been either to continue its diffusionist program shorn of its systematic and theoretical depth (e.g., the Myth-Ritual school) or to cut loose the pattern and systematics from history (e.g., Eliade).  We have yet to develop the responsible alternative: the integration of a complex notion of pattern and system with an equally complex notion of history.&quot;</i> (J. Smith, &quot;In Comparison a Magic Dwells,&quot; in <i>Imagining Religion</i>, 1982, pp26-29)<br /><br /><b>Late 19th Century:</b> The Writings of Isaac Newton Vail<br /><br />- Vail is known for his &quot;Canopy Theory&quot;, the idea that Earth had once been girdled by a doughnut-like structure of ice crystals which, in keeping with Genesis 1:6-8, was alluded to as the firmament.<br /><br /><b>1851:</b> <i>The Serpent Symbol, and the Worship of the Reciprocal Principles of Nature in America</i>, by EG Squier<br /><br /><b>1883:</b> <i>The Natural Genesis: Or Second Part of a Book of the Beginnings, Containing an Attempt to Recover and Reconstitute the Lost Origines of the Myths and Mysteries, Types an Symbols, Religion and Language, with Egypt for the Mouthpiece and Africa as the Birthplace</i> (2 volumes), by Gerald Massey<br /><br />- <i>&quot;The controversial and erratic Gerald Massey, in two large works (The Natural Genesis and Ancient Egypt), claimed that the religion and mythology of a polar god was first formulated by the priest-astronomers of ancient Egypt and spread from Egypt to the rest of the world.&quot;</i> (<i>The Saturn Myth</i>, by David Talbott)<br /><br /><b>1883:</b> <i>Ragnarok: The Age of Fire and Gravel</i>, by Ignatius Donnelly<br /><br />- Unlike Whiston and most other scientific investigators on the list, Donnelly was a lawyer, Minnesota state legislator and a member of Congress.<br />- <b><i>&quot;Man, by an inherited instinct, regards the comet as a great terror and a great foe; and the heart of humanity sits uneasily when one blazes in the sky.&quot;</i></b> (p430)<br />- We today understand through laboratory experimentation that <i>&quot;mice can pass on learned information about traumatic or stressful experiences&quot;</i> (Dias, Brian G, and Kerry J Ressler, Nature Neuroscience 17, 89-96 (2014))<br /><br /><b>1885:</b> <i>Paradise Found: The Cradle of the Human Race at the North Pole</i>, by William F. Warren<br /><br />- One of the first writers to recognize the pole as the special domain of the great god: <b><i>&quot;The religions of all ancient nations ... associate the abode of the supreme God with the North Pole, the centre of heaven; or with the celestial space immediately surrounding it. [Yet] no writer on comparative theology has ever brought out the facts which establish this assertion.&quot;</i></b><br />- wikipedia: <i>&quot;In this work Warren placed Atlantis at the North Pole, as well as the Garden of Eden, Mount Meru, Avalon and Hyperborea.[1] Warren believed all these mythical lands were folk memories of a former inhabited far northern seat where man was originally created [2] ... Warren&#39;s identification of Atlantis with the North Pole was maintained by positioning Atlas in the far north by mapping out ancient Greek cosmology. Warren equated the primordial Titan Atlas of Greek mythology who supported the Heavens on his shoulders (or supported the earth on a pillar) to the Atlas described in Plato&#39;s dialogue Critias as the first ruler of Atlantis (Critias, 114a). In Warren&#39;s view, all the axis mundi or cosmic-axis of ancient legends (Yggdrasil, Irminsul and Atlas&#39; pillar) had to be in the far north &#39;at the top of the world&#39;&quot;</i><br /><br /><b>1876:</b> <i>The Chaldean Account of Genesis</i>, by George Smith<br /><br />- <i>&quot;George Smith (1840-1876) was an English Assyrologist, apprentice engraver, but self-taught in cuneiform in the corridors of the British Museum. Eventually he was hired by Sir Henry Rawlinson, prominent archaeologist. Smith achieved world-wide attention when he discovered an account of the flood with obvious Biblical parallels in 1872, related in The Chaldean Account of the Deluge. This book expands on the previous work&quot;</i> (<a href=\"http://sacred-texts.com/ane/caog/index.htm\" class=\"ot-anchor\">http://sacred-texts.com/ane/caog/index.htm</a>)<br /><br /><b>1894:</b> <i>The Dawn of Astronomy: A Study of the Temple-Worship and Mythology of Ancient Egyptians</i>, by Norman Lockyer<br /><br />- <i>&quot;This book is a tour of the Egyptian monuments uncovered and examined archeologically in his day, and he looks at their orientation and astronomical alignments with expertise and enthusiasm&quot;</i> (review at <a href=\"https://archive.org/details/dawnastronomyas00lockgoog\" class=\"ot-anchor\">https://archive.org/details/dawnastronomyas00lockgoog</a>)<br /><br /><b>1894:</b> <i>The Swastika: The Earliest Known Symbol and Its Migrations; With Observations on the Migration of Certain Industries in Prehistoric Times</i><br /><br /><b>1897:</b> <i>The Night of the Gods: An Inquiry Into Cosmic and Cosmogenic Mythology and Symbolism</i> (Vol 2), by John J O&#39;Neill<br /><br />- <b><i>&quot;In a general survey of ancient language, symbolism, and mythology, John O&#39;Neill (The Night of the Gods) insisted that mankind&#39;s oldest religion centered on a god of the celestial pole.&quot;</i></b> (<i>The Saturn Myth</i>, by David Talbott)<br />- <i>&quot;It was while engaged in these researches and on literary work, reviewing books on religious symbols for one of the Quarterlies, that it occurred to him that the common origin of the religious myths and symbols is to be found in the impression made on the mind of every race in every clime, by the phenomena of the revolution of the earth, and its relation, real or apparent, to the sun, the moon and the stars. The phenomena being universally observed by primitive man in the same stage of development, and under very similar conditions, gave rise to similar, if not to identical, interpretations. This general principle, applied to the symbols and myths of races and climes so diverse as those of the Aryan races of Europe and India, the yellow races of Eastern Asia, the red races of America, the black races of Africa and Australia, yields some striking and unexpected results.&quot;</i> (from first pages of book)<br /><br /><b>1899:</b> <i>Star-Names &amp; their Meanings</i>, by Richard Hinckley Allen<br /><br />wikipedia: <i>&quot;this work collected the origins of the names of stars and constellations from a panoply of sources, some primary but most secondary; also telling briefly the various myths and folklore connected with stars in the Greco-Roman tradition; as well as in the Arabic, Babylonian, Indian and Chinese traditions, for which, however, some modern criticism having taken it to task, claiming it to be largely superseded ... The book also provides some cursory details about astronomy at the knowledge level of the end of the 19th century. Similarly, astrology and its history are dealt with briefly in the introduction, and some other basic astrological references are scattered throughout the book, although downplayed.&quot;</i><br /><br /><b>1901:</b> <i>The Fundamental Principles of Old and New World Civilizations</i>, by Zelia Nuttall<br /><br />- <i>&quot;Zelia Nuttall, in Fundamental Principles of Old and New World Civilizations, undertook an extensive review of ancient Mexican astronomy, concluding that the highest god was polar. From Mexico she shifted to other civilizations, </i><b><i>finding the same unexpected role of a polar god.&quot;</i></b> (<i>The Saturn Myth</i>, by David Talbott)<br /><br /><b>1909:</b> <i>The Legends of the Jews</i> (Vol I - IV) - A popular reference source for historians<br /><br /><b>1909:</b> <i>Les origines de l&#39;astronomie chinoise</i> (The Original Chinese Astronomy), by Léopold_de_Saussure<br /><br />- <b><i>&quot;showed that primitive Chinese religion and astronomy honour the celestial pole as the home of the supreme god&quot;</i></b> (<i>The Saturn Myth</i>, by David Talbott)<br /><br /><b>1909:</b> <i>The Earliest Cosmologies: The Universe as Pictured in Thought by the Ancient Hebrews, Babylonians, Egyptians, Greeks, Iranians, and Indo-Aryans</i>, by William Fairfield Warren<br /><br /><b>1913:</b> <i>The Dragon in China and Japan</i><br /><br /><b>1922:</b> <i>The Tree of Life</i>, by Uno Holmberg Harva<br /><br />- <b><i>&quot;documented the preeminence of the polar god in the ritual of Altaic and neighbouring peoples</i></b><i>, suggesting ancient origins in Hindu and Mesopotamian cosmologies&quot;</i> (<i>The Saturn Myth</i>, by David Talbott)<br /><br /><b>1925:</b> <i>The Riddle of the Earth</i>, by Comyns Beaumont<br /><br />- <i>&quot;&#39;His main subjects were mythology, early history, geology, and ancient astronomical records. In all of them he found convincing evidence that the earth had suffered many cataclysms in the course of its history, the most recent having occurred in about 1322 B.C.&#39;. Beaumont suggested they &#39;were caused by meteoric impacts which in turn are closely related to cometary movements. That belief was the cornerstone of Beaumont&#39;s revolution theories and history and geography&#39;</i> (summary from <a href=\"http://www.velikovsky.info/Comyns_Beaumont\" class=\"ot-anchor\">http://www.velikovsky.info/Comyns_Beaumont</a>)<br />- Alfred de Grazia summarizes Beaumont&#39;s main ideas as follows (list from <a href=\"http://www.velikovsky.info/Comyns_Beaumont\" class=\"ot-anchor\">http://www.velikovsky.info/Comyns_Beaumont</a>):<br /> + The geology of the world&#39;s surface is largely catastrophic.<br /> + The catastrophe was caused by a cometary collision.<br /> + All geological formations were shifted as a result.<br /> + Cosmic lightning played a major role.<br /> + Hydrocarbons were present in cometary tails.<br /> + Ancient chronology was several hundred years too old.<br /> + The Ancient calendars had to be revised because of the catastrophe.<br /> + Many species were extinguished catastrophically.<br /> + Religion was born in cometary worship and tied to phallic forms because of the shape of comets.<br /> + Fear of cometary collisions is inherited by mankind.<br /> + Vermin were deposited by comets which also provoked plagues.<br /> + Deities from Egypt, Greece, Meso-America, and elsewhere were identified with planets.<br /> + Pyramids were both astronomical observatories and &#39;air-raid shelters&#39; for nobility and kings.<br /> + Planet Saturn, as a comet, caused the Noachian Deluge.<br /> + The Atlantis date (c. 9500 BC) given by Plato had to be shortened.<br /> + Extensive legendary evidence pictures the &#39;hairy&#39;, &#39;bearded&#39;, &#39;blazing stars&#39; that were comets.<br /> + Stonehenge, Avebury Circle and similar monuments were astronomical instruments.<br /> + Central American legends (and cultures) were contemporaneous with those of the Old World.<br /> + The intercalary &#39;five evil days&#39; were cursed because they coincided with a world disaster and the ending of an age.<br /> + The serpent, dragon, winged-globe, caduceus, and other ancient symbols are traceable to cometary catastrophes.<br /> + Religious festivals are dated by cometary catastrophes.<br /> + Cometary conflagrations are the origin of coal deposits.<br /> + The ancients had a true 360 day year.<br /> + The planet Venus underwent great changes in color, diameter, figure, and orbit in the time of Ogyges.<br /> + Quetzalcoatl (Coculkan-Hurakan) commemorated the cometary dragon for the Meso-Americans.<br />- <b><i>&quot;Did Immanuel Velikovsky knowingly present ideas someone else had developed many years earlier as his own? While this question seems bizarre even to his most ardent opponents, it was recently raised in a paper by Robert Stephanos. [..] Beaumont&#39;s theses are almost identical to those of Velikovsky. Yet Beaumont developed and published them as early as the 1920s and 1930s.&quot;</i></b> (Benny J Peiser, quoted at <a href=\"http://www.velikovsky.info/Comyns_Beaumont\" class=\"ot-anchor\">http://www.velikovsky.info/Comyns_Beaumont</a>)<br />- <i>&quot;De Grazia was suspicious: &#39;Could there have been a &#39;Bridie Murphy Effect&#39;?&#39; Had Velikovsky simply forgotten that he had already come across Beaumont&#39;s books (or ideas) in the 1920s or 1930s? ... In hindsight, de Grazia was too quick to rule out direct influence. He failed to check whether Beaumont&#39;s books were stored in the Public Library on 42nd Street, the other big library which Velikovsky had frequently used during the 1940s. It holds all of Beaumont&#39;s early books [13], so they were readily available to Velikovsky during his ten years of research.&quot;</i> (Benny J Peiser, quoted at <a href=\"http://www.velikovsky.info/Comyns_Beaumont\" class=\"ot-anchor\">http://www.velikovsky.info/Comyns_Beaumont</a>)<br /><br /><b>1926:</b> <i>The Migration of Symbols and Their Relations to Belief and Customs</i>, by Donald A Mackenzie<br /><br />- <i>&quot;In this interesting volume Mr. Mackenzie deals with the occurrence of certain widely spread symbols, such as the swastika and the spiral, on the basis of the Diffusionist theory of a common origin, which seems to him a far more plausible hypothesis than that of the &#39;psychic unity of man.&#39;  He rejects the latter mainly, it seems, because we do not know enough about the workings of the primitive mind to justify the assumption that different tribes and races will think alike in like circumstances.  His book is therefore mainly a contribution to the attractive but still very speculative theory that the Diffusionist school of anthropology has built up.  He regards the &#39;winged-disc&#39; of the Egyptians and the &#39;whirling-logs&#39; of the American Indians as identical in origin, and would trace a common source all the variations of the spiral ornament or symbol.  This sees to be going far beyond what is warranted by our present state of knowledge ... [paywall]&quot;</i> (<a href=\"http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=3911944\" class=\"ot-anchor\">http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=3911944</a>)<br /><br /><b>1927:</b> <i>The Celestial Ship of North</i>, by E Valentia Straiton<br /><br />- <i>&quot;This is an extraordinary work. It is one of most [comprehensive] examinations of mythology ever produced. I highly recommend it to anyone looking to expand his or her knowledge base in mythology, metaphysics and/or astrology.&quot;</i> (Amazon.com review)<br />- <i>&quot;An important and informative work. An incredible resource of occult knowledge.&quot;</i> (Amazon.com review)<br />- <i>&quot;Very good. Amazing information&quot;</i> (Amazon.com review)<br /><br /><b>1931:</b> <i>Symbolism of the Cross</i>, by René Guénon<br /><br />- <i>&quot;sought to outline a universal doctrine centering on the polar gods and principles of ancient man.&quot;</i> (<i>The Saturn Myth</i>, by David Talbott)<br /><br /><b>1932:</b> <i>The Mysterious Comet: Or the Origin, Building up, and Destruction of Worlds, by means of Cometary Contacts</i>, by Comyns Beaumont<br /><br />- <i>&quot;The Mysterious Comet examines the relationship between the Earth and other bodies in space, with or without &#39;fixed&#39; orbits, noting the effects on various systems, weather and otherwise. He notes the interaction between comets, meteors, meteorites and volcanoes, and the manner in which all life is sustained by the replenishment of air and water and other energies brought by cometary bodies ... Comyns Beaumont brings together accounts of the earthquakes and eruptions of his day and those from history along with meteorological and astronomical accounts and  mythical references. He builds on neo-platonic and catastrophist theories and places Atlantis in the region of the British Isles ... A fascinating and thorough book with lots of well indexed references for readers interested in doing further research.&quot;</i> (summary from <a href=\"http://www.twelvearound1.com/cb2.html\" class=\"ot-anchor\">http://www.twelvearound1.com/cb2.html</a>)<br /><br /><b>1934:</b> <i>Ancient Oriental Seals in the collection of Edward T Newell</i>, by Edward Theodore Newell<br /><br />- Does not appear to show the original seals, but it does show many drawings of the archetypal forms which appear within these seals, with some discussion of what they might refer to.<br />- <i>&quot;The collection of cylinders and other seals here presented was assembled by Mr. Edward T. Newell during a period of about thirty years ... This publication should be considered primarily as a collection of source materials.  In cataloguing these seals I have endeavored to describe them as accurately as possible without discussing their designs ...&quot;</i><br /><br /><b>1936:</b> <i>Moons, Myths and Man: A reinterpretation</i>, by HS Bellamy<br /><br />- <i>&quot;H S Bellamy was a writer who helped popularise the pre-WWI Cosmic Ice Theory of Hans Hörbiger aka Welteislehre or Glazial-Kosmogonie (Glacial Cosmology) which theorised that ice was the basic building block of all cosmological processes. Hörbiger was an old school engineer whose theories were later pounced on by the Nazis in their search to counter what they saw as Jewish dominance in Astronomy and Physics - it&#39;s fascinating stuff, this: the universe seen through the eyes and mind of a jobbing engineer/designer/inventor who lived and worked at the grimey/greasy/steamy cuttin&#39; edge of the early 20th century Industrial-Modernist world...these are the thoughts and theories and background stuff that helped shape the imaginations of the first wave of Golden Age SF writers ... Bellamy (whose pen-name hides his own German origins) chucks a trough-load of ancient-myths into the mix and asks: did the Babylonians, etc and assorted pre-History civilisations see the fall of the other, earlier Earth-satellites that once accompanied Luna. There&#39;s some compelling connections made here (and lots of great diagrams!) ... Bellamy&#39;s descriptions of stars colliding and planet-sized plumes of steam erupting from a Red Giant star after billions of tonnes of cosmic ice spiral down thru its gravity-well read like they&#39;ve been lifted straight from the apocalypto-carnage of E E &#39;Doc&#39; Smith&#39;s Lensmen series or early Heinlein, etc&quot;</i> (review from <a href=\"http://kidshirt.blogspot.com/2008/06/moons-myths-and-man.html\" class=\"ot-anchor\">http://kidshirt.blogspot.com/2008/06/moons-myths-and-man.html</a>)<br /><br /><b>1942:</b> <i>Book of Revelation is History</i>, by HS Bellamy<br /><br />- <i>&quot;Mr. Bellamy discovers in the Book of Revelation three cycles of myths, all concerned with a most appalling cataclysm which occurred thousands of centuries ago -the attraction and disintegration by the earth’s gravitation of the satellite which was the predecessor of our present moon. He discovers further mythological material which he traces to a subsequent cataclysm which occurred when the planet Luna was captured by the earth’s gravitation about fifteen thousand years ago, causing the sudden submergence of the continent of Atlantis.&quot;</i> (summary from <a href=\"http://www.nature.com/nature/journal/v150/n3804/abs/150363a0.html\" class=\"ot-anchor\">http://www.nature.com/nature/journal/v150/n3804/abs/150363a0.html</a>)<br /><br /><b>1943:</b> <i>Built Before the Flood: The Problem of the Tiahuanaco Ruins</i>, by HS Bellamy<br /><br />- <i>&quot;argues like others, that the evidence of calcified remains of marine plants 11,500 feet up on the plateau shows that it was originally at sea level, and therefore predates the great catastrophe and flood ... Bellamy ascribes this to a great planetary girdle-tide. This raised the sea-level by over two miles on either side of the Equator, caused in turn by a predecessor of our Moon orbiting into Earth; an even greater catastrophe was caused by its impact, which also destroyed Atlantis and the remains of Lemuria. Though this, based in Hans Hoerbiger’s cosmological theory popular before the war, is not now fashionable, it is one explanation of the sea-fossils and the extraordinary height of Tiahuanaco, in rarified air not easy for westerners to breathe ... It also records (pages 41-2) the legends of the (then) local Indians, a proud and quiet people calling themselves &#39;Urus&#39;, &#39;Men of the Light&#39;, who hinted that they had lived in these Bolivian highlands before the Age of Darkness (Chamak-Pacha), a terrible cataclysm, and that they remember the time when the Sun re-appeared after it. The great ruined city, for which &#39;Tiahuanaco&#39; was the traditional local name, was built before this cataclysm, and was by a great sea rich in big fish; eventually there was a flood which destroyed the city’s builders.&quot;</i> (<a href=\"https://fallencreature.wordpress.com/genesis-files/built-before-the-flood-the-mystery-of-tiahuanaco/\" class=\"ot-anchor\">https://fallencreature.wordpress.com/genesis-files/built-before-the-flood-the-mystery-of-tiahuanaco/</a>)<br /><br /><b>1945:</b> <i>The Riddle of Prehistoric Britain</i>, by Comyns Beaumont<br /><br />- <i>&quot;Not until Jacob in a somewhat obscure manner was told to call himself Israel was that name adopted and accorded to his twelve &quot;sons:&quot; but if we accept the explanation of Sanchoniathon, a Phoenician of Tyre, Cronus &quot;whom Phoenicians called Israel&quot; was king of Phoenicia, and it signified that these Chaldeo-Phoenician tribes were worshippers of Cronus-Saturn...for Jehovah was a far later importation. The name Israel has subsequently been misappropriated, for those Biblical Christians who term themselves Israelites in fact label themselves followers of a pagan deity&quot;</i> (quote from the text, pulled from <a href=\"http://www.truthcontrol.com/comyns-beaumont\" class=\"ot-anchor\">http://www.truthcontrol.com/comyns-beaumont</a>)<br />- <i>&quot;Another feature to be observed of the Giants is that nearly always they are described as red-haired, as were the Adamites and Edomites, names derived from the Hebrew adom or edom, red or ruddy...The same applies to the Phoenicians of like stock, whose name is but a derivation of the word meaning red or carroty, relating to a ruddy-faced people&quot;</i> (another quote from text)<br />- <i>&quot;The Flood, to the world generally a vague and nebulous tradition, really conceals the most appalling visitation and its ravages in the British Isles and Scandinavian lands may be retraced to more considerable extent by the effects of what geologists term the &#39;Drift&#39; Age. It was no mere ice drift. It was sudden and terribly swift and violent ... It permanently affected the world’s climate towards greater extremes of cold and damp, lengthened the solar year by enlarging the world’s orbit. It shaped world history by compelling the flight of survivors to other less inhospitable climes and led in considerable degree to the dispersion of the Aryans. It inundated the British Isles for a period to a great extent except the higher lands. It was the drowning of Atlantis. </i><b><i>The Flood immortalizes the collision of a fallen planet, later termed Satan, actually a cometary body, with our earth.&quot;</i></b> (Notice similarity between terms <i>Satan</i> and <i>Saturn</i> ...)<br /><br /><b>1945:</b> <i>In the Beginning: God. A new scientific vindication of cosmogonic myths in the Book of Genesis</i>, by HS Bellamy<br /><br /><b>1948:</b> <i>The Atlantis Myth</i>, by HS Bellamy<br /><br />- <i>&quot;Bellamy argues that the sinking of Atlantis was caused by the quakes which attended the earth&#39;s capture of its present moon.&quot;</i> (<a href=\"https://books.google.com/books?id=X0HCAgAAQBAJ&amp;pg=PT176&amp;lpg=PT176&amp;dq=%22the+atlantis+myth%22+bellamy&amp;source=bl&amp;ots=v_phc0nBPl&amp;sig=KDbyU4rfEwhuMdSnxb6TlfbDKDM&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjC_fqFjurNAhUT5mMKHUsRBU4Q6AEIHDAA#v=onepage&amp;q=%22the%20atlantis%20myth%22%20bellamy&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=X0HCAgAAQBAJ&amp;pg=PT176&amp;lpg=PT176&amp;dq=%22the+atlantis+myth%22+bellamy&amp;source=bl&amp;ots=v_phc0nBPl&amp;sig=KDbyU4rfEwhuMdSnxb6TlfbDKDM&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjC_fqFjurNAhUT5mMKHUsRBU4Q6AEIHDAA#v=onepage&amp;q=%22the%20atlantis%20myth%22%20bellamy&amp;f=false</a>)<br /><br /><b>1950:</b> <i>Worlds in Collision</i>, by Immanuel Velikovsky<br /><br />- <b>Likely the most controversial book and most controversial researcher in the history of modern science.</b><br />- <b>The scientific community forced the book&#39;s publisher, Macmillan, to cease publishing it just one month after its publication on May 3, 1950.  It would nevertheless become a bestseller in the United States in 1952.</b><br />- Velikovsky leveled two major attacks at the dominant attitude of uniformitarianism with roughly the same intensity: <b>His first claim was that catastrophes had wracked Earth; the second was that these happened in the recent past and were witnessed and recorded by humanity.</b><br />- <b>Velikovsky argued in it that the terrifying gods of the ancient world were planets. The book recounted two close encounters of the comet or protoplanet Venus with the Earth.</b><br />- <b>Velikovsky argued that the planet Mars was known as the warrior god, and that centuries after the Venus catastrophes, Mars moved on an unstable orbit intersecting that of Earth</b>, leading to a series of Earth-disturbing events in the 8th and 7th centuries BC.<br />- These claims were made at a time when the solar system was considered an extremely stable configuration, <i>&quot;on the basis of approximate analytical analysis of the stability of dynamical n-body systems, on the basis of properties of the standard model (condensation from a gas cloud) for the formation of the solar system ... The analysis made using the modern very sophisticated analytical instruments has indeed shown that </i><b><i>nonlinear complex dynamical system, including planetary systems, have generally a behavior of the type defined chaotic, whose long term behavior cannot be predicted</i></b><i> and whose dynamical structure is extremely rich. Now it is estimated that, even disregarding the very possible interactions with other bodies and structures in the galaxy, the solar system cannot be back integrated in time for more than a few million years, a factor one thousand less than estimated fifty years ago.&quot;</i> (<a href=\"http://www.bibliotecapleyades.net/ciencia/esp_ciencia_velikovsky12.htm\" class=\"ot-anchor\">http://www.bibliotecapleyades.net/ciencia/esp_ciencia_velikovsky12.htm</a>).<br />- Velikovsky was shunned by nearly all of academia -- <b>except for his friend, Albert Einstein</b>, who claims to have read the book twice.<br />- <b>Velikovsky used his mythological knowledge of the Venus archetype to predict in the book that Venus would be found to be between 800-900F -- which differed significantly from the mainstream view at the time of 200-300 F.  Velikovsky&#39;s prediction was shown to be accurate in 1962.</b><br />- Velikovsky also stressed the importance of electromagnetic interactions in astronomy, particularly in the case of large bodies encroaching upon one another -- a subject which he spoke at length to Einstein about.<br />- Some have claimed that the great success of Velikovsky&#39;s <i>Worlds in Collision</i> was <i>&quot;partly related to a postwar reawakening of interests in religious traditions and widespread critical sentiments against a science that had led to the atomic weapon and to the risk of a nuclear obliteration of humankind.&quot;</i> (<a href=\"http://www.bibliotecapleyades.net/ciencia/esp_ciencia_velikovsky12.htm\" class=\"ot-anchor\">http://www.bibliotecapleyades.net/ciencia/esp_ciencia_velikovsky12.htm</a>)<br /><br /><b>1951:</b> <i>A Life History of Our Earth: Based on the Geological Application of Hoerbiger&#39;s Theory</i>, by HS Bellamy<br /><br /><b>1952:</b> <i>Ages in Chaos</i>, by Immanuel Velikovsky<br /><br />- <b>In this book, Velikovsky challenged the authenticity of radiocarbon dates on the basis of historical documents.</b><br />- <i>&quot;The basic idea of Velikovsky is that the official chronology of the first and second millennium BC of Egyptian and other civilizations dated by anchoring them to the Egyptian one (Micenean, Cananean, Ugaritic, Cretese, Anatolian…) is affected by a substantial error. This is for Velikovsky the main reason why scholars have essentially been unable to fit the events described in the Bible with the events described in Egyptian or other histories ... Velikovsky claims that the fundamental error lies in the absolute anchoring of the Egyptian chronology that was made about two hundred years ago, at the beginning of Egyptology&quot;</i> (<a href=\"http://www.bibliotecapleyades.net/ciencia/esp_ciencia_velikovsky12.htm\" class=\"ot-anchor\">http://www.bibliotecapleyades.net/ciencia/esp_ciencia_velikovsky12.htm</a>)<br />- Current historical timelines of the period BC depend on the accepted &quot;gold standard&quot; of Egyptian Pharaoh dynasties.  Velikovsky challenged the accuracy of the timeline.<br />- He used the Ipuwer papyrus to realign the catastrophic events in Biblical Exodus with reports in Egyptian history of similar chaos that in the standard version contains no reference to equivalent events of the period.<br /><br /><b>1955:</b> Jupiter observed to emit radio noises, as Velikovsky predicted<br /><br />- Velikovsky had made the prediction in the midst of discussions with Einstein.  Although Einstein himself disputed Velikovsky&#39;s reasoning, he nevertheless used his influence to have Jupiter surveyed for radio emissions.<br />- <b>In April 1955, radio noises were indeed discovered from Jupiter -- a surprise for scientists at the time who had assumed that Jupiter was too cold and inactive to emit radio waves.</b><br />- <b>The discovery led Einstein to agree to assist in developing other tests for Velikovsky&#39;s thesis, but Einstein unfortunately died only weeks later.</b><br /><br /><b>1958:</b> The first American rockets are sent to space<br /><br />- It is quickly realized that space is not just an empty vacuum; space contains charged particles, known in the laboratory as a plasma.<br />- <b>All of our theories for space, up to that point, were based upon the premise that gravity was the only real force available; but plasmas can also carry the electromagnetic force, as Velikovsky suggested.</b><br />- A 1963 Popular Science article covering the discovery says in big bold letters: <b><i>&quot;&#39;Space&#39; was invented on Earth before we knew what was out there&quot;</i></b> (<a href=\"https://books.google.com/books?id=wiADAAAAMBAJ&amp;pg=PA73&amp;lpg=PA73&amp;dq=van+allen+empty+space&amp;source=bl&amp;ots=-3Y7jZrrqx&amp;sig=PjtIX3XnKqJ7ZRkZU8cnCq2id7U&amp;hl=en&amp;sa=X&amp;ved=0CDwQ6AEwBGoVChMIheuM2ZDBxwIVhJmICh1rrwlT#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=wiADAAAAMBAJ&amp;pg=PA73&amp;lpg=PA73&amp;dq=van+allen+empty+space&amp;source=bl&amp;ots=-3Y7jZrrqx&amp;sig=PjtIX3XnKqJ7ZRkZU8cnCq2id7U&amp;hl=en&amp;sa=X&amp;ved=0CDwQ6AEwBGoVChMIheuM2ZDBxwIVhJmICh1rrwlT#v=onepage&amp;q&amp;f=false</a>)<br />- Numerous astrophysical and plasma physics textbooks are updated to reflect the new understanding that 99%+ of what we see with our telescopes is matter in the plasma state; our textbook theories for space have yet to change in any significant manner as a result, a half-century later.<br /><br /><b>1950-1969:</b> Rough times for mythology research<br /><br />- Unfounded claim that ancient myths are purely fictional.<br />- Unfounded belief that people investigated mythology only to try to force-fit the data into supporting a particular religious claim.<br />- Challenges to uniformity (like changes to the solar system) were outright rejected by academia.<br /><br /><b>1964:</b> <i>Bombarded Earth</i>, by René Louis Charles Gallant<br /><br />- <i>&quot;The first comprehensive treatment of large meteorite impacts and their effects on Earth was given by René Gallant in his book Bombarded Earth (1964). Gallant&#39;s work has gone largely unnoticed&quot;</i> (summary from <a href=\"http://www.velikovsky.info/Ren%C3%A9_Gallant\" class=\"ot-anchor\">http://www.velikovsky.info/Ren%C3%A9_Gallant</a>)<br />- <i>&quot;Gallant argued that the seismic activity and climate changes which, according to the evidence provided by (Claude) Schaeffer, occurred at the times of the destructions [of Bronze Age Civilisation], were both likely to have resulted from large meteoritic impacts&quot;</i> (summary from <a href=\"http://www.velikovsky.info/Ren%C3%A9_Gallant\" class=\"ot-anchor\">http://www.velikovsky.info/Ren%C3%A9_Gallant</a>)<br />- <i>&quot;It is the author&#39;s assertion that impact by meteorites over long stretches of geological time has played a vital and significant part in the history of our planet and the evolution of its life ... that the impact of meteorites of planetary dimensions must have caused the Earth&#39;s polar axis to shift, changes in the tempo of the Earth&#39;s spin, and slips in the upper part of the Earth&#39;s crust. The results must have been to change the face of the earth, to create mountain ranges, to alter climates and to slaughter species. Moreover these impacts must have released large quantities of lethal radiation which extinguished living species and permitted the appearance of new ones ... If this were not dramatic enough, such catastrophes must also render questionable the work of the various &#39;radioactive clocks&#39; nowadays used to measure geological and prehistoric time on the assumption of a slow evolution. Nor can we escape the possibility that the theory of the gradual and orderly evolution of our planet and its living species may have been repeatedly disturbed by global cataclysmic events.&quot;</i> (from the text)<br /><br /><b>1969:</b> <i>Hamlet&#39;s Mill: An Essay on Myth and the Frame of Time</i>, by Giorgio De Santillana and Hertha Von Dechend<br /><br />- <b>de Santillana was possibly the greatest historian of Renaissance science at the the time of publication</b>. Hertha von Dechend was professor of the history of science at the University of Frankfurt, and a research associate at the Massachusetts Institute of Technology for five winters, 1962 to 1967. Giorgio de Santillana was for many years professor of the history and philosophy of science at M.I.T. By 1969, when Hamlet’s Mill was published, he had authored numerous articles and books, but no university press accepted <i>Hamlet&#39;s Mill</i>, thus respectable scholrs were not obligated to take his work into account.<br />- They concluded ancient myths have a common origin and that the places mentioned in myths were in the heavens and not on the Earth.<br />- <b><i>&quot;The real actors on the stage of the universe are very few, if their adventures are many. The most &#39;ancient treasure&#39; -- in Aristotle&#39;s word -- that was left to us by our predecessors of the High and Far-Off Times was the idea that the gods are really stars, and that there are no others. The forces reside in the starry heavens, and all the stories, characters and adventures narrated by mythology concentrate on the active powers among the stars, who are the planets.&quot;</i></b><br />- Noted that the ancients seemed to universally refer to the planets as gods and to stars or groups of stars as animals.<br />- <b>Tried to force-fit the ancient stories into a gradualist uniformitarian solar system model</b> (which asserts that we can understand the past by simply observing the present).<br />- They were staunch uniformitarians.  They considered the ancient sky as only slightly different, but nevertheless asserted that the ancients saw a sky that is not seen now.<br />- Claimed that the ancient mythmakers were not <i>&quot;even mildly interested in unusual sensational &#39;catastrophes&#39; caused by meteors, and the like&quot;</i>.  Ev Cochrane responds to this claim by pointing out that <i>&quot;it would be difficult to point to a single recurring theme in all of ancient myth that does not have direct reference to &#39;sensational catastrophes&#39; associated with the tumultuous behavior and terrifying appearance of the various planets.&quot;</i><br />- Like the Pan-Babyloniasts, Santillana and Dechend argued that astronomical understanding dated to very ancient times. <b>They claimed that such advanced concepts as the precession of the equinoxes were common knowledge already during the Paleolithic Period. It then diffused around the globe, whereby it somehow became encoded into ancient myth, but -- oddly -- was otherwise forgotten.</b><br />- Their approach made it difficult to explain many of the ancient writings, such as why Saturn was called the Sun god and Mars was thought to create havoc with the other gods and with the Earth.<br /><br /><b>1966:</b> <i>Prehistory and Earth Models</i>, by Melvin A Cook<br /><br />- <i>&quot;The author is a physical chemist (PhD, Yale, 1937) with a career in explosives, high velocity impact, solid state, surface chemistry and related problems in chemistry, physics and metallurgy, but no formal training in geology, having studied it only by self discipline for the past decade. The present volume is, therefore, the work of &#39;an outsider&#39;. His interest in the problems concerning earth science became strong after discovery of evidence that the earth&#39;s atmosphere may not be in steady state as far as radiocarbon is concerned even though only about 30,000 years are required to establish this steady state as shown by Libby. Presentation of this evidence in open forums usually met with vigorous debate. In order, therefore, to provide the facts pertaining to this and related matters, the bulletin &#39;Geological Chronometry&#39; [Cook, 1956] was presented. It made available for careful study arguments on this matter presented in lectures given in the Geology Department, University of Utah in 1955.One of the more interesting arguments concerned the helium balance of the earth and its implications concerning age, which was later published [Cook, 1957]. An attempt to publish another manuscript giving direct evidence for the short-time chronometry of the atmosphere and oceans entitled &#39;Anomalous Chronometry in the Atmosphere and Hydrosphere&#39;, not unexpectedly nor without some cause, met with considerable opposition and was not published. Interestingly enough, the apparent fact that radiocarbon is not in steady state in the atmosphere was the main objection. Lingenfelter [1963] was only recently able to point this out ... Chapters I to IV, inclusive, are the updated result of studies on problems of &#39;geologic time&#39; which it is hoped have taken full account of all objective criticism so far obtained. The author&#39;s conclusion based on this work, is that there really are no reliable time clocks despite an almost overwhelming contrary opinion.&quot;</i> (from Preface of text)<br /><br /><b>1967:</b> <i>Cataclysms of Earth</i>, by Hugh Auchincloss Brown<br /><br />- Amazon review: <i>&quot;I found reading Cataclysms of the Earth by Hugh Auchincloss Brown to be a really mind-opening experience. The first part of the book discusses the theory that the polar ice caps regularly build up such size and weight over thousands of years that our spinning earth eventually sends them careening toword the equator. In many respects this makes much more sense that the usual &quot;ice ages&quot; and warm ages stuff we have been taught. Here&#39;s why ...</i><br />- <i>It explains the ubiquitous &quot;great flood&quot; stories found in so many ancient societies around the globe. There&#39;s not enough water on the earth for the oceans to cover the tall mountains. But if the land suddenly started moving rapidly, then the oceans, by inertia, would be seen to flood onto and over the land.</i><br />- <i>Entire flash frozen mastadons have been found in Siberia with temperate climate grasses in their stomachs and mouths. One minute they were eating grass and the next they were frozen solid. Careening poles is the best explanation for this.</i><br />- <i>Hudson bay, in Canada, was evidently the north pole in recent geologic history. The land is rebounding rapidly and there are ice scratches on the surrounding rocks which radiate out in all directions from the center.</i><br />- <i>There is also so much more than I can even get into here in this brief review. Clearly Hugh Auchincloss Brown was a man who dared to think for himself. He goes on to state his own theory of what gravity is and shoots some big holes in the ideas we associate with Newton.</i><br />- <i>If you totally believe in the modern day religion of &quot;science&quot; as promoted in the media and universities then you will have great trouble with this book.&quot;</i><br /><br /><b>1974:</b> <i>Recollections of a Fallen Sky: Velikovsky and Cultural Amnesia</i><br /><br />- Collection of the papers presented at the University of Lethbridge conferences &#39;Velikovsky and Cultural Amnesia&#39; which ran May 9 and 10, 1974<br />- <i>&quot;The papers presented in this volume are revised versions of the papers originally presented at the Symposium and from the first collection of papers on the subject of cultural Amnesia since Velikovsky introduced the topic in Worlds in Collision. The papers have been examined by other experts in the field concerned, criticisms were collected, and the authors were allowed to make minor changes in the hope that a more accomplished volume could be produced ... Although the papers all relate to some aspect of Cultural Amnesia, they deal with subjects as diverse as anthropology, geology, narrative art, and psychiatry. While the task of showing relationships between them is desirable, it is difficult. It is may hope that the interpretation presented here, with which the authors might not agree, will stimulate readers to consider carefully the papers and their relation to Cultural Amnesia.&quot;</i> (summary from <a href=\"http://www.velikovsky.info/Recollections_of_a_Fallen_Sky\" class=\"ot-anchor\">http://www.velikovsky.info/Recollections_of_a_Fallen_Sky</a>)<br /><br /><b>1974:</b> AAAS Conference, &quot;Velikovsky&#39;s Challenge to Science&quot;<br /><br />- It was summarized as: <i>&quot;&#39;Velikovsky&#39;s Challenge to Science&#39;: From his study of historical records, Immanuel Velikovsky has concluded that close encounters between earth and the planets Mars and Venus occurred at about 1500 B.C. and 775 B.C. This suggestion has met with great disbelief from most astronomers, though public interest in Velikovsky&#39;s ideas has continued to maintain the controversy. What are the arguments for and against Velikovsky&#39;s suggestions? How has the scientific establishment dealt with his work. Can any nonscientist make contributions to the advancement of scientific thought, or is the day of the independent scholar now over -- at least in astronomy?&quot;</i><br />- Most of the papers resulting from the symposium were published in the book, <i>Scientists Confront Velikovsky</i> (edited by Donald Goldsmith).  Velikovsky&#39;s presentation appeared in Pensee.<br />- It was marked by a now-famous confrontation between the wildly popular Carl Sagan and Immanuel Velikovsky.  By most accounts, Carl Sagan was the victor.  But, one science writer, James Hazelwood of the <i>Oakland Tribune</i> acknowledged that Velikovsky <i>&quot;gave at least as much punishment as he received.&quot;</i> (<a href=\"http://www.velikovsky.info/Velikovsky%27s_Challenge_to_Science\" class=\"ot-anchor\">http://www.velikovsky.info/Velikovsky&#39;s_Challenge_to_Science</a>)<br />- <i>&quot;I received a phone call from Ivan King, the galactic dynamicist from UC Berkeley who would be the program moderator at the AAAS Symposium, and he expressed some unhappiness at a news item I had submitted for publication in the December issue of Industrial Research. King disclaimed in this conversation what I had written, that the recent upsurge of interest had prompted a rehearing of Velikovsky&#39;s hypothesis and that, additionally, the projected symposium was a move to stem criticisms leveled at the AAAS by various groups for past excesses of the more militant, zealous members of that august body against Velikovsky. King&#39;s disclaimer was a complete turnabout from what I had been informed in a previous chat with him some months before, and I began to smell something fishy in the air. However, this last conversation with King closed on an upnote, leaving the door ajar for further discussions. But, as it turned out, there were no more discussions until my confrontation with King at the symposium itself. </i><b><i>And, when I learned that he had sent a letter to Pensée stating that, &#39;None of us in the scientific community believes that a debate about Velikovsky&#39;s views of the solar system would be remotely justified at a serious scientific meeting,&#39; it was clear that the original raison d&#39;etre of the symposium had been replaced by the opportunistic chance to discredit Velikovsky once-and-for-all. If this wasn&#39;t to have been a scientific debate, one was led to wonder why it was being held in the first place.&quot;</i></b> (from &quot;Velikovsky: A Personal Chronological Perspective of His Final Years&quot; by Frederic B. Jueneman)<br />- <b><i>&quot;So it was that none of the participants, including the friendly Michelson, addressed themselves to the specifics of Velikovsky&#39;s ideas, but posed additional questions for which they themselves had no answers</i></b><i> ... Further talks with other newsmen that afternoon confirmed my estimates that the spectacular displays of dialectics on the part of several speakers would overshadow the more sedate, scholarly commentaries, and even these latter would be misinterpreted or ignored altogether. Sagan&#39;s &quot;ten plagues&quot; as a rejoinder to Velikovsky, in spite of inherent weaknesses, were considered inviolable truths and a final refutation to Velikovsky&#39;s arguments.&quot;</i> (from &quot;Velikovsky: A Personal Chronological Perspective of His Final Years&quot; by Frederic B. Jueneman)<br /><br /><b>1975:</b> Vera Rubin announces at a 1975 meeting of the American Astronomical Society the discovery that most stars in spiral galaxies orbit at roughly the same speed<br /><br />- Initially met with skepticism, her results have come to be known as the dark matter problem.<br />- The Big Bang cosmology today requires that around 27% of the mass of the universe must be dark matter.<br /><br /><b>1976:</b> <i>The Age of Velikovsky</i>, by CJ Ransom<br /><br />- <i>&quot;The Age of Velikovsky is written by C.J. Ransom, a PhD in Plasma Physics who conducted research for General Dynamics&#39; Convair Aerospace Division. It is important reading for anyone interested in the Velikovsky affair. Ransom gives an overview of Velikovsky&#39;s early life, then Chapter 1 begins with the events surrounding the publication of Velikovsky&#39;s first book in 1950: </i><b><i>&#39;The reaction to Worlds in Collision by many members of the scientific community can provide research material to interested psychoanalysts for decades</i></b><i> ... unscientific and unethical actions did take place, and no amount of rationalization can justify these actions ... Many of the actions were personal asaults on Velikovsky, or others who happened to be in the line of fire. These actions were inexcusable even if Velikovsky were wrong ...&#39; Ransom is given to bursts of humor, as can be seen in the following: &quot;*Before the book was published, review articles ... appeared [which] did not accurately portray the conclusions or the scholarship which led to these conclusions. Unfortunately ... [s]ome writers never came closer to the original than a review article, and they wrote articles refuting what others thought Velikovsky might have said.* Oddly enough, some of these same people claimed that Velikovsky did not use proper sources.&#39; (emphasis in original). So much pressure was exerted on the publisher by the scientific community that the book was transferred to another publisher, though it had remained the best-seller almost from the first day of its release. This attempt at supression of Velikovsky&#39;s work can be clearly seen as one of the darkest and shabbiest actions in the history of modern science. The author devotes almost 200 pages to showing many different aspects of Velikovsky&#39;s discoveries, with evidence that indicates their accuracy. There is the correction of history by over 500 years and the inconsistencies that are eliminated by the revised chronology. There are numerous characteristics of the sun, planets, and moons, realization of which were surprises to scientists, but had either been explicitly predicted by Velikovsky or fit his theory better than they did into existing scientific dogma. Then we read about the AAAS convention of 1974: &#39;[it] was advertised as a scientific appraisal of Velikovsky&#39;s theory. But, at the opening of the meeting, the attendees were informed that the subject was not worthy of scientific discussion and the meeting was being held to point this out to any minds which had strayed from the uniformitarian fath. Several of the participants certainly lived up to the claim that they would not discuss the subject scientifically.&#39; Paramount among these was Carl Sagan. </i><b><i>&#39;His paper contained nothing which furthered scientific debate. However, his paper was presented exceptionally well, and his charisma added to the effectiveness of the presentation. Most of the audience did not know and, because of his captivating delivery, did not care that many of his points were irrelevant, incorrect, or misleading. His entrancingly arrogant delivery exuded the air of a great evangelist who had come to lead the people along the true uniformitarian path ... Perhaps Sagan&#39;s most quoted statement from the sumposium was this: &#39;My conclusion will be that where V is original, he is very likely wrong; and that where he is right, the idea has been pre-empted by earlier workers.&#39; Whether this lie was original with Sagan or was fabricated by an earlier worker, it is flatly untrue.&#39;</i></b><i> In Ransom&#39;s conclusion he says, &#39;We have seen that a number of irrational acts have occurred in the Velikovsky afffair, and that there are divergent reasons for these actions ... However, it is time to look ahead. We could argue forever over what Velikovsky did or did not mean, what he did or did not predict, and miss the total concept he presented. Enough information now exists to show that his ideas are worthy of continued study. Whether he as an individual is right or wrong on some point is irrelevant. </i><b><i>Velikovsky&#39;s work now belongs to the world, and the world will lose by continuing to ignore it.&#39;</i></b><i> Wonderful reading if you can find the book!&quot;</i><br /><br /><b>1977:</b> &quot;Frequency of Nearby Supernovae and Climatic and Biological Catastrophes&quot;, by Clark, DH, McCrea, WY, and Stephenson, FR, <i>Nature</i> 265, p318-19\n\n- Claimed that a supernova may have influenced the recent history of Earth, including the aurora.<br />- This mechanism could have caused catastrophic events and could have created the enhanced auroras.<br />- It&#39;s one way in which stories from ancient cultures can be fitted with physical events.<br /><br /><b>1977:</b> <i>Immanuel Velikovskys Jewish Science</i><br /><br />- An unpublished Velikovsky biography<br /><br /><b>1978:</b> <i>Velikovsky and His Critics</i><br /><br />- <i>&quot;This little book is a study of a scientific controversy. The particular science central to this controversy is history, for it is a debate about the past of our planet and that solar system of which it is a part, about events observed and recorded by human societies. But, even more than the most complex historical topics, the issues raised by Velikovsky have crucial significance for the widest range of particular sciences, and it is appropriate that they have become the matter of broad interdisciplinary discussion. The participants in the phase of this debate here considered are qualified scientific workers in the fields of history, medicine, astronomy, sociology, philosophy, statistics, chemistry, engineering, and physics, and the author of this book is an economist.&quot;</i> (from the text)<br /><br /><b>1979:</b> Immanuel Velikovsky dies Nov 17<br /><br />- Various pro-Velikovsky advocates (Frederic Juneman, Alfred De Grazia, David Talbott) fragment into related-but-distinct mythological interpretations.<br /><br /><b>1981:</b> <i>Velikovsky&#39;s Sources</i>, by Bob Forrest (Six Volumes), collection goes by the name <i>A Guide to Velikovsky&#39;s Sources</i><br /><br />- <i>&quot;I first read Worlds in Collision whilst at university in the late 1960s, but actually my real interest in Velikovsky came about when I teamed up with Patrick Moore to do a follow-up to his book about Independent Thinkers, Can You Speak Venusian? A chapter of that book was to be devoted to &#39;Catastrophists in Collision&#39;, dealing with Velikovsky, Beaumont, Donnelly and others. In preparation for that, I started delving into Velikovsky&#39;s use of sources, the results of which subsequently far outstripped what was needed for the book with Patrick. The project took on a life of its own and I came to write Velikovsky&#39;s Sources as a result. I&#39;d known Donald Cyr (the editor of Stonehenge Viewpoint) for a few years before that, and it was he that suggested that I did a series of articles, based on Velikovsky&#39;s Sources, for his journal. Eventually those articles were gathered together in book form and published by him as A Guide to Velikovsky&#39;s Sources&quot;.</i> (private communication by email, from Bob Forrest to a third party, March 12, 2009)<br />- <i>&quot;After that I rather retired from the Velikovsky scene ... the reason was simply that having done my book(s), I had said everything that I wanted to say. If people thereafter disagreed with my findings and chose to continue to believe in Velikovsky, then my attitude was, &#39;Fine - that&#39;s your choice, but to my mind you are sailing in the face of plain common sense, and you are in the same boat as people who continue to believe that Bacon wrote Shakespeare, despite all reasonable refutation.&#39; In brief, I have continued to hold the view that Worlds in Collision is a crank book.&quot;</i> (private communication by email, from Bob Forrest to a third party, March 12, 2009)<br /><br /><b>Early 80&#39;s:</b> Government plasma researcher, Anthony Peratt, claims to solve the dark matter problem<br /><br />- He demonstrates through simulation that the galactic rotation curve problem can be resolved if galaxies are simulated as interactions of electromagnetic plasma filaments (<a href=\"https://www.plasma-universe.com/Galaxy_formation\" class=\"ot-anchor\">https://www.plasma-universe.com/Galaxy_formation</a>).<br />- No dark matter is required.<br /><br /><b>Early 80&#39;s:</b> Venus Pioneer mission returns back data confirming that Venus&#39; heat is coming from it surface<br /><br />- The scientific community latched onto Carl Sagan&#39;s Super Greenhouse Theory in order to explain Venus&#39; unexpectedly high temperature.<br />- A mission was sent to Venus -- the Venus Pioneer mission -- in large part to establish some evidence for a greenhouse effect on Venus.<br />- <b>That mission, much to their dismay, again confirmed Velikovsky&#39;s thesis, in that all of the probes reported back that Venus&#39; heat was originating from the planet&#39;s surface.  Such an observation ruled out a greenhouse effect (which requires thermal equilibrium, heat in = heat out).  The observations seemed to suggest that the planet was cooling down from some recent significant event.</b><br />- <b>Venus Pioneer engineers were then tasked with demonstrating how all of these instruments could be in error. Eventually, after significant effort, they came up with possible explanations. The data for that mission was &quot;corrected&quot; to reflect Sagan&#39;s Super Greenhouse Theory.</b><br />- These papers were published in the early 80&#39;s, long after the brouha over Velikovsky had largely died down. <b>Very few advocates of terrestrial climate change today are aware that this dataset was corrected to reflect the greenhouse theory.</b><br /><br /><b>1980:</b> <i>The Saturn Myth</i>, by David Talbott<br /><br />- <b>Attempts a &quot;natural history of mythology&quot; by tracing the mythical patterns found around the globe to a shared experience of planetary catastrophe; diffusion is not, according to these scholars, the primary source of the patterns of mythology.</b><br />- Comparative mythologist David Talbott proposes that ancient myths and traditions describe the planet Saturn as the the dominant celestial body in the sky, appearing <i>&quot;fixed at the north celestial pole&quot;</i>.  This interpretation requires the existence of misunderstandings in textbook planetary, stellar and cosmological theories (but it would be many years of research before David Talbott would settle on a physical explanation).<br />- Dave Talbott proposes, alongside other mythologists like Dwardu Cardona and Ev Cochrane, that the solar system of former times beared little resemblance to its current state, and that the transition was marked by a global catastrophe which humans barely survived.<br />- <b>They claim that, taken at face value, the odd similarities in the oldest mythological archetypes can be well explained by a quasi-stable colinear arrangement (a &quot;conjunction&quot;) -- as if the Earth, Mars, Venus and Saturn are on a common sausage-like trajectory, and all share a cental column.  In this configuration, the planets were far closer to one another, and thus more visible in the Pagan sky -- which is what permitted the Pagans to tell detailed stories about them.</b><br />- <b>Critics have claimed for many years that this arrangement of planets is physically impossible.  Yet, this scenario is astronomically recognized as a Herbig-Haro object</b> -- which astronomers have recognized as a distinct type of emission nebula as far back as the 1940&#39;s.  Crucially, Herbig-Haro are recognized as <i>&quot;ubiquitous in star-forming regions ... HH objects are a transient phenomena that last less than a few thousand years.&quot;</i> The Milky Way is currently estimated to harbor 150,000 of them. (<a href=\"https://en.wikipedia.org/wiki/Herbig%E2%80%93Haro_object\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Herbig%E2%80%93Haro_object</a>).  As strange as the configuration may sound to us, it is a logical configuration within the context of a story about human origins, as well as catastrophe (due to its observable lack of stability).<br />- These peculiar claims exhibit surprisingly broad explanatory capabilities for the mythological archetypes.  The explanation spans numerous books besides <i>The Saturn Myth</i>, including <i>God Star</i>, <i>Flare Star</i>, <i>Metamorphic Star</i>, <i>Primordial Star</i>, <i>The Many Faces of Venus</i>, <i>On Fossil Gods and Forgotten Worlds</i> and <i>Martian Metamophosis</i>. <b>It may very well turn out that the scenario proposed sounds odd to us simply because we are still in a state of gaining a &quot;common sense&quot; for the universe and the origin of its life.  We&#39;d be wise to avoid judging ideas on the basis of how they compare to our own current solar system, which we know today to be uncommon.</b><br />- Ev Cochrane: An advantage of this approach is that <i>&quot;it is complemented at every point by ancient art.  Thus it is that ancient traditions of &#39;lost&#39; suns are complemented by prehistoric pictographs depicting a wheel-like &#39;sun&#39; bearing little resemblance to the current sun.  Ancient traditions of Inanna / Venus standing in the &#39;heart&#39; of heaven together with Shamash and Sin are complemented by pictures showing Venus as an eight-pointed star set upon the disc of Shamash while enclosed within the crescent of Sin -- this in striking contradiction to current astronomical reality.  Even the ancient traditions describing an ascent to heaven along a luminous extraterrestrial ladder are complemented by prehistoric pictures of &#39;suns&#39; with ladder-like appendages ... </i><b><i>The truth is that the pictorial evidence commemorating a catastrophic solar system is so abundant and compelling that it is impossible to do it justice in a single volume.&quot;</i></b> (<i>The Many Faces of Venus</i>, p191)<br /><br /><b>1980:</b> Carl Sagan expresses regrets about the scientific community&#39;s treatment of Velikovsky in the <i>Cosmos</i> television series<br /><br />- Carl Sagan: <i>&quot;The worst aspect of the Velikovsky affair is not that many of his ideas were wrong or silly or in gross contradiction to the facts. Rather, </i><b><i>the worst aspect is that some scientists attempted to suppress Velikovsky&#39;s ideas. The suppression of uncomfortable ideas may be common in religion or in politics, but it is not the path to knowledge. And there&#39;s no place for it in the endeavor of science. We do not know beforehand where fundamental insights will arise from about our mysterious and lovely solar system. And the history of the study of our solar system shows clearly that accepted and conventional ideas are often wrong -- and that fundamental insights can arise from the most unexpected sources.&quot;</i></b><br />- Nobody notices.<br /><br /><b>1985:</b> <i>The Mystery of Comets</i>, by Fred Whipple<br /><br />- <i>&quot;The fortunate find in the seventies, at Mawangdui, China, of a Han dynasty silk comet atlas [see image halfway down page at </i><i><a href=\"http://abob.libs.uga.edu/bobk/bronze.html\" class=\"ot-anchor\">http://abob.libs.uga.edu/bobk/bronze.html</a></i><i>] sheds considerable light on earlier enigmatic motifs. Most illuminating is the drawing, described by text on the artifact as a long-tailed pheasant star. This rendering of a jetting comet viewed down its axis of rotation has a considerable history, and, as a motif, appears on artifacts found in most areas of the world. The artist who illustrated this silk twenty-two hundred, or so, years ago was not likely a first-hand observer. What is produced here is a schematic of received comet caricatures with claims that specific things will happen if a represented type appears. The pinwheel-like image is unique to the compilation in that an omen is given for an appearance in each of the four seasons, implying that this comet was seen more often than the others represented. This may illustrate a frequently viewed aspect of comet Encke which has a 3.3 year orbit and rotational axis that occasionally points toward Earth.&quot;</i><br />- See the Han dynasty &quot;silk comet atlas&quot; midway down the page at <a href=\"http://abob.libs.uga.edu/bobk/bronze.html\" class=\"ot-anchor\">http://abob.libs.uga.edu/bobk/bronze.html</a>, where <b>the Swastika is very obviously included in a collection of drawings of comets</b>.<br /><br /><b>1990:</b> <i>Carl Sagan and Immanuel Velikovksy</i>, by Charles Ginenthal<br /><br />- <i>&quot;Carl Sagan and Immanuel Velikovsky is an analysis of a scientific debate respecting the theory of Immanuel Velikovsky held at a symposium under the auspices of the American Association for the Advancement of Science [AAAS] in San Francisco in 1974. One of the participants in the debate, Professor Carl Sagan of Cornell University, raised, there and elsewhere, a host of questions and points of severe criticism of Velikovsky&#39;s thesis. Charles Ginenthal, the author of this work, has gone into all aspects of this criticism and has exhaustively outlined the evidence with data from the scientific literature. The analysis is devastating: the definitive response to the Sagan-Velikovsky debacle. It debunks in lucid terms, point by point, each and every issue raised. It is sometimes humorous and often scathingly critical. No one who reads this material will have the slightest doubt about the nature of this further chapter in the Velikovsky Affair. Carl Sagan and Immanuel Velikovsky will be hailed by some and decried by others. It reopens this episode of the Velikovsky debate with a thunderous roar. It is must reading for anyone interested in how scientific debates of revolutionary concepts are actually conducted, how evidence is handled, how ethics are trampled.&quot;</i> (back cover of text)<br /><br /><b>1992:</b> <i>Ka: A Handbook of Mythology</i>, by Hugh Crosthwaite<br /><br />- <i>&quot;Hugh Crosthwaite has gathered 45 years of classical study into a uniquely valuable compendium. He connects the rites and language of our forebears by a collection of common roots and usages, shows that they relate in a great many cases to electrical phenomena and tricks and that they all have to do with the gods, in heaven and descending upon the earth.&quot;</i> (SIS Chronology and Catastrophism Workshop 1993 No 1, Aug 1993)<br /><br /><b>1993:</b> <i>Catastrophism, Neocatastrophism and Evolution</i>, by Trevor Palmer<br /><br />- Neocatastrophism: <i>&quot;Neo-catastrophism is (1) a geological term, that refers to the doctrine that the gradual processes we see on Earth, have been supplemented by huge natural catastrophes, and also (2) a palaeontological term referring to faunal discontinuities in the fossil record.&quot;</i><br />- Anthony Hallam writes: <i>&quot;In the latter part of the twentieth century a new school of thought emerged in geology that has been dubbed &#39;neo-catastrophism&#39; as opposed to the gradualism of Lyell and Darwin. Its adherents embrace a punctuated view of geological and biological history.&quot;</i><br />- <i>&quot;Catastrophism, Neocatastrophism and Evolution (Cover illustration, 14K) tells how prevailing views of patterns and processes in the evolution of life on Earth have changed in a significant fashion over the past few years. In 1959, the centenary of the publication of Charles Darwin&#39;s On the Origin of Species by means of Natural Selection, the Modern Synthesis of neo-Darwinism which incorporated developments in genetics into traditional Darwinism, seemed completely secure. Together with the contemporary geological paradigm, the Modern Synthesis was widely seen as representing the triumph of a gradualistic-uniformitarian view of Earth history over the catastrophist alternative. Evolutionary change was slow (an essential feature of gradualism), imperceptible (except over long periods of time) and progressive (though not because of linear development, but as a result of competition between variant forms). Later, however, it became increasingly clear that the course of evolution had been much less even paced and much more erratic than previously realised, the fossil record revealing episodic rapid bursts and abrupt transitions which could no longer (as formerly) be dismissed as artifacts. Moreover, from 1980 onwards neocatastrophism made remarkable advances so that, today, impacts of asteroids and comets have to be regarded as perfectly plausible agents of evolutionary change, and evidence for such an extraterrestrial involvement at any particular time considered on its merits.&quot;</i> (<a href=\"http://www.sis-group.org.uk/catneo.htm\" class=\"ot-anchor\">http://www.sis-group.org.uk/catneo.htm</a>)<br />- <i>&quot;Something of a revolution in academic thought has taken place, but as with other paradigm shifts in the scientific world, there are those who deny that it has happened. As Archie Roy, Professor of Astronomy at Glasgow University, said at an SIS meeting in London in 1985, it is not unknown for the response to a new idea to progress from &#39;This man is nuts&#39;, through &#39;We&#39;ll really have to look at the problem just to dismiss it&#39;, to &#39;Of course I&#39;ve always known that this was the case&#39;. With the subject of evolutionary biology, which is our particular concern, the imprecise and subjective nature of some of the terminology makes it possible for those who wish to do so to maintain that the essentials of the gradualistic viewpoint remain intact. For example, a significant evolutionary change taking place over several thousand years (rather than several hundred thousand years) might be cited by some as an example of a rapid transition, whilst others would have no difficulty in accepting it as falling within the range that could be regarded as gradualistic. Hence, one of the aims of the present work is to go beyond assertions and give the full flavour of views about contemporary and historical evolutionary theory which have received wide circulation in books and general science journals such as Nature, Science, Scientific American and New Scientist. The change in tone of such writings over the stars is very apparent, leaving little scope for misrepresentation. Nevertheless, in order to convey opinions and arguments with maximum accuracy, extensive quotations are given. In the light of these, it should be evident that a major change of perspective, if not a revolution, has occurred over the past decade or so.&quot;</i> (<a href=\"http://www.sis-group.org.uk/catneo.htm\" class=\"ot-anchor\">http://www.sis-group.org.uk/catneo.htm</a>)<br /><br /><b>1993:</b> <i>Scientific Prehistory: A Sequel of Prehistory and Earth Models</i>, by Melvin A Cook<br /><br />- <i>&quot;It must be said at the outset that this book is not an easy read, in more ways than one. As with Prehistory and Earth Models, many sections are so densely packed with facts and ideas that they need to be read carefully more than once to be fully understood. Unfortunately, Cook&#39;s eyesight was fading badly when the book was being written, so there are more printing errors than normal to be negotiated. Then there is also the matter of the author&#39;s unusual mix of strongly-held beliefs (he is a Mormon and a convinced creationist), yet he also has a formidable scientific background, based largely on a lifetime&#39;s work on high explosives, which has brought him into contact with not only the world of rocks and mines where they are used but also the frontiers of chemistry and plasma physics. (He is particularly proud of his Nitro-Nobel Gold Medallion, awarded in 1968 for work on slurry explosives.) Add to this an obvious love of a good argument and an unwillingness to suffer those he sees as fools gladly and no one is guaranteed a smooth ride. Atheists like me will not be happy with his unashamed creationism, yet I suspect that few in the creationist camp will be totally comfortable either as he argues for his own particular view (and timetable) of creation and pursues ideas about fundamental cosmology with scientific gusto.&quot;</i> (summary from &quot;SIS Chronology &amp; Catastrophism Review&quot; 1994, Vol XVI, Oct 1995)<br />- The contradictions are nicely caught in a quotation given from J. Keith Rigby of the Brigham Young University of Utah concerning a talk Cook gave to a Chemistry seminar there in 1962 (p. 110): <i>&quot;We did attend, I suppose largely because of the reputation Cook has as an uninhibited thinker. I had heard many of his ideas second-hand and wanted to hear them without the usual colouring of others. I was impressed with his attention to mathematical and physical detail but appalled at his abysmal ignorance of geology and his avowed intent to discredit observations of geology. His was a stimulating presentation.... I had anticipated religious overtones in his paper but there were none, fortunately, even though the person inviting him down is a professed &#39;short history&#39; chemist. Effects of his paper difficult to evaluate, for those who are in agreement with him are more deeply convinced his thinking is correct and those who were opposed are still opposed, only more intently ...&quot;</i> (quote is from &quot;SIS Chronology &amp; Catastrophism Review&quot; 1994, Vol XVI, Oct 1995)<br /><br /><b>1993:</b> &quot;Written in the Stars: Celestial Origin of Maya Creation Myth&quot;, R. Wertime &amp; A. Schuster, <i>Archaeology</i> (July/August 1993), p32<br /><br />- Arguably the most important Mayan scholar.<br />- <i>&quot;It seems that the interaction of astronomy and mythology was common in other cultures as well [as it was among the Maya].  Scholars working in South America have found similar kinds of systems in the Amazon ... The Maya may have been using a way of thinking about the sky and using it in their mythology that was very ancient indeed.  I&#39;m even prepared to accept that much of the cosmology/mythology came straight across the Bering Strait, and that it may be 10,000 or 15,000 years old; it may be 20,000 years old.  I think it may be possible that we have tapped into a very ancient stratum of human thought.  If it did come across with the first Americans, then we may be in touch with one of the two or three great human intellectual traditions that we as a species have ever evolved, part of the fundamental &#39;software&#39; that all of the peoples of the Americas and Asia have utilized.&quot;</i><br />- What is remarkable is that David Talbott&#39;s Saturnian theory can easily explain the Mayan creation story known as the Popul Vuh.<br /><br /><b>1994:</b> Comet Shoemaker-Levy impacts into Jupiter<br /><br />- The scientific community wakes up to the real possibility of catastrophic events over human historical timespans (Velikovsky&#39;s main thesis).<br /><br /><b>1995:</b> &quot;Recent Results in Cometary Astronomy: Implications for the Ancient Sky&quot;, <i>Vistas in Astronomy</i> Vol 39, Pergamon Press: p647-671\n\n- <i>&quot;The extreme preoccupation of most early societies with celestial imagery ... appears to be part of a world wide phenomenon.&quot;</i><br /><br /><b>1996:</b> <i>Stephen J. Gould and Immanuel Velikovsky: Essays in the Continuing Velikovsky Affair</i>, by Charles Ginenthal<br /><br />- Several essays commenting on some of the people who have criticised Velikovsky. It includes contributions by Irving Wolfe, Lynn E. Rose, Dwardu Cardona, David Talbott and Ev Cochrane.<br />- Covers 4 topics:<br /> <i>&quot;1. Velikovsky&#39;s highly revolutionary, catastrophic theories,</i><br /> <i>2. the Velikovsky Affair, which is the sordid and vicious response to those ideas by mainstream, American science,</i><br /> <i>3. the impressive amount of evidence, especially from the space probes, which has accumulated in mainstream science since 1950 in support of Velikovsky&#39;s theories and predictions, and</i><br /> <i>4. catastrophist science, or research by Velikovsky&#39;s supporters in a number of fields, published in hundreds of scholarly articles, which constitutes a parallel universe to mainstream, traditional, uniformitarian science.&quot;</i> (Amazon.com description)<br /><br /><b>1997:</b> <i>A Fire Not Blown: Investigations of Sacral Electrical Roots in Ancient Languages of the Mediterranean Region</i>, by Hugh Crosthwaite<br /><br />- <i>&quot;In this work I have tried to develop some of the ideas that I put forward in my previous book Ka. The chief aim has been to apply my first work’s electrical interpretation of ancient myths and cosmology to a particular area of the ancient Mediterranean world, then to quote further examples of religious practice and the relevant vocabulary from a wider area. There has inevitably been repetitions of examples and interpretations from my earlier work ... In my first book I gave about twenty cases of reversals of direction of writing, suggesting that something more than coincidence was involved. The present work contains more than eighty examples for consideration, and there are more possibilities which may justify mention at a later stage.&quot;</i> (from the preface to the text)<br /><br /><b>1997:</b> <i>Larousse Encyclopedia of Mythology</i> (Anonymous)<br /><br />- <i>&quot;Are all these legends a confused account of great events on a planetary scale which were beheld in terror simultaneously by men scattered everywhere over the world?&quot;</i><br /><br /><b>1997 - present:</b> The Electric Universe electrical cosmology is announced<br /><br />- This is an electrical cosmology which extends plasma cosmology (much of it published in IEEE&#39;s <i>Transactions on Plasma Science</i>), and which offers a potential explanation for how the Saturn myth might have come to be.<br />- Inspired by Immanuel Velikovsky&#39;s own work, and marked by the collaboration of David Talbott (comparative mythologist, <i>The Saturn Myth</i>), Wal Thornhill (maverick plasma physicist) and Don Scott (electrical engineer).<br />-  Wal Thornhill has substantially elaborated Immanuel Velikovsky&#39;s earlier belief in the importance of electromagnetic interactions in astronomy through claims about plasmas, the universe&#39;s dominant state of matter (EM effects appear more pronounced at interstellar scales, and up).<br />- Extends the analysis of mythology into the realm of petroglyphs by incorporating modern (oftentimes government-run) high-intensity plasma physics laboratory observations.<br />- <b>The cosmology and the mythology benefit from one another&#39;s existence, but can be learned independent of one another.</b><br />- <b>The idea has in recent years developed a substantial following, but is so controversial that Wikipedia has been scrubbed of most references.</b><br /><br /><b>1997:</b> <i>Rain of Iron and Ice</i>, by John S. Lewis<br /><br />- <i>&quot;Our understanding of astronomy, geology, and biology is illuminated by this new insight: we see Earth&#39;s surface not in quasi-mystical terms as a uniquely sheltered refuge for life, but as a part of the fabric of the solar system, subject like other bodies to rare, cataclysmic change.&quot;</i><br /><br /><b>1981:</b> Results are presented at a geophysical conference that are suggestive of a massive impact crater north of the Yucatan peninsula measuring 190 miles wide<br /><br />- In due time, it would become thought of as the impact that led to the demise of the dinosaurs.<br />- As the notion of global catastrophe (one of Velikovsky&#39;s core theses) has gone mainstream, the core assumption of uniformity (his other assertion) which props up most of the geological and planetary sciences has never been thoughtfully questioned.  This premise that we can use our observations of today to understand what happened in the past has acted as the basis for centuries of inferences in these fields.  The direct observation of evidence for catastrophes should, in principle, lend some credence to the notion of capricious planetary gods, and it should inspire questions about many of the most popular planetary and geological theories of our day (which only look to gradual, rather than sudden, processes).  The scientific community continues to refuse to question their assumption of uniformity.<br /><br /><b>2000:</b> <i>Marking Time: The Epic Quest to Invent the Perfect Calendar</i>, by Duncan Steel<br /><br />- He noticed that geographically disconnected people have similar start times for the Earth (Often four to six thousand years before the Common Era).<br />- <b>This &quot;start time&quot; should not necessarily be taken literally; it also makes sense if prehistoric peoples on all of the various continents witnessed and experienced a major event.</b><br />- Dr. Steel created a model for multiple events he believes caused the start times in ancient cultures.  He&#39;s not the only investigator who believes in multiple events.<br />- He said the ancient sky was the location of a continuing phenomenon, which persisted for some centuries, and that all ancient cultures witnessed basically the same activity in the sky.<br />- Dr. Steel and his co-workers indicated that a large comet, which arrived sometime within the last ten thousand years, caused multiple events.<br />- He proposed that a comet broke into a number of smaller comets whose orbits intersected the orbit of the Earth at different times and caused different start times of events for the various groups of ancient people.<br />- The ancients then became interested in calendars in order to predict when the next major event would occur.<br /><br /><b>1982-2003:</b> Dr. Victor Clube, Oxford University Department of Astrophysics<br /><br />- Clube and colleages such as Dr. Bill Napier said the last ice age and the current interglacial age represent a microcosm of Earth history, dominated by a single giant comet.<br />- Says the comet caused catastrophic conditions to occur repeatedly during the course of geological and biological evolution.<br />- Also suggested punctuated equilibrium applies to recent sociological evolution as well as biological and geological evolution (punctuated equilibrium proposes that once species appear in the fossil record they will become stable, showing little net evolutionary change for most of their geological history, but then occasionally punctuated by rapid changes)<br /><br /><b>1982-1990:</b> Dr. Bill Napier, Armagh Observatory<br /><br />- Said recent terrestrial research has provided evidence to support theories of close encounters with comets.<br />- Analysis of tree rings in Irish Oaks depicted a dramatic climatic downturn circa 2354-2345 BCE.<br />- Suggested a cause - Earth entering the path of a comet&#39;s meteor stream.<br />- <i>&quot;Blast from the sky&quot;</i> was blamed by ancients for a cataclysmic environmental event accompanied by destruction of mud-brick buildings in Northern Syria about the time of the Irish event (Napier, WM, <i>Ancient Myths &amp; Tree Rings Point to Giant Comet&#39;s Visit to Earth</i>, 1998).<br />- He said that this also pointed to an encounter with a giant comet.<br /><br />Notes on Napier and Clube:<br />- After the sky became stable, the ancients still had the myths but with no gods left in the sky.<br />- Therefore, according to Clube and Napier, the ancients transferred the comet characteristics to the visible but dull planets.<br />- They said, <b><i>&quot;The planets were too few in number and too simple in behavior to carry the rich complexity of myths; but they had become the only candidates available.&quot;</i></b> (<i>The Cosmic Winter</i>, Clube, Victor, and Napier, Bill, 1990, p88)<br /><br /><b>2003:</b> Government plasma researcher, Anthony Peratt investigates an electrical plasma physics origin for petroglyphs<br /><br />- Peratt noted the similarity of fundamental, laboratory-observed plasma forms to an unusually large portion of petroglyphs.<br />- This was merely an image-to-image comparison, and did not involve an analysis of the myths.<br />- The research originated as a purely scientific investigation of high energy z-pinch plasmas (extreme electrical discharges over gas in a vacuum).<br />- Peratt and others have postulated models involving significant plasma influences that created aurora forms which they claim contributed to the myth-making period.<br />- 40% of all petroglyph types can be accounted for<br />- <i>&quot;Eighty-four distinct high-energy-density Z-pinch categories have been identified in petroglyphs, nearly all of which belong to the archaic class.  Only a small percentage of these petroglyphs, or parts of petroglyph patterns, do not fall into any of these categories.&quot;</i> (&quot;Characteristics for the Occurrence of a High-Current, Z-Pinch Aurora as Recorded in Antiquity - Part I&quot;)<br /><br /><b>2004:</b> <i>Introduction to Comets</i>, by John C. Brandt and Robert D. Chapman<br /><br />- Earth-comet collisions are now accepted as a possibility by mainstream science<br /><br /><b>2006:</b> <i>The Cycle of Cosmic Catastrophes: How a Stone-Aged Comet Changed the Course of World Culture</i>, by Richard Firestone, Allen West and Simon Warwick-Smith<br /><br />- <b>Firestone and his group uncover 8 instances of mammoth tusks peppered with small meteorites on one side, and a mammoth carcass partially covered with a black radioactive mat filled with exotic isotopes; the find is significant because the mammoths are thought to have gone extinct in human historical times (and many theorists continue to blame humans for their extinction).</b> The dates for these artifacts, however, suggest a much older age for the event of around 35,000 years ago.<br />- Firestone&#39;s model started essentially with a supernova visible about 41,000 years ago, which they assert created a shockwave 34,000 years ago that increased comet and asteroid impacts throughout the solar system.<br />- About 16,000 years ago, they assert a second shock wave arrived, accompanied by increased radiation and comet and asteroid collisions.<br />- Firestone and about 26 others also published evidence for an extraterrestrial impact 12,900 years ago that contributed to the megafaunal extinctions and the Younger Dryas (YD) cooling. They asserted numerous impacts of material hit the Northern Hemisphere and debris also impacted the Sun, Moon and other planets.<br />- In the spirit of Peratt&#39;s claims, they also asserted that such impacts could create a series of solar flares, which can in turn influence the aurora activity (and possibly explain mythological stories).<br />- <i>&quot;When we did [&#39;look beyond pure science&#39;], we found remarkable and widespread corroboration for our theory in the myths and legends from more than fifty diverse ancient cultures around the world.&quot;</i> (p129)<br />- Firestone&#39;s discussion of &quot;c14 plateaus&quot; observed in the dating record indicate that c14 dates cannot be relied upon for dating artifacts related to impact events (but it seems that these arguments have not been well received).<br />- <i>&quot;We do not believe or suggest that just the supernova or just the impacts caused the extinction of the megafauna -- it was more complicated than that ... &#39;Something&#39; happened long ago, although many details may be lost.&quot;</i> (p150)<br />- Critics objected that <i>&quot;there was no mechanism to hold such a dense swarm of impactors together in space. To the suggestion that a large comet had broken up just before hitting Earth, they replied that this lacked a physical mechanism ... While it is true that some comets have been seen to spontaneously disintegrate in space, the chances of this happening just before an impact with Earth is negligible -— something that might have happened at most once in the past four billion years. There was apparently no way to get a swarm of impactors to target North America alone.&quot;</i> (<a href=\"http://www.csicop.org/si/show/did_a_cosmic_impact_kill_the_mammoths\" class=\"ot-anchor\">http://www.csicop.org/si/show/did_a_cosmic_impact_kill_the_mammoths</a>).<br /><br /><b>2008:</b> Astronomer Geoffrey W. Marcy and others ...<br /><br />- ... note that there may be many different physical processes that lead to the formation of planets.<br />- <i>&quot;Look around our solar system and you could be forgiven for thinking its eight planets drifted in from completely different parts of the cosmos.&quot;</i> (Clark, Stuart, &quot;Unknown Earth: Our Planet&#39;s Seven Biggest Mysteries&quot;, <i>New Scientist</i>, September 27, 2008, p28)<br /><br /><b>2011:</b> &quot;Lone Planetary-Mass Object Found in Family of Stars&quot; at <a href=\"http://www.jpl.nasa.gov/wise/newsfeatures.cfm?release=2016-107\" class=\"ot-anchor\">http://www.jpl.nasa.gov/wise/newsfeatures.cfm?release=2016-107</a><br /><br /><i>&quot;In 2011, astronomers announced that our galaxy is likely teeming with free-floating planets. In fact, these lonely worlds, which sit quietly in the darkness of space without any companion planets or even a host sun, might outnumber stars in our Milky Way galaxy.&quot;</i><br /><br /><b>2011:</b> Marinus Anthony van der Sluijs publishes a 4-volume review of the mythological archetypes which seeks to separate out all interpretation<br /><br />- See <a href=\"http://mythopedia.info/books.html\" class=\"ot-anchor\">http://mythopedia.info/books.html</a><br />- Rens&#39; approach to mythology seeks to align with Anthony Peratt&#39;s in that it focuses on an enhanced polar auroral column as a possible cause for the mythological archetypes.<br /><br /><b>2014:</b> &quot;Dark Matter as a Trigger for Periodic Comet Impacts&quot;, Randall &amp; Reece, <i>Physics Review Letters</i>, 112.<br /><br />- <i>&quot;Our proposal is that during the Sun&#39;s passage through the mid-plane of the Milky Way -- the stripe of stars and bright dust that you can observe in a clear night sky -- the Solar System encountered a disk of dark matter that dislodged the distant object, thereby precipitating this cataclysmic impact&quot;</i> that killed the dinosaurs (Randall, Lisa, <i>Dark Matter and the Dinosaurs</i>, 2015).<br /><br /><b>2014:</b> The number of exoplanet discoveries dramatically increases to 800 in just 2014<br /><br />- Nature Magazine: <b><i>&quot;The discovery of thousands of star systems wildly different from our own has demolished ideas about how planets form. Astronomers are searching for a whole new theory ...&quot;</i></b><br />- <b><i>&quot;... Following the launch of NASA&#39;s planet-finding Kepler satellite in 2009, the number of possible exoplanets quickly multiplied into the thousands — enough to give astronomers their first meaningful statistics on other planetary systems, and to undermine the standard theory for good. Not only were there lots of exoplanet systems bearing no resemblance to ours, but the most commonly observed type of planet — a &#39;super-Earth&#39; that falls between the sizes of our world and Neptune, which is four times bigger — does not even exist in our Solar System. Using our planetary family as a model, says astronomer Gregory Laughlin of the University of California, Santa Cruz, &#39;has led to no success in extrapolating what&#39;s out there.&#39; ...&quot;</i></b><br />- <b><i>&quot;... The findings have triggered controversy and confusion, as astronomers struggle to work out what the old theory was missing. They are trying ideas, but are still far from sure how the pieces fit together. The field in its current state &#39;doesn&#39;t make much sense&#39;, says Norm Murray of the Canadian Institute for Theoretical Astrophysics in Toronto. &#39;It&#39;s impossible right now to account for everything,&#39; agrees Kevin Schlaufman, an astrophysicist at the Massachusetts Institute of Technology (MIT) in Cambridge. Until researchers reach a new consensus, they will not be able to understand how our own Solar System fits into the grand scheme of things, let alone predict what else might exist.&quot;</i></b> (<a href=\"http://www.nature.com/news/astronomy-planets-in-chaos-1.15480\" class=\"ot-anchor\">http://www.nature.com/news/astronomy-planets-in-chaos-1.15480</a>)<br /><br /><b>2015:</b> &quot;Centaurs as a hazard to civilization&quot;, Bill Napier, David Asher,<br />Mark Bailey and Duncan Steel, A&amp;G, December 2015, vol. 56, pp. 6.24-6.30\n\n- The discovery of giant comets called Centaurs -- typically 50 - 100km across, or larger -- in the outer planetary system over the last two decades means that these objects pose a much greater hazard to life than asteroids.<br />- Just one of them is more massive than the total of all known Earth-crossing asteroids.<br />- Calculations indicate that a Centaur will be deflected onto a path crossing the Earth&#39;s orbit about once every 40,000-100,000 years.<br />- They argue that episodes of environmental upheaval around 10,800 BCE and 2,300 BCE are consistent with this.<br />- The ages of sub-millimeter craters identified in lunar rocks returned in the Apollo program are almost all younger than 30,000 years, indicating a vast enhancement in the amount of dust in the inner Solar System since then.<br /><br /><b>Today:</b> Observations increasingly raise questions about the thoughtless application of the uniformitarian assumption<br /><br />- <b><i>&quot;As a special term, methodological uniformitarianism was useful only when science was debating the status of the supernatural in its realm; for if God intervenes, then laws are not invariant and induction becomes invalid.  It was useful for those who, as Lyell, needed a guide to combat what we now consider unscientific notions of divine intervention and the resultant discordance of past and present modes of change.  Their battle has been won, and the weapon which secured the victory deserves to be honorably retired, lest it appear like the crossbow in a nuclear age.  The term today is an anachronism; for we need no longer take special pains to affirm the scientific nature of our discipline.&quot;</i></b> (was formerly at <a href=\"http://courses.washington.edu/ess408/Gould1965.pdf\" class=\"ot-anchor\">http://courses.washington.edu/ess408/Gould1965.pdf</a>)<br /><br /><b>Today:</b> The most popular cosmology can only identify around 5-6% of the universe&#39;s matter; without the addition of 95% invisible, hypothetical matters and forces, no gravitational model can be made to explain observations.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Why Mythology is So Controversial","summary":"Mythology is Infused with Astronomical Content, But Taking these Interpretations at Face Value Requires that the Sky Must Have Changed / The Fact that these Frequently Impossible, Arbitrary, (to Us) Meaningless, and Absurd Motifs are to be Found Around the World Significantly Constrains their Explanation / It's Worth Thinking About Because if the Code Was Cracked, We'd Likely Learn a Lot About Our Human Origins and Minds","image":"https://lh4.googleusercontent.com/-18YVzNJZvhE/V4B3TBbRfMI/AAAAAAAAIPg/N1LU_uaiWxEzTr-pavuUqUFwMqE5ItkwQCL0B/why-mythology-is-controversial-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/a59SXKa7UJv","publishDate":"2016-07-09T04:02:55.701Z","updateDate":"2017-01-03T02:26:52.264Z","text":"<b>Why Mythology is So Controversial: Mythology is Infused with Astronomical Content, But Taking these Interpretations at Face Value Requires that the Sky Must Have Changed / The Fact that these Frequently Impossible, Arbitrary, (to Us) Meaningless, and Absurd Motifs are to be Found Around the World Significantly Constrains their Explanation / It&#39;s Worth Thinking About Because if the Code Was Cracked, We&#39;d Likely Learn a Lot About Our Human Origins and Minds</b><br /><br />This graphic features a small portion of a much larger argument which is made in two Youtube videos ...<br /><br /><b>Thunderbolts of the Gods</b><br /><a href=\"https://www.youtube.com/watch?v=5AUA7XS0TvA\" class=\"ot-anchor\">https://www.youtube.com/watch?v=5AUA7XS0TvA</a><br /><br /><b>Symbols of an Alien Sky</b><br /><a href=\"https://www.youtube.com/watch?v=t7EAlTcZFwY\" class=\"ot-anchor\">https://www.youtube.com/watch?v=t7EAlTcZFwY</a><br /><br /><b>The Big Picture of Attempts to Decode Mythology</b><br /><br />Bruce Masse, environmental archaeologist, US Air Force and University of Hawaii:<br /><br /><i>&quot;Past cultures worldwide often shared in a single cosmic vision, and we must not let our own present fragmented fields of knowledge hinder our attempt to recapture that vision.&quot;</i><br /><br />From Dwardu Cardona&#39;s <i>God Star</i> (p23):<br /><br /><b><i>&quot;The casual reader of popular mythology may not realize to what extent the subject is infused with astronomical content. This is partly due to the aversion displayed by most modern mythologists to anything in their studies that smacks of astronomical connections.</i></b><i> This is so true that when Girogio de Santillana and Hertha von Dechend dared to go against the common trend in their monumental work, Hamlet&#39;s Mill, they found the doors of academia closed shut in their face.  As Martin Bernal found reason to state:</i><br /><br /><i>&#39;Having written a major book on Galileo, de Santillana [whom Bernal qualifies as possibly the greatest historian of Renaissance science] became interested in the Hermetic Egyptian tradition; then, late in life, he read Dupuis&#39;s Origine de tous les cultes, and was convinced by its argument that much of ancient mythology was indeed allegory for scientific astronomy.</i><br /><br /><i>Despite de Santillana&#39;s enormous reputation, Hamlet&#39;s Mill -- the book in which he and a younger German colleague [Hertha von Dechend] set out [such a mytho-astronomical] scheme -- was not accepted by any university press and was published commercially.  This means that respectable scholars are not obliged to take such work into account. In addition, de Santillana&#39;s having stuck his neck out so far lessened his effectiveness as a [historian].  Furthermore, his work ... could be lumped together with more or less &#39;lunatic fringe&#39;; this allowed or even compelled orthodox scholars to ignore it.&#39;</i><br /><br /><b><i>Earlier in this century, mythologists were quite aware of the astronomical nature of ancient myths, but, of lately, the general assumption has been that these astral connections are of late derivation, having been foisted on an already existing mythology.</i></b><i> So much can be said against this that volumes, of which this is one, can be written.</i><br /><br /><i>The problem, however, goes deeper than that because even those earlier mythologists who had accepted the astronomical nature of the myths were themselves often guilty of tampering with their sources so that, today, it is not easy to discern what these ancient sources really had to say about the cosmic powers our ancestors venerated. </i><b><i>This is not to be wondered at because, if one had to pursue the astronomical motifs inherent in most of mythology, one would be forced to assume that the ancients&#39; perception of the celestial sphere was entirely different from that of modern man.  Thus, when modern mythologists pursue these astral myths, they tend to look up at the sky, if only figuratively, and realize that what the ancient records state about the planets, and even the Sun and Moon, bears absolutely no resemblance to the present Solar System.  And yet one must keep in mind that these self-same ancients ... were quite sophisticated when it came to observational astronomy.&quot;</i></b><br /><br />From <i>The Many Faces of Venus: The Planet Venus in Ancient Myth and Religion</i> by Ev Cochrane, p185-187:<br /><br /><i>&quot;Levi-Strauss emphasized this problem [scholars&#39; inability to explain the recurrence of mythical themes around the globe] in an influential essay many years ago: </i><b><i>&#39;How are we going to explain the fact that myths throughout the world are so similar?&#39;</i></b><br /><br /><b><i>Particularly troubling are the countless mythical details that don&#39;t make sense in the real world -- flying or fire-breathing dragons; the dwarf-like hero who suddenly assumes a gigantic form spanning heaven; lightning emanating from the &#39;eye&#39; of the ancient sun-god; and countless others.  One is naturally inclined to attribute these bizarre details to creative imagination and the art of storytelling, but this &#39;explanation&#39; runs up against an insuperable difficulty: these seemingly meaningless and &#39;impossible&#39; motifs are to be found around the globe.</i></b><i> As Levi-Strauss emphasized, it is highly unlikely that creative imagination could explain such endlessly recurring motifs:</i><br /><br /><b><i>&#39;Mythic stories are, or seem, arbitrary, meaningless, absurd, yet nevertheless they seem to reappear all over the world.  A &#39;fanciful&#39; creation of the mind in one place would be unique -- you would not find the same creation in a completely different place.&#39;</i></b><br /><br /><i>There would appear to be but three possible explanations for the presence of such recurring motifs: (1) They originated in creative imagination and subsequently became diffused around the globe; (2) They are natural products of the human mind; and (3) They have some reference to celestial phenomena, observed by ancient man the world over.</i><br /><br /><i>Levi-Strauss has rightly rejected the possibility that creative imagination can ever account for the striking content and commonality characteristic of ancient myth.  Diffusion from a single Ur-source (Africa?) can hardly be imagined nowadays, in light of what we know about the antiquity and complexity of human migration.  Nor is there any evidence that a single Ur-culture once existed which could have produced a Homer from whom all peoples derived their favorite tales.</i><br /><br /><i>Levi-Strauss and Jung are perhaps the best known proponents of the view that myth is a natural product of the human mind.  The former, as we have seen, would understand myth as reflecting the brain&#39;s innate tendency to organize reality in terms of binary operations.  Yet even if one grants that Levi-Strauss has offered a valid theory of cognitive perception, it is exceedingly difficult to imagine how this theory will explain the specific recurring details of ancient myth. </i><b><i>Why is Venus, rather than Mercury, interpreted as female?  Why is Mars, rather than Jupiter, deemed the paramour of Venus and driller of fire?  This list of questions could easily be multiplied a thousand-fold.</i></b><br /><br /><i>Carl Jung&#39;s theory that myth represents a projection of the contents of the Collective Unconscious suffers at once from a vagueness of expression as well as from its mystical nature.  For as Jung himself admits, the objective referents of the Collective Unconscious can never be known:</i><br /><br /><i>&#39;The methdological principle in accordance with which psychology treats the products of the unconscious is as follows: Contents of an archetypal character are manifestations of processes in the collective unsconscious.  Hence they do not refer to anything that is or has been conscious, but to something essentially unconscious. </i><b><i>In the last analysis, therefore, it is impossible to say what they refer to.&#39;</i></b><br /><br /><b><i>It will be seen that Jung&#39;s theory only removes the mystery of mythical imagery one step further back and thus is of little use in understanding its historical origins.</i></b><br /><br /><i>The third theory -- that ancient myth reflects mankind&#39;s enduring fascination with heavenly phenomena -- has much to recommend it.  That ancient man observed and worshipped the celestial bodies very early on is well-documented.  The astral theory of myth has the additional advantage that it readily and logically explains the apparent universality of various motifs as the product of common experience.&quot;</i><br /><br />Mel Acheson writes at <a href=\"https://www.thunderbolts.info/wp/2011/11/07/that-one-story-again/\" class=\"ot-anchor\">https://www.thunderbolts.info/wp/2011/11/07/that-one-story-again/</a><br /><br /><b><i>&quot;Petroglyphs, myths, and rituals around the world are composed of the same motifs: the ladder to heaven, the great star, and the thunderbolt</i></b><i> -- to name only three. The motifs are linked within each ancient society’s oeuvre and therefore are implicitly defined: the thunderbolt springs from the eye of the hero as he descends the ladder from heaven. Furthermore, </i><b><i>the linkages are the same around the world.</i></b><br /><br /><i>This is not a new insight. Many mythologists have remarked on the one story told around the world. The difficulty lies in explaining that pattern of recurrence. There would seem to be only three possibilities.</i><br /><br /><b><i>One Story:</i></b><i> Some physical event with a global occurrence inspired the anthropomorphic narrative. If the story were about sunrises and thunderstorms, there would be no difficulty. However, the story is about planetary gods hurling hammers and fiery wheels from a celestial column that is fixed along the axis of heaven. If those planetary gods are the same planets that we see today, the story violates the Law of Gravity and is impossible.</i><br /><br /><b><i>One Storyteller:</i></b><i> This was the first story told by the handful of first humans as they huddled around the first campfire in Africa. Their descendants took it with them as they spread around the world. They changed the names of the characters as they invented different languages, but they kept the motifs and plots and even the specific details of the images and interrelationships.</i><br /><br /><i>Thus, a primitive tribe in the Amazon today tells of the same ladder to heaven as the ancient Babylonians told of because their ancestors carried it unchanged across the Bering land bridge during the Ice Age. This explanation trades impossibility for incredibility.</i><br /><br /><b><i>One Storytelling:</i></b><i> The human brain is hardwired to generate this myth in all its details. Then why did it generate those myths and glyphs only during the Age of Mythmaking, not before, not now? Today’s comparable creation myth, the Big Bang, retains the explosive initial event but puts it far in the past beyond human witnessing and devotes most of the narrative to slowly changing uniformity.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Scientism","summary":"Conventional Science Instruction Does Not Normally Include an Education ","image":"https://lh3.googleusercontent.com/-agaWPpFThxw/V3iZC8oBzTI/AAAAAAAAINs/DuWWRtf0IPUHjZ7dv2yLNGLhSBii7DHMA/w1800-h2520/scientism-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/RzN5VuqtGkt","publishDate":"2016-07-03T04:48:57.959Z","updateDate":"2017-01-03T02:26:31.125Z","text":"<b>Scientism: Conventional Science Instruction Does Not Normally Include an Education </b><b><i>About</i></b><b> Science / By Implication, in the Absence of Any Deliberate Discussion of the Real Situation, Most People are Schooled in a Tradition of Scientism - Not Explicitly, But Rather by What Science Education Typically Leaves Out</b><br /><br /><b>Susan Haack&#39;s Six Signs of Scientism:</b><br /><br /><a href=\"http://www.uta.edu/philosophy/faculty/burgess-jackson/Haack,%20Six%20Signs%20of%20Scientism.pdf\" class=\"ot-anchor\">http://www.uta.edu/philosophy/faculty/burgess-jackson/Haack,%20Six%20Signs%20of%20Scientism.pdf</a><br /><br /><i>&quot;1. Using the words &#39;science,&#39; &#39;scientific,&#39; &#39;scientifically,&#39; &#39;scientist,&#39; etc., honorifically, as generic terms of epistemic praise.</i><br /><br /><i>2. Adopting the manners, the trappings, the technical terminology, etc., of the sciences, irrespective of their real usefulness.</i><br /><br /><i>3. A preoccupation with demarcation, i.e., with drawing a sharp line between genuine science, the real thing, and “pseudo-scientific” imposters.</i><br /><br /><i>4. A corresponding preoccupation with identifying the “scientific method,” presumed to explain how the sciences have been so successful.</i><br /><br /><i>5. Looking to the sciences for answers to questions beyond their scope.</i><br /><br /><i>6. Denying or denigrating the legitimacy or the worth of other kinds of inquiry besides the scientific, or the value of human activities other than inquiry, such as poetry or art.&quot;</i><br /><br />The following excerpts on scientism come from chapter 3 of a text titled <i>Teaching and Learning About Science and Society</i><br /><br /><b><i>&quot;[T]here is a real need, within the formal educational system, for some teaching, and learning, and discussion, and maturing of understanding, about science ... science education traditionally repudiates any such responsibility. Its concern is solely with the content of science, as defined by the principle of &#39;validity&#39;. If there is to be any such teaching, so the science educators say, then it must be under the name of some other subject, such as history, or philosophy, or ethics.</i></b><br /><br /><b><i>Unfortunately, the teachers of other, more humanistic subjects do not necessarily feel this responsibility, and are not always confident enough of their understanding of science to make a satisfactory job of it.</i></b><i> It is really not quite fair for departments and faculties of science, who often have a strongly developed sense of the competence of science as a coherent method, discipline and force in human affairs, to pass this buck. </i><b><i>It is hard to justify a separation of science teaching from teaching about science.</i></b><br /><br /><b><i>They cannot in fact be separated. By implication, in the absence of any deliberate discussion of the real situation, science education is naively &#39;for&#39; science, everyday, in every way, without qualification or limitation as to its reliability, scope or relevance. More by what it leaves out than by what it actually says, &#39;valid&#39; science is deeply imbued with scientism. It reinforces, without question or comment, the widespread sentiment that science should be the only authority for belief and the only criterion for action.&#39;</i></b><br /><br /><i>Scientism is not new, and is not simply a product of our present system of science education. In various philosophical and political manifestations it is natural to our civilization, and has been a major factor in European thought since the rise of modern science in the 17th century. In its fundamental insistence on the value of scientific methods of investigation and argument, where these are applicable, it is entirely admirable. It is only really dangerous in its more extreme and doctrinaire forms, which conflict with good sense and common humanity, and are often complete perversions of science itself.</i><br /><br /><i>The trouble with scientism is that it takes as given an attitude &#39;for&#39; science, without deeper analysis. This attitude provokes naive forms of antiscientism which are equally sterile. The very questions that are to be answered in the attempt to formulate satisfactory opinions about the role, value, use, etc., of science have already been begged.</i><br /><br /><i>It would be quite unreasonable, of course, to expect a teacher to preach against the very subject that he or she was also expounding. To be a good teacher of physics, say, one must have considerable confidence in the value of physics. But that does not mean that one must also be quite favourable to every application of physics, such as nuclear weapons, or that one should assert that all other branches of science are quite inferior to physics, to which, hypothetically, in the distant future, they might all be reduced. </i><b><i>There is no necessary incompatibility between asserting the &#39;validity&#39; of a particular discipline and accepting that other disciplines, other intellectual orientations and values, are also valid in their rightful places.</i></b><i> Presumably those who teach German in English schools have deep affection for German literature and culture, but that should not blind them to some of the crimes and follies of German history, or encourage them to believe that it would be better if we all spoke German ourselves!</i><br /><br /><i>This chapter is concerned with the scientistic attitudes encouraged by conventional science education. </i><b><i>But these attitudes arise more by implication than by deliberate instruction.</i></b><i> The proper response to scientism is not antiscientism, but a deeper, more carefully balanced analysis of the ethical, philosophical or political issues that are really at stake in each particular case. Such an analysis seldom needs to be concerned with the &#39;validity&#39; of the relevant scientific discipline, and in no way threatens the commitment of its teachers to that discipline ...</i><br /><br /><i>The &#39;scientific world picture&#39; is thus something that people believe in, and talk about - but it is seldom actually taught as a single topic. In the very early stages of science education some of its elementary aspects are explained, but the &#39;General Science&#39; curriculum is soon divided up along traditional disciplinary lines for 0 level and A level. Beyond this point, science education has no place for an overall scientific representation of the natural world. Students of physics and chemistry are not taught anything about geology or physiology. Medical students acquire only the merest acquaintance with behavioural psychology. The academic boxes may be even smaller than a conventional discipline. In one department of physics, astronomy and cosmology may never be mentioned; in another, the physical behaviour of elementary materials such as metals and polymers may never be explained.</i><br /><br /><i>No doubt such matters should be left to &#39;general education&#39;, semiformally in school, quite informally from newspapers, TV and personal reading. It is foolish to suppose that every student of a scientific subject could become encyclopaedic about science in all its aspects. But just because he or she is specializing in a scientific subject, that subject needs to be seen in its intellectual context. This is larger, and far less coherent than is normally suggested in science education.</i><br /><br /><i>For the existence of a single, unique, eventually-to-be-discovered scientific world picture is a myth. In and around the traditional physical and biological sciences, the local maps can be made to fit together with acceptable tolerances; beyond this region, the scientific representations shade off, and become infernally controversial. How much is known &#39;scientifically&#39; in psychology? Is there such a thing as a &#39;social science&#39;? Does the &#39;scientific world picture&#39; have any place for man and his works? Where do human values come into this picture? These are questions that are deeply disputed. </i><b><i>By taking for granted that there is, or could be, a unique scientific view of things, yet ignoring all questions concerning the scope of this particular view, or the legitimacy of alternative points of observation, science education is, by implication, profoundly scientistic. If science is held to be entirely valid in its own familiar regions, and if no other systems of thought are given serious consideration, the implication must be that science is the sole legitimate authority in &#39;mapping&#39; the world.</i></b><br /><br /><b><i>It is paradoxical to suggest that by not trying to expound a complete scientific world picture, science education is keeping alive the myth that such a picture exists. But that, of course, is the very nature of myths - that they should not be challenged by rational investigation, and should be taught only as matters of faith. When seriously studied, they dissolve in contradiction and uncertainty ...</i></b><br /><br /><i>The notion that only the scientific or technological specialist can give really reliable advice leads immediately to the ideology of technocracy. In this manifestation of scientism, the authority of science is given human form, in the persons of the scientists themselves. By their mastery of all that is known to science, they have become the most worthy to govern society, either indirectly as &#39;backroom boys&#39; manipulating the politicians and administrators or, less modestly, by learning the various arts of &#39;decision making&#39; and sitting in the seats of power themselves ...</i><br /><br /><b><i>(a) Science is not an end in itself. It is only to be valued as a means towards the satisfaction of various human needs, both material and spiritual. The search for truth is not an absolutely privileged activity: there are occasions when it must give way to other moral imperatives, such as respect for life, for beauty, for justice, or for charity.</i></b><br /><br /><b><i>(b) Science is not the only source of knowledge that is relevant to our lives on earth. The wisdom of the poet, the prophet, the artist, the lawyer, the humanistic scholar, or the statesman, may give us better guidance, or deeper insight than any formal scientific analysis.</i></b><br /><br /><b><i>(c) Scientific knowledge is never absolutely objective and cannot be known for certain to be true. It is not generated by mechanical robots, but by human beings, with all their failings of blindness and prejudice. It cannot be validated by rigorous logic, only by the exercise of personal judgement. It is always to some extent subject to the material interests, the historical experience, the cultural traditions, the social relationships of those who create it. Even where it seems soundly based and uncontroversial, accepted scientific knowledge may still contain significant errors.</i></b><br /><br /><b><i>(d) Scientific knowledge is reliable only over certain aspects of the natural world; its strength lies principally in those aspects that are studied in the physical and biological sciences. And even in these traditional disciplines many valid questions remain unanswered, or are tacitly ignored, because they do not seem to come within the grasp of the &#39;art of the soluble&#39;.</i></b><br /><br /><i>(e) Even within the traditional natural sciences, it is far more difficult to arrive at a reliable scientific answer than is imagined by most people, who know only about the great triumphs of research and discovery. </i><b><i>For every successful investigation there are dozens that fail to reach a convincing conclusion.</i></b><i> The main principles of physics, chemistry and biology may well be satisfactorily understood; but the work of elucidating the details of all the phenomena that are supposed to be governed by these principles is lengthy, laborious and grotesquely incomplete. </i><b><i>Much of our science-based technology works well enough, not because it was designed rationally from first principles with a real understanding of what was going on, but simply because it has been tested in practice by old-fashioned methods of trial and error.</i></b><br /><br /><i>(f) In many spheres of rational knowledge, dealing with many observable aspects of the natural world - especially the individual and social behaviour of biological organisms - there is an almost complete lack of reliable, fundamental theory. Many particular facts are known, but there is inadequate evidence to support and make fully convincing a general set of rigorous principles analogous to the &#39;laws&#39; of the physical sciences. Every claim to have discovered such overriding principles (from which strictly verifiable predictions would follow) must be regarded with very cautious scepticism.</i><br /><br /><b><i>(g) Notwithstanding many successes in the technological applications of the physical sciences and applied mathematics, the behaviour of any complex, strongly interacting system cannot be accurately predicted over a long period ...</i></b><br /><br /><i>But the challenge to conventional science education is whether it is really neutral concerning the philosophy or ideology of science. By its austere concentration on objectivity and &#39;validity&#39;, by its disciplinary specialization and fragmentation, by the problems that are chosen for investigation, by its examples of technological application, by its glorification of the intellectual achievements of its creators without reference to their human qualities, and in many other ways, science education conveys an attitude towards science that is very different from the moderate, eclectic, pluralistic attitude that has been sketched above. </i><b><i>It warns, by implication, against &#39;soft&#39;, subjective, value-laden, humanistic points of view, without revealing the converse weaknesses of &#39;hard&#39;, objective, value-free, scientific analysis. Even if it is not overtly scientistic, it effectively insulates the student from any serious criticism of the naive materialism, primitive positivism and complacent technocracy that seems to be the official public face of science.</i></b><br /><br /><b><i>This is why there is an urgent need to teach more about science in schools and colleges. It is not a question of radically subverting established scientists and technological institutions, nor of diverting the main stream of science teaching from its traditional channels. The health of the scientific enterprise depends upon people having a much more accurate picture of science and technology than they get from the existing curriculum. They need to look inside the Black Box that encloses science, conceived as an instrument of social action (Fig. 3). This should be the fundamental objective of the movement for STS education - not to replace conventional science education nor to modify it out of all recognition, but to correct its unconscious bias with complementary themes.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The History of Peer Review","summary":"The First Referees Were Never Intended to Play the Part of Supreme Scientific Gatekeepers / That Notion Emerged Around 1900, and When it Did, Suspicions Immediately Arose that the System Might be Fundamentally Flawed","image":"https://lh3.googleusercontent.com/-b1pOQXhsNy8/V3coXDB1fQI/AAAAAAAAIMw/gqn1AgPBmr0Rue73b0LaxETxM87Xc21dw/w3600-h5040/the-history-of-peer-review-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/a465vDkKrSd","publishDate":"2016-07-02T02:36:03.302Z","updateDate":"2017-01-03T02:27:20.999Z","text":"<b>The History of Peer Review: The First Referees Were Never Intended to Play the Part of Supreme Scientific Gatekeepers / That Notion Emerged Around 1900, and When it Did, Suspicions Immediately Arose that the System Might be Fundamentally Flawed</b><br /><br />If <i>Nature</i> published more articles like this, I&#39;d have to get a subscription ...<br /><br /><b>Peer review: Troubled from the start</b><br />Alex Csiszar<br />19 April 2016<br /><br /><a href=\"http://www.nature.com/news/peer-review-troubled-from-the-start-1.19763\" class=\"ot-anchor\">http://www.nature.com/news/peer-review-troubled-from-the-start-1.19763</a><br /><br /><b><i>&quot;Pivotal moments in the history of academic refereeing have occurred at times when the public status of science was being renegotiated, explains Alex Csiszar</i></b><br /><br /><i>Referees are overworked. The problem of bias is intractable. The referee system has broken down and become an obstacle to scientific progress. Traditional refereeing is an antiquated form that might have been good for science in the past but it&#39;s high time to put it out of its misery.</i><br /><br /><b><i>What is this familiar litany? It is a list of grievances aired by scientists a century ago.</i></b><i> If complaining about the faults of referee systems is nothing new, such systems are not as old as historical accounts often claim. Investigators of nature communicated their findings without scientific referees for centuries. Deciding whom and what to trust usually depended on personal knowledge among close-knit groups of researchers. (Many might argue it still does.)</i><br /><br /><i>The first referee systems that we would recognize as such were set in place by English scientific societies in the early nineteenth century. </i><b><i>But these referees were never intended to play the part of supreme scientific gatekeepers. That notion emerged in around 1900</i></b><i> (see &#39;Past notes&#39;). </i><b><i>It was exactly then that some began to wonder whether referee systems might be fundamentally flawed. In this sense, peer review has always been broken.</i></b><br /><br /><i>Today, with the debate about the future of peer review more fraught than ever, it is crucial to understand the youth of this institution. What&#39;s more, its workings and its imagined goals have evolved continually, and its current tensions bear the marks of this. The referee system has become a mishmash of practices, functions and values. But </i><b><i>one thing stands out: pivotal moments in the history of peer review have occurred when the public status of science was being renegotiated.</i></b><br /><br /><b><i>Scientific publicists</i></b><br /><br /><i>In 1831, William Whewell, a Cambridge professor and philosopher of science, proposed a scheme to the Royal Society of London. He suggested that it commission reports on all papers sent for publication in the semi-annual Philosophical Transactions. Written by teams of eminent scholars, these reports might, he argued, be &#39;often more interesting than the memoirs themselves&#39; and thus a great source of publicity for science [1]. Besides, authors would be grateful to know that their papers would be read carefully by at least two or three people. The society was just then launching a new journal to be called the Proceedings of the Royal Society, a cheaper monthly periodical to include abstracts of papers presented at the society. It had pages to fill and seemed the ideal place for these new reports.</i><br /><br /><i>At the time, editors of scientific journals made publishing decisions by personal fiat, perhaps in consultation with some trusted helpers. For publications that belonged to a scientific academy or society — such as the Philosophical Transactions — the vote of some committee of eminent persons would determine a manuscript&#39;s fate. </i><b><i>(The temptation to conflate these practices with modern referee systems has led to the stubborn myth that the origins of the scientific referee can be traced back as far as the seventeenth century.)</i></b><br /><br /><b><i>Whewell was not much concerned about preventing shoddy papers from being printed; he was not proposing a new mechanism to inform publishing decisions. Instead, he was one of many people campaigning to increase the public visibility of science and give a unified identity to the scientific enterprise in England.</i></b><i> (It was he who, a few years later, coined the word &#39;scientist&#39; to this end.) This movement had begun in 1830 and is now most remembered for Charles Babbage&#39;s Reflections on the Decline of Science in England, a screed about the paucity of state funding for, and public recognition of, science. But its more consequential legacy is the referee system.</i><br /><br /><i>Whewell was cribbing from a century-old custom at the French Academy of Sciences in Paris of writing reports that evaluated inventions and discoveries in the service of the king. There, researchers who were elected to the academy were paid by the state as a reward for scientific eminence, and politicians seemed to value their opinions. Indeed, to be an expert (a French word not yet common in English) was almost by definition to be a writer of reports. Whewell reckoned that those French académiciens must be doing something right.</i><br /><br /><i>The proposal to turn the Royal Society into a corps of expert judges in the style of the French academy was met with enthusiasm. But translating the report-writing practice across the Channel proved more complicated than Whewell expected.</i><br /><br /><b><i>News or views?</i></b><br /><br /><i>Whewell agreed to write the first report. His collaborator was a former student at Cambridge, John William Lubbock, a mathematically inclined astronomer who was also the Royal Society&#39;s treasurer. They jointly selected a manuscript submitted by George Airy, another up-and-coming astronomer. The paper, &#39;On an inequality of Long Period in the Motions of the Earth and Venus&#39;, used sophisticated mathematical methods to calculate how the orbits of these planets were influenced by the gravitational force each exerted on the other.</i><br /><br /><i>Whewell and Lubbock took turns reading the manuscript — copying technologies at the time left much to be desired. Both instantly knew what they thought of it. And they completely disagreed.</i><br /><br /><b><i>They argued about the paper for months. Both wrote draft reports, which could not have been more different. Whewell&#39;s focused on the significance of the problem and on Airy&#39;s remarkable conclusions. Lubbock&#39;s picked at the inelegant ways in which Airy had constructed his equations. Most fundamentally, they argued about what a reader&#39;s report ought to be. Whewell wanted to spread word of the discovery and to place it in the bigger picture (think Nature&#39;s News &amp; Views and Science&#39;s Perspectives). &#39;I do not think the office of reporters ought to be to criticize particular passages of a paper but to [show] its place,&#39; he told Lubbock. If they picked out flaws, he warned, authors would be put off. Lubbock had other priorities: &#39;I do not see how we can pass over grievous errors,&#39; he wrote.</i></b><br /><br /><b><i>Feeling that they had reached an impasse, Lubbock went to the author himself to deliver his suggestions for improvement. Airy was understandably irritated that his manuscript was being subjected to this strange new procedure. &#39;There the paper is,&#39; he wrote to Whewell, &#39;and I am willing to let my credit rest on it.&#39; He had no intention of changing his text. Lubbock threatened to pull out, but ultimately relented and swallowed his criticisms, acknowledging that this was &#39;the first report which the Council have ever made&#39; and trying to see the bigger picture. He thanked Whewell for putting his &#39;shoulder to the wheel&#39; and signed his name to the report [2].</i></b><br /><br /><i>With disaster averted, Whewell&#39;s version of the report was read publicly at the society on 29 March 1832, and was printed in the Proceedings, while Airy&#39;s full paper appeared in the Transactions. Lubbock&#39;s critiques never became public.</i><br /><br /><i>Not long before, the Astronomical Society of London (now the Royal Astronomical Society) and the Geological Society of London had also begun to experiment with similar reports. It was a geologist, George Greenough, who introduced the term &#39;referee&#39; in 1817, importing into science a term he knew from his days as a law student [3]. But </i><b><i>it was the Royal Society&#39;s system of reports that caused the British scientific world to take notice. The practice gradually spread to other societies, including the Royal Society of Edinburgh and the Linnean Society of London. But it was not really until the twentieth century that journals unaffiliated with any society slowly followed suit.</i></b><br /><br /><b><i>Anonymous Judges</i></b><br /><br /><b><i>The struggle between Whewell and Lubbock represented two distinct visions of what a referee might be. Whewell was the authoritative generalist, glancing down on the landscape of knowledge. He was unconcerned with — and probably not in a position to critique — the details. Such referees were, according to the Royal Society&#39;s president, &#39;Elevated by their character and reputation above the influence of personal feelings of rivalry or petty jealousy&#39; [4]. Lubbock was a younger specialist, Airy&#39;s equal. This allowed him to take a fine-tooth comb to Airy&#39;s arguments; it also put him in the position of reviewing a direct competitor.</i></b><br /><br /><b><i>Initially, Whewell&#39;s vision won out. But the system began to transform even as it lurched into existence.</i></b><i> After a couple of years, the reports became shrouded in secrecy. The last Proceedings issue to include one was in mid-1833, and no negative reports were ever published. </i><b><i>A letter Whewell wrote in 1836 shows that he himself had changed his view: he describes the referee as a defender of a society&#39;s reputation, working behind the scenes to exclude publications that do not belong. Neither the Royal Society&#39;s archives — nor the personal papers of those involved — are clear on how this happened, but we should not be surprised that it did. In England, unlike France, there was little precedent for public authorities judging from on high what constituted good or bad science. Signing one&#39;s name to explicit criticism of a colleague would have been ungentlemanly.</i></b><br /><br /><b><i>More familiar was the anonymous critic who purported to speak for the public, epitomized by the anonymous book reviews that dominated English periodicals throughout the period, from the Quarterly Review to the lowly Mechanics&#39; Magazine (the practice survives today in The Economist). Through anonymity, as one uncredited editor argued in 1833, &#39;the individual is merged in the court which he represents, and he speaks not in his own name, but ex cathedra (with full authority)&#39; [5]. Justifications of the anonymity of the scientific referee took a similar view.</i></b><br /><br /><b><i>It took just a decade for the referee to become an established scientific persona, and not a noble one. An 1845 exposé in a London magazine painted a picture of referees as scheming judges quite possibly &#39;full of envy, hatred, malice, and all uncharitableness&#39;. Hidden away in some secret chamber, this scientific judiciary, the article implied, used the cover of anonymity to advance their personal interests — perhaps through undetectable acts of piracy — at the expense of helpless authors [6].</i></b><br /><br /><b><i>It was only near the turn of the twentieth century that the idea began to take hold that editors and referees, taken as one large machinery of judgement, ought to ensure the integrity of the scientific literature as a whole. Amid calls to curtail the &#39;veritable sewage thrown into the pure stream of science&#39; (a suggestion [7] by the physiologist Michael Foster in 1894), English scientific societies debated combining their publishing apparatuses, with a standardized referee system overseeing all of scientific publishing.</i></b><i> (The plan was abandoned, in part because it would have meant convincing publishers of independent journals, such as the Philosophical Magazine, to go out of business.)</i><br /><br /><b><i>Nonetheless, the referee was gradually reimagined as a sort of universal gatekeeper with a duty to science. As this idea gained ground, many began to worry that the system itself might be intrinsically flawed, a force that impeded creative science and which ought to be abolished. Such worries culminated in what was surely the first formal inquiry into the workings of referee systems — in 1903, by the Geological Society of London. The inquiry found that opinion was sharply divided on the subject, receiving several vitriolic statements about the injustices and inefficiencies of the systems in use. The &#39;referee&#39; was in such disrepute that they nearly banned the use of the term in all society business.</i></b><br /><br /><b><i>But referee systems survived, and were slowly set up by independent journals as well. Outside the Anglophone scientific world, referee systems remained rare. Albert Einstein, for example, was shocked when an American journal sent a paper of his to a referee in 1932. The idea that any legitimate scientific journal ought to implement a formal referee system began to take hold in the decades following the Second World War.</i></b><br /><br /><b><i>Apotheosis and Fall</i></b><br /><br /><b><i>In the 1960s, refereeing emerged as a symbol of objective judgement and consensus in science. The referee was, in the words of the physicist and science writer John Ziman, &#39;the lynchpin about which the whole business of Science is pivoted&#39; [8]. Just as in 1830s England, the relationship of science to the public was at the foreground of these changes. The scientific community was once again working hard to solidify perceptions of its role in society. The very phrase &#39;scientific community&#39; dates from this time. Researchers wanted to preserve autonomy while holding on to the massive government funding that had come their way since the Second World War. Allocations for basic research in the United States, for instance, swelled by a factor of 25 in less than a decade [9].</i></b><br /><br /><i>&#39;Peer review&#39; was a term borrowed from the procedures that government agencies used to decide who would receive financial support for scientific and medical research. </i><b><i>When &#39;referee systems&#39; turned into &#39;peer review&#39;, the process became a mighty public symbol of the claim that these powerful and expensive investigators of the natural world had procedures for regulating themselves and for producing consensus, even though some observers quietly wondered whether scientific referees were up to this grand calling.</i></b><br /><br /><b><i>Current attempts to reimagine peer review rightly debate the psychology of bias, the problem of objectivity, and the ability to gauge reliability and importance, but they rarely consider the multilayered history of this institution. Peer review did not develop simply out of scientists&#39; need to trust one another&#39;s research. It was also a response to political demands for public accountability. To understand that other practices of scientific judgement were once in place ought to be a part of any responsible attempt to chart a future path. The imagined functions of this institution are in flux, but they were never as fixed as many believe.&quot;</i></b><br /><br />[1] W. Whewell to P. M. Roget, 22 March 1831; Royal Society of London Library [DM/1].<br /><br />[2] J. W. Lubbock to W. Whewell, 27 January 1832; Trinity College Library, Cambridge [a/216/61].<br /><br />[3] George Greenough Papers; University College London [Add. 7918/1621].<br /><br />[4] Proc. R. Soc. Lond. 3, 140–155 (1832).<br /><br />[5] New Monthly Magazine 39, 2–6 (1833).<br /><br />[6] Wade&#39;s London Rev. 1, 351–369 (1845).<br /><br />[7] Nature 49, 563–564 (1894).<br /><br />[8] Ziman, J. Public Knowledge: An Essay Concerning the Social Dimension of Science (Cambridge Univ. Press, 1968).<br /><br />[9] Kaiser, D. Nature 505, 153–155 (2014).<br />ArticlePubMed<br /><br />[10] Baldwin, M. Making Nature: The History of a Scientific Journal (Univ. Chicago Press, 2015).<br /><br />The original Nature article has additional resources and interesting related links embedded within the article.  It&#39;s worth checking out ...<br /><br /><a href=\"http://www.nature.com/news/peer-review-troubled-from-the-start-1.19763\" class=\"ot-anchor\">http://www.nature.com/news/peer-review-troubled-from-the-start-1.19763</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"Science's Gatekeeping Problem","summary":"It is Today Widely Understood by Academic Researchers that the Process of Reviewing Scientific Research Tends to Reject the Exceptional or Unconventional / What is Less Acknowledged by Academics is the Idea that this Gatekeeping Might be the Cause for this Century's Observable Decline in Conceptual Scientific Revolutions","image":"https://lh3.googleusercontent.com/-Kr9SF6oXnAo/V3aHhkmfu8I/AAAAAAAAILc/6UPW5t2RppUUV3W2CNkUICPD6FDqieLPg/w3600-h5040/peer-reviews-gatekeeping-problem-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/foFFtfhcv1r","publishDate":"2016-07-01T15:09:32.492Z","updateDate":"2017-01-03T02:28:18.825Z","text":"<b>Science&#39;s Gatekeeping Problem: It is Today Widely Understood by Academic Researchers that the Process of Reviewing Scientific Research Tends to Reject the Exceptional or Unconventional / What is Less Acknowledged by Academics is the Idea that this Gatekeeping Might be the Cause for this Century&#39;s Observable Decline in Conceptual Scientific Revolutions</b><br /><br />The quote in the image comes from <a href=\"http://faculty.washington.edu/ghp/images/stories/pubPDF/2005-Pollack-Revitalizing_Science.pdf\" class=\"ot-anchor\">http://faculty.washington.edu/ghp/images/stories/pubPDF/2005-Pollack-Revitalizing_Science.pdf</a><br /><i>&quot;Peer review seems to be researchers’ favourite whipping boy. Whenever two or three academics gather together, they tell each other horror stories of the journals, granting bodies and peer reviewers who have failed to recognise the latest great work. My perspective on this may be skewed by a couple of decades spent as an editor at selective journals — </i><b><i>but there is serious work to suggest that the business of selecting and rewarding research is not working optimally</i></b><i> ...</i><br /><br /><i>14 of the most highly cited papers in their sample of 757 (that were eventually published elsewhere) were rejected by a highly ranked medical journal. In 12 of these cases, the articles were </i><b><i>&#39;desk rejected&#39; -- by editors rather than peer reviewers -— and Siler et al suggest that this means that scientific gatekeeping has a problem with the exceptional or unconventional</i></b><i> ...</i><br /><br /><i>[P]eer review is not the same as gatekeeping by the editors of journals. When peer review is blamed for all rejections from journals, the complainers are, in my view, conflating two processes. One is the technical assessment of a piece of work: has it been properly done, and are the conclusions adequately supported? This type of technical assessment must of necessity be done by peers—those in the specialist area who know the issues best. Another is the selection of the most interesting or important work, or the pieces that are most likely to appeal to a particular journal’s readership. </i><b><i>This type of assessment is routinely made by editors — whether they are full time staff or academics working part time as editors. (In making this assessment, the editors often consult with the same peers whose expert advice they have asked on the technical issues, which muddies the distinction.)&quot;</i></b><br /><br /><a href=\"http://blogs.bmj.com/bmj/2014/12/23/theodora-bloom-peer-review-and-gatekeeping/\" class=\"ot-anchor\">http://blogs.bmj.com/bmj/2014/12/23/theodora-bloom-peer-review-and-gatekeeping/</a><br /><br /><i>&quot;A half-century ago, breakthroughs were fairly common events. They could be counted on to occur on an unpredictable but not infrequent basis. Pioneering such breakthroughs were now-legendary figures such as Linus Pauling, Jonas Salk, Richard Feynman, James Watson, Francis Crick — heroic names familiar even to lay people ...&quot;</i><br /><br />For more information, see ...<br /><br /><b>The Decline in Conceptual Revolutions</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/FXj2NzSzjSo\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/FXj2NzSzjSo</a><br /><br /><i>&quot;... But things have changed. While the past 30 years have brought a great outpouring of scientific results, breakthroughs are less common. Modern equivalents of Pauling, Salk, and Watson-Crick are not easy to identify. Can you name more than a handful of breakthroughs that have emerged during the past three decades? I mean realized breakthroughs such as revelation of the biochemical nature of heredity or a vaccine to eradicate a ravaging affliction, not incipient breakthroughs whose realization always seems just around the corner.</i><br /><br /><i>Considering the currently massive investment in science, why are there so few scientific breakthroughs? Why is it that scientific heroes are now so scarce?</i><br /><br /><i>Some argue that this settling down is all but inevitable. After all, science today is far more complicated than it has been, often requiring teams of investigators and large groups to pursue effectively. Others argue that there is simply not much more to be discovered — that the breakthroughs have had their heyday and we need content ourselves with merely filling in the gaps. Thus, breakthroughs might not be expected to occur on an everyday basis.</i><br /><br /><i>Perhaps some of this is true — but a significant role may also be played by another factor: the growing aversion to risk taking. Although funding agencies have much to be proud of for past achievements, it is broadly perceived that they have become less agile in dealing with proposals that dissent from orthodoxy. Challengers of the status quo rarely succeed in today’s scientific climate. Hence, the approaches most apt to generate conceptual breakthroughs are frequently throttled before they can emerge from the scientific womb.</i><br /><br /><i>The funding agencies are aware of this problem. Both the NSF and the NIH have held recent workshops to deal with the issue, and some measures have been taken over and above existing remedial programs. The term &#39;high risk&#39; now permeates NIH review guidelines. The NIH has established five to ten &#39;Pioneer Awards&#39; each year to encourage novel approaches. And R21 &#39;high risk&#39; grants have been available from many Institutes for some time.</i><br /><br /><i>While these responses acknowledge the problem, it is broadly felt that they are nominal. Few dissenters from orthodoxy report any success with the R21 system, and even some NIH administrators admit that these grants do not solve the problem. Admonishing reviewers to be &#39;less conservative&#39; comes with no guarantee that they will be ...</i><br /><br /><b><i>... The scenario is akin to a business that does not need to compete.</i></b><i> Ordinary businesses must innovate to beat the competition, but the scientific business suffers little such concern, for so long as the promise of incipient breakthroughs continues to be as well advertised as it is now, the public will continue to invest ...</i><br /><br /><i>... Compare this review system with that of a court.  The &#39;court&#39; is set up here to adjudicate the fate of a grant application.  A plaintiff makes the case.  The case is against the defendant (establishment) -- who then proceeds to act as judge. </i><b><i>The party making judgment is the very party who stands to lose most if the application prevails.&quot;</i></b><br /><br />(&quot;Revitalizing Science in a Risk-Averse Culture: Reflections on the Syndrome and Prescriptions for Its Cure&quot; by Gerald Pollack at <a href=\"http://faculty.washington.edu/ghp/images/stories/pubPDF/2005-Pollack-Revitalizing_Science.pdf\" class=\"ot-anchor\">http://faculty.washington.edu/ghp/images/stories/pubPDF/2005-Pollack-Revitalizing_Science.pdf</a>)<br /><br /><i>&quot;The number of unconventional schools of thought around today is surprisingly large.  Those who take the trouble to look will find meaningful alternatives distributed throughout practically all domains of science, many of them rich with promise.  All but a few are ignored or repressed by the prevailing orthodoxy -- either because of perceived threats to self-interest or because of suspicion stemming from a cultural bias that fundamental discoveries are no longer possible.  These unconventional schools of thought represent potentially ripe fruit, waiting to be plucked&quot;</i><br /><br />(<a href=\"http://conceptualrevolutions.com/2014/06/06/qa-with-gerald-pollack-on-the-institute-for-venture-science/\" class=\"ot-anchor\">http://conceptualrevolutions.com/2014/06/06/qa-with-gerald-pollack-on-the-institute-for-venture-science/</a>)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Institute for Venture Science Process","summary":"Peer Review Can Be Like a Court where the Party Making Judgment is the Very Party Who Stands to Lose Most if the Application Prevails / The IVS is a New Approach to Peer Review Designed Specifically for Cases where Ideas Challenge Entrenched Textbook Theories / It Specifically Addresses Peer Review's Gatekeeping Problem, Where Specialist Scientists Reject through Oftentimes Anonymous Review Ideas Which Challenge their Own Lines of Investigation","image":"https://lh3.googleusercontent.com/-9YGns7ptiQw/V3S0OP8FHjI/AAAAAAAAIKE/JoaRHVXseus9zftl_5f3MqDBnfGCa94zg/w7142-h9999/the-institute-for-venture-science-process-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/GtfYqmXUYKX","publishDate":"2016-06-30T05:55:23.197Z","updateDate":"2017-01-03T02:27:37.317Z","text":"<b>The Institute for Venture Science Process: Peer Review Can Be Like a Court where the Party Making Judgment is the Very Party Who Stands to Lose Most if the Application Prevails / The IVS is a New Approach to Peer Review Designed Specifically for Cases where Ideas Challenge Entrenched Textbook Theories / It Specifically Addresses Peer Review&#39;s Gatekeeping Problem, Where Specialist Scientists Reject through Oftentimes Anonymous Review Ideas Which Challenge their Own Lines of Investigation</b><br /><br /><b>How Will the IVS Foster Revolutionary Advances?</b><br /><b>(The More Detailed Answer)</b><br /><br /><b>1. The IVS invites groundbreaking proposals, doesn&#39;t matter what field</b><br /><br /><i>&quot;Applications must address big issues.&quot;</i><br /><br /><b>2. These ideas go through a vetting process to weed out flaky proposals</b><br /><br /><i>&quot;Pre-proposals: Pre-proposals should be required in order to discourage submissions that fail to address big issues, and for offering feedback, which can be useful for helping to shape full proposals.  This strategy will reduce the number of full proposals that need review. </i><b><i>The feedback may also help shift the scientific culture to one that is more receptive to unconventional approaches rather than suspicious of them.</i></b><br /><br /><i>Screening: Applications are first screened for seriousness.  The reviewers </i><b><i>from outside the applicant&#39;s field, reviewing independently</i></b><i>, assume responsibility for this initial gatekeeping function.  Applications dealing with issues of very narrow significance, or of obvious flakiness, are to be turned back.  Split vote warrants discussion, and a majority vote then prevails.  Written comments from each administrator are returned to the applicant, who may challenge the panel&#39;s negative decision.  In that case, a panel of out-of-the-field scientists chosen from the reviewer pool reconsiders the gate-keeping decision and decides one way or the other.  Admitted applicants move to the review stage.&quot;</i><br /><br /><b>3. With what&#39;s left, the IVS asks for a scathing critique from the field&#39;s leaders, and establishes a debate between the two sides</b><br /><br /><i>Proposals: Not to exceed 20 pages, </i><b><i>a proposal will detail the problems with the prevailing school of thought, present an alternative; it will offer detailed evidence and/or reasons why this alternative might be superior.  The proposal should also include a brief outline of how to proceed initially.  The material should be presented in a way that is understandable to non-experts.</i></b><i> Included with the proposal could be a limited number of letters from scientists in support of the proposed idea.</i><br /><br /><i>Reviewer Pool: Since the review principles differ qualitatively from ordinary grant review, special care needs to be exercised in choosing reviewers. </i><b><i>Broad and experienced &#39;generalists&#39; are desirable</i></b><i>, particularly those who command the respect of the scientific community. </i><b><i>The most important qualification, however, is openness to the notion that revolutions are still possible in our day and age, and willingness to entertain promising ideas that may seem radical.</i></b><i> Those who, themselves, have posed major challenges to prevailing orthodoxies may be particularly well suited, for they are well acquainted with the obstacles faced by challengers and may therefore be especially able to judge the merit of challenges in other fields.&quot;</i><br /><br /><b>4. Evaluators from outside the field listen to the debate for how well the applicants can defend themselves</b><br /><br /><i>&quot;Proposal Review: The goal is for a panel of say, five to seven open-minded scientists to hear challenger and defender in an impartial setting, </i><b><i>and determine whether the challenge has a reasonable chance of prevailing.</i></b><i> The first phase of the debate is written.  The panel sees the proposer&#39;s evidence </i><b><i>and invites written responses from the field&#39;s leading defendants.</i></b><i> Defendants&#39; responses should be signed; Not only does this promote responsibility, but it also confirms his or her leadership position in the field -- an incentive for their participation. </i><b><i>The proposer is then invited to prepare a written response. This debate is made available on the web, for interested members of the community to digest and consider.</i></b><br /><br /><i>In examining both sets of arguments, panel members may have questions.  These questions could concern the applicant&#39;s proposal and/or the defenders&#39; comments.  Any question that arises should be promptly communicated and promptly answered, the answers made available to all relevant parties </i><b><i>and posted on the web</i></b><i>.  This process should quickly clear up questions whose answers might make a major difference in the evaluation.</i><br /><br /><i>The panel may then move to evaluation; or, if it feels the amplification is needed, </i><b><i>it may request oral debate</i></b><i>, either as a forum or by video conferencing.  Again, such debate should be made available to the public on the web, stimulating attention to the challenge.  Indeed, the vetting process itself may be considered the first stage of public alert to the possbility of meaningful challenge to the status quo. </i><b><i>Fierce community debate would be a sure sign of progress, for, as Claude Bernard put it, &#39;Controversy is the lifeblood of science.&#39;&quot;</i></b><br /><br /><b>5. A judgment is made as to how well they did and how important their subject is</b><br /><br /><i>&quot;Decisions: </i><b><i>The challenge paradigm need not be proven better than the prevailing one, only of reasonably high potential to be proved better.</i></b><i> Hence, the panel&#39;s decision should not be taken by the public as an on-high pronouncement on which school of thought is correct -- a scenario that carries the danger of inciting backlash.  It is merely an evaluation of potential.  Each panel member awards two scores, higher numbers indicating higher quality: </i><b><i>1-10 for potential impact</i></b><i> and </i><b><i>1-10 for the strength of current evidence and hence the likelihood of prevailing. The two scores are multiplied to obtain the total score, which will range from 1 to 100. Reviewer scores are then averaged.</i></b><i> A single outlier might invite additional panel discussion and possible re-scoring.  In order to diminish the possibility that a single biased reviewer could tilt the result unfairly, any reviewer&#39;s total that remains more than one standard deviation from the mean is removed from consideration, and the remaining scores are averaged to yield the final panel score.</i><br /><br /><i>All applications are scored in this manner. </i><b><i>Funding begins from the highest score and continues until available funds are exhausted.&quot;</i></b><br /><br /><b>6. The IVS funds selected proposals liberally in order to guarantee that money will be there for years</b><br /><br /><b>7. Multiple groups using different techniques are funded to pursue the same theme, in order to establish a critical mass</b><br /><br /><i>&quot;Funding amounts and durations: To ensure a critical mass of activity, each funded idea must come with a substantial commitment to fund multiple groups*; otherwise, the idea is unlikely to take hold no matter how promising it might be.  The review panel might suggest the appropriate funding level for the applicant, as well as for the overall effort.  As a rough guideline, a minimum of ten standard-size grants would be reasonable, and for those proposals that warrant, more groups could accelerate the pace.&quot;</i><br /><br /><b>8. If the challenge really has the goods, in 2-4 years it&#39;ll be obviouis that the evidence for this is convincing</b><br /><br /><i>&quot;As for duration, a reasonable minimum would be five years. </i><b><i>Five years should be long enough to determine whether the challenge is making headway.</i></b><i> The five-year grant should come with the understanding that </i><b><i>success in gaining substantial influence on the field would create a reasonably low threshold for follow-up funding</i></b><i>, to continue any necessary buildup of momentum.  Advance knowledge of the low threshold should help entice otherwise reluctant investigators to enter into the high-risk arena with understanding that funding cutoff would not be arbitrary.  This second-period funding would be evaluated similarly to the initial evaluation, the initial document being a report of progress. </i><b><i>Evaluation should not be based solely on the number of publications and/or journal-impact factors</i></b><i>, which are not necessarily measures of influence; rather, it should be based primarily on </i><b><i>signs of increasing influence on the field, which may be best judged not only by increasing citations in the field, but mainly by interviews with the fields&#39; scientists.&quot;</i></b><br /><br />See <a href=\"http://www.theinstituteforventurescience.net/\" class=\"ot-anchor\">http://www.theinstituteforventurescience.net/</a> for the most up-to-date explanation of the IVS.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"An Expert in Science Misconceptions Explains Critical Thinking","summary":"The Internet Has Given a Voice to the World, But the Noise of the Internet Tends to Crowd Out the Best Ideas / An Anti-Pseudoscience Movement which Emphasizes Experts and Facts Over Becoming a Better Thinker Has Become Popular / Meanwhile, a New Robot Economy Will Favor the Best Thinkers / How Are You Preparing for the Robot Economy?","image":"https://lh3.googleusercontent.com/-CV-dBfCS66U/V3MrgKSSN-I/AAAAAAAAIIo/mpLfUgVPEMIcBdF6MRSN8OiykLQlqlbbg/w1800-h2520/an-expert-in-scientific-misconceptions-explains-critical-thinking-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/4LzsWzeD7HQ","publishDate":"2016-06-29T02:03:27.774Z","updateDate":"2017-01-03T02:28:58.720Z","text":"<b>An Expert in Science Misconceptions Explains Critical Thinking: The Internet Has Given a Voice to the World, But the Noise of the Internet Tends to Crowd Out the Best Ideas / An Anti-Pseudoscience Movement which Emphasizes Experts and Facts Over Becoming a Better Thinker Has Become Popular / Meanwhile, a New Robot Economy Will Favor the Best Thinkers / How Are You Preparing for the Robot Economy?</b><br /><br />This text is very famous in the physics education research community.  It&#39;s high resale value keeps it rare.<br /><br /><i>Teaching Introductory Physics</i><br />Arnold B Arons<br /><br />Chapter 13: Critical Thinking<br />(p375-386)<br /><br /><i>&quot;The simple but difficult arts of paying attention, copying accurately, following an argument, detecting an ambiguity or a false inference, testing guesses by summoning up contrary instances, organizing one&#39;s time and one&#39;s thought for study -- all of these arts ... cannot be taught in the air but only through the difficulties of a defined subject; they cannot be taught in one course in one year, but must be acquired gradually in dozens of connections.&quot;</i> (Jacques Barzun)<br /><br /><b>13.1 INTRODUCTION</b><br /><br /><i>&quot;No curricular recommendation, reform, or proposed structure has ever been made without some obeisance [deferential respect] to the generic term &#39;critical thinking&#39; or one of its synonyms.  The flood of reports on education in our schools and colleges that has been unleashed in recent years is no exception; every report, at every level of education, calls for attention to the enhancement of thinking-reasoning capacities in the young.  A currently prominent formula is &#39;higher order thinking skills.&#39;  Few of the documents that come to us, however, attempt to supply some degree of specificity -- some operational definition of the concept, with illustrations of what might be done in day-to-day teaching to move toward the enunciated goals.</i><br /><br /><i>It is the object of this chapter to try to &#39;unpack&#39; the term &#39;critical thinking&#39; -- to list a few simpler, underlying processes of abstract logical reasoning that are common to many disciplines and that can be cultivated and exercised separately in limited contexts accessible to the student.  Subsequently, the individual&#39;s conscious weaving together of these various modes results in the larger synthesis we might characterize as &#39;critical thought.&#39;  As Barzun points out in the quotation cited above, </i><b><i>this can be done only through practice in, preferably, more than one field of subject matter.</i></b><br /><br /><i>To glimpse some of the ways in which effective schooling might enhance students&#39; reasoning capacities, it is instructive to examine a few of the thinking and reasoning processes that underlie analysis and inquiry.  These are processes that teachers rarely articulate or point out to students; yet these processes are implicit in many different studies.  The following listing is meant to be illustrative; it is neither exhaustive nor prescriptive.  Readers are invited to add or elaborate items they have identified for themselves or sense to be more immediately relevant in their own disciplines.</i><br /><br /><b><i>1 Consciously raising questions &#39;What do we know ... ?  How do we know ... ?  Why do we accept or believe ... ?  What is the evidence for ... ?&#39; when studying some body of material or approaching a problem.</i></b><br /><br /><i>Consider the assertion, which virtually every student and adult will make, that the moon shines by reflected sunlight.  How many people are able to describe the simple evidence, available to anyone who can see, that leads to this conclusion (which was, incidentally, perfectly clear to the ancients)?  This does not require esoteric intellectual skills; young children can follow and understand; all one need do is lead them to watch the locations of both the sun and moon, not just the moon alone, as a few days go by.  Yet for the majority of our population the &#39;fact&#39; that the moon shines by reflected sunlight is received knowledge, not sustained by understanding.</i><br /><br /><i>Exactly the same must be said about the contention that the earth and planets revolve around the sun.  The validation and acceptance of this view marked a major turning point in our intellectual history and in our collective view of man&#39;s place in the universe. </i><b><i>Although the basis on which this view is held is more subtle and complex than that for the illumination of the moon, the &#39;How do we know ... ?&#39; should be an intrinsic part of general education; it is, for most people, however, received knowledge</i></b><i> -- as is also the view that matter is discrete in its structure rather than continuous.</i><br /><br /><i>Similar questions should be asked and addressed in other disciplines: How does the historian come to know how the Egyptians, or Babylonians, or Athenians lived?  On what basis does the text make these assertions concerning consequences of the revocation of the Edict of Nantes?  What is the evidence for the claim that such and such tax and monetary policies promote economic stability?  What was the basis for acceptance of the doctrine of separation of church and state in our political system?</i><br /><br /><i>Cognitive development researchers [e.g., Anderson (1980); Lawson (1982)] describe two principal classes of knowledge: figurative or declarative on the one hand, and operative or procedural on the other. </i><b><i>Declarative knowledge consists of knowing &#39;facts&#39;</i></b><i> (matter is composed of atoms and molecules; animals breathe oxygen and expel carbon dioxide; the United States entered the Second World War after the Japanese attack on Pearl Harbor in December 1941). </i><b><i>Operative knowledge involves understanding where the declarative knowledge comes from or what underlies it</i></b><i> (What is the evidence that the structure of matter is discrete rather than continuous?  What do we mean by the terms &#39;oxygen&#39; and &#39;carbon dioxide&#39; and how do we recognize these as different substances?  What worldwide political and economic events underlay the American declaration of war?). </i><b><i>And operative knowledge also involves the capacity to use, apply, transform, or recognize the relevance of declarative knowledge in new situations.</i></b><br /><br /><i>&#39;Above all things,&#39; says Alfred North Whitehead in a well-known passage on the first page of The Aims of Education, </i><b><i>&#39;we must beware of what I will call &#39;inert ideas&#39; -- that is to say, ideas that are merely received into the mind without being utilized, or tested, or thrown into fresh combinations.&#39;</i></b><i> And John Gardner once deplored our tendency ... &#39;to hand our students the cut flowers while forbidding them to see the growing plants.&#39;</i><br /><br /><b><i>Preschool children almost always ask &#39;How do we know ... ?  Why do we believe ... ?&#39; questions until formal education teaches them not to. Most high school and college students then have to be pushed, pulled, and cajoled into posing and examining such questions; they do not do so spontaneously. Rather, our usual pace of assignments and methods of testing all too frequently drive students into memorizing end results, rendering each development inert. Yet given time and encouragement, the habit of inquiry can be cultivated, the skill enhanced, and the satisfaction of understanding conveyed.  The effect would be far more pronounced and development far more rapid if this demand were made deliberately and simultaneously in science, humanities, history, and social science courses rather than being left to occur sporadically, if at all, in one course or discipline.</i></b><br /><br /><b><i>2 Being clearly and explicitly aware of gaps in available information. Recognizing when a conclusion is reached or a decision made in absence of complete information and being able to tolerate the attendant ambiguity and uncertainty. Recognizing when one is taking something on faith without having examined the &#39;How do we know ...? Why do we believe ...?&#39; questions.</i></b><br /><br /><i>Interesting investigations of cognitive skill and maturity are conducted by administering test questions or problems in which some necessary datum or bit of information has been deliberately omitted, and the question cannot be answered without securing the added information or making some plausible assumption that closes the gap.  Most students and many mature adults perform very feebly on these tests.  They have had little practice in such analytical thinking and fail to recognize, on their own, that information is missing.  If they are told that this is the case, some will identify the gap on reexamining the problem, but many will stil fail to make the specific identification.</i><br /><br /><b><i>In our subject matter courses, regardless of how carefully we try to examine evidence and validate our models and concepts, it will occasionally be necessary to ask students to take something on faith. This is a perfectly reasonable thing to do, but it should never be done without making students aware of what evidence is lacking and exactly what they are taking on faith [Realize that press releases in some disciplines do this as a matter of routine]. Without such care, they do not establish a frame of reference from which to judge their level of knowledge, and they fail to discriminate clearly those instances in which evidence has been provided from those in which it has not.</i></b><br /><br /><b><i>3 Discriminating between observation and inference, between established fact and subsequent conjecture.</i></b><br /><br /><b><i>Many students have great trouble making such discriminations even when the situation seems patently obvious to the teacher. They are unused to keeping track of the logical sequence</i></b><i>, and they are frequently confused by technical jargon they have previously been exposed to but never clearly understood.</i><br /><br /><i>In the case of the source of illumination of the moon cited earlier, for example, students must be made explicitly conscious of the fact that they see the extent of illumination increasing steadily as the angular separation between moon and sun increases, up to full illumination at a separatin of 180 degrees.  This direct observation leads, in turn, to the inference that what we are seeing is reflected sunlight.</i><br /><br /><i>In working up to the concept of &#39;oxygen&#39; (without any prior mention of this term at all) with a group of elementary school teachers some years ago, I had them do an experiment in which they heated red, metallic copper in an open crucible and weighed the crucible periodically.  What they saw happening, of course, was the copper turning black and the weight of crucible and contents steadily increasing.  When I walked around the laboratory and asked what they had observed so far, many answered, &#39;We observed oxygen combining with the copper.&#39;  When I quizzically inquired whether that was what they had actually seen happening, their reaction was one of puzzlement.  It took a sequence of Socratic questioning to lead them to state what they had actually seen and to discern the inference that something from the air must be joining the copper to make the increasing amount of black material in the crucible.  It had to be brought out explicitly that this &#39;something from the air&#39; was the substance to which we would eventually give the name &#39;oxygen.&#39; </i><b><i>What they wanted to do was to use the technical jargon they had acquired previously without having formed an awareness of what justified it.</i></b><br /><br /><i>This episode illustrates the importance of exposing students to repeated opportunity to discriminate between observation and inference. </i><b><i>One remedial encounter in one subject matter context is not nearly enough</i></b><i>, but opportunities are available at almost every turn.  Mendel&#39;s observations of nearly integral ratios of population members having different color and size characteristics must be separated from inference of the existence of discrete elements controlling inheritance.  In the study of literature, analysis of the structure of a novel or a poem must be distinguished from an interpretation of the work.  In the study of history, primary historical data or information cited by the historian must be separated from the historian&#39;s interpretation of the data.</i><br /><br /><i>A powerful exercise once employed by some of my colleagues in history was to give the students a copy of the Code of Hammurabi accompanied by the assignment: &#39;Write a short paper addressing the following question: From this code of laws, what can you infer about how these people lived and what they held to be of value?&#39;  This exercise obviously combines exposure to both processes 1 and 3.</i><br /><br /><b><i>4 Recognizing that words are symbols for ideas and not the ideas themselves. Recognizing the necessity of using only words of prior definition, rooted in shared experience, in forming a new definition and in avoiding being misled by technical jargon.</i></b><br /><br /><i>From the didactic manner in which concepts (particularly scientific concepts) are forced on students in early schooling, it is little wonder that they acquire almost no sense of the process of operational definition and that they come to view concepts as rigid, unchanging entities with only one absolute significance that the initiated automatically &#39;know&#39; and that the breathless student must acquire in one intuitive gulp. </i><b><i>It comes as a revelation and a profound relief to many students when they are allowed to see that concepts evolve; that they go through a sequence of redefinition, sharpening, and refinement; that one starts at crude, initial, intuitive levels and, profiting from insights gained in successive applications, develops the concept to final sophistication.</i></b><br /><br /><i>In my own courses, I indicate from the first day that we will operate under the precept &#39;idea first and name afterwards&#39; and that scientific terms acquire meaning only through the description of shared experience in words of prior definition.  When students try to exhibit erudition (or take refuge from questioning) by name dropping technical terms that have not yet been defined, I and my staff go completely blank and uncomprehending.  Students catch on to this game quite quickly.  They cease name dropping and begin to recognize, on their own, when they do not understand the meaning of a term.  Then they start drifting in to tell us of instances in which they got into trouble in psychology, or sociology, or economics, or political science course by asking for operational meaning of technical terms.  It is interesting that this is an aspect of cognitive development to which many students break through relatively quickly and easily.  Unfortunately, this is not true of most other modes of abstract logical reasoning.</i><br /><br /><b><i>5 Probing for assumptions (particularly the implicit, unarticulated assumptions) behind a line of reasoning.</i></b><br /><br /><i>In science courses, this is relatively easy to do.  Idealizations, approximations, and simplifications lie close to the surface and are quite clearly articulated in most presentations. </i><b><i>They are ignored or overlooked by the students, however, principally because explicit recognition and restatement are rarely, if ever, called for on tests or examinations.</i></b><i> In history, humanities, and the social sciences, underlying assumptions are frequently more subtle and less clearly articulated; probing for them requires careful and self-conscious attention on the part of instructors and students.</i><br /><br /><b><i>6 Drawing inferences from data, observations, or other evidence and recognizing when firm inferences cannot be drawn. This subsumes a number of processes such as elementary syllogistic reasoning (e.g., dealing with basic propositional, &#39;if ... then&#39; statements), correlational reasoning, recognizing when relevant variables have or have not been controlled.</i></b><br /><br /><i>Separate from the analysis of another&#39;s line of reasoning is the formulation of one&#39;s own.  &#39;If ... then&#39; reasoning from data or information must be undertaken without prompting from an external &#39;authority.&#39;  One must be able to discern possible cause-and-effect relations in the face of statistical scatter and uncertainty.  One must be aware that failure to control a significant variable vitiates [spoils] the possibility of inferring a cause-and-effect relation. </i><b><i>One must be able to discern when two alternative models, explanations, or interpretations are equally valid and cannot be discriminated on logical grounds alone.</i></b><br /><br /><i>As an illustration of the latter situation, I present a case I encounter very frequently in my own teaching.  When students in a general education science course begin to respond to assignments leading them to watch events in the sky (diurnal changes in rising, setting and elevation of the sun, waxing and waning of the moon, behavior of the stars and readily visible planets), </i><b><i>they immediately expect these naked eye observaitons to allow them to &#39;see&#39; the &#39;truth&#39; they have received from authority, namely that the earth and planets revolve around the sun. When they first confront the fact that both the geo- and heliocentric models rationalize the observations equally well and that it is impossible to eliminate one in favor of the other on logical grounds at this level of observation, they are quite incredulous. They are shocked by the realization that either model might be selected provisionally on the basis of convenience, or of aesthetic or religious predilection. In their past experience, there has always been a pat answer. They have never been led to stand back and recognize that one must sometimes defer, either [temporarily] or permanently, to unresolvable alternatives. They have never had to wait patiently until sufficient information and evidence were accumulated to develop an answer to an important question; the answer has always been asserted (for the sake of &#39;closure&#39;) whether the evidence was at hand or not, and the ability to discriminate decidability versus undecidability has never evolved.</i></b><br /><br /><i>An essentially parallel situation arises in the early stages of formation of the concepts of static electricity (see Sections 6.7 and 6.8).  Students are very reluctant to accept the fact that, before we know anything about the microscopic constitution of matter and the role of electrical charge at that level, it is impossible to tell from observable (macroscopic) phenomena whether positive charge, negative charge, or both charges are mobile or being displaced. </i><b><i>They wish to be told the &#39;right answer&#39; and fail to comprehend that any one of the three models accounts equally well for what we have observed and predicts equally well in new situations.</i></b><i> They want to use the term &#39;electron&#39; even though they have no idea what it means or what evidence justifies it, and they apply it incorrectly to irrelevant and inappropriate situations.</i><br /><br /><b><i>If attention is explicitly given, experiences such as the ones just outlined can play a powerful role in opening student minds to spontaneous assessment of what they know and what they do not know, of what can be inferred at a given juncture and what cannot.</i></b><br /><br /><b><i>7 Performing hypothetico-deductive reasoning; that is, given a particular situation, applying relevant knowledge of principles and constraints and visualizing, in the abstract, the plausible outcomes that might result from various changes one can imagine to be imposed on the system.</i></b><br /><br /><i>Opportunities for such thinking abound in almost every course. </i><b><i>Yet students are most frequently given very circumscribed [restricted] questions that do not open the door to more imaginative hypothetico-deductive reasoning.</i></b><i> The restricted situations are important and provide necessary exercises as starting points, but they should be followed by questions that impel the student to invent possible changes and pursue the plausible consequences.</i><br /><br /><b><i>8 Discriminating between inductive and deductive reasoning; that is, being aware when an argument is being made from the particular to the general [inductive] or from the general to the particular [deductive].</i></b><br /><br /><i>The concepts of &#39;electric circuit,&#39; &#39;electric current,&#39; and &#39;resistance&#39; can be induced from very simple observations made with electric batteries and arrangements of flashlight bulbs.  This leads to the </i><b><i>inductive</i></b><i> construction of a &#39;model&#39; of operation of an electric circuit.  The model then forms the basis for </i><b><i>deductive</i></b><i> reasoning, that is, predictions of what will happen to brightness of bulbs in new configurations or when changes (such as short circuiting) are imposed on an existing configuation.</i><br /><br /><i>Exactly similar thinking can be developed in connection with economic models or processes.  Hypothetico-deductive reasoning is intimately involved in virtually all such instances, but one should always be fully conscious of the distinction between the inductive and the deductive modes.</i><br /><br /><b><i>9 Testing one&#39;s own line of reasoning and conclusions for internal consistency and thus developing intellectual self-reliance.</i></b><br /><br /><i>The time is long past when we could teach our students all they need to know.  The principal function of education -- higher education in particular -- must be to help individuals to their own intellectual feet: To give them conceptual starting points and an awareness of what it means to learn and understand something so that they can continue to read, study, and learn as need and opportuninty arise, without perpetual formal instruction.</i><br /><br /><b><i>To continue genuine learning on one&#39;s own (not just accumulating facts) requires the capacity to judge when understanding has been achieved and to draw conclusions and make inferences from acquired knowledge. Inferring, in turn, entails testing one&#39;s own thinking, and the results of such thinking, for correctness or at least for internal coherence and consistency.</i></b><i> This is, of course, a very sophisticated level of intellectual activity, and students must first be made aware of the process and its importance.  Then they need practice and help.</i><br /><br /><i>In science courses, they should be required to test and verify results and conclusions by checking that the results make sense in extreme or special cases that can be reasoned out simply and directly.  They should be led to solve a problem in alternative ways when that is possible.  Such thinking should be conducted in both quantitative and qualitative situations.  In the humanities and social sciences, the checks for internal consistency are more subtle, but they are equally important and should be cultivated explicitly.  Students should be helped to sense when they can be confident of the soundness, consistency, or plausibility of their own reasoning so that they can consciously dispense with the teacher and cease relying on someone else for the &#39;right answer.&#39;</i><br /><br /><b><i>10 Developing self-consciousness concerning one&#39;s own thinking and reasoning processes.</i></b><br /><br /><i>This is perhaps the highest and most sophisticated reasoning skill, presupposing the others that have been listed. </i><b><i>It involves standing back and recognizing the processes one is using, deliberately invoking those most appropriate to the given circumstances, and providing the basis for conscious transfer of reasoning methods from familiar to unfamiliar contexts.</i></b><br /><br /><i>Given such awareness, one can begin to penetrate new situations by asking oneself probing questions and constructing answers.  Starting with artificial, idealized, oversimplified versions of the problem, one can gradually penetrate to more realistic and complex versions.  In an important sense, this is the mechanism underlying independent research and investigation.</i><br /><br /><b><i>13.3 WHY BOTHER WITH CRITICAL THINKING?</i></b><br /><br /><i>The preceding list of thinking and reasoning processes underlying the broad generic term &#39;critical thinking&#39; is neither complete nor exhaustive.  For illustrative purposes, I have tried to isolate and describe processes and levels of awareness that appear to be bound up with clear thinking and genuine understanding in a wide variety of disciplines and to show a deep commonality in this respect among very different kinds of subject matter.  These processes underlie the capacity defined by Jacques Barzun in the quotation that heads this chapter.</i><br /><br /><b><i>Developing these intellectual skills requires extensive, sustained practice. Such practice is not possible in a space devoid of subject matter. It is only through contact with, and immersion in, rich areas of subject matter that interesting and significant experience can be generated.</i></b><i> Although it may be possible, in principle, to generate limited aspects of such practice through artificial kinds of exercises and puzzle solving, or even through analysis of scores in sports contests, it seems a waste of time to resort to such sterile channels when all the vital disciplines of our culture lie at our disposal.</i><br /><br /><i>Why should we want to cultivate skills such as those I have listed?  There are many obvious reasons having to do with </i><b><i>quality of life</i></b><i>, with </i><b><i>professional competence</i></b><i>, with </i><b><i>the advance of culture and of society in general</i></b><i>, but I particularly wish to suggest a socio-political reason: </i><b><i>the education of an enlightened democractic citizenry</i></b><i>.  What capacities characterize such a citizenry?</i><br /><br /><b><i>Justice Learned Hand, the distinguished jurist of the precedeing generation, argued with telling irony that we would be able to preserve civil liberties only so long as we were willing to engage in the &#39;intolerable labor thought, that most distasteful of all our activities.&#39; John Dewey in Democracy and Education contends that &#39;The opposite to thoughtful action are routine or capricious behavior. Both refuse to acknowledge responsibility for the future consequences which flow from present action.&#39;</i></b><br /><br /><i>The requirements set by Barzun, Hand, and Dewey can be broken down to more fundamental components.  The sophisticated distinction between enlightened and short range self-interest is based on hypothetico-deductive reasoning.  Such reasoning is also inevitably involved in visualizing possible outcomes of decisions and policies in economic and political domains.</i><br /><br /><b><i>There is need to discriminate between facts and inferences in the contentions with which one is surrounded. There is the necessity of making tentative judgments or decisions, and it is better that this be done in full awareness of gaps in available information than in an illusion of certainty. There is the highly desirable capacity to ask critical, probing, fruitful questions concerning situations in which one has little or no expertise. There is the need to be explicitly conscious of the limits of one&#39;s own knowledge and understanding on a given issue.</i></b><br /><br /><i>Each of these capacities appears on the preceding list, and I believe that each can be cultivated and enhanced, at least to some degree, in the great majority of college students through properly designed experiences embracing a wide variety of subjects.</i><br /><br /><i>I hasten to emphasize that these skills alone are not sufficient to assure good citizenship or other desirable qualities of mind and person.  Other ingredients are necessary, not the least of which are moral and ethical values, which impose their own constraints on the naked processes of thinking and reasoning.  Although values are not disconnected from thinking and reasoning, the educational problems they pose transcend the limits of this short essay and require discussion in their own right.</i><br /><br /><b><i>13.4 EXISTING LEVEL OF CAPACTITY FOR ABSTRACT LOGICAL REASONING</i></b><br /><br /><i>In the United States some investigators have rather belatedly come to realize that much of our science curricular material, and the volume and pace with which we thrust it at our students, are badly mismatched to the existing levels of student intellectual development at virtually every age.  I am convinced that the same is true in other disciplines, but the fact is less readily discerned because assignments and tests concentrate on end results and procedures rather than on reasoning and understanding.</i><br /><br /><i>I say that &#39;some&#39; have become aware of this problem because, despite the unequivocal and relentlessly accumulating statistics, many who teach in the schools, colleges, and universities remain unaware of the emerging data; others fail to see any relevance to their own teaching.</i><br /><br /><i>Beginning about 1971, investigators began administering elementary tasks in abstract logical reasoning (such as those pioneered by Jean Piaget [see Piaget and Inhelder (1958)] in his studies of the development of abstract reasoning capacity in children) to adolescents and adults of college age and beyond [see, for example, Chiapetta (1976); McKinnon and Renner (1971)].  The tests have centered principally on arithmetical reasoning with ratios or division and on awareness of the necessity of controlling variables in deducing cause-effect relationship.</i><br /><br /><i>Although the results vary significantly from one population to another (economically disadvantaged versus economically advantaged; concentrating in science and engineering versus concentrating in humanities or fine arts versus concentrating in the social sciences, etc.), the overall averages have remained essentially unchanged with increasing volume of data since the first small samples were reported in 1971, and, most suggestively, the averages do not change appreciably with increasing age beyond about 12 or 13: Roughly one third of the total number of individuals tested solve the tasks correctly; roughly one third perform incorrectly but show a partial, incipient grasp of the necessity mode of reasoning; the remaining third fail completely.  In Piagetian terminology, the first group might be described as using formal patterns of reasoning, the third group as using principally concrete patterns, and the middle group as being in transition between the two modes [Arons and Karplus (1976)].</i><br /><br /><i>The weaknesses revealed by these two specific tasks would mean relatively little if they stood by themselves, but, in fact, these weaknesses are closely correlated with weaknesses in other modes of abstract logical reasoning such as discriminating between observation and inference; dealing with elementary syllogisms involving inclusion, exclusion, and serial ordering; recognizing gaps in available information; doing almost any kind of hypothetico-deductive reasoning.</i><br /><br /><i>Most of the curricular materials thrust at students in the majority of their courses at secondary and college level implicitly require well-developed reasoning capacity in the modes that have been listed in this discussion.  In fact, only a small proportion of the students (less than one third) are ready for such performance. </i><b><i>The rest, lacking the steady, supportive help and explicit exercises required, resort, in desperation, to memorization of end results and procedures.</i></b><i> Failing to develop the processes underlying critical thinking, they fail to have experience of genuine understanding and come to believe that knowledge is inculcated by teachers and consists of recognizing juxtapositions of arcane vocabulary on multiple choice tests.  (Readers familiar with studies of William G. Perry will recognize his first category of intellectual outlook among college students [Perry (1970].)</i><br /><br /><b><i>13.5 CAN CAPACITY FOR ABSTRACT LOGICAL REASONING BE ENHANCED?</i></b><br /><br /><i>In our Physics Education Group at the University of Washington, we have worked intensively for some years with populations of pre- and in-service elementary school teachers and other nonscience majors ranging in age from 18 to over 30.  Initially no more than about 10% were using formal patterns of reasoning.  By starting with very basic, concrete observations and experiences, forming concepts out of such direct experience, going slowly, allowing students to make and rectify mistakes by confronting contradiction or inconsistency, insisting that they speak and write out their lines of reasoning and explanation, repeating the same modes of reasoning in new contexts days and weeks apart, we have been able to increase the fraction who successfully use abstract patterns of reasoning to perhaps 70 to 90%, depending on the nature of the task.</i><br /><br /><b><i>The most important practical lesson we have learned is that repetition is absolutely essential -- not treading water in the same context until &#39;mastery&#39; is attained, but in altered and increasingly richer context, with encounters spread out over time.</i></b><i> Quick, remedial exercises in artificial situations preceding &#39;real&#39; course work are virtually useless.  One must patiently construct repeated encounters with the same modes of reasoning in regular course work and allow students to benefit from their mistakes.  Progress becomes clearly visible in the sense that the percentage of successful students increases with each repetition.</i><br /><br /><i>It is still a very long step from the development of specific abstract reasoning processes in one area of subject matter, such as elementary science, to more advanced levels of subject matter in the same area, not to speak of transfer to entirely different areas. </i><b><i>What little evidence exists suggests that very little transfer occurs from experience acquired in only one discipline. I myself am strongly convinced, however (mostly by fragmentary, anecdotal evidence, and perhaps some admixture of wishful thinking), that very great progress could be effected if students were simultaneously exposed to such intellectual experience in entirely different disciplines.  This is largely a matter of conjecture since an organized experiment at the college level has not really been tried ...&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"The Strong Inference Plus Method","summary":"When Scientific Controversies Persist or Theories Remain Speculative After Many Decades, Scientific Discourse & Research Should Switch to a Strong Inference-PLUS Approach / It's an Approach to Scientific Research Which Elaborates Chamberlain's Method of Multiple Working Hypotheses Into a Process for Handling the Most Difficult Scientific Questions","image":"https://lh3.googleusercontent.com/-7zxR5hxeX_U/V3AwKz0jlSI/AAAAAAAAIHg/FmldTDViCJw2EyqZAHT3y8hZkL-gNu9Sw/w7142-h9999/the-strong-inference-plus-method-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/SCjsAKhJpKh","publishDate":"2016-06-26T19:44:17.568Z","updateDate":"2017-01-03T02:28:37.355Z","text":"<b>The Strong Inference Plus Method: When Scientific Controversies Persist or Theories Remain Speculative After Many Decades, Scientific Discourse &amp; Research Should Switch to a Strong Inference-PLUS Approach / It&#39;s an Approach to Scientific Research Which Elaborates Chamberlain&#39;s Method of Multiple Working Hypotheses Into a Process for Handling the Most Difficult Scientific Questions</b><br /><br />References:<br /><br />[1] [2] <a href=\"http://www.internationalskeptics.com/forums/showthread.php?t=147374&amp;page=37\" class=\"ot-anchor\">http://www.internationalskeptics.com/forums/showthread.php?t=147374&amp;page=37</a><br /><br />[3] <a href=\"http://en.wikipedia.org/wiki/Strong_inference\" class=\"ot-anchor\">http://en.wikipedia.org/wiki/Strong_inference</a><br /><br />[4] <a href=\"http://pages.cs.wisc.edu/~markhill/science64_strong_inference.pdf\" class=\"ot-anchor\">http://pages.cs.wisc.edu/~markhill/science64_strong_inference.pdf</a><br /><br />[5] [6] <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2048741/\" class=\"ot-anchor\">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2048741/</a><br /><br />[7] [8] [9] [10] <a href=\"http://www.bus.ucf.edu/faculty/rfolger/file.axd?file=2012%2F11%2FDavis+%282006%29+Strong+inference.pdf\" class=\"ot-anchor\">http://www.bus.ucf.edu/faculty/rfolger/file.axd?file=2012%2F11%2FDavis+%282006%29+Strong+inference.pdf</a><br /><br />[11] <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2048741/\" class=\"ot-anchor\">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2048741/</a><br /><br />[12] <a href=\"http://pages.cs.wisc.edu/~markhill/science64_strong_inference.pdf\" class=\"ot-anchor\">http://pages.cs.wisc.edu/~markhill/science64_strong_inference.pdf</a><br /><br />[13] <a href=\"http://www.blc.arizona.edu/courses/schaffer/249/Darwin%20and%20Philosophers/ODonohue%20-%20Strong%20Inference.pdf\" class=\"ot-anchor\">http://www.blc.arizona.edu/courses/schaffer/249/Darwin%20and%20Philosophers/ODonohue%20-%20Strong%20Inference.pdf</a><br /><br />[14] <a href=\"http://pages.cs.wisc.edu/~markhill/science64_strong_inference.pdf\" class=\"ot-anchor\">http://pages.cs.wisc.edu/~markhill/science64_strong_inference.pdf</a><br /><br />[15] [16] [17] <a href=\"http://pages.cs.wisc.edu/~markhill/science64_strong_inference.pdf\" class=\"ot-anchor\">http://pages.cs.wisc.edu/~markhill/science64_strong_inference.pdf</a><br /><br />[18] <a href=\"http://www.bus.ucf.edu/faculty/rfolger/file.axd?file=2012%2F11%2FDavis+%282006%29+Strong+inference.pdf\" class=\"ot-anchor\">http://www.bus.ucf.edu/faculty/rfolger/file.axd?file=2012%2F11%2FDavis+%282006%29+Strong+inference.pdf</a><br /><br />[19] [20] [21] <a href=\"http://pages.cs.wisc.edu/~markhill/science64_strong_inference.pdf\" class=\"ot-anchor\">http://pages.cs.wisc.edu/~markhill/science64_strong_inference.pdf</a><br /><br />[22] <a href=\"http://arstechnica.com/science/2014/09/is-there-a-creativity-deficit-in-science/\" class=\"ot-anchor\">http://arstechnica.com/science/2014/09/is-there-a-creativity-deficit-in-science/</a><br /><br />[23] <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2048741/\" class=\"ot-anchor\">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2048741/</a><br /><br />[24] <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2048741/\" class=\"ot-anchor\">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2048741/</a><br /><br />[25] [26] <a href=\"http://www.bus.ucf.edu/faculty/rfolger/file.axd?file=2012%2F11%2FDavis+%282006%29+Strong+inference.pdf\" class=\"ot-anchor\">http://www.bus.ucf.edu/faculty/rfolger/file.axd?file=2012%2F11%2FDavis+%282006%29+Strong+inference.pdf</a><br /><br />[27] <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2048741/\" class=\"ot-anchor\">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2048741/</a><br /><br />[28] [29] <a href=\"http://pages.cs.wisc.edu/~markhill/science64_strong_inference.pdf\" class=\"ot-anchor\">http://pages.cs.wisc.edu/~markhill/science64_strong_inference.pdf</a><br /><br />[30] <a href=\"http://advan.physiology.org/content/31/1/93.full-text.pdf+html\" class=\"ot-anchor\">http://advan.physiology.org/content/31/1/93.full-text.pdf+html</a><br /><br />[31] <a href=\"http://www.bus.ucf.edu/faculty/rfolger/file.axd?file=2012%2F11%2FDavis+%282006%29+Strong+inference.pdf\" class=\"ot-anchor\">http://www.bus.ucf.edu/faculty/rfolger/file.axd?file=2012%2F11%2FDavis+%282006%29+Strong+inference.pdf</a><br /><br />[32] [33] [34] <a href=\"http://pages.cs.wisc.edu/~markhill/science64_strong_inference.pdf\" class=\"ot-anchor\">http://pages.cs.wisc.edu/~markhill/science64_strong_inference.pdf</a><br /><br />[35] <a href=\"http://www.bus.ucf.edu/faculty/rfolger/file.axd?file=2012%2F11%2FDavis+%282006%29+Strong+inference.pdf\" class=\"ot-anchor\">http://www.bus.ucf.edu/faculty/rfolger/file.axd?file=2012%2F11%2FDavis+%282006%29+Strong+inference.pdf</a><br /><br />[36] <a href=\"http://www.bus.ucf.edu/faculty/rfolger/file.axd?file=2012%2F11%2FDavis+%282006%29+Strong+inference.pdf\" class=\"ot-anchor\">http://www.bus.ucf.edu/faculty/rfolger/file.axd?file=2012%2F11%2FDavis+%282006%29+Strong+inference.pdf</a><br /><br />[37] <a href=\"http://pages.cs.wisc.edu/~markhill/science64_strong_inference.pdf\" class=\"ot-anchor\">http://pages.cs.wisc.edu/~markhill/science64_strong_inference.pdf</a><br /><br />[38] [39] <a href=\"http://www.bus.ucf.edu/faculty/rfolger/file.axd?file=2012%2F11%2FDavis+%282006%29+Strong+inference.pdf\" class=\"ot-anchor\">http://www.bus.ucf.edu/faculty/rfolger/file.axd?file=2012%2F11%2FDavis+%282006%29+Strong+inference.pdf</a><br /><br />[40] <a href=\"http://www.blc.arizona.edu/courses/schaffer/249/Darwin%20and%20Philosophers/ODonohue%20-%20Strong%20Inference.pdf\" class=\"ot-anchor\">http://www.blc.arizona.edu/courses/schaffer/249/Darwin%20and%20Philosophers/ODonohue%20-%20Strong%20Inference.pdf</a><br /><br />[41] <a href=\"http://www.blc.arizona.edu/courses/schaffer/249/Darwin%20and%20Philosophers/ODonohue%20-%20Strong%20Inference.pdf\" class=\"ot-anchor\">http://www.blc.arizona.edu/courses/schaffer/249/Darwin%20and%20Philosophers/ODonohue%20-%20Strong%20Inference.pdf</a><br /><br />[42] [43] <a href=\"http://www.bus.ucf.edu/faculty/rfolger/file.axd?file=2012%2F11%2FDavis+%282006%29+Strong+inference.pdf\" class=\"ot-anchor\">http://www.bus.ucf.edu/faculty/rfolger/file.axd?file=2012%2F11%2FDavis+%282006%29+Strong+inference.pdf</a><br /><br />[44] - [58] <a href=\"http://www.blc.arizona.edu/courses/schaffer/249/Darwin%20and%20Philosophers/ODonohue%20-%20Strong%20Inference.pdf\" class=\"ot-anchor\">http://www.blc.arizona.edu/courses/schaffer/249/Darwin%20and%20Philosophers/ODonohue%20-%20Strong%20Inference.pdf</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Two Systems of the Mind","summary":"Kahneman's Two Systems of the Mind Hypothesis is a Crucial Tool for Becoming a Better Thinker / The Idea is that the Mind is Two Minds in One / System 2 is the Rational Mind Which We Can Use to Solve Complex Problems / System 1, By Contrast, Employs a Form of Pattern-Matching / Questioning Assumptions Requires Taking Control of the Interaction of these Two Systems","image":"https://lh3.googleusercontent.com/-zrqk7VypiPo/V3AJ7_NiX9I/AAAAAAAAIGs/8vU7IvlR3zsaWFDw0aN9q0TEe0BwatxCA/w1800-h2520/the-two-systems-of-the-mind-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/YL5TQWx5U6a","publishDate":"2016-06-26T17:02:22.202Z","updateDate":"2017-01-03T02:29:28.215Z","text":"<b>The Two Systems of the Mind: Kahneman&#39;s Two Systems of the Mind Hypothesis is a Crucial Tool for Becoming a Better Thinker / The Idea is that the Mind is Two Minds in One / System 2 is the Rational Mind Which We Can Use to Solve Complex Problems / System 1, By Contrast, Employs a Form of Pattern-Matching / Questioning Assumptions Requires Taking Control of the Interaction of these Two Systems</b><br /><br /><b>A Psychological Perspective on Rationality</b><br />Daniel Kahneman<br />(Nobel Prize Winner in Economics)<br />2013 Arthur M Okun Public Policy Lecture<br />Yale University<br /><br /><a href=\"https://www.youtube.com/watch?v=MgQutgSwY88\" class=\"ot-anchor\">https://www.youtube.com/watch?v=MgQutgSwY88</a><br /><br />10:52:<br /><br /><i>&quot;I don&#39;t like the word, irrationality, because it conveys to me, you know, frothing at the mouth and a lot of things that are just not descriptive of humans. But, it is a fact that ... the rational agent model, which plays a central role in economics and other social sciences ... is built on the assumption that ... economic agents are rational -- or at least, are rational enough so that you can assume rationality as the basis of a lot of economic modeling.</i><br /><br /><i>And, in a way, this is obviously not true, and everybody knows it&#39;s not true. </i><b><i>And Amos Tversky used to say that all of the economists he knows are fully aware of their spouse not being rational or their dean not being rational. But, when it comes to economic agents, in principle, or in general, then they assume and defend the assumption of rationality.</i></b><br /><br /><i>I propose a very simple diagnostic for imperfect rationality. And when you look at a judgment or at a preference, you can represent it as some combination of considerations. Each of these considerations is assigned some kind of a weight, and it&#39;s a very general scheme for representing judgments and preferences. And </i><b><i>a failure of perfect rationality, in the broadest sense, is that the weighting is not quite right. That is, there are some considerations and some facts that are given too much weight -- more than they should have</i></b><i> -- and I&#39;m not going to define what is the correct weight, because that depends upon a lot of things. And there are many ways of defining correct weights. But, </i><b><i>once you define a correct weight in an acceptable way, then it turns out that people don&#39;t use those weights</i></b><i>, don&#39;t apply those weights. Considerations are over-weighted or under-weighted. And this is one, I think, convenient way of thinking of failures of rationality.</i><br /><br /><i>Most of the familiar definitions of failure of rationality are of this kind. You know, we commonly assume and recognize that </i><b><i>we over-weight the present over the future; that we over-weight losses over gains</i></b><i>; when they look at lottery tickets that specify an amount to be won, and you know that the probability is minuscule, </i><b><i>people over-weight the amount rather than the probability</i></b><i>, and they find those lottery tickets too attractive -- more attractive than they should -- if they were perfectly rational ...</i><br /><br /><b><i>People over-weight information that is currently available to them over information that they do not have. So, information that I need, but don&#39;t have, gets much less weight than information that I do happen to have.</i></b><br /><br /><b><i>People over-weight emotional factors ... They over-weight vivid examples over ... statistical description.</i></b><br /><br /><i>People under-weight certain things, and actually, one very general characteristic of the way that people think is that people under-weight statistical characteristics of ensembles. </i><b><i>They tend to think about individual cases.</i></b><i> And they focus on individual cases ...</i><br /><br /><i>There are other diagnostics. And I&#39;ll mention three of them. One that Amos Tversky and I did a lot of work on is violations of the rule that is called </i><b><i>rule invariance</i></b><i>. And the invariance rule specified that certain factors should be given weight of zero. So, for example, if you describe a cold cut of meat as being 20% fat or 80% fat-free, in terms of the substance that you are saying, you are saying exactly the same thing. And if people were assigning a monetary value to acquiring such a piece of such meat, then they should not be influenced by whether it is described by 20% fat or 80% fat-free. They ARE influenced by that, so they over-weight something that they should neglect. The content -- the meat itself, in this case -- is invariant. But, it&#39;s described in different ways. You should be indifferent to these ways. You should not be affected by them. But, in fact, all of us are.</i><br /><br /><i>So, this is one family of violations of rationality. There is another which I think is extremely important. And this is that ...</i><br /><br /><b><i>we tend to focus on the problem at hand. And we tend to focus very narrowly on the problem at hand</i></b><br /><br /><i>... And this takes the form of investors deciding on a particular investment, and very frequently without regard for their other investments, or for what else they have. People are making a decision without considering the fact that there will be further decisions in the future that, you know, they should be considering what they will do then when deciding what to do now. And it turns out people actually don&#39;t.</i><br /><br /><i>Then, there are failures of what I call objectivity, and that one isn&#39;t discussed a lot, but it&#39;s quite straightforward ...</i><br /><br /><b><i>We tend to think about reasoning as going from premises to conclusions. So, reasoning is directional. There are certain things that you assume, and then from those assumptions, you get somewhere. And it&#39;s supposed to be directional. When you see influences that go the other way -- that is, when people believe in arguments because they believe in the conclusions -- that is a very characteristic failure of perfect rationality. And it&#39;s one that we very commonly find.</i></b><br /><br /><i>Now, there has been a lot of work along those lines, documenting these types of violations of the rules of perfect rationality. And, economists in particular complain, and I think rightly so, about the work of psychologists in this area. And they accuse us, quite correctly, of not having a theory. Economists have theories. Psychologists don&#39;t. In fact, you could say without exaggeration that economists have theories, and psychologists tend to have lists. They have lists of phenomena. And, you know, long lists of phenomena. And, this is not considered elegant in some circles. And among my economist friends, it is not viewed kindly.</i><br /><br /><i>What psychologists do have -- and it is true, we really do not have a general theory -- we do have a lot of what I would call intermediate-level generalizations. We have facts that are not very limited facts. They have a certain range. But, they are not linked very closely to each other. There isn&#39;t a body -- a theory -- to hold all of this together.</i><br /><br /><i>Now, when I settled down to write the book, Thinking, Fast and Slow, it&#39;s in a way an attempt to go a little beyond what we have in psychology in general. </i><b><i>By trying to take a lot of phenomena, and trying to put them within a single unified framework.</i></b><i> Now, the framework, I must say, is nothing to be proud of. It&#39;s in fact a somewhat disreputable approach -- the one that I have taken. And I&#39;ve taken it very deliberately.&quot;</i><br /><br />20:00:<br /><br /><i>&quot;I speak in the book of two agents in the mind. I speak of </i><b><i>system 1</i></b><i> and </i><b><i>system 2</i></b><i> as if they were things, and actually, as if they were persons. I&#39;m trying to endow them with a certain character -- with propensities, with skills, with habits, with things that they tend to do, or are unable to do. So, that&#39;s the way that I go at it. Now, I really don&#39;t believe that there are such agents in the mind. This, by the way, is contrary not only to the rules of economics, because it&#39;s not a proper theory. It&#39;s actually contrary to the rules of doing proper psychology, because the psychologists learn not to explain behavior by assuming little people inside of the head. They&#39;re called homunculi. And you try not to explain the behavior of the agent by population inside the head. And I do precisely that. And I do it without shame. I will tell you why I do it.</i><br /><br /><i>I will speak of system 1, and </i><b><i>that&#39;s the quick one</i></b><i>. That&#39;s thinking fast. That&#39;s the </i><b><i>intuitive</i></b><i> one, and I&#39;ll try to describe it. And I&#39;ll speak of system 2. That&#39;s the </i><b><i>slow thinking</i></b><i> in the title. This is the </i><b><i>controlled</i></b><i> one, the </i><b><i>careful</i></b><i> one. And I&#39;ll speak of their interactions as if they&#39;re interacting, and sort of, one of them trying to control the other, and vice-versa. And of course I don&#39;t believe that there are such systems in the brain, or that there are these little people. So, why do I do it? Well, I&#39;m going to tell you the correct way of describing everything that I&#39;m going to say. When I say that system 1 does x, or system 2 does x, or system 2 cannot do x, what I really mean by that is, well, there are classes of behaviors. There are classes of mental activities that you could call type 1 and type 2. Now, psychologists have absolutely no objections about calling classes of behavior type 1 and type 2. Now, when I say that system 1 does all of the behaviors that are of type 1, and system 2 does all of the behaviors of type 2, so I&#39;ve invented an agent. Why have I invented an agent? You&#39;re going to find out, actually, within the next hour, and I&#39;m quite sure that I&#39;m right. This makes it easy to think about ... psychology ... because our minds appear to be wired for processing information about agents. We&#39;re just very, very good at endowing agents -- you know, animal, human -- at endowing agents with traits, with propensities, with skills. And once we have endowed an entity with a propensity or with a trait, we tend to remember them. And eventually, when we have a rich description of an agent, that agent does develop a sort of personality. And then we&#39;re able to, in intuitive ways, of guessing how that agent will behave in other circumstances. This is really, I believe, some thing we have that is built in, that we can do. And I&#39;m deliberately building on that. This is the way I think when I think of system 1 and system 2. I can translate that into proper language -- type 1 and type 2 -- any time. So could you. By and large, I think, unless you&#39;re trying to impress some purists, it&#39;s not worth the effort. You can just go on thinking in terms of system 1 and system 2, and of the psycho-drama between them.&quot;</i><br /><br />23:45:<br /><br /><i>&quot;So, let me tell you a bit about the two systems. I&#39;ll start with system 2. System 2, that&#39;s the slow one. And ... basically you can think of it as </i><b><i>the system which engages in effortful activities</i></b><i>. Type 2 activities, if you will, are </i><b><i>activities that demand effort</i></b><i>. Now, how do we know that they demand effort? Well, there are two main tests: One is physiological. There are many manifestations of effort, including dilation of the pupil, heart rate quickens, lots of things happen ... You can tell when people are exerting mental effort.</i><br /><br /><i>And the main diagnostic is that </i><b><i>when people exert effort in one activity, they cannot perform other activities at the same time</i></b><i>. So ... my diagnostic is that, if it is something that you cannot do while making a left turn into traffic, then it&#39;s effortful. Because when you make a left turn into traffic, you stop talking. And if people have any sense who are sitting next to you, they stop talking. And that&#39;s why speaking on the cell phone is so dangerous when you drive, it&#39;s because the people who talk to you don&#39;t know that you&#39;re overtaking a car or that you&#39;re making a left turn into traffic. So ... we have what is called a limited capacity for effort, which means that if we do one thing, there are other things we cannot do. We cannot do several effortful things at the same time.</i><br /><br /><b><i>Effortful activities are experienced, by and large, as MY activities ... There is an impression -- a feeling -- of agency attached to most of these effortful activities</i></b><i>. This is something that I do. So, multiplying two numbers is effortful. Focusing intently on a particular spot, and trying not to let attention wander. That&#39;s effortful. Being polite to somebody you don&#39;t like, that&#39;s effortful. We know about these things because they strain the physiology. </i><b><i>They leave people tired</i></b><i>. And there are many indications that when people are tired, after exerting effort, they find it more difficult to exert effort in other activities. So, this is what system 2 does. And, </i><b><i>by and large, when we think of our minds, we think of what system 2 does</i></b><i>. We think of ourselves as </i><b><i>thinking</i></b><i> people, as </i><b><i>deliberate</i></b><i> people, </i><b><i>as going in an orderly way from one state of mind to another -- and of making choices and making decisions</i></b><i>, and </i><b><i>of being quite conscious</i></b><i>, because what goes on unconsciously, well, we&#39;re not conscious of. So, the little that we see of our internal life, that&#39;s largely system 2. That&#39;s the thesis of the book, actually.</i><br /><br /><b><i>We greatly exaggerate the importance of system 2 in our lives and in human affairs in general.&quot;</i></b><br /><br />27:15:<br /><br /><i>&quot;Now, what is system 1? This is the fast thinking of the title, Thinking, Fast and Slow, and mostly I&#39;ll talk about one aspect of system 1, which is memory. You know, we have this marvelous organ between our ears. </i><b><i>It makes sense of the world second by second, so that anything that happens, we sort of understand</i></b><i>. We get the right ideas about it. </i><b><i>We anticipate what comes next</i></b><i>. All of this happens courtesy of that thing that we have, our </i><b><i>associative memory</i></b><i>. It creates an interpretation of the world as we go, so I see that scene. You see me. This is really not something that we choose to see. I don&#39;t choose to see you. </i><b><i>It happens to you</i></b><i>. And this is the characteristic experience that is associated with system 1, with automatic activities. They happen to you. You don&#39;t do them. They happen.</i><br /><br /><i>And, so, </i><b><i>events and stimuli from the outside world, they create sensations automatically, they evoke memories automatically</i></b><i>. So, if I say 2 plus 2, a number came to your mind. You didn&#39;t have to ask for it. You didn&#39;t have to decide to perform that computation. You couldn&#39;t help carrying out that computation to save your life, any more than you could help yourself from reading a word on the billboard. Those things just happen.</i><br /><br /><i>But, system 1 doesn&#39;t merely call for well-learned associations of this kind. If I ask you, well, you think it was really pre-ordained that World War I ... would occur? And, for those of you who know anything about the history of World War I, this instantly brought a lot of information to mind. That happened automatically. You didn&#39;t ask for it. We have not only an interpretation of the world as it exists, but we can call out complex stories that would take a long time to describe.</i><br /><br /><b><i>And, in a way, they all come at once.</i></b><br /><br /><i>So, you know, if I ask you what are the chances of any changes to the rules of filibusters in the next ten years, something will happen to you. You know a fair amount about this. Now, articulating it will be effortful, but a lot will happen without you having to think about it. </i><b><i>And these things really happen very fast.</i></b><br /><br /><i>My favorite example which is, of course, in the book ... is of ... I won&#39;t try to imitate it. It&#39;s a male voice speaking in an upper-class British accent, and saying a sentence, and the sentence -- while ... events in the individual&#39;s brain are recorded continuously, so that&#39;s the background. That&#39;s the story -- the sentence is &#39;I have large tattoos all down my back&#39;. It takes about a third of a second, and you get a characteristic brain response, and the characteristic brain response is surprise. And it has a clear signature. If you think about what it takes for someone to be surprised by such an event, it is really extraordinary. You&#39;ve got to identify the voice as an upper-class British voice. You&#39;ve got to bring up the stereotype of upper-class British male. You&#39;ve got to bring up the idea that it&#39;s unlikely that many of them have large tattoos down their back. That is an incongruity, hence a surprise. And that is detectable within less than half a second. This is amazing. And that is ... system 1. All of this happens unbidden. You don&#39;t have to think about it. This just happens to you. And a lot of complicated things like that.&quot;</i><br /><br />32:00:<br /><br /><i>&quot;System 1 is not dumb, so ... You know, we tend to think of system 2 as the higher system, but </i><b><i>what we do really well, we do with system 1, because skills are system 1</i></b><i>. So, driving -- </i><b><i>when we drive, it&#39;s almost effortless</i></b><i> -- except when we take left turns into traffic, or overturn cars on a narrow road -- almost effortless, you can talk when you drive. But, so that&#39;s system 1. It&#39;s a complicated activity, and it is an activity that we perform almost without effort, and automatically. And there are many such activities.</i><br /><br /><i>They&#39;re intuitive skills. We are very impressed by intuitive skills. We&#39;re impressed by the skills of firefighters, and chess players, and physicians, great diagnosticians. But, all of us have skills at that level, and many of them -- especially in the social domain. We can recognize each others&#39; moods. We can say to someone, you&#39;re tired. You know, we can recognize a spouse&#39;s mood from one word on the telephone, and you know, many other things from that general nature. The skills are in system 1, in that automatic memory system that interprets the world. So, our mind --</i><br /><br /><b><i>what we think of as our mind, the real products of our mind -- most of it is system 1. Most of it happens through events that are experienced passively as, you know, my memory provides that information. Then, we do something with it. We articulate it. We respond with it. We elaborate on it, and that tends to be system 2.</i></b><br /><br /><i>So, what&#39;s the image that I&#39;m drawing here? Many people have two systems. Psychologist Jonathan Haidt had a very similar idea, and he speaks of the elephant and the writer.</i><br /><br /><b><i>I propose another image, and it&#39;s the image of a ... whole bunch of reporters and an editor. And the reporters are system 1. And the editor is system 2. And what happens is the reporter provides stories, and the stories are ready-made. But, they haven&#39;t been approved yet. They haven&#39;t been endorsed yet. And then there is the editor. The editor is busy. But the editor just has time to have a quick look at the story, and if it&#39;s okay, it goes to print. It gets endorsed. And a LOT of what system 2 does, in my view, is endorse system 1.</i></b><br /><br /><i>And then ... you know, vague impressions become beliefs, and sort of vague tendencies become decisions. There is a lot that happens when system 2 becomes involved, </i><b><i>but the action in many of these cases comes from system 1.&quot;</i></b><br /><br />35:30:<br /><br /><b><i>&quot;So, in many cases, the editor just, you know, checks the story, and off it goes. It gets endorsed. In some cases, the editor stops the story, and either for a minor rewrite, or completely suppressed. We better not publish this. That happens a lot</i></b><i> ... If you&#39;re talking with someone, you know, and you have that irresistible thought that this is a real idiot, it&#39;s unlikely that you&#39;re going to say it. And by the way, it&#39;s effortful not to say it in some cases. So, this is system 2. </i><b><i>The inhibition is system 2, and it&#39;s the editor suppressing something. Quite often, the editor may say, well, let&#39;s have another story. Let&#39;s have another take on this. Or, let me reason very slowly, and then you have the editor himself or herself going over the story and recreating it</i></b><i>. So, that&#39;s ... system 1.</i><br /><br /><i>And, let me add something else about that editor -- an essential characteristic of system 2 ...</i><br /><br /><b><i>System 2 is really lazy, and it is really governed by a rule of least effort. And there is a LOT of evidence to that effect. Now, there are large individual differences in how lazy system 2 is. It&#39;s much lazier for some than for others ...</i></b><br /><br /><i>... Shane came up with a very good example, and the example which, I&#39;m sure many of you are familiar with, is a puzzle. And, the puzzle is the following: A bat and a ball, together, cost a dollar ten. The bat costs a dollar more than the ball. How much does the ball cost? And the beauty of this example is that a number comes to everybody&#39;s mind. It&#39;s ten cents. And this is an associative reaction, and, you know, most people -- including those who don&#39;t say ten cents, ultimately, that&#39;s the first response that comes to mind ... The ... ten cents is wrong. And it&#39;s very easy to see that ten cents is wrong, because if the ball costs ten cents, then the bat costs a dollar and ten cents. The total is $1.20, so that&#39;s false.</i><br /><br /><i>Now, I don&#39;t know the proportion at Yale, but I suppose it couldn&#39;t be higher than at Princeton. About 50% of Princeton students get that wrong, in writing, in a written test. They just say ten cents.</i><br /><br /><b><i>Now, this is, I think, a VERY important discovery. And it&#39;s an important fact, because there is something basic that you know about everybody who wrote ten cents: They didn&#39;t check.</i></b><br /><br /><i>... You know, if they had checked, they would have left it blank, they would have found the correct solution -- which is 5 cents, if you&#39;re still working on it ... They would have done something else. Something else would have happened.</i><br /><br /><b><i>So, system 2 ... does as little as possible.</i></b><i> That&#39;s sort of the image that I&#39;m proposing. You have those two systems. System 2 is in control. It can suppress, like an editor ... You know, not now, or not at all, not this story, or let&#39;s do something else, or let&#39;s recompute. But, by and large, the action comes from system 1 ... Proposing, suggesting, having impressions, having feelings that become emotions that become intentions that become decisions that become beliefs. And if we want to understand how the mind works, actually it turns out that we had better understand what are the operating rules of system 1.&quot;</i><br /><br />40:15:<br /><br /><i>&quot;... There is a basic heuristic that you can follow in trying to predict behavior. It doesn&#39;t always work. Heuristics don&#39;t always work ...</i><br /><br /><b><i>But, it works a fair amount of the time, which is to ask: What is the first reaction? What&#39;s the first reaction to a situation, to a problem, to a word? The first reaction is very diagnostic is a system 1 reaction. It&#39;s the first emotion. It&#39;s whether you approve or disapprove. It&#39;s whether you want to approach or avoid. There are many things which happen instantly which are the first reaction, and either that first reaction will be carried over directly into beliefs and actions and so on, or it will ... provide the anchor for what happens next, so that even if if is not really endorsed, it is still influential. Even if system 2, the editor, has assigned somebody else to rewrite the story, something of the original story is still there. And so, as a heuristic, it&#39;s a useful heuristic.</i></b><br /><br /><i>Now, let me talk a bit about the operating characteristics of system 1. And what I will do is mainly focus on characteristics that explain failure of perfect rationality.</i><br /><br /><b><i>I think of system 1 as a storyteller. And I&#39;m using a lot of metaphors, but by storytelling, what I mean is, system 1 sees connections between events. And you have to think of ... memory as, you know, a vast network of ideas which are linked associatively, which are linked not by mere associations, but by specific links, from cause to effect, from token to type ...</i></b><br /><br /><i>And what we have with that machine, it looks for connections. And it does this automatically. One of my best examples is one I learned from Nassim Taleb. It&#39;s in The Black Swan. And it talks about Bloomberg News on the day that Saddam Hussein was captured. And I don&#39;t recall the exact sequence, but at one stage, the bond market went up. And at that stage, the ... very big headline was: Saddam Hussein creates fears in the markets ... the bond market goes up. Then, the bond market went radically down. And there was a story: Saddam Hussein&#39;s capture reassures the public, the bond market went down.</i><br /><br /><i>What happens -- this is completely typical -- you have a fact, and the fact is something happened in the market. It went up or it went down. It looks for an explanation. In memory, there is a search for an explanation. The explanation has to be an event that is possibly, potentially, causally related, and that happened earlier, and that is sufficiently salient and surprising to be the cause. Now, here, you get the bond market doing something, and anything that was going to happen to the bond market that day was going to be attribute to Saddam Hussein, because that was an event which was surprising and sort of consequential. And that&#39;s the way it works.</i><br /><br /><i>So, we tend to tell stories. Those stories, we don&#39;t even have to tell them. They occur to us in that way ...</i><br /><br /><b><i>The world comes interpreted. And it comes interpreted in causal terms.</i></b><br /><br /><i>... This emphasis on causality ... I can&#39;t emphasize causality enough, because it really is a characteristic of the way that the system works -- looking for causes, inferring causes, and working from causes forward.</i><br /><br /><i>So, it tells stories. Then there is something else that happens that is a characteristic of system 1, I think, which is that ...</i><br /><br /><b><i>it tells the best possible story. So, it tells a story that is coherent. And I call that associative coherence. It makes sense ... It makes sense ... for an individual to be all good or all bad.</i></b><br /><br /><i>... We really don&#39;t like the idea of Hitler loving little children or flowers, when in fact he did ... This bothers us because it is not emotionally consistent. It is not associatively consistent. We tend to look for stories that have that form of consistency and coherence.</i><br /><br /><b><i>Now, you can make a coherent story from VERY little information. In fact, the less information, in some cases, the easier it is to make a coherent story. Now, what matters here is that subjective confidence -- the comfort people have -- appears not to be determined by the amount of information. The confidence that people have in their impressions in the stories that system 1 is telling them, the confidence is determined by the associative coherence of the story -- by whether the story makes internal sense. If we have a good story, we feel confident in it. It&#39;s the internal contradictions that lower our confidence. This is radically different from the rational way of assigning probability to an event, or to a story, or to a hypothesis.</i></b><br /><br /><i>When we rationally assign a probability to a hypothesis, we weigh the evidence. But, here, it&#39;s not about the evidence. </i><b><i>It&#39;s about the internal coherence. It is really not about the AMOUNT of evidence. We see that all over the place in research: Radical insensitivity to the size of samples, radical insensitivity to the overall amount of evidence. We have ... I&#39;ve described it as a machine for jumping to conclusions ...</i></b><br /><br /><i>I asked somewhere about someone who is a national leader. And that national leader is intelligent and firm. Now, is she a good one? And actually, you have an answer ... Intelligent and firm, that&#39;s really good. You&#39;re already there. You have an answer. You have an answer as you go, you are evaluating it. Now, if the third word had been corrupt, you&#39;d change your mind ... And by the way, you wouldn&#39;t change your mind enough, because it would have been worse if I had first said she was corrupt ...</i><br /><br /><i>We jump to conclusions. We form immediate impressions on the basis of very little evidence. This will cause a failure of perfect rationality, and many failures have that character. </i><b><i>System 1 tends to suppress ambiguity, and not to recognize ambiguity</i></b><i>. So, I have many examples of that, but the one I use most often is, &#39;Anne approached the bank&#39;. And when you hear, &#39;Anne approached the bank,&#39; most of you think of the bank as a financial institution. But, if the previous sentence had been something about floating down the river, the word bank would mean something else. And ... really the sentence, &#39;Anne approach the bank&#39; is ambiguous. The ambiguity is suppressed. Most of you, I suppose, see more banks than rivers, and ... something gets decided. You don&#39;t do it. It happens to you. Ambiguity is suppressed. </i><b><i>A solution is adopted</i></b><i>, and this is very characteristic of system 1. So, there is a suppression of ambiguity and telling the best story possible ...</i><br /><br /><b><i>This has important implications. It has the implication that we live in a world that is radically simpler than the real world. We simplify the world as we construct stories about it.&quot;</i></b><br /><br />49:50:<br /><br /><i>&quot;Now, I spoke of associative coherence, and let me develop that a little bit. And I already mentioned something that psychologists call the Halo Effect, which is that </i><b><i>the various traits that we&#39;ve assigned to an individual or to a group or to society tend to be emotionally coherent. They tend to make sense together in emotional terms.</i></b><br /><br /><i>But, there are other manifestations to that. So, here&#39;s an experiment ... In the experiment, people are required to evaluate the validity of a line of reasoning, the validity of a syllogism. And the syllogism is: All roses are flowers, some flowers fade quickly, therefore some roses fade quickly. Valid or invalid? it is not valid. For those that are still working on it, it is not valid because it is entirely possible that all of the flowers that fade quickly are not roses. But, it is true that some roses fade quickly.</i><br /><br /><i>Now, I forget the exact number, but it&#39;s well over 70% of undergraduates say the reasoning is valid. Now, this is enormously important as a finding, because it tells us something about the way we think in politics, for example.</i><br /><br /><b><i>We have a conclusion -- or we believe that the conclusion is true -- and that is enough to make us believe in arguments that favor that conclusion. This is a violation in what I call objectivity. Here, the reasoning goes the wrong way, from conclusion to ...</i></b><br /><br /><i>This characteristic, it explains something which seems at first to be the opposite of jumping to conclusions ...</i><br /><br /><b><i>It is why, in a serious sense, it is really almost impossible to change peoples&#39; minds about things that matter to them.</i></b><i> And if you ask yourself why you believe in what you believe -- be it religion or global warming or, you know, in whatever else you believe, why do you believe it?</i><br /><br /><b><i>We have been socialized to think that we believe in things because there is evidence to support them. But, this is not the way that we actually form beliefs. We believe in these things because we trust other people who believe in them. That&#39;s the main reason that we have religious beliefs. We&#39;re not waiting for evidence to have those beliefs. But, what there is, is there is an emotional connection -- a powerful emotional connection between love and trust and belief, and other people believing.</i></b><br /><br /><i>And, what is really very important, I think -- I had the occasion to speak about that to the National Academy of Sciences -- why there is difficulty in conveying scientific evidence to the public -- that ...</i><br /><br /><b><i>scientists tend to think that we believe in things because there is evidence to support them. This is NOT the way that most people believe in things. Actually, it&#39;s not even true of all scientist always, but it certainly is not true of the public that scientists are trying to address. Evidence really doesn&#39;t really seem to work.</i></b><br /><br /><b><i>And WHY evidence doesn&#39;t seem to work seems to be that the kinds of evidence that scientists bring does not resonate with system 1. System 1 needs stories, preferably stories about individual cases, preferably clear causal stories that have emotional impact. It&#39;s very unfortunate in a way, but that&#39;s the way I think things are.&quot;</i></b><br /><br />54:20:<br /><br /><b><i>&quot;So ... the same system 1 that makes us jump to conclusion when there is very little evidence makes us extraordinarily resistant to change -- not when there is a lot of evidence, but when there is an ingrained collection of attitudes, and emotions, and beliefs. They are going to be held, even against massive opposing evidence.</i></b><br /><br /><i>Another characteristic of system 1: When we ask people a question, they tend to do more than we ask them to do. And there was an experiment, when I was 40 years old that had a big impact on me, where people listened to words on speakers, and they were asked to make rhyming judgments. Do the words rhyme or not? And here are two pairs of words: And one is vote, note. They rhyme. And the other is vote goat. Well, they rhyme too, but people are much slower saying that vote goat rhymes than vote, note. So, what did they do?</i><br /><br /><i>They heard the word. They spelled it, and the mismatch on the spelling slowed them down in detecting the match on the rhyming. They did something entirely superfluous, or a computation was done which was entirely unnecessary. This is the way that the system works. It works in parallel. It generates not sequentially, like ordinary reasoning. </i><b><i>But, a lot of things happen in parallel. And that one question evokes much more than one answer.</i></b><i> In the same way, people that are asked about the syllogism, is it valid? But, what happened was that system 1 immediately detected, it is true, it is true! And true and valid are associatively correlated, and it&#39;s that correlation that cause people to answer the question, valid, incorrectly.</i><br /><br /><i>By the way, the same undergraduates who failed the question about roses, if you formulate exactly the same question in terms of x&#39;s and y&#39;s, they answer it with no difficulty whatsoever, and correctly -- most of them. </i><b><i>So, it is the case that when an incorrect answer comes very readily to mind -- incorrect answer being a correct answer to a different question -- we tend to adopt ... And we answer a question that we haven&#39;t been asked. This turns out to be quite an important mechanism in judgment and decision-making ...</i></b><br /><br /><i>I speak of a young woman named Julie, and she is a graduating senior. And I&#39;m going to tell you one fact about Julie, which is that she read fluently when she was age 4. And now I&#39;m asking you: What&#39;s her GPA? Now, the striking thing is -- that, actually, although you may be embarrassed to admit it, a number came to mind. You have an idea of her GPA, on the basis of that information. You know, it&#39;s not precise. But, you know, it&#39;s certainly over 3.2, and it&#39;s, you know, less than 3.9. You know, it&#39;s actually -- agreement on those kinds of things is remarkably close, because we are in the same culture, and we understand each other.</i><br /><br /><i>Now, we absolutely -- I think -- understand the mechanism that creates this impression. And the mechanism is that when we hear that she read fluently at age 4, we have an impression of how precocious she was. Now, we weren&#39;t asked how precocious was Julie. We were asked what&#39;s her GPA now, 20 years later or something. But, you can go from the answer to one to the answer to the other directly, and it turns out that peoples&#39; answer to what&#39;s her GPA ... Her GPA is about as extreme as the judgment of how precocious she was in her reading. That&#39;s the way that people do this. Do is not the right word. That&#39;s the way that things happen in peoples&#39; mind. We have a matching rule, and ... you could match Julie&#39;s reading age to a lot of things ... Like to how tall a building ... in New Haven would be as tall as Julie&#39;s reading speed, people would give you a number of floors for that building. That is a fundamental ability of system 1, and one that is often used to answer questions.&quot;</i><br /><br />1:00:05:<br /><br /><i>&quot;I think you get the point that I&#39;m trying to make, which is that we have an absolutely wonderful mind. You know, it&#39;s just incredibly marvelous what it can do. You know, it can learn to drive. It can learn to identify, you know, that a particular combination of a voice and a statement is surprising. It can do a lot of great things. It can provide an immediate diagnosis. It can identify very good move in chess. Intuition, in short, is possible. Intuition is marvelous.</i><br /><br /><i>But, we also have a mind that is really incompatible with the basic requirements of rationality as put forward, as explained in decision theory, or the basis of economics. </i><b><i>We have a mind that is not equipped for invariance</i></b><i>, because your reaction to fat-free or to fat food is not going to be the same. It is a mind that is not equipped for dealing adequately with quantity of evidence, because we tend to make up stories, and evaluate the coherence of these stories.</i><br /><br /><i>And so, the verdict is really not a negative verdict on the human mind. It&#39;s a complicated verdict. But, it&#39;s clear that the theory of rationality within decision theory, or within the standard models of what rationality you should think of, is profoundly non-psychological. The psychological truth -- you know, we don&#39;t know the truth -- but what I try to summarize in this book is not compatible with the rational model.</i><br /><br /><i>What I&#39;ve presented to you is not a theory. But, I hope it&#39;s a little bit on the way to a theory, that there is some internal coherence to the story that I&#39;ve been telling you. And so, even if the story is not good -- if it is coherent -- then you might believe it, if I am lucky.</i><br /><br /><i>So, thank you.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"When Can We Trust Expertise?","summary":"Daniel Kahneman Received the Nobel in Economics for His Two Systems of the Mind Hypothesis / His Book ","image":"https://lh3.googleusercontent.com/-wheiHRFKpQs/V27wiu9ztsI/AAAAAAAAIFk/auiL-oXC9ScBAHWsLosEquoaCHpL_sj9w/w1800-h2520/when-experts-can-be-trusted-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/3Q72PNoCCNH","publishDate":"2016-06-25T20:59:31.496Z","updateDate":"2017-01-03T02:29:13.399Z","text":"<b>When Can We Trust Expertise?: Daniel Kahneman Received the Nobel in Economics for His Two Systems of the Mind Hypothesis / His Book </b><b><i>Thinking, Fast and Slow</i></b><b> Has Become a Classic / The Advice He Provides on Expertise is Surprisingly Actionable, and It Will in Due Time Prove to be a Valuable Metric for Evaluating Expert Claims</b><br /><br />(Do not worry too much about what System 1 and 2 are for now; we&#39;ll be getting to that next ...)<br /><br /><i>Thinking, Fast and Slow</i><br />From Chapter: &quot;Expert Intuition: When Can We Trust It&quot;<br />Daniel Kahneman<br /><br /><i>&quot;If subjective confidence is not to be trusted, </i><b><i>how can we evaluate the probable validity of an intuitive judgment? When do judgments reflect true expertise? When do they display an illusion of validity?</i></b><i> The answer comes from the two basic conditions for acquiring a skill:</i><br /><br />- <b><i>an environment that is sufficiently regular to be predictable</i></b><br />- <b><i>an opportunity to learn these regularities through prolonged practice</i></b><br /><br /><b><i>When both these conditions are satisfied, intuitions are likely to be skilled.</i></b><i> Chess is an extreme example of a regular environment, but bridge and poker also provide robust statistical regularities that can support skill. Physicians, nurses, athletes, and firefighters also face complex but fundamentally orderly situations. The accurate intuitions that Gary Klein has described are due to highly valid cues that the expert’s System 1 has learned to use, even if System 2 has not learned to name them. In contrast, stock pickers and political scientists who make long-term forecasts operate in a zero-validity environment. Their failures reflect the basic unpredictability of the events that they try to forecast.</i><br /><br /><b><i>Some environments are worse than irregular. Robin Hogarth described &#39;wicked&#39; environments, in which professionals are likely to learn the wrong lessons from experience.</i></b><i> He borrows from Lewis Thomas the example of a physician in the early twentieth century who often had intuitions about patients who were about to develop typhoid. Unfortunately, he tested his hunch by palpating the patient’s tongue, without washing his hands between patients. When patient after patient became ill, the physician developed a sense of clinical infallibility. His predictions were accurate — but not because he was exercising professional intuition!</i><br /><br /><i>Meehl’s clinicians were not inept and their failure was not due to lack of talent. </i><b><i>They performed poorly because they were assigned tasks that did not have a simple solution</i></b><i>. The clinicians’ predicament was less extreme than the zero-validity environment of long-term political forecasting, but they operated in </i><b><i>low-validity situations that did not allow high accuracy</i></b><i>. We know this to be the case because the best statistical algorithms, although more accurate than human judges, were never very accurate. Indeed, the studies by Meehl and his followers never produced a &#39;smoking gun&#39; demonstration, a case in which clinicians completely missed a highly valid cue that the algorithm detected. An extreme failure of this kind is unlikely because human learning is normally efficient. If a strong predictive cue exists, human observers will find it, given a decent opportunity to do so. Statistical algorithms greatly outdo humans in noisy environments for two reasons: they are more likely than human judges to detect weakly valid cues and much more likely to maintain a modest level of accuracy by using such cues consistently.</i><br /><br /><i>It is wrong to blame anyone for failing to forecast accurately in an unpredictable world. However, it seems fair to blame professionals for believing they can succeed in an impossible task. </i><b><i>Claims for correct intuitions in an unpredictable situation are self-delusional at best</i></b><i>, sometimes worse. </i><b><i>In the absence of valid cues, intuitive &#39;hits&#39; are due either to luck or to lies</i></b><i>. If you find this conclusion surprising, you still have a lingering belief that intuition is magic. </i><b><i>Remember this rule: intuition cannot be trusted in the absence of stable regularities in the environment.</i></b><br /><br /><i>Some regularities in the environment are easier to discover and apply than others. Think of how you developed your style of using the brakes on your car. As you were mastering the skill of taking curves, you gradually learned when to let go of the accelerator and when and how hard to use the brakes. Curves differ, and the variability you experienced while learning ensures that you are now ready to brake at the right time and strength for any curve you encounter. The conditions for learning this skill are ideal, because you receive immediate and unambiguous feedback every time you go around a bend: the mild reward of a comfortable turn or the mild punishment of some difficulty in handling the car if you brake either too hard or not quite hard enough. </i><b><i>The situations that face a harbor pilot maneuvering large ships are no less regular, but skill is much more difficult to acquire by sheer experience because of the long delay between actions and their noticeable outcomes. Whether professionals have a chance to develop intuitive expertise depends essentially on the quality and speed of feedback, as well as on sufficient opportunity to practice.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"Why It's Probably Not a Conspiracy","summary":"Edward Bernays Discovered that it is Possible to Persuade People to Behave Irrationally if You Link Products or Ideas to their Emotional Desires and Feelings / The Story of the Adaptation of German Propaganda Into American Public Relations is an Important One, in Part Because It Shows that Groups of People Can Be Convinced to Behave Against their Own Rational Interests / It's Not a Claim that there Are No Conspiracies, But People Who Are Quick to Point to Conspiracies Should Look Here, Inwards, First","image":"https://lh3.googleusercontent.com/-uMfl2cp3qxM/V27Vv1DdQzI/AAAAAAAAIEs/gDuGyIcAEocMouBedBcJzndpqXAO_4r-w/w1800-h2520/why-its-probably-not-a-conspiracy-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/DgeDsg8vdud","publishDate":"2016-06-25T19:05:15.261Z","updateDate":"2017-01-03T02:30:11.837Z","text":"<b>Why It&#39;s Probably Not a Conspiracy: Edward Bernays Discovered that it is Possible to Persuade People to Behave Irrationally if You Link Products or Ideas to their Emotional Desires and Feelings / The Story of the Adaptation of German Propaganda Into American Public Relations is an Important One, in Part Because It Shows that Groups of People Can Be Convinced to Behave Against their Own Rational Interests / It&#39;s Not a Claim that there Are No Conspiracies, But People Who Are Quick to Point to Conspiracies Should Look Here, Inwards, First</b><br /><br /><b>Century of the Self Transcript - Part 1 - Happiness Machines</b><br /><a href=\"https://www.youtube.com/watch?v=eJ3RzGoQC4s\" class=\"ot-anchor\">https://www.youtube.com/watch?v=eJ3RzGoQC4s</a><br /><br /><i>&quot;A hundred years ago a new theory about human nature was put forth by Sigmund Freud. He had discovered he said, primitive and sexual and aggressive forces hidden deep inside the minds of all human beings. Forces which if not controlled led individuals and societies to chaos and destruction.</i><br /><br /><i>This series is about how those in power have used Freud&#39;s theories to try and control the dangerous crowd in an age of mass democracy.</i><br /><br /><i>But the heart of the series is not just Sigmund Freud but other members of the Freud family.</i><br /><br /><i>This episode is about Freud&#39;s American nephew </i><b><i>Edward Bernays</i></b><i>.</i><br /><br /><i>Bernays is almost completely unknown today but his influence on the 20th century was nearly as great as his uncles. Because </i><b><i>Bernays was the first person to take Freud&#39;s ideas about human beings and use them to manipulate the masses. He showed American corporations for the first time how to they could make people want things they didn&#39;t need by linking mass produced goods to their unconscious desires.</i></b><br /><br /><i>Out of this would come a new political idea of how to control the masses. </i><b><i>By satisfying people&#39;s inner selfish desires one made them happy and thus docile</i></b><i>. It was the start of the all-consuming self which has come to dominate our world today.</i><br /><br /><i>Part One</i><br /><br /><i>Happiness Machines</i><br /><br /><i>Freud&#39;s ideas about how the human mind works have now become an accepted part of society. As have psychoanalysts.</i><br /><br /><i>Every year the psychotherapists ball is held in a grand place in Vienna.</i><br /><br /><i>&#39;This is the psychotherapy ball. Psychotherapists come, some advanced patients come, former patients come, and many other people - friends as well as people from the Viennese society who like to come to a nice elegant comfortable ball.&#39; - Dr. Alfred Fritz, President World Council for Psychotherapy</i><br /><br /><i>But it was not always so. A hundred years ago Freud&#39;s ideas were hated by Viennese society. At that time Vienna was the center of a vast empire leading central Europe. And to the powerful nobility of the Hoffman accord Freud&#39;s ideas were not only embarrassing, but the very idea of examining and analyzing ones inner feelings was a threat to their absolute control.&quot;</i><br /><br />Countess Erzie Karolyi - Budapest: <i>&quot;&#39;You see at that time these people had the power and of course you just weren&#39;t allowed to show your bloody feelings, I mean you just couldn&#39;t. You know if you were unhappy, can you imagine for instance you see someone in the country in a castle you are deeply unhappy you are a woman; you couldn&#39;t go to your mate and cry on her shoulders, you couldn&#39;t go into the village and complain about your feelings, it was assailing yourself to someone you just couldn&#39;t. You know. Because they had to respect you. Now of course Freud put that very much into question - you see </i><b><i>to examine yourself you would have to put other things into question - society, everything that surrounds you</i></b><i> and that was not a good thing at that time. Why? Because your self-created empire to a certain extent would have fallen to bits much earlier already.&#39;</i><br /><br /><i>But what frightened the rulers of the empire even more was Freud&#39;s idea hidden inside all human beings were dangerous instinctual drives. Freud had devised a method he called psychoanalysis. By analyzing dreams and free association he had unearthed he said powerful sexual and aggressive forces which were the remnants of our animal past. Feelings we repressed because they were too dangerous.&quot;</i><br /><br />Dr. Earnest Jones - Colleague of Freud: <i>&quot;&#39;Freud devised a method for exploring the hidden part of the mind which we nowadays call </i><b><i>the unconscious this the part is totally unknown to our consciousness. That there exists a barrier in all our minds which prevents these hidden and welcome impulses from the unconscious from emerging.&#39;</i></b><br /><br /><i>In 1914 the Austria Hungarian Empire led Europe into war. As the horror mounted Freud saw it as terrible evidence of the truth of his findings. The saddest thing he wrote, that this is exactly the way we should expect people to behave from our knowledge of psychoanalysis. Governments had unleashed the primitive forces in humans beings and no one seemed to know how to stop them.</i><br /><br /><i>At that time, Freud&#39;s young nephew Edward Bernays was working as a press agent in America. His main client was the world famous opera singer Caruso who was touring the United States. Bernays&#39; parents had emigrated to America 20 years before, but he kept in touch with his Uncle who joined him for Holidays in the Alps. But Bernays was now about to return to Europe for a very different reason. On the night that Caruso opened in Toledo Ohio America announced that it was entering the war against Germany and Austria. As part of the war effort the US government set up a committee on public information and Bernays was employed to promote America&#39;s war aims in the press. The president Woodrow Wilson had announced that the United States would fight not to restore the old empires but to bring democracy to all of Europe. Bernays proved extremely skillful at promoting this idea both at home and abroad and at the end of the war was asked to accompany the President to the Paris Peace Conference.&quot;</i><br /><br />Edward Bernays - 1991: <i>&quot;&#39;Then to my surprise they asked me to go with Woodrow Wilson to the peace conference. And at the age of 26 I was in Paris for the entire time of the peace conference that was held in the suburb of Paris and we and worked to make the world safe for democracy. That was the big slogan.&#39;</i><br /><br /><i>Wilson&#39;s reception in Paris astounded Bernays and the other American propagandists. They had portrayed Wilson as a liberator of the people. The man who would create a new world in which the individual would be free. They had made him a hero of the masses. And as he watched the crowd surge around Wilson, </i><b><i>Bernays began to wonder if it would be possible to do the same type of mass persuasion but in peace time.&quot;</i></b><br /><br /><b>Edward Bernays - 1991: </b><b><i>&quot;&#39;When I came back to the United States I decided that if you could use propaganda for war you could certainly use it for peace. And propaganda got to be a bad word because of the Germans using it. So what I did is try to find some other words so we found the word Council on Public Relations.&#39;</i></b><br /><br /><i>Bernays returned to New York and set up as a Public Relations Councilman in small office off Broadway. Which was the first time the term had even been used. Since the end of the 19th century, America had become a mass industrial society with millions clustered together in the cities. Bernays was determined to find a way to manage and alter the way these new crowds thought and felt. To do this he turned to the writings of his Uncle Sigmund. While in Paris Bernays had sent his Uncle a gift of some Havana cigars. In return Freud had sent him a copy of his General Introduction to Psychoanalysis. Bernays read it and the picture of hidden irrational forces inside human beings fascinated him. He wondered whether he might be able to make money manipulating the unconscious.&quot;</i><br /><br /><b>Pat Jackson - Public Relations Adviser and Colleague of Bernays: </b><b><i>&quot;&#39;What Eddie got from Freud was indeed this idea that there is a lot more going on in human decision making. Not only among individuals but even more importantly among groups that this idea that information drives behavior. So Eddie began to formulate this idea that you had to look at things that will play to people&#39;s irrational emotions. You see that immediately moved Eddie into a different category from other people in his field and most government officials and managers of the day who thought if you just hit people with all this factual information they would look at that say go &quot;of course&quot; and Eddie knew that was not the way the world worked.&#39;</i></b><br /><br /><i>Bernays set out to experiment with the minds of the popular classes. His most dramatic experiment was to persuade women to smoke. At that time there was a taboo against women smoking and one of his early clients George Hill, the President of the American Tobacco corporation asked Bernays to find a way to break it.&quot;</i><br /><br />Edward Bernays - 1991: <i>&quot;&#39;He says we&#39;re losing half of our market. Because men have invoked a taboo against women smoking in public. Can you do anything about that? I said let me think about it. If I may have permission to see psychoanalyst to see what cigarettes mean to women. He said what&#39;ll cost? So I called up Dr Brille, AA Brille who was the leading psychoanalyst in New York at the time.&#39;</i><br /><br /><i>AA Brille was one of the first psychoanalysts in America. And for a large fee he told Bernays that cigarettes were a symbol of the penis and of male sexual power. He told Bernays that if he could find a way to connect cigarettes with the idea of challenging male power then women would smoke because then they would have their own penises.</i><br /><br /><i>Every year New York held an Easter day parade to which thousands came. Bernays decided to stage an event there . He persuaded a group of rich debutants to hide cigarettes under their clothes. Then they should join the parade and at a given signal from him they were to light up the cigarettes dramatically. Bernays then informed the press that he had heard that a group of suffragettes were preparing to protest by lighting up what they called torches of freedom.&quot;</i><br /><br />Pat Jackson - Public Relations Adviser and Colleague of Bernays: <i>&quot;&#39;He knew this would be an outcry, and he knew that all of the photographers would be there to capture this moment so he was ready with a phrase which was torches of freedom. So here you have a symbol, women, young women, debutantes, smoking a cigarette in public with a phrase that means anybody who believes in this kind of equality pretty much has to support them in the ensuing debate about this, because I mean torches of freedom. What&#39;s our American point, it&#39;s liberty, she&#39;s holding up the torch, you see and so all this there together, there&#39;s emotion there&#39;s memory and there&#39;s a rational phrase, all of this is in there together. So the next day this was not just in all the New York papers it was across the United States and around the world. And from that point forward the sale of cigarettes to woman began to rise. He had made them socially acceptable with a single symbolic ad.&#39;</i><br /><br /><i>What Bernays had created was the idea that if a women smoked it made her more powerful and independent. An idea that still persists today. It made him realize that it was possible to persuade people to behave irrationally if you link products to their emotional desires and feelings. The idea that smoking actually made women freer, was completely irrational. But it made them feel more independent. It meant that irrelevant objects could become powerful emotional symbols of how you want to be seen by others.&quot;</i><br /><br /><b>Peter Strauss - Employee of Bernays 1948-1952: </b><b><i>&quot;&#39;Eddie Bernays saw a way to sell product was not to sell it to your intellect, that you ought to buy an automobile, but that you will feel better about it if you have this automobile. I think he originated that idea that they weren&#39;t just purchasing something that they were engaging themselves emotionally or personally in a product or service. It&#39;s not that you think you need a piece of clothing but that you will feel better if you have a piece of clothing. That was his contribution in a very real sense. We see it all over the place today but I think he originated the idea, the emotional connect to a product or service.&#39;</i></b><br /><br /><i>What Bernays was doing fascinated Americas corporations. They had come out of the war rich and powerful, but they had a growing worry. The system of mass production had flourished during the war and now millions of goods were pouring off production lines. What they were frightened of was the danger of overproduction, that there would come a point when people had enough goods and would simply stop buying. Up until that point the majority of products were still sold to the masses on the basis of need. While the rich had long been used to luxury goods for the millions of working class Americans most products were still advertised as necessities. Goods like shoes stockings even cars were promoted in functional terms for their durability. The aim of the advertisements were simply to show people the products practical virtues, nothing more.</i><br /><br /><i>What the corporations realized they had to do was transform the way the majority of Americans thought about products. One leading Wall Street banker, Paul Mazer of Leahman Brothers was clear about what was necessary. We must shift America, he wrote, from a needs to a desires culture. People must be trained to desire, to want new things even before the old had been entirely consumed. We must shape a new mentality in America. Man&#39;s desires must overshadow his needs.&quot;</i><br /><br />Peter Solomon - Investment Banker - Leahman Brothers: <b><i>&quot;&#39;Prior to that time there was no American consumer</i></b><i>, there was the American worker. And there was the American owner. And they manufactured, and they saved and they ate what they had to and the people shopped for what they needed. And while the very rich may have bought things they didn&#39;t need, most people did not. And Mazer envisioned a break with that where you would have things that you didn&#39;t actually need, but you wanted as opposed to needed.&#39;</i><br /><br /><i>And the man who would be at the center of changing that mentality for the corporations was Edward Bernays.</i><br /><br />Stuart Ewen - Historian of Public Relations: <i>&quot;&#39;Bernays really is the guy within the United States more than anybody else who sort of brings to the table psychological theory as something that is an essential part of how, from the corporate side, of how we are going to appeal to the masses effectively and the whole sort of merchandising establishment and the sales establishment is ready for Sigmund Freud. I mean they are ready for understanding what motivates the human mind. And so there&#39;s this real openness to Bernays techniques being used to sell products to the masses.&#39;</i><br /><br /><i>Beginning in the early 20&#39;s the New York banks funded the creation of chains of department stores across America. They were to be the outlets for the mass produced goods. And Bernays&#39; job was to produce the new type of customer. Bernays began to create many of the techniques of mass consumer persuasion that we now live with. He was employed by William Randolph Hurst to promote his new women&#39;s magazines, and Bernays glamorized them by placing articles and advertisements that linked products made by others of his clients to famous film stars like Clara Bow, who was also his client. Bernays also began the practice of product placement in movies, and he dressed the stars at the films premieres with clothes and jewelry from other firms he represented.</i><br /><br /><i>He was, he claimed, the first person to tell car companies they could sell cars as symbols of male sexuality. He employed psychologists to issue reports that said products were good for you and then pretended they were independent studies. He organized fashion shows in department stores and paid celebrities to repeat the new and essential message, </i><b><i>you bought things not just for need but to express your inner sense of your self to others ...&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"The Method of Multiple Working Hypotheses","summary":"Even by 1890, it Was Understood that the Habit of Some to Hastily Conjure Up an Explanation for Every New Phenomenon Leads to the Development of Weak Theories / Groupthink Can then Transform these Weak Theories Into \"Ruling\" Theories, and the End Result Can Suffer from the Investigators' Hasty & Narrowed Focus / By Contrast, the Method of Multiple Working Hypotheses Can Generate a More Thorough Theoretical Coverage by Addressing the Inherent Partiality of Intellectual Parentage","image":"https://lh3.googleusercontent.com/-5kfYAGClcwE/V2yfLZvMCfI/AAAAAAAAIC8/Opcn3sVZ4-wUWp9uEbjnAlmvidybMK9MQCL0B/the-method-of-multiple-working-hypotheses-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/hBEP2oCp8Pb","publishDate":"2016-06-24T02:48:17.879Z","updateDate":"2017-01-03T02:29:52.295Z","text":"<b>The Method of Multiple Working Hypotheses: Even by 1890, it Was Understood that the Habit of Some to Hastily Conjure Up an Explanation for Every New Phenomenon Leads to the Development of Weak Theories / Groupthink Can then Transform these Weak Theories Into &quot;Ruling&quot; Theories, and the End Result Can Suffer from the Investigators&#39; Hasty &amp; Narrowed Focus / By Contrast, the Method of Multiple Working Hypotheses Can Generate a More Thorough Theoretical Coverage by Addressing the Inherent Partiality of Intellectual Parentage</b><br /><br />It&#39;s a long piece; if you cannot read it all, then use the bold text to skim ...<br /><br /><b>The Method of Multiple Working Hypotheses</b><br />Science, Vol. 15, No. 366<br />Feb. 7, 1890<br />pp. 92-96<br />Published by: American Association for the Advancement of Science<br /><a href=\"http://www.jstor.org/stable/1764336\" class=\"ot-anchor\">http://www.jstor.org/stable/1764336</a><br /><br /><i>&quot;As methods of study constitute the leading theme of our session, I have chosen as a subject in measurable consonance the method of multiple working hypotheses in its application to investigation, instruction, and citizenship.</i><br /><br /><b><i>There are two fundamental classes of study. The one consists in attempting to follow by close imitation the processes of previous thinkers, or to acquire by memorizing the results of their investigations. It is merely secondary, imitative, or acquisitive study. The other class is primary or creative study. In it the effort is to think independently, or at least individually, in the endeavor to discover new truth, or to make new combinations of truth, or at least to develop an individualized aggregation of truth.</i></b><i> The endeavor is to think for one&#39;s self, whether the thinking lies wholly in the fields of previous thought or not. It is not necessary to this habit of study that the subject-material should be new; but the process of thought and its results must be individual and independent, not the mere following of previous lines of thought ending in predetermined results. The demonstration of a problem in Euclid precisely as laid down is an illustration of the former; the demonstration of the same proposition by a method of one&#39;s own or in a manner distinctively individual is an illustration of the latter; both lying entirely within the realm of the known and the old.</i><br /><br /><i>Creative study, however, finds its largest application in those subjects in which, while much is known, more remains to be known. Such are the fields which we, as naturalists, cultivate; and we are gathered for the purpose of developing improved methods lying largely in the creative phase of study, though not wholly so.</i><br /><br /><b><i>Intellectual methods have taken three phases in the history of progress thus far.</i></b><i> What may be the evolutions of the future it may not be prudent to forecast. Naturally the methods we now urge seem the highest attainable. </i><b><i>These three methods may be designated, first, the method of the ruling theory; second, the method of the working hypothesis; and, third, the method of multiple working hypotheses.</i></b><br /><br /><i>In the earlier days of intellectual development the sphere of knowledge was limited, and was more nearly within the compass of a single individual; and those who assumed to be wise men, or aspired to be thought so, felt the need of knowing, or at least seeming to know, all that was known as a justification of their claims. So, also, there grew up an expectancy on the part of the multitude that the wise and the learned would explain whatever new thing presented itself. Thus pride and ambition on the one hand, and expectancy on the other, developed the putative wise man whose knowledge boxed the compass, and whose acumen found an explanation for every new puzzle which presented itself. This disposition has propagated itself, and has come down to our time as an intellectual predilection, though the compassing of the entire horizon of knowledge has long since been an abandoned affectation. </i><b><i>As in the earlier days, so still, it is the habit of some to hastily conjure up an explanation for every new phenomenon that presents itself. Interpretation rushes to the forefront as the chief obligation pressing upon the putative wise man. Laudable as the effort at explanation is in itself, it is to be condemned when it runs before a serious inquiry into the phenomenon itself. A dominant disposition to find out what is, should precede and crowd aside the question, commendable at a later stage, &#39;How came this so?&#39; First full facts, then interpretations.</i></b><br /><br /><i>The habit of precipitate explanation leads rapidly on to the development of tentative theories. The explanation offered for a given phenomenon is naturally, under the impulse of self-consistency, offered for like phenomena as they present themselves, and there is soon developed a general theory explanatory of a large class of phenomena similar to the original one. </i><b><i>This general theory may not be supported by any further considerations than those which were involved in the first hasty inspection.</i></b><i> For a time,it is likely to be held in a tentative way with a measure of candor. With this tentative spirit and measurable candor, the mind satisfies its moral sense, and deceives itself with the thought that it is proceeding cautiously and impartially toward the goal of ultimate truth. It fails to recognize that </i><b><i>no amount of provisional holding of a theory, so long as the view is limited and the investigation partial, justifies an ultimate conviction. It is not the slowness with which conclusions are arrived at that should give satisfaction to the moral sense, but the thoroughness, the completeness, the all-sidedness, the impartiality, of the investigation.</i></b><br /><br /><i>It is in this tentative stage that the affections enter with their blinding influence. </i><b><i>Love was long since represented as blind</i></b><i>, and what is true in the personal realm is measurably true in the intellectual realm. Important as the intellectual affections are as stimuli and as rewards, they are nevertheless dangerous factors, which menace the integrity of the intellectual processes. The moment one has offered an original explanation for a phenomenon which seems satisfactory, that moment affection for his intellectual child springs into existence; and as the explanation grows into a definite theory, his parental affections cluster about his intellectual offspring, and it grows more and more dear to him, so that, while he holds it seemingly tentative, it is still lovingly tentative, and not impartially tentative. </i><b><i>So soon as this parental affection takes possession of the mind, there is a rapid passage to the adoption of the theory. There is an unconscious selection and magnifying of phenomena that fall into harmony with the theory and support it, and an unconscious neglect of those that fail of coincidence. The mind lingers with pleasure upon the facts that fall happily into the embrace of the theory, and feels a natural coldness toward those that seem refractory.</i></b><i> Instinctively there is a special searching-out of phenomena that support it, for the mind is led by its desires. There springs up, also, an unconscious pressing of the theory to make it fit the facts, and a pressing of the facts to make them fit the theory. When these biasing tendencies set in, the mind rapidly degenerates into the partiality of paternalism. The search for facts, the observation of phenomena and their interpretation, are all dominated by affection for the favored &#39;theory until it appears to its author or its advocate to have been overwhelmingly established. The theory then rapidly rises to the ruling position, and investgaton. observation, and interpretation are controlled and directed by it. From an unduly favored child, it readily becomes master, and leads its author whithersoever it will. The subsequent history of that mind in respect to that theme is but the progressive dominance of a ruling idea.</i><br /><br /><i>Briefly summed up, the evolution is this: </i><b><i>a premature explanation passes into a tentative theory, then into an adopted theory, and then into a ruling theory.</i></b><br /><br /><i>When the last stage has been reached, unless the theory happens, perchance, to be the true one, all hope of the best results is gone. To be sure, truth may be brought forth by an investigator dominated by a false ruling idea. His very errors may indeed stimulate investigation on the part of others. But the condition is an unfortunate one. Dust and chaff are mingled with the grain in what should be a winnowing process.</i><br /><br /><i>As previously implied, the method of the ruling theory occupied a chief place during the infancy of investigation. It is an expression of the natural infantile tendencies of the mind, though in this case applied to its higher activities, for in the earlier stages of development the feelings are relatively greater than in later stages.</i><br /><br /><i>Unfortunately it did not wholly pass away with the infancy of investigation, but has lingered along in individual instances to the present day, and finds illustration in universally learned men and pseudo-scientists of our time.</i><br /><br /><i>The defects of the method are obvious, and its errors great. If I were to name the central psychological fault, I should say that it was the admission of intellectual affection to the place that should be dominated by impartial intellectual rectitude.</i><br /><br /><i>So long as intellectual interest dealt chiefly with the intangible, so long it was possible for this habit of thought to survive, and to maintain its dominance, because the phenomena themselves, being largely subjective, were plastic in the hands of the ruling idea; but so soon as investigation turned itself earnestly to an inquiry into natural phenomena, whose manifestations are tangible, whose properties are rigid, whose laws are rigorous, the defects of the method became manifest, and an effort at reformation ensued. The first great endeavor was repressive. The advocates of reform insisted that theorizing should be restrained, and efforts directed to the simple determination of facts. The effort was to make scientific study factitious instead of causal. Because theorizing in narrow lines had led to manifest evils, theorizing was to be condemned. The reformation urged was not the proper control and utilization of theoretical effort, but its suppression. We do not need to go backward more than twenty years to find ourselves in the midst of this attempted reformation. Its weakness lay in its narrowness and its restrictiveness. There is no nobler aspiration of the human intellect than desire to compass the cause of things. The disposition to find explanations and to develop theories is laudable in itself. It is only its ill use that is reprehensible. The vitality of study quickly disappears when the object sought is a mere collocation of dead unmeaning facts.</i><br /><br /><i>The inefficiency of this simply repressive reformation becoming apparent, improvement was sought in the method of the working hypothesis. This is affirmed to be the scientific method of the day, but to this I take exception. </i><b><i>The working hypothesis differs from the ruling theory in that it is used as a means of determining facts, and has for its chief function the suggestion of lines of inquiry; the inquiry being made, not for the sake of the hypothesis, but for the sake of facts. Under the method of the ruling theory, the stimulus was directed to the finding of facts for the support of the theory.</i></b><i> Under the working hypothesis, the facts are sought for the purpose of ultimate induction and demonstration, the hypothesis being but a means for the more ready development of facts and of their relations, and the arrangement and preservation of material for the final induction.</i><br /><br /><i>It will be observed that the distinction is not a sharp one, and that a working hypothesis may with the utmost ease degenerate into a ruling theory. Affection may as easily cling about an hypothesis as about a theory, and the demonstration of the one may become a ruling passion as much as of the other.</i><br /><br /><i>Conscientiously followed, the method of the working hypothesis is a marked improvement upon the method of the ruling theory; but it has its defects, -- defects which are perhaps best expressed by the ease with which the hypothesis becomes a controlling idea. </i><b><i>To guard against this, the method of multiple working hypotheses is urged.</i></b><i> It differs from the former method in the multiple character of its genetic conceptions and of its tentative interpretations. </i><b><i>It is directed against the radical defect of the two other methods; namely, the partiality of intellectual parentage. The effort is to bring up into view every rational explanation of new phenomena, and to devleop every tenable hypothesis respecting their cause and history. The investigator thus becomes the parent of a family of hypotheses; and, by his parental relation to all, he is forbidden to fasten his affections unduly upon any one.</i></b><i> In the nature of the case, the danger that springs from affection is counteracted, and therein is a radical difference between this method and the two preceding. The investigator at the outset puts himself in cordial sympathy and in parental relations (of adoption, if not of authorship) with every hypothesis that is at all applicable to the case under investigation. Having thus neutralized the partialities of his emotional nature, he proceeds with a certain natural and enforced erectness of mental attitude to the investigation, knowing well that some of his intellectual children will die before maturity, yet feeling that several of them may survive the results of final ivestigation, </i><b><i>since it is often the outcome of inquiry that several causes are found to be involved instead of a single one.</i></b><i> In following a single hypothesis, the mind is presumably led to a single explanatory conception. But an adequate explanation often involves the co-ordination of several agencies, which enter into the combined result in varying proportions. The true explanation is therefore necessarily complex. Such complex explanations of phenomena are specially encouraged by the method of multiple hypotheses, and constitute one of its chief merits. We are so pione to attribute a phenomenon to a single cause, that, when we find an agency present, we are liable to rest satisfied therewith, and fail to recognize that it is but one factor, and perchance a minor factor, in the accomplishment of the total result. Take for illustration the mooted question of the origin of the Great Lake basins. We have this, that, and the other hypothesis urged by different students as the cause of these great excavations; and all of these are urged with force and with fact, urged justly to a certain degree. It is practically demonstrable that these basins were river-valleys antecedent to the glacial incursion, and that they owe their origin in part to the pre-existence of those valleys and to the blocking-up of their outlets. And so this view of their origin is urged with a certain truthfulness. So, again, it is demonstrable that they were occupied by great lobes of ice, which excavated them to a marked degree, and therefore the theory of glacial excavation finds support in fact. I think it is furthermore demonstrable that the earth&#39;s crust beneath these basins was flexed downward, and that they owe a part of their origin to crust deformation. But to my judgment neither the one nor the other; nor the third, constitutes an adequate explanation of the phenomena. All these must be taken together, and possibly they must be supplemented by other agencies. The problem, therefore, is the determination not only of the participation, but of the measure and the extent, of each of these agencies in the production of the complex result. This is not likely to be accomplished by one whose working hypothesis is pre-glacial erosion, or glacial erosion, or crust deformation, but by one whose staff of working hypotheses embraces all of these and any other agency which can be rationally conceived to have taken part in the phenomena.</i><br /><br /><i>A special merit of the method is, that by its very nature it promotes thoroughness. </i><b><i>The value of a working hypothesis lies largely in its suggestiveness of lines of inquiry that might otherwise be overlooked. Facts that are trivial in themselves are brought into significance by their bearings upon the hypothesis, and by their causal indications.</i></b><i> As an illustration, it is only necessary to cite the phenomenal influence which the Darwinian hypothesis has exerted upon the investigations of the past two decades. But a single working hypothesis may lead investigation along a given line to the neglect of others equally important; and thus, while inquiry is promoted in certain quarters, the investigation lacks in completeness. But if all rational hypotheses relating to a subject are worked co-equally, thoroughness is the presumptive result, in the very nature of the case.</i><br /><br /><i>In the use of the multiple method, the re-action of one hypothesis upon another tends to amplify the recognized scope of each, and their mutual conflicts whet the discriminative edge of each. The analytic process, the development and demonstration of criteria, and the sharpening of discrimination, receive powerful impulse from the co-ordinate working of several hypotheses.</i><br /><br /><i>Fertility in processes is also the natural outcome of the method. </i><b><i>Each hypothesis suggests its own criteria, its own means of proof, its own methods of developing the truth; and if a group of hypotheses encompass the subject on all sides, the total outcome of means and of methods is full and rich.</i></b><br /><br /><i>The use of the method leads to certain peculiar habits of mind which deserve passing notice, since as a factor of education its disciplinary value is one of importance. When faithfully pursued for a period of years, it develops a habit of thought analogous to the method itself, which may be designated a habit of parallel or complex thought. Instead of a simple succession of thoughts in linear order, the procedure is complex, and the mind appears to become possessed of the power of simultaneous vision from different standpoints. Phenomena appear to become capable of being viewed analytically and synthetically at once. It is not altogether unlike the study of a landscape, from which there comes into the mind myriads of lines of intelligence, which are received and co-ordinated simultaneously, producing a complex impression which is recorded and studied directly in its complexity. My description of this process is confessedly inadequate, and the affirmation of it as a fact would doubtless challenge dispute at the hands of psychologists of the old school; but I address myself to naturalists who I think can respond to its verity from, their own experience.</i><br /><br /><i>The method has, however, its disadvantages. No good thing is without its drawbacks; and this very habit of mind, while an invaluable acquisition for purposes of investigation, introduces difficulties in expression. It is obvious, upon consideration, that this method of thought is impossible of verbal expression. We cannot put into words more than a single line of thought at the same time; and even in that the order of expression must be conformed to the idiosyncrasies of the language, and the rate must be relatively slow. When the habit of complex thought is not highly developed, there is usually a leading line to which others are subordinate, and the difficulty of expression does not rise to serious proportions; but when the method of simultaneous vision along different lines is developed so that the thoughts running in different channels are nearly equivalent, there is an obvious embarrassment in selection and a disinclination to make the attempt. Furthermore, the impossibility of expressing the mental operation in words leads to their disuse in the silent processes of thought, and hence words and thoughts lose that close association which they are accustomed to maintain with those whose silent as well as spoken thoughts run in linear verbal courses. There is therefore a certain predisposition on the part of the practitioner of this method to taciturnity.</i><br /><br /><b><i>We encounter an analogous difficulty in the use of the method with young students. It is far easier, and I think in general more interesting, for them to argue a theory or accept a simple interpretation than to recognize and evaluate the several factors which the true elucidation may require.</i></b><i> To illustrate: it is more to their taste to be taught that the Great Lake basins were scooped out by glaciers than to be urged to conceive of three or more great agencies working successively or simultaneously, and to estimate how much was accomplished by each of these agencies. The complex and the quantitative do not fascinate the young student as they do the veteran investigator.</i><br /><br /><i>It has not been our custom to think of the method of working hypotheses as applicable to instruction or to the practical affairs of life. We have usually regarded it as but a method of science. But I believe its application to practical affairs has a value co-ordinate with the importance of the affairs themselves. I refer especially to those inquiries and inspections that precede the coming-out of an enterprise rather than to its actual execution. The methods that are superior in scientific investigation should likewise be superior in those investigations; that are the necessary antecedents to an intelligent conduct of affairs. But I can dwell only briefly on this phase of the subject.</i><br /><br /><i>In education, as in investigation, it has been much the practice to work a theory. The search for instructional methods has often proceeded on the presumption that there is a definite patent process through which all students might be put and come out with results of maximum excellence; and hence pedagogical inquiry in the past has very largely concerned itself with the inquiry, &#39;What is the best method?&#39; rather than with the inqiry, &#39;What are the special values of different methods, and what are their several advantageous applicabilities in the varied work of instruction?&#39; The past doctrine has been largely the doctrine of pedagogical uniformitarianism. But the faculties and functions of the mind are almost, if not quite, as varied as the properties and functions of matter; and it is perhaps not less absurd to assume that any specific method of instructional procedure is more effective than all others, under any and all circumstances, than to asume that one principle of interpretation is equally applicable to all the phenomena of nature. As there is an endless variety of mental processes and combinations and an indefinite number of orders of procedure, the advantage of different methods under different conditions is almost axiomatic. This being granted, there is presented to the teacher the problem of selection and of adaptation to meet the needs of any specific issue that may present itself. It is important, therefore, that the teacher shall have in mind a full array of possible conditions and states of mind which may be presented, in order that, when any one of these shall become an actual case, he may recognize it, and be ready for the emergency.</i><br /><br /><b><i>Just as the investigator armed with many working hypotheses is more likely to see the true nature and significance of phenomena when they present themselves, so the instructor equipped with a full panoply of hypotheses ready for application more readily recognizes the actuality of the situation, more accurately measures its significance, and more appropriately applies the methods which the case calls for.</i></b><br /><br /><i>The application of the method of multiple hypotheses to the varied affairs of life is almost as protean as the phases of that life itself, but certain general aspects may be taken as typical of the whole. What I have just said respecting the application of the method to instruction may apply, with a simple change of terms, to almost any other endeavor which we are called upon to undertake. We enter upon an enterprise in most cases without full knowledge of all the factors that will enter into it, or all of the possible phases which it may develop. It is therefore of the utmost importance to be prepared to rightly comprehend the nature, bearings, and influence of such unforeseen elements when they shall definitely present themselves as actualities. </i><b><i>If our vision is narrowed by a preconceived theory as to what will happen, we are almost certain to misinterpret the facts and to misjudge the issue. If, on the other hand, we have in mind hypothetical forecasts of the various contingencies that may arise, we shall be the more likely to recognize the true facts when they do present themselves.</i></b><i> Instead of being biased by the anticipation of a given phase, the mind is rendered open and alert by the anticipation of any one of many phases, and is free not only, but is predisposed, to recognize correctly the one which does appear. The method has a further good effect. The mind, having anticipated the possible phases which may arise, has prepared itself for action under any one that may come up, and it is therefore ready-armed, and is predisposed to act in the line appropriate to the event. It has not set itself rigidly in a fixed purpose, which it is predisposed to follow without regard to contingencies. It has not nailed down the helm and predetermined to run a specific course, whether rocks lie in the path or not; but, with the helm in hand, it is ready to veer the ship according as danger or advantage discovers itself.</i><br /><br /><i>It is true, there are often advantages in pursuing a fixed predetermined course without regard to obstacles or adverse conditions. Simple dogged resolution is sometimes the salvation of an enterprise; but, while glorious successes have been thus snatched from the very brink of disaster, overwhelming calamity has in other cases followed upon this course, when a reasonable regard for the unanticipated elements would have led to success. So there is to be set over against the great achievements that follow on dogged adherence great disasters which are equally its result.</i><br /><br /><i>The tendency of the mind, accustomed to work through multiple hypotheses, is to sway to one line of policy or another, according as the balance of evidence shall incline. This is the soul and essence of the method. It is in general the true method. Nevertheless there is a danger that this yielding to evidence may degenerate into unwarranted vacillation. It is not always possible for the mind to balance evidence with exact equipoise, and to determine, in the midst of the execution of an enterprise, what is the measure of probability on the one side or the other; and as difficulties present themselves, there is a danger of being biased by them and of swerving from the course that was really the true one. Certain limitations are therefore to be placed upon the application of the method, for it must be remembered that a poorer line of policy consistently adhered to may bring better results than a vacillation between better policies.</i><br /><br /><i>There is another and closely allied danger in the application of the method. In its highest development it presumes a mind supremely sensitive to every grain of evidence. Like a pair of delicately poised scales, every added particle on the one side or the other produces its effect in oscillation. But such a pair of scales may be altogether too sensitive to be of practical value in the rough affairs of life. The balances of the exact chemist are too delicate for the weighing-out of coarse commodities. Despatch may be more important than accuracy. So it is possible for the mind to be too much concerned with the nice balancings of evidence, and to oscillate too much and too long in the endeavor to reach exact results. It may be better, in the gross affairs of life, to be less precise and more prompt. Quick decisions, though they may contain a grain of error, are oftentimes better than precise decisions at the expense of time.</i><br /><br /><i>The method has a special beneficent application to our social and civic relations. Into these relations there enter, as great factors, our judgment of others, our discernment of the nature of their acts, and our interpretation of their motives and purposes. The method of multiple hypotheses, in its application here, stands in decided contrast to the method of the ruling theory or of the simple working hypothesis. The primitive habit is to interpret the acts of others on the basis of a theory. Childhood&#39;s unconscious theory is that the good are good, and the bad are bad. </i><b><i>From the good the child expects nothing but good; from the bad, nothing but bad. To expect a good act from the bad, or a bad act from the good, is radically at variance with childhood&#39;s mental methods. Unfortunately in our social and civic affairs too many of our fellow-citizens have never outgrown the ruling theory of their childhood.</i></b><br /><br /><i>Many have advanced a step farther, and employ a method analogous to that of the working hypothesis. A certain presumption is made to attach to the acts of their fellow beings, and that which they see is seen in the light of that presumption, and that which they construe is construed in the light of that presumption. They do not go to the lengths of childhood&#39;s method by assuming positively that the good are wholly good, and the bad wholly bad; but there is a strong presumption in their minds that he concerning whom they have an ill opinion will act from corresponding motives. It requires positive evidence to overthrow the influence of the working hypothesis.</i><br /><br /><i>The method of multiple hypotheses assumes broadly that the acts of a fellow-being may be diverse in their nature, their motives, their purposes, and hence in their whole moral character; that they may be good though the dominant character be bad; that they may be bad though the dominant character be good; that they may be partly good and partly bad, as is the fact in the greater number of the complex activities of a human being. </i><b><i>Under the method of multiple hypotheses, it is the first effort of the mind to see truly what the act is, unbeclouded by the presumption that this or that has been done because it accords with our ruling theory or our working hypothesis.</i></b><i> Assuming that acts of similar general aspect may readily take any one of several different phases, the mind is freer to see accurately what has actually been done. So, again, in our interpretations of motives and purposes, the method assumes that these may have been any one of many, and the first duty is to ascertain which of possible motives and purposes actually prompted this individual action. Going with this effort there is a predisposi tion to balance all evidence fairly, and to accept that interpretation to which the weight of evidence inclines, not that which simply fits our working hypothesis or our dominant theory. The outcome, therefore, is better and truer observation and juster and more righteous interpretation.</i><br /><br /><i>There is a third result of great importance. The imperfections of our knowledge are more likely to be detected, for there will be less confidence in its completeness in proportion as there is, a broad comprehension of the possibilities of varied action, under similar circumstances and with similar appearances, so, also, the imperfections of evidence as to the motives and purposes inspiring the action will become more discernible in proportion to the fulness of our conception of what the evidence should be to distinguish between action from the one or the other of possible motives. </i><b><i>The necessary result will be a less disposition to reach conclusions upon imperfect grounds. So, also, there will be a less inclination to misapply evidence; for, several constructions being definitely in mind, the indices of the one motive are less liable to be mistaken for the indices of another.</i></b><br /><br /><i>The total outcome is greater care in ascertaining the facts, and greater discrimination and caution in drawing conclusions. I am confident, therefore, that the general application of this method to the affairs of social and civic life would go far to remove those misunderstandings, misjudgments, and misrepresentations which constitute so pervasive an evil in our social and our political atmospheres, the source of immeasurable suffering to the best and most sensitive souls. The misobservations, the misstatements, the misinterpretations, of life may cause less gross suffering than some other evils; but they, being more universal and more subtle, pain. </i><b><i>The remedy lies</i></b><i>, indeed, partly in charity, but more largely </i><b><i>in correct intellectual habits, in a predominant, ever-present disposition to see things as they are, and to judge them in the full light of an unbiased weighing of evidence applied to all possible constructions, accompanied by a withholding of judgment when the evidence is insufficient to justify conclusions.</i></b><br /><br /><i>I believe that one of the greatest moral reforms that lies immediately before us consists in the general introduction into social and civic life of that habit of mental procedure which is known in investigation as the method of multiple working hypotheses.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Decline in Conceptual Revolutions","summary":"Science Magazine Charted the Milestones of Science in its July 1, 2005 Issue / Their List of Milestones Reveals a Serious Problem Not Commonly Discussed / A Comparison of Two Eras, 1865-1905 with 1965-2005, Plainly Reveals that the Number of Conceptual Scientific Revolutions is on the Decline","image":"https://lh3.googleusercontent.com/-aDHTBfXZY-4/V2qp429qJzI/AAAAAAAAICg/7DGVGQviVOge4UcD7vRPchrapu8iSSGuA/w7200-h10080/the-decline-in-conceptual-revolutions-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/FXj2NzSzjSo","publishDate":"2016-06-22T15:11:13.928Z","updateDate":"2017-01-03T02:31:26.524Z","text":"<b>The Decline in Conceptual Revolutions: Science Magazine Charted the Milestones of Science in its July 1, 2005 Issue / Their List of Milestones Reveals a Serious Problem Not Commonly Discussed / A Comparison of Two Eras, 1865-1905 with 1965-2005, Plainly Reveals that the Number of Conceptual Scientific Revolutions is on the Decline</b><br /><br />Idea based upon talk given by Dr. Gerald Pollack at <a href=\"http://www.theinstituteforventurescience.net/#!who-we-are/c19n2\" class=\"ot-anchor\">http://www.theinstituteforventurescience.net/#!who-we-are/c19n2</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The 5 Stages of the Mind","summary":"Many, if Not Most, of the Change Challenges You Face Today and Will Face Tomorrow Require Something More than Incorporating New Technical Skills Into Your Current Mindset / These are the \"Adaptive Challenges\" / They Can Only be Met by Transforming Your Mindset, by Advancing to a More Sophisticated Stage of Mental Development / The Biggest Error that Leaders Make is to Think that Adaptive Challenges Can Be Solved by Purely Technical Means","image":"https://lh3.googleusercontent.com/-vrgD2qV_FVk/V2oIoyWU9DI/AAAAAAAAIBs/J98Eoa8Ecpw7EzAqoWnBawOAjbRrU5uBQ/w7142-h9999/the-5-stages-of-the-mind.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/ZNuUvggTdrf","publishDate":"2016-06-22T03:41:01.632Z","updateDate":"2017-01-03T02:30:35.417Z","text":"<b>The 5 Stages of the Mind: Many, if Not Most, of the Change Challenges You Face Today and Will Face Tomorrow Require Something More than Incorporating New Technical Skills Into Your Current Mindset / These are the &quot;Adaptive Challenges&quot; / They Can Only be Met by Transforming Your Mindset, by Advancing to a More Sophisticated Stage of Mental Development / The Biggest Error that Leaders Make is to Think that Adaptive Challenges Can Be Solved by Purely Technical Means</b><br /><br />[1] <a href=\"http://www.nhcue.edu.tw/~aca/fte/95-2/4.pdf\" class=\"ot-anchor\">http://www.nhcue.edu.tw/~aca/fte/95-2/4.pdf</a><br /><br />[2] Adapted from Robert Kegan, <i>In Over Our Heads: the Mental Demands of Modern Life</i> (Cambridge: Harvard University Press, 1994), pp 314-315 by Peter W Pruyn, <a href=\"http://developmentalobserver.blog.com\" class=\"ot-anchor\">http://developmentalobserver.blog.com</a>.<br /><br />[3]-[8] <i>Immunity to Change: How to Overcome It and Unlock the Potential in Yourself and Your Organization</i>, by Robert Kegan and Lisa Laskow Lahey<br /><br />[9][10] <a href=\"http://www.nhcue.edu.tw/~aca/fte/95-2/4.pdf\" class=\"ot-anchor\">http://www.nhcue.edu.tw/~aca/fte/95-2/4.pdf</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"The Extinction of the Mammoths","summary":"A 2012 Press Release Pins the Extinction of the Mammoths on \"Climate Warming, Encroaching Humans and Habitat Change — the Same Threats Facing Many Species Today\" / But a 2006 Book on the Subject Showed 8 Instances of Mammoth Tusks Peppered with Meteorites / That Group Revealed a Skeleton Partially Covered by a Black Radioactive Mat with Exotic Isotopes / And they Point to Recent, Coinciding Extinctions Across the Planet for a Variety of Animals","image":"https://lh3.googleusercontent.com/-gQtNOM6_n1w/Vy1swZomtdI/AAAAAAAAH7o/JpVbkC-EqQc1f5m9rx7HV3vZuu0O2icOQ/w6250-h8750/the-extinction-of-the-mammoths-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/ZqknTz57wfZ","publishDate":"2016-05-07T04:19:46.222Z","updateDate":"2017-01-03T02:31:56.322Z","text":"<b>The Extinction of the Mammoths: A 2012 Press Release Pins the Extinction of the Mammoths on &quot;Climate Warming, Encroaching Humans and Habitat Change — the Same Threats Facing Many Species Today&quot; / But a 2006 Book on the Subject Showed 8 Instances of Mammoth Tusks Peppered with Meteorites / That Group Revealed a Skeleton Partially Covered by a Black Radioactive Mat with Exotic Isotopes / And they Point to Recent, Coinciding Extinctions Across the Planet for a Variety of Animals</b><br /><br />The arguments within the graphic are not today as popular as the following narrative.  Why is that?<br /><br /><a href=\"http://phys.org/news/2012-06-climate-habitat-humans-cahoots-mammoths.html\" class=\"ot-anchor\">http://phys.org/news/2012-06-climate-habitat-humans-cahoots-mammoths.html</a><br /><br /><b><i>&quot;Woolly mammoth extinction has lessons for modern climate change</i></b><br /><i>June 12, 2012</i><br /><br /><i>Although humans and woolly mammoths co-existed for millennia, the shaggy giants disappeared from the globe between 4,000 and 10,000 years ago, and scientists couldn&#39;t explain until recently exactly how the Flinstonian behemoths went extinct.</i><br /><br /><i>In a paper published June 12 in the journal Nature Communications, UCLA researchers and colleagues reveal that not long after the last ice age, the last woolly mammoths succumbed to a lethal combination of climate warming, encroaching humans and habitat change — the same threats facing many species today.</i><br /><br /><i>&#39;We were interested to know what happened to this species during the climate warming at the end of the last ice age because we were looking for insights into what might happen today due to human-induced climate change,&#39; said Glen MacDonald, director of UCLA&#39;s Institute of the Environment and Sustainability (IoES). &#39;The answer to why woolly mammoths died off sounds a lot like what we expect with future climate warming.&#39;</i><br /><br /><i>MacDonald, a professor of geography and of ecology and evolutionary biology, worked with UCLA IoES scientists Robert Wayne and Blaire Van Valkenburgh, UCLA geographer Konstantine Kremenetski, and researchers from UC Santa Cruz, the Russian Academy of Science and the University of Hawaii Manoa.</i><br /><br /><i>Their work shows that although hunting by people may have contributed to the demise of woolly mammoths, contact with humans isn&#39;t the only reason this furry branch of the Elephantidae family went extinct. By creating the most complete maps to date of all the changes happening thousands of years ago, the researchers showed that the extinction didn&#39;t line up with any single change but with the combination of several new pressures on woolly mammoths.</i><br /><br /><i>When the last ice age ended about 15,000 years ago, woolly mammoths were on the rise. Warming melted glaciers, but the still-chilly temperatures were downright comfy for such furry animals and kept plant life in just the right balance. It was good weather for growing mammoths&#39; preferred foods, while still too cold for the development of thick forests to block their paths or for marshy peatlands to slow their stride.</i><br /><br /><i>But the research explains that the end was coming for the last of the woolly mammoths, who inhabited Beringia, a chilly region linked by the Bering Strait that included wide swaths of Alaska, the Yukon and Siberia.</i><br /><br /><i>Though humans had hunted woolly mammoths in Siberia for millennia, it wasn&#39;t until the last ice age that people crossed the Bering Strait and began hunting them in Alaska and the Yukon for the first time. After a harsh, 1,500-year cold snap called the Younger Dryas about 13,000 years ago, the climate began to get even warmer. The rising temperatures led to a decline in woolly mammoths&#39; favored foods, like grasses and willows, and encouraged the growth of low-nutrient conifers and potentially toxic birch. Marshy peatlands developed, forcing the mammoths to struggle through difficult and nutritionally poor terrain, and forests became more abundant, squeezing mammoths out of their former territory.</i><br /><br /><i>&#39;It&#39;s not just the climate change that killed them off,&#39; MacDonald said. &#39;It&#39;s the habitat change and human pressure. Hunting expanded at the same time that the habitat became less amenable.&#39;</i><br /><br /><i>Most of the woolly mammoths died about 10,000 years ago, with the final small populations, which were living on islands, lingering until about 4,000 years ago.</i><br /><br /><i>Many previous theories about the mammoths&#39; extinction tended to blame only one thing: hunting, climate changes, disease or even an ice-melting, climate-changing meteor, MacDonald said. The new research marks the first time scientists mapped out and dated so many different aspects of the era at once. Using radiocarbon dating of fossils, the researchers were able to trace the changing locations of peatlands, forests, plant species, mammoth populations and human settlements over time, and they cross-referenced this information with climate-change data.</i><br /><br /><i>The research used 1,323 mammoth radiocarbon dates, 658 peatland dates, 447 tree dates, and 576 dates from Paleolithic archaeological sites. Scientists from IoES and other UCLA departments obtained samples and worked on radiocarbon dating of the peatlands and the forests, and they created a database uniting information on hundreds of previously dated mammoth samples, developing the final map from thousands of dates and latitude and longitude records.</i><br /><br /><i>That&#39;s what drew Van Valkenburgh, a paleontologist and professor of ecology and evolutionary biology, to the project.</i><br /><br /><i>&#39;Glen&#39;s project combined paleobotanical, paleontological, genetic, archaeological and paleoclimate data and did it in a bigger way, with many more data points, than has been done before,&#39; said Van Valkenburgh, who interpreted the archaeological record. &#39;I was excited to be able to contribute to such an ambitious and exciting study.&#39;</i><br /><br /><i>She and Wayne, a UCLA molecular geneticist and professor of ecology and evolutionary biology who studies ancient DNA, used different methods of examining the mammoth fossils to reconstruct the ancient population size.</i><br /><br /><i>&#39;It&#39;s a dramatic advance in the amount of data,&#39; said Wayne, who reconstructed mitochondrial DNA from radiocarbon-dated woolly mammoth remains. &#39;Essentially, larger populations should have greater genetic diversity. However, in this case, the extent of fossil remains provided a more high-resolution picture of how the population size changed through time than genetic diversity.&#39;</i><br /><br /><i>Mapping the size and location of both mammoth and human populations alongside temperature changes and plant locations through time gave the researches a uniquely complete view of what happened, MacDonald said.</i><br /><br /><i>&#39;We are, in a sense, time-traveling with our maps to look at the mammoths,&#39; he said.</i><br /><br /><i>It&#39;s something MacDonald has dreamed of for a long time, he said. He was working in Siberia several years ago when a colleague found a woolly mammoth tooth.</i><br /><br /><i>&#39;We looked at it and held it, and just the thought that those immense creatures had been there not that long ago in geologic time and yet completely disappeared was really amazing,&#39; MacDonald said. &#39;How warming in the past has been involved in extinction might help us prevent extinctions in the future.&#39;&quot;</i><br /><br />GRAPHIC REFERENCES:<br /><br />[1] 3 images on left + excerpt: <a href=\"http://bickersteth.blogspot.com/2014/06/mammoth-yarns.html\" class=\"ot-anchor\">http://bickersteth.blogspot.com/2014/06/mammoth-yarns.html</a><br /><br />[2][3] Hibben, Frank, <i>The Lost Americans</i> (New York: Thomas &amp; Crowell Co. 1946), from <a href=\"http://www.sott.net/article/128992-Forget-About-Global-Warming-We-re-One-Step-From-Extinction\" class=\"ot-anchor\">http://www.sott.net/article/128992-Forget-About-Global-Warming-We-re-One-Step-From-Extinction</a><br /><br />[4] Sanderson, Ivan T., &quot;Riddle of the Frozen Giants&quot;, Saturday Evening Post, No 39, January 16, 1960, from <a href=\"http://www.sott.net/article/128992-Forget-About-Global-Warming-We-re-One-Step-From-Extinction\" class=\"ot-anchor\">http://www.sott.net/article/128992-Forget-About-Global-Warming-We-re-One-Step-From-Extinction</a><br /><br />[5][6] <a href=\"http://www.sott.net/article/128992-Forget-About-Global-Warming-We-re-One-Step-From-Extinction\" class=\"ot-anchor\">http://www.sott.net/article/128992-Forget-About-Global-Warming-We-re-One-Step-From-Extinction</a>, note that [6] does not specifically cite the 10 million animal claim<br /><br />[7] Farrand, William R., &quot;Frozen Mammoths and Modern Geology&quot;, <i>Science</i>, Vol 133, No 3455, March 17, 1961, from <a href=\"http://www.sott.net/article/128992-Forget-About-Global-Warming-We-re-One-Step-From-Extinction\" class=\"ot-anchor\">http://www.sott.net/article/128992-Forget-About-Global-Warming-We-re-One-Step-From-Extinction</a><br /><br />[8] <a href=\"https://www.sott.net/article/128992-Forget-About-Global-Warming-We-re-One-Step-From-Extinction\" class=\"ot-anchor\">https://www.sott.net/article/128992-Forget-About-Global-Warming-We-re-One-Step-From-Extinction</a><br /><br />[9] Farrand, William R., &quot;Frozen Mammoths and Modern Geology&quot;, <i>Science</i>, Vol.133, No. 3455, March 17, 1961, from <a href=\"http://www.sott.net/article/128992-Forget-About-Global-Warming-We-re-One-Step-From-Extinction\" class=\"ot-anchor\">http://www.sott.net/article/128992-Forget-About-Global-Warming-We-re-One-Step-From-Extinction</a><br /><br />[10] <a href=\"http://www.lwl.org/pressemitteilungen/mitteilung.php?urlID=16070\" class=\"ot-anchor\">http://www.lwl.org/pressemitteilungen/mitteilung.php?urlID=16070</a><br /><br />[11]-[13] <a href=\"http://phenomena.nationalgeographic.com/2014/07/18/baby-mammoths-yield-hi-res-details-for-paleontologists/\" class=\"ot-anchor\">http://phenomena.nationalgeographic.com/2014/07/18/baby-mammoths-yield-hi-res-details-for-paleontologists/</a><br /><br />[14] <a href=\"http://bickersteth.blogspot.com/2014/06/mammoth-yarns.html\" class=\"ot-anchor\">http://bickersteth.blogspot.com/2014/06/mammoth-yarns.html</a><br /><br />[15] Many of the images on this graphic come from this book: <i>The Cycle of Cosmic Catastrophes - Flood, Fire and Famine in the History of Civilization</i> by Richard Firestone and Allen West (June 5, 2006), <a href=\"http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/\" class=\"ot-anchor\">http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/</a><br /><br />[16] Valentine, quoted by Berlitz, Charles, <i>The Mystery of Atlantis</i> (New York, 1969)<br /><br />[17] Simpson, George G., <i>Horses</i>, New York: Oxford University Press, 1961; and Martin, P.S. &amp; Guilday, J.E., &quot;Bestiary for Pleistocene Biologists&quot;, Pleistocene Extinction, Yale University, 1967<br /><br />[18] <a href=\"http://www.sott.net/article/128992-Forget-About-Global-Warming-We-re-One-Step-From-Extinction\" class=\"ot-anchor\">http://www.sott.net/article/128992-Forget-About-Global-Warming-We-re-One-Step-From-Extinction</a><br /><br />[19] Leonard, R. Cedric, Appendix A in &quot;A Geological Study of the Mid-Atlantic Ridge&quot;, Special Paper No. 1 (Bethany: Cowen Publishing 1979)<br /><br />[20] <i>The Cycle of Cosmic Catastrophes - Flood, Fire and Famine in the History of Civilization</i> by Richard Firestone and Allen West (June 5, 2006), <a href=\"http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/\" class=\"ot-anchor\">http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/</a><br /><br />[21] <a href=\"http://cosmictusk.com/seek-and-ye-shall-find-et-material-confirmed-in-murray-springs-black-mat/\" class=\"ot-anchor\">http://cosmictusk.com/seek-and-ye-shall-find-et-material-confirmed-in-murray-springs-black-mat/</a><br /><br />[22] <a href=\"http://phys.org/news150097682.html\" class=\"ot-anchor\">http://phys.org/news150097682.html</a><br /><br />[23]-[28] <i>The Cycle of Cosmic Catastrophes - Flood, Fire and Famine in the History of Civilization</i> by Richard Firestone and Allen West (June 5, 2006), <a href=\"http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/\" class=\"ot-anchor\">http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/</a><br /><br />[29] <a href=\"http://skywalker.cochise.edu/wellerr/students/murray-springs/project.htm\" class=\"ot-anchor\">http://skywalker.cochise.edu/wellerr/students/murray-springs/project.htm</a><br /><br />[30] <i>The Cycle of Cosmic Catastrophes - Flood, Fire and Famine in the History of Civilization</i> by Richard Firestone and Allen West (June 5, 2006), <a href=\"http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/\" class=\"ot-anchor\">http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/</a><br /><br />[31][32] <a href=\"http://news.bbc.co.uk/2/hi/science/nature/7130014.stm\" class=\"ot-anchor\">http://news.bbc.co.uk/2/hi/science/nature/7130014.stm</a><br /><br />[33] Some minor processing was performed on this image to clean up the compression artifacts, original at <a href=\"http://www.nature.com/news/2007/071212/full/news.2007.372.html\" class=\"ot-anchor\">http://www.nature.com/news/2007/071212/full/news.2007.372.html</a><br /><br />[34] <a href=\"http://news.bbc.co.uk/2/hi/science/nature/7130014.stm\" class=\"ot-anchor\">http://news.bbc.co.uk/2/hi/science/nature/7130014.stm</a><br /><br />[35]-[38] <i>The Cycle of Cosmic Catastrophes - Flood, Fire and Famine in the History of Civilization</i> by Richard Firestone and Allen West (June 5, 2006), <a href=\"http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/\" class=\"ot-anchor\">http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/</a><br /><br />[39] <a href=\"http://news.bbc.co.uk/2/hi/science/nature/7130014.stm\" class=\"ot-anchor\">http://news.bbc.co.uk/2/hi/science/nature/7130014.stm</a><br /><br />[40] <i>The Cycle of Cosmic Catastrophes - Flood, Fire and Famine in the History of Civilization</i> by Richard Firestone and Allen West (June 5, 2006), <a href=\"http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/\" class=\"ot-anchor\">http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/</a><br /><br />[41] <a href=\"https://www.thunderbolts.info/tpod/2005/arch05/050325blueberries.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2005/arch05/050325blueberries.htm</a><br /><br />[42][43] <a href=\"http://www.pnas.org/content/109/28/E1903/F1.expansion.html\" class=\"ot-anchor\">http://www.pnas.org/content/109/28/E1903/F1.expansion.html</a><br /><br />[44] <a href=\"http://cosmictusk.com/page/3/\" class=\"ot-anchor\">http://cosmictusk.com/page/3/</a><br /><br />[45]-[47] <i>The Cycle of Cosmic Catastrophes - Flood, Fire and Famine in the History of Civilization</i> by Richard Firestone and Allen West (June 5, 2006), <a href=\"http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/\" class=\"ot-anchor\">http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/</a><br /><br />[48] <i>Metamorphic Star</i>, Dwardu Cardona (2011)<br /><br />[49] <i>The Cycle of Cosmic Catastrophes - Flood, Fire and Famine in the History of Civilization</i> by Richard Firestone and Allen West (June 5, 2006), <a href=\"http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/\" class=\"ot-anchor\">http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/</a><br /><br />[50] Guthrie, <i>Frozen Fauna</i>, op. cit., pp. 9-10. (I&#39;m not exactly sure what the original citation was, but it might be this: <a href=\"http://www.amazon.com/Frozen-Fauna-Mammoth-Steppe-Story/dp/0226311236\" class=\"ot-anchor\">http://www.amazon.com/Frozen-Fauna-Mammoth-Steppe-Story/dp/0226311236</a>)<br /><br />[51] Radiocarbon, Vol. 4, (1962), p. 180.<br /><br />[52][53] <i>The Cycle of Cosmic Catastrophes - Flood, Fire and Famine in the History of Civilization</i> by Richard Firestone and Allen West (June 5, 2006), <a href=\"http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/\" class=\"ot-anchor\">http://www.amazon.com/Cycle-Cosmic-Catastrophes-Stone-Age-Changed/dp/1591430615/</a><br /><br />[54] <i>Metamorphic Star</i>, Dwardu Cardona (2011), see below for Dwardu&#39;s references<br /><br />[55] moqui marbles, utah Iron oxide concretions (Moqui marbles) Moqui Marbles, hematite concretions, from the Navajo Sandstone of southeast Utah. <a href=\"https://www.pinterest.com/pin/237635317811793018/\" class=\"ot-anchor\">https://www.pinterest.com/pin/237635317811793018/</a><br /><br />[56] <a href=\"http://siberiantimes.com/science/casestudy/features/did-dogs-help-drive-the-woolly-mammoth-to-extinction/\" class=\"ot-anchor\">http://siberiantimes.com/science/casestudy/features/did-dogs-help-drive-the-woolly-mammoth-to-extinction/</a><br /><br />DWARDU CARDONA&#39;S REFERENCES:<br /><br />[1] But see Flare Star, pp 443 ff.<br /><br />[2] That a supernova preceded the eventual choice of a cometary impact as the<br />cause of this extinction is recounted in Primordial Star, pp 228 ff.<br /><br />[3] RB Firestone &amp; W Topping, &quot;Terrestrial Evidence of a Nuclear Catastrophe in<br />Paleoindian Times,&quot; Mammoth Trumpet (March 2001), pp 9-16.<br /><br />[4] F Hoyle &amp; C Wickramsinghe, &quot;Cometary Impacts and Ice Ages,&quot; Astrophysics and Space Science (March 2001), p 374.<br /><br />[5] Ibid.<br /><br />[6] JB Kloosterman, &quot;The Usselo Horizon: A Worldwide Charcoal-Rich Layer of Alleroed Age,&quot; Proceedings of the Conference: New Scenarios on the Evolution of the Solar System and Consequences on History of Earth and Man (Bergamo, 2002), p 153.<br /><br />[7] Ibid<br /><br />[8] Ibid<br /><br />[9] R Dalton, &quot;Blast in the Past?&quot; Nature (May 17, 2007), p 257.<br /><br />[10] D Krotz, &quot;Supernova Explosion May Have Caused Mammoth Extinction,&quot; Lawrence Berkeley Laboratory Research News at <a href=\"http://ibl.gov/Science-Articles/Archive/NSD\" class=\"ot-anchor\">ibl.gov/Science-Articles/Archive/NSD</a> (September 23, 2005), p 1.<br /><br />[11] R Dalton, op cit, p 256; H Pringle, &quot;Firestorm From Space Wiped Out Prehistoric Americans,&quot; New Scientist (May 26, 2007), pp 8, 9.<br /><br />[12] C Lee, &quot;Comet Over Canada Linked to Extinction of Mammoths,&quot; The Vancouver Sun (June 12, 2007), p A7.<br /><br />[13] J Ruvinsky, &quot;The Great American Extinction,&quot; Discover (August 2007), p 11.<br /><br />[14] R Mitchum, &quot;Scientists Say Comet Killed Off Mammoths, Saber-Toothed Tigers,&quot;<a href=\"http://physorg.com\" class=\"ot-anchor\">physorg.com</a> (January 2, 2009).<br /><br />[15] Ibid<br /><br />[16] D Krotz, loc cit.<br /><br />[17] R Firestone, et al, The Cycle of Cosmic Catastrophes: Flood, Fire, and Famine in the History of Civilization (Rochester, Vermont, 2006), in toto<br /><br />[18] The Intuitive-Connections Network at <a href=\"http://intuitive-connections.net/2007/bookcosmiccatastrophes.htm\" class=\"ot-anchor\">intuitive-connections.net/2007/bookcosmiccatastrophes.htm</a><br /><br />[19] A Minard, &quot;Ancient Meteor Blast Peppered Mammoths with &#39;Shrapnel&#39;,&quot; <a href=\"http://nationalgeographic.com\" class=\"ot-anchor\">nationalgeographic.com</a> (December 14, 2007); R Dalton, loc cit<br /><br />[20] F Largent, &quot;The Clovis Comet -- Part I: Evidence for a Cosmic Collision 12,900 Years Ago,&quot; Mammoth Trumpet (January 2008), p 25.<br /><br />[21] J Ruvinsky, loc cit<br /><br />[22] Ibid<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Denial of the Failure of Planetary Formation Theory","summary":"A Nature Article from July 2014 Declared that \"The Discovery of Thousands of Star Systems Wildly Different from Our Own has Demolished Ideas About How Planets Form\" / The Headline Announced that \"Astronomers are Searching for a Whole New Theory\" / They Said \"The First Meaningful Statistics Undermined the Standard Theory for Good\" / That Announcement was Apparently Long Forgotten Just Four Months Later When a Single Example of Textbook Planetary Formation Was Said to Provide \"Direct Proof that Nature and Theory are Very Much in Agreement\"","image":"https://lh3.googleusercontent.com/-jERnR4BQ4PY/Vyl_O_TCZJI/AAAAAAAAH28/bj14DpGyRZsy69reRPEnfz4CYi4d0MCIA/w3600-h5040/the-failure-and-denial-of-planetary-formation-theory-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/ZZ1azpB378D","publishDate":"2016-05-04T04:49:51.136Z","updateDate":"2017-01-03T02:32:34.090Z","text":"<b>The Denial of the Failure of Planetary Formation Theory: A Nature Article from July 2014 Declared that &quot;The Discovery of Thousands of Star Systems Wildly Different from Our Own has Demolished Ideas About How Planets Form&quot; / The Headline Announced that &quot;Astronomers are Searching for a Whole New Theory&quot; / They Said &quot;The First Meaningful Statistics Undermined the Standard Theory for Good&quot; / That Announcement was Apparently Long Forgotten Just Four Months Later When a Single Example of Textbook Planetary Formation Was Said to Provide &quot;Direct Proof that Nature and Theory are Very Much in Agreement&quot;</b><br /><br /><a href=\"http://www.nature.com/news/astronomy-planets-in-chaos-1.15480\" class=\"ot-anchor\">http://www.nature.com/news/astronomy-planets-in-chaos-1.15480</a><br /><br /><b><i>&quot;Astronomy: Planets in chaos</i></b><br /><br /><b><i>The discovery of thousands of star systems wildly different from our own has demolished ideas about how planets form. Astronomers are searching for a whole new theory.</i></b><br /><br /><i>Ann Finkbeiner</i><br /><i>02 July 2014&quot;</i><br /><br />Image caption reads:<br /><br /><b><i>&quot;Astronomers know that both stars and planets form from interstellar gas clouds — but the details are still murky.&quot;</i></b><br /><br />The article ...<br /><br /><b><i>&quot;Not so long ago — as recently as the mid-1990s, in fact — there was a theory so beautiful that astronomers thought it simply had to be true.</i></b><br /><br /><i>They gave it a rather pedestrian name: the core-accretion theory. But its beauty lay in how it used just a few basic principles of physics and chemistry to account for every major feature of our Solar System. </i><b><i>It explained why all the planets orbit the Sun in the same direction; why their orbits are almost perfectly circular and lie in or near the plane of the star&#39;s equator; why the four inner planets (Mercury, Venus, Earth and Mars) are comparatively small, dense bodies made mostly of rock and iron; and why the four outer planets (Jupiter, Saturn, Uranus and Neptune) are enormous, gaseous globes made mostly of hydrogen and helium. And because the same principles of physics and astronomy must apply throughout the Universe, it predicted that any system of &#39;exoplanets&#39; around another star would look pretty much the same.</i></b><br /><br /><b><i>But in the mid-1990s, astronomers actually started finding those exoplanets — and they looked nothing like those in our Solar System. Gas giants the size of Jupiter whipped around their stars in tiny orbits, where core accretion said gas giants were impossible. Other exoplanets traced out wildly elliptical orbits. Some looped around their stars&#39; poles. Planetary systems, it seemed, could take any shape that did not violate the laws of physics.</i></b><br /><br /><i>Following the launch of NASA&#39;s planet-finding Kepler satellite in 2009, the number of possible exoplanets quickly multiplied into the </i><b><i>thousands — enough to give astronomers their first meaningful statistics on other planetary systems, and to undermine the standard theory for good.</i></b><i> Not only were there lots of exoplanet systems bearing no resemblance to ours, but </i><b><i>the most commonly observed type of planet — a &#39;super-Earth&#39; that falls between the sizes of our world and Neptune, which is four times bigger — does not even exist in our Solar System. Using our planetary family as a model, says astronomer Gregory Laughlin of the University of California, Santa Cruz, &#39;has led to no success in extrapolating what&#39;s out there&#39;.</i></b><br /><br /><i>The findings have triggered controversy and confusion, as astronomers struggle to work out what the old theory was missing. They are trying ideas, </i><b><i>but are still far from sure how the pieces fit together. The field in its current state &#39;doesn&#39;t make much sense&#39;</i></b><i>, says Norm Murray of the Canadian Institute for Theoretical Astrophysics in Toronto. </i><b><i>&#39;It&#39;s impossible right now to account for everything,&#39;</i></b><i> agrees Kevin Schlaufman, an astrophysicist at the Massachusetts Institute of Technology (MIT) in Cambridge. Until researchers reach a new consensus, they will not be able to understand how our own Solar System fits into the grand scheme of things, let alone predict what else might exist.</i><br /><br /><i>A planet is born</i><br /><br /><i>In the search for an overarching theory, astronomers do agree that core accretion has some things right: planets are leftovers from the birth of stars, a process in which interstellar clouds of hydrogen and helium gas contract until their cores grow dense and hot enough to ignite (see &#39;Planetary standard model&#39;).</i><br /><br /><i>Some hydrogen and helium does not fall straight into the newborn star, but instead swirls around it, forming a thin, flat disk that orbits the star&#39;s equator. Carried along with this gas are tiny solid grains of heavier elements such as carbon, oxygen, nitrogen, silicon and iron, all made in earlier generations of stars. As the disk cools, electrostatic charges stick these grains together to form loose conglomerates that eventually grow into kilometre-scale bodies known as planetesimals. At that point gravity takes over, and the planetesimals collide, fragment, mash together and grow into full-sized planets. As that happens, friction with the surrounding gas forces them into almost circular orbits.</i><br /><br /><i>This core-accretion process happens throughout the disk but has different results in different locations. Towards the centre, the only grains that can survive the heat from the newborn star are materials with high melting points, such as iron and various minerals — essentially, rock. The result is an inner system of rock–iron planets, limited to an Earth-mass or less by the disk&#39;s relative scarcity of solid materials.&quot;</i><br /><br />(... What they have failed to mention here, btw, is the obvious contradiction that the Sun is largely made of hydrogen and helium, the two lightest elements.  Metals make up less than 0.1% of it ...)<br /><br /><i>Farther away from the star, however, the disk is cool enough to preserve ices that are much more abundant than iron and rock, and that accrete readily on the planetesimals. Once the planetesimals grow to maybe ten times the mass of Earth, they can start pulling in the surrounding hydrogen and helium, quickly accreting into Jupiter- and Saturn-like gas giants tens or hundreds of times Earth&#39;s mass. They stop growing only when they have cleared all the gas from their orbits.</i><br /><br /><i>Space oddities</i><br /><br /><i>This is also where the standard theory of planetary formation stops, mainly because it fits our Solar System so well: rocky planets on the inside, gas giants on the outside. </i><b><i>But in 1995, when observers in Switzerland reported the discovery of the first unambiguous exoplanet in orbit around a Sun-like star, it was clear that the standard model had left something out. Precise measurements of the radial velocity of the star 51 Pegasi showed minuscule repeated changes caused by a planet&#39;s gravitational pull. The data showed that the planet&#39;s mass was 150 times that of Earth, or nearly half that of Jupiter. This clearly put it in the gas-giant category. Yet the planet, 51 Pegasi b, orbited its star every four Earth days at a distance of just 7.5 million kilometres, or 0.05 astronomical units (1 AU is the distance between Earth and the Sun). This is much smaller than the 0.47-AU orbit of Mercury, and puts the planet in a region where the temperature of the gas disk during formation would have been about 2,000 kelvin, much too hot for solid ice and gases. &#39;It was like, &#39;What! We weren&#39;t even looking for that,&#39; says Derek Richardson, an astronomer at the University of Maryland at College Park.</i></b><br /><br /><i>Astronomers called it a hot Jupiter. They soon turned up a family of such giant exoplanets between one-third and ten times the mass of Jupiter, orbiting between 0.03 AU and 3 AU from their stars. And there were other oddities: </i><b><i>WASP-7b orbits its star&#39;s poles instead of its equator; the orbit of HD 80606b is highly elliptical, ranging from 0.03 AU at one end to 0.8 AU at the other; HAT-P-7b&#39;s orbital direction is opposite to its star&#39;s spin.</i></b><br /><br /><i>By 2000, astronomers had found 30 exoplanets; by the end of 2008, 330. Then NASA launched Kepler, which spent the next four years searching for exoplanets in a single patch of sky containing some 150,000 Sun-like stars. Kepler identifies planets by detecting the slight dimming in a star&#39;s light that occurs when an object passes in front of it. This &#39;transit&#39; method can find planets much smaller than the radial-velocity technique can, giving astronomers a chance to detect other Earths. Kepler has now found 974 exoplanets, with 4,254 further candidates waiting for confirmation by ground-based measurements. If all of Kepler&#39;s candidates are confirmed — and they do tend to be — then the techniques taken together will have found well over 5,000 exoplanets.</i><br /><br /><i>Kepler&#39;s planets run in odd systems. The Kepler-56 system, for example, has two planets, of 22 and 181 Earth masses, </i><b><i>both orbiting at 45° to the star&#39;s plane. In the Kepler-47 system, two planets both orbit a binary star.</i></b><i> Kepler-36&#39;s planets are closer together than any others yet seen: they orbit the star every 14 days and 16 days, respectively. One is rocky and is eight times as dense as the other, which is ice. &#39;How did they get so close together?&#39; wonders Richardson. </i><b><i>&#39;And how are they so different?&#39;</i></b><i> Kepler-11 is orbited by six planets, five of which are among the smallest and least massive ever found. Their densities, says David Charbonneau of the Harvard–Smithsonian Center for Astrophysics in Cambridge, Massachusetts, &#39;are shockingly low, they must be mostly ice or have significant gas envelopes&#39; — </i><b><i>yet all five are tucked in together within 0.25 AU of their star.</i></b><br /><br /><i>Not like the others</i><br /><br /><i>Kepler&#39;s biggest surprise has come from statistical summaries of its findings. The planets seen so far can be said to fall into three categories: hot Jupiters; giant planets with idiosyncratic orbits; and super-Earths. Worlds in this third category are generally found in compact systems of two to four planets each, orbiting their stars at distances from 0.006 to 1 AU in periods ranging from more than 100 days down to hours. Although there are no super-Earths in our Solar System, they orbit at least 40% of all nearby Sun-like stars, </i><b><i>which makes them the most common type of planet found.</i></b><i> &#39;The hot Jupiters are freaks, less than 1%,&#39; says Joshua Winn, a physicist who studies exoplanets at MIT. &#39;The long-period eccentric giants are maybe 10%. </i><b><i>The 40% — that makes you wonder.&#39;</i></b><br /><br /><i>The question is how to account for all this planetary-system diversity. In general, astronomers begin with the standard core-accretion theory then add in processes that probably did not play out in our own Solar System.</i><br /><br /><i>To explain hot Jupiters, for example, they suggest that the planets did not stick around at their birth place in the cold outer reaches of stellar disks. Instead, the infant giants spiralled inwards as viscous gas in the disk slowed their orbits. At some point, for reasons unknown, they stopped their death spirals and settled into stable orbits close to their stars. Despite the extreme temperatures, the giant planets had strong-enough gravity to keep hold of their gas.</i><br /><br /><i>Eccentric giants could be the result of gravitational interaction. If several giant planets started to migrate, they might have passed one another closely enough for their gravity to sling them in crazy new directions. They could have scattered out of alignment with the rest of the system, got knocked into orbits opposite to the star&#39;s rotation or even been flung from the system entirely.</i><br /><br /><i>Super-Earths are harder to account for. For one thing, the term has no agreed definition, says Winn: some of the smallest, closest-in planets might actually be the stripped cores of migrating giants that came too close to their stars and got their gas blown off. &#39;Super-Earths are probably </i><b><i>not</i></b><i> nice, </i><b><i>stereotypical</i></b><i> birds,&#39; says Eric Ford, an astrophysicist at the Pennsylvania State University in University Park. </i><b><i>&#39;Maybe some are more like penguins.&#39;&quot;</i></b><br /><br />(Has the denial already begun before the end of the article admitting defeat?)<br /><br /><b><i>&quot;The sheer size of the super-Earth flock requires explanation. The standard theory cannot do that because in existing models, the central regions of stellar disks contain much too little material to create several close-in super-Earths.</i></b><i> But theorists have found ways around that problem. Laughlin and Eugene Chiang, an astronomer at the University of California, Berkeley, have shown that compact systems of super-Earths can grow from disks with much greater masses, distributed closer to their stars. Murray and Brad Hansen, an astrophysicist at the University of California, Los Angeles, have also proposed a more massive disk, but one in which super-Earths are born from planetesimals that formed farther out in the disk, then migrated in before they collected into planets.</i><br /><br /><i>Astronomer Douglas Lin of the University of California, Santa Cruz, and his colleagues have tried to merge all the categories of planet into what Winn calls &#39;an all-singing, all-dancing model&#39; that can account for all the systems seen. It starts by assuming that the distribution of mass in the disk will vary from system to system. After that, says Lin, it&#39;s &#39;migration, migration, migration&#39;: all types of planet grow to full size in the middle to outer part of the disk, and then move inwards in order.&quot;</i><br /><br />What is remarkable is that they have up to this moment rejected any role for actual history.  They only include it to save their model.<br /><br /><b><i>&quot;Such models are appealing, but the concept of migration, especially of the smaller planets, gives some researchers pause — if only because no one has ever seen it happening.</i></b><i> The necessary observations may not be possible: stars young enough to have planets migrating through protoplanetary disks are still surrounded by dust, and their light flickers, making it extremely unlikely that current methods will be able to pick out the dimming caused by a transiting planet. The theory is not settled, either. </i><b><i>Modellers have found it hard to explain why migrating planets, big or small, would stop in the orbits that astronomers have observed. In simulations, says Winn, they don&#39;t: &#39;the planets plop right down on the star&#39;.</i></b><br /><br /><b><i>Perhaps the biggest question is why our Solar System is so different. Why doesn&#39;t it contain the one kind of planet most common around other Sun-like stars? Why are there no planets inside Mercury&#39;s orbit when that&#39;s where most of the exoplanets are in other systems? Why do we have a balance of large and small planets when most other systems seem to choose one or the other but not both?</i></b><br /><br /><i>Astronomers still don&#39;t know how different we are. Observations of exoplanets are seriously biased: neither of the two main techniques would find our widely spread-out Solar System, nor are they sensitive to systems with both large and small planets. It might be that we are not unusual at all.</i><br /><br /><i>Future observations may give some answers. Kepler has been hobbled by a failure of the mechanisms that keep it pointing at its original target patch of sky, but last month it was approved to keep taking data. The longer it does so, the larger the exoplanet orbits it will be able to see. Ground-based programmes are starting to operate with improved instruments, some also capable of seeing planets 5 AU or more from their stars. And from 2017, NASA&#39;s planned Transiting Exoplanet Survey Satellite (TESS) will look for planetary transits across all the bright stars in the sky. The wider range of possible exoplanet candidates makes it more likely that astronomers will spot a Solar System like ours — if one exists.</i><br /><br /><i>Meanwhile, researchers continue to nurture their mess of models, </i><b><i>which have grown almost as exotic and plentiful as the planets they seek to explain. And if the current theories are disjointed, ad hoc and no longer beautiful, that is often how science proceeds, notes Murray. &#39;Life,&#39; he says, &#39;is like that.&#39;&quot;</i></b><br /><br />Nature 511, 22–24 (03 July 2014) doi:10.1038/511022a<br /><br /><a href=\"https://public.nrao.edu/static/pr/planet-formation-alma.html\" class=\"ot-anchor\">https://public.nrao.edu/static/pr/planet-formation-alma.html</a><br /><br /><i>&quot;For Release: 7 a.m. E.S.T. Thursday, November 6, 2014</i><br /><b>Birth of Planets Revealed in Astonishing Detail in ALMA’s ‘Best Image Ever’</b>_<br /><br /><b><i>Astronomers have captured the best image ever of planet formation around an infant star</i></b><i> as part of the testing and verification process for the Atacama Large Millimeter/submillimeter Array’s (ALMA) new high-resolution capabilities.</i><br /><br /><b><i>This revolutionary new image reveals in astonishing detail the planet-forming disk surrounding HL Tau, a Sun-like star located approximately 450 light-years from Earth in the constellation Taurus.</i></b><br /><br /><i>ALMA uncovered never-before-seen features in this system, including multiple concentric rings separated by clearly defined gaps. </i><b><i>These structures suggest that planet formation is already well underway around this remarkably young star.&quot;</i></b><br /><br />(So they say ... we were also just informed that this arrangement is enormously rare!)<br /><br /><b><i>&#39;These features are almost certainly the result of young planet-like bodies that are being formed in the disk.</i></b><i> This is surprising since HL Tau is no more than a million years old and such young stars are not expected to have large planetary bodies capable of producing the structures we see in this image,&#39; said ALMA Deputy Director Stuartt Corder.</i><br /><br /><i>All stars are believed to form within clouds of gas and dust that collapse under gravity. Over time, the surrounding dust particles stick together, growing into sand, pebbles, and larger-size rocks, which eventually settle into a thin protoplanetary disk where asteroids, comets, and planets form.</i><br /><br /><i>Once these planetary bodies acquire enough mass, they dramatically reshape the structure of their natal disk, fashioning rings and gaps as the planets sweep their orbits clear of debris and shepherd dust and gas into tighter and more confined zones.</i><br /><br /><i>The new ALMA image reveals these striking features in exquisite detail, providing the clearest picture to date of planet formation. Images with this level of detail were previously only seen in computer models and artist concepts. </i><b><i>ALMA, living up to its promise, has now provided direct proof that nature and theory are very much in agreement.</i></b><br /><br /><b><i>&#39;This new and unexpected result provides an incredible view of the process of planet formation. Such clarity is essential to understand how our own Solar System came to be and how planets form throughout the Universe,&#39;</i></b><i> said Tony Beasley, director of the National Radio Astronomy Observatory (NRAO) in Charlottesville, Virginia, which manages ALMA operations for astronomers in North America.</i><br /><br /><i>HL Tau is hidden in visible light behind a massive envelope of dust and gas. Since ALMA observes at much longer wavelengths, it is able to peer through the intervening dust to study the processes right at the core of this cloud. &quot;This is truly one of the most remarkable images ever seen at these wavelengths. The level of detail is so exquisite that it&#39;s even more impressive than many optical images. </i><b><i>The fact that we can see planets being born will help us understand not only how planets form around other stars but also the origin of our own Solar System,&#39;</i></b><i> said NRAO astronomer Crystal Brogan.&quot;</i><br /><br />(We must consider the possibility that we do not actually understand how planets form.)<br /><br /><i>&quot;ALMA&#39;s new high-resolution capabilities were achieved by spacing the antennas up to 15 kilometers apart. This baseline at millimeter wavelengths enabled a resolution of 35 milliarcseconds, which is equivalent to a penny as seen from more than 110 kilometers away.</i><br /><br /><i>&#39;Such a resolution can only be achieved with the long baseline capabilities of ALMA and provides astronomers with new information that is impossible to collect with any other facility, including the best optical observatories,&#39; noted ALMA Director Pierre Cox.</i><br /><br /><i>These long baselines fulfill one of ALMA’s major objectives and mark an impressive technological and engineering milestone. Future observations at ALMA&#39;s longest possible baseline of 16 kilometers will produce even clearer images and continue to expand our understanding of the cosmos.</i><br /><br /><i>&#39;This observation illustrates the dramatic and important results that come from NSF supporting world-class instrumentation such as ALMA,&#39; said Fleming Crim, the National Science Foundation assistant director for Mathematical and Physical Sciences. &#39;ALMA is delivering on its enormous potential for revealing the distant Universe and is playing a unique and transformational role in astronomy.&#39;&quot;</i><br /><br />The image is indeed amazing, but the data appears to be taking us somewhere else, and there is at least one other alternative explanation for what this might be ...<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Why Venus is Not a Greenhouse","summary":"Not Even Science Journalists Have Taken the Time to Investigate the Origin of the Idea / The Venus Pioneer Probes Directly Sampled the Venusian Atmosphere all the Way to the Surface, But this Data Ruled Out Carl Sagan's Super-Greenhouse / Scientists Were Subsequently Tasked with Hypothesizing How the Instruments Might Have Failed / When Possible Failures were Found, the Data Was then Corrected to Reflect the Greenhouse Theory, Under the Assumption that the Instruments Must Have Failed / None of this is Ever Mentioned Today When People Say that Venus is a Greenhouse","image":"https://lh3.googleusercontent.com/-X69c6k2j7LQ/VyWF5ZPPefI/AAAAAAAAH1M/7oAd70jO2vUxuqiRUvFnR30MLLdOwLQ4A/w6250-h8750/why-venus-is-not-a-greenhouse-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/hdDa75Zk3sd","publishDate":"2016-05-01T04:31:16.145Z","updateDate":"2017-01-03T02:33:15.615Z","text":"<b>Why Venus is Not a Greenhouse: Not Even Science Journalists Have Taken the Time to Investigate the Origin of the Idea / The Venus Pioneer Probes Directly Sampled the Venusian Atmosphere all the Way to the Surface, But this Data Ruled Out Carl Sagan&#39;s Super-Greenhouse / Scientists Were Subsequently Tasked with Hypothesizing How the Instruments Might Have Failed / When Possible Failures were Found, the Data Was then Corrected to Reflect the Greenhouse Theory, Under the Assumption that the Instruments Must Have Failed / None of this is Ever Mentioned Today When People Say that Venus is a Greenhouse</b><br /><br />From Charles Ginenthal&#39;s <i>Newton, Einstein &amp; Velikovsky: Celestial Mechanics, Electromagnetism, Solar System Instability &amp; Cosmology</i>, excerpts from p225 - 235:<br /><br /><b><i>&quot;Taken together, the top of Venus&#39; atmosphere exhibits a thermal imbalance of 15 to 20%, the middle and lower atmosphere, a thermal imbalance of around 50%, while the surface, on average, the thermal imbalance was 40 times greater than sunlight could provide</i></b><i> ... The tremendous rise in infrared radiation, as one nears the surface of Venus, is similar to the heat emitted, say, by a white-hot block of metal, in that as one puts one&#39;s hand closer and closer to it, the heat rises to greater and greater levels, but at a certain point close to it, the heat rises immensely.&quot;</i> (p228)<br /><br /><i>&quot;Taylor, et al., assumed what they had not proved was responsible for heating Venus&#39;s atmosphere at the cloud tops, and to escape their assumption, called this &#39;the most [probable] value.&#39;  They made that choice not based on scientific evidence, but on an assumption set ... up as a kind of psychological reality and admitted it was their &#39;most [probable] value.&#39;  This, then, became the mantra upon which all the other readings were to be hammered into place.&quot;</i> (p226)<br /><br /><b>Appendix: A Commentary on Venus&#39; Albedo Calculations by Lucy Skywalker</b> <br /><br /><a href=\"http://www.thunderbolts.info/forum/phpBB3/viewtopic.php?p=45823&amp;sid=b9180ef0854851540151487f6254408c\" class=\"ot-anchor\">http://www.thunderbolts.info/forum/phpBB3/viewtopic.php?p=45823&amp;sid=b9180ef0854851540151487f6254408c</a><br /><br /><i>&quot;Astronomers cite Tomasko&#39;s article on pages 611 - 612 of the definitive book on Venus, Venus, by Hunten, Colin, Donahue, Moroz, publ. Univ. of Arizona Press, 1983). This is a monstrous size and costs $80: difficult for ordinary people to gain access. Strangely, there are two articles about thermal equilibrium, the Tomasko article on pages 611-612, AND an article by F.W. Taylor on page 658. It turns out that </i><b><i>only by adopting the most myopic view possible can you get thermal equilibrium from the evidence, and that is what Tomasko does.</i></b><br /><br /><b><i>For thermal equilibrium to pertain, two numbers must match up; the first is emissions which all parties involved agree reads correctly as 0.76. The second number is the planetary albedo. Tomasko claims (pp 611-612) &quot;For the whole planet to be in equilibrium with absorbed sunlight, the bolometric albedo would have to be 0.76...&quot; But the Pioneer Venus readings on albedo (Taylor&#39;s article, page 658) was 0.80 plus/minus 0.02, and the calculations from Venera data (also page 658) are 0.79, plus 0.02, minus 0.01. The closest you could get and stay within error bounds is 0.78.</i></b><br /><br /><i>Now you might ask, what&#39;s a lousy 0.02 amongst friends; doesn&#39;t sound like much ... This is one of those cases in which a little bit appears to go a long way. Consider what Taylor claims would have to be believed if the 0.80 figure for albedo were to hold good (also page 658): &#39;Clearly, the Pioneer measurements of emission and reflection are not consistent with each other if radiative balance applies. A source inside Venus equal in magnitude to 20% of the solar input (i.e. accounting for the difference between 0.76 and 0.80) is very unlikely since Venus is thought to have an Earth-like makeup which would imply heat sources several orders of magnitude less than this. Also, even if such sources were postulated, it is difficult to construct a model in which these fairly large amounts of heat can be transported from the core to the atmosphere via a rocky crust without the later becoming sufficiently plastic to collapse the observed surface relief. This could only be avoided if the transport were very localized, i.e., via a relatively small number of giant volcanoes. Although large, fresh-looking volcanoes do appear to exist on Venus (see chapter 6), and the content of the atmosphere is consistent with vigorous output from these, a simple comparison with terrestrial volcanism shows that the volcanic activity on Venus would have to be on an awesome scale to account for the missing 10^15 W or so of power.&#39;</i><br /><br /><i>... Taylor does not particularly want to believe what the data is telling him: that there is no way that Venus is within error bounds of thermal equilibrium, but he does not deny the data. How then does Tomasko make a claim of equilibrium? </i><b><i>Tomasko cites one 1968 calculation of albedo of 0.77 +- 0.07 without bothering to tell you that that estimate was later revised upwards to 0.80 +- 0.07 in 1975</i></b><i> (Taylor tells us that on page 657), and notes that Taylor&#39;s indication of 0.02 error bounds for the Pioneer reading (the most recent, and done with the best instruments from the best distance) may be &#39;too small&#39;.&quot;</i><br /><br /><i>Taylor notes (page 758): &#39;A more acceptable alternative is that the preliminary estimate of 0.80 +- .02 for the albedo from the PV measurements is too high, since the uncertainty limit is now known from further work to be too conservative (J. V. Martinchik, personal communication). A fuller analysis of PV albedo data - still the best in terms of wave length, spatial and phase coverage, and radiometric precision, which is likely to be obtained for the forseeable future, is likely to resolve this puzzle. In conclusion then, the best thermal measurements of Venus, with the assumption of global energy balance, yield a value of the albedo of 0.76 +- .01; </i><b><i>this is the most probable value.&#39;</i></b><br /><br /><b><i>Tomasko is basing his entire case on one outdated calculation, and upon a &#39;personal communication from Martinchik&#39;. Taylor is saying that the best measurements available tell us that thermal balance is not to be had on Venus, and that Sagan and his super greenhouse theory are FUBAR, but that that can&#39;t really be, that he and others are probably, hopefully looking at something the wrong way, but he doesn&#39;t know what that something is.</i></b><br /><br /><i>That&#39;s a long way from claiming that Venus is &#39;within error bounds&#39; of equilibrium.</i><br /><br />* Charles Ginenthal&#39;s texts include crucial arguments to many ongoing debates today which have largely been forgotten by modern scientists.  Unfortunately, his texts are sometimes littered with spelling mistakes -- even the quotes he provides.  This is why it&#39;s particularly important to provide snapshots of the original quotes which he is pointing to, because many of his rare arguments retain their validity in spite of these spelling mistakes.<br /><br />** Also note that I have intentionally removed all references to Immanuel Velikovsky in this piece.  This is not because I am rejecting his ideas, but rather because many people will refuse to read anything about him -- even when those people are already climate change skeptics.  This is a common practice in my work.  We can as desired return to the subject of Velikovsky at any moment, but we should permit people to engage the basic claims and observations without upfront imposing a worldview.  The catastrophists of course raised many crucial questions in the sciences which are still alive to this day.<br /><br />REFERENCES:<br /><br />The graphic is a combination of the works of Charles Ginenthal and Ted Holden.  Many of the papers come from the massive text, <i>Venus</i> by D.M. Hunten, et al, eds. (Tucson, AZ 1983).  Some papers were retrieved via email request.  Sourcing these claims takes quite a bit of effort.<br /><br />[1] <i>Storms of My Grandchildren: The Truth About the Coming Climate Catastrophe and Our Last Chance to Save Humanity</i> at <a href=\"http://www.amazon.com/Storms-My-Grandchildren-Catastrophe-Humanity/dp/1608195023/\" class=\"ot-anchor\">http://www.amazon.com/Storms-My-Grandchildren-Catastrophe-Humanity/dp/1608195023/</a><br /><br />[2] <a href=\"http://www.universetoday.com/117425/why-is-venus-so-horrible/\" class=\"ot-anchor\">http://www.universetoday.com/117425/why-is-venus-so-horrible/</a><br /><br />[3] &quot;The mystery of Venus&#39; internal heat&quot;, Nov. 13 1980 issue of <i>New Scientist</i><br /><br />[4] <i>Pioneering Venus - A Planet Unveiled</i> at <a href=\"http://www.amazon.com/Pioneering-Venus-unveiled-Richard-Fimmel/dp/B009RXS7S8/\" class=\"ot-anchor\">http://www.amazon.com/Pioneering-Venus-unveiled-Richard-Fimmel/dp/B009RXS7S8/</a><br /><br />[5] Quote comes from <i>Newton, Einstein &amp; Velikovsky: Celestial Mechanics, Electromagnetism, Solar System Instability &amp; Cosmology</i>, Charles Ginenthal, p225<br /><br />[6] &quot;Thermal Structure of the Atmosphere of Venus&quot;, <i>Venus</i>, p226<br /><br />[7][8] Richard A. Kerr, &quot;Venus: Not Simple of Familiar, But Interesting,&quot; <i>Science</i>, Vol. 207 (1980), p. 289<br /><br />[9] W.F. Taylor, et al, &quot;The Thermal Balance of the Middle and Upper Atmosphere of Venus,&quot; <i>Venus</i>, p657-658<br /><br />[10] <i>Pioneering Venus - A Planet Unveiled</i> at <a href=\"http://www.amazon.com/Pioneering-Venus-unveiled-Richard-Fimmel/dp/B009RXS7S8/\" class=\"ot-anchor\">http://www.amazon.com/Pioneering-Venus-unveiled-Richard-Fimmel/dp/B009RXS7S8/</a><br /><br />[11] W.F. Taylor, et al, &quot;The Thermal Balance of the Middle and Upper Atmosphere of Venus,&quot; <i>Venus</i>, p657-658<br /><br />[12] Martin G. Tomasko, &quot;The Thermal Balance of the Lower Atmosphere of Venus,&quot; got this copy online, but also available in <i>Venus</i>, p606<br /><br />[13][14] H. E. Revercomb, L. A. Sromovsky, and V. E. Suomi, &quot;Net Thermal Radiation in the Atmosphere of Venus,&quot; Icarus 61, p521-538 (1985)<br /><br />[15] Richard A. Kerr, &quot;Venus: Not Simple of Familiar, But Interesting,&quot; Science, Vol. 207 (1980), p. 289<br /><br />[16] H. E. Revercomb, L. A. Sromovsky, and V. E. Suomi, &quot;Net Thermal Radiation in the Atmosphere of Venus,&quot; Icarus 61, p521-538 (1985)<br /><br />[17] G. M. Tomasko, &quot;The Thermal Balance of the Lower Atmosphere of Venus,&quot; <i>Venus</i>, p. 606.<br /><br />[18][19] John Ackerman, &quot;An Alternative View of Venus&quot;<br /><br />[20] L.G. Young, &quot;Infrared Spectra of Venus,&quot; <i>Exploration of the Planetary System</i> (Boston 1974), p.139<br /><br />[21] Richard A. Kerr, &quot;Venus: Not Simple of Familiar, But Interesting,&quot; Science, Vol. 207 (1980), p. 289<br /><br />[22] Michael Shermer&#39;s book is available at <a href=\"http://www.amazon.com/Skeptic-Encyclopedia-Pseudoscience-Set/dp/1576076539/\" class=\"ot-anchor\">http://www.amazon.com/Skeptic-Encyclopedia-Pseudoscience-Set/dp/1576076539/</a><br /><br />[23] <a href=\"http://ajae.oxfordjournals.org/content/early/2014/02/24/ajae.aau001.abstract\" class=\"ot-anchor\">http://ajae.oxfordjournals.org/content/early/2014/02/24/ajae.aau001.abstract</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"The Petroglyphs as Records of Cosmic Events","summary":"What Would Happen to the Aurora if the Earth Was Suddenly Impacted by a Violent Electrical Storm? / Decades of Plasma Physics Experimentation has Provided a Detailed Answer / The Patterns Observed in these Laboratories Can Explain 40% of all Petroglyph Types / The Scientific Community has so far Failed to Pay Attention / Not Even Most Petroglyph Experts Know this / Since these Petroglyphs are Only a Few Thousand Years Old, these Events Must Logically be Related to the 1st Stories Man Told","image":"https://lh3.googleusercontent.com/-POfxkcfglmY/VyBfePkuZgI/AAAAAAAAHyI/FamDHs1mvOgMSJcY_HoeC03GKXBuDe32A/w6250-h8750/the-petroglyphs-as-records-of-events-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/WaCFTUodH7q","publishDate":"2016-04-27T07:22:03.848Z","updateDate":"2017-01-03T02:33:42.099Z","text":"<b>The Petroglyphs as Records of Cosmic Events: What Would Happen to the Aurora if the Earth Was Suddenly Impacted by a Violent Electrical Storm? / Decades of Plasma Physics Experimentation has Provided a Detailed Answer / The Patterns Observed in these Laboratories Can Explain 40% of all Petroglyph Types / The Scientific Community has so far Failed to Pay Attention / Not Even Most Petroglyph Experts Know this / Since these Petroglyphs are Only a Few Thousand Years Old, these Events Must Logically be Related to the 1st Stories Man Told</b><br /><br />This is based upon the incredible research of Dr. Anthony Peratt, a former advisor for the Department of Energy and one of the world&#39;s most talented plasma physicists.  Dr. Peratt traveled the world to create this paper.<br /><br />The details about the aurora and all of the petroglyph images come from one single paper [1], here:<br /><br /><b>Characteristics for the Occurrence of a High-Current, Z-Pinch Aurora as Recorded in Antiquity - Part I</b><br /><a href=\"http://www.everythingselectric.com/product/anthony-peratt-z-pinch-aurora/\" class=\"ot-anchor\">http://www.everythingselectric.com/product/anthony-peratt-z-pinch-aurora/</a><br /><br />There is a follow-up Part II paper that discusses directionality, in an attempt to localize the events.<br /><br /><b>It&#39;s worth mentioning that critics of the Electric Universe launched a letter-writing campaign to the laboratories involved with this research in an effort to put a stop to it.</b> In some regards, their attempts to interfere succeeded.<br /><br />It&#39;s always seemed to me that the presentation of this paper is fatally flawed.  I personally had to read through it a number of times before I could fully appreciate the depth of analysis, and IMO you need look no further than this presentation problem to understand why nobody knows about this paper.  This graphic is my own attempt to set the situation right.<br /><br />This is not even close to comprehensive.  There is considerably more evidence illustrated in the paper; but this graphic will get you to second base on the subject.<br /><br />The 40% quote [reference 2] comes from one of the other researchers involved, Marinus van der Sluijs, one of the world&#39;s leading experts on the mythological archetypes.  But, the 40% claim is made in a different paper, here:<br /><br /><b>Rock-Solid Evidence for a Prehistoric Aurora</b><br /><a href=\"http://www.mythopedia.info/rock-solid.pdf\" class=\"ot-anchor\">http://www.mythopedia.info/rock-solid.pdf</a><br /><br />Reference [3] mostly comes from a text titled <i>An Introduction to Synchrotron Radiation - Techniques and Applications</i>.<br /><br />I need to go out of my way to point out that Dr. Anthony Peratt does not (apparently) subscribe to the exotic reconstructions of planetary history advocated by Electric Universe theorists.  Yet, some have claimed that this line of investigation represents a stunning corroboration of their pre-existing analysis of mythology.  Dr. Peratt and Marinus have a more traditional perspective on the findings.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Dark Energy Fudge Factor","summary":"Dark Energy Was Proposed as a Way to Resolve an Inconsistency Between Two Different Cosmic Yardsticks / The Consensus on Dark Energy was Achieved with Just One Paper in Nature and Agreement Between Just Two Teams of Scientists / The Problem is that Every Flawed Model Can be Saved by Fiddling Around with Numbers / Yet, there Are Few Introspective Voices in the Cosmological Community Today / And the Cosmology Conferences Are Likened to Big Parties Where People Enjoy their Shared Feelings About 'Consensus Cosmology'","image":"https://lh3.googleusercontent.com/-o85DFu49nw8/Vxvr6dagaOI/AAAAAAAAHww/VUoqm_HGUOMG1iAb26bJkRrDiSBqFivpg/w6250-h8750/the-dark-energy-fudge-factor-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/eGWiSdB778a","publishDate":"2016-04-23T21:43:35.814Z","updateDate":"2017-01-03T02:34:05.154Z","text":"<b>The Dark Energy Fudge Factor: Dark Energy Was Proposed as a Way to Resolve an Inconsistency Between Two Different Cosmic Yardsticks / The Consensus on Dark Energy was Achieved with Just One Paper in Nature and Agreement Between Just Two Teams of Scientists / The Problem is that Every Flawed Model Can be Saved by Fiddling Around with Numbers / Yet, there Are Few Introspective Voices in the Cosmological Community Today / And the Cosmology Conferences Are Likened to Big Parties Where People Enjoy their Shared Feelings About &#39;Consensus Cosmology&#39;</b><br /><br />The comic is a section of a larger comic from <a href=\"http://phdcomics.com/comics.php\" class=\"ot-anchor\">http://phdcomics.com/comics.php</a>.  There&#39;s a copy of the original at <a href=\"http://www.ph.unimelb.edu.au/~kmack/personal_files/phd111907s.gif\" class=\"ot-anchor\">http://www.ph.unimelb.edu.au/~kmack/personal_files/phd111907s.gif</a>.<br /><br />Quotes are from ...<br /><br /><i>Bankrupting Physics: How Today&#39;s Top Scientists Are Gambling Away Their Credibility</i><br />Alexander Unzicker and Sheilla Jones<br /><br /><b>Unzicker on Free Parameters</b><br /><br /><i>&quot;The current paradigm, the &#39;concordance model&#39; of the universe, is supported by most cosmologists. </i><b><i>It is described by six numbers in total (two of which are dark matter and dark energy)</i></b><i> ... But this is the question that really bothers me: Why is the universe as a whole described by these six numbers, and not by others?  And why just six?  Are we able to accurately calculate them, and if not, why not?  And what happens if new, still more precise observations come up with even more unexplained numbers?&quot;</i> (<i>Bankrupting Physics: How Today&#39;s Top Scientists Are Gambling Away Their Credibility</i>, Alexander Unzicker and Sheilla Jones, p9)<br /><br /><i>&quot;Despite all drawbacks, cosmology is still in good shape when compared to the sum of findings physicists have gathered from observations of the microscopic world -- the standard model of particle physics. </i><b><i>Cosmology&#39;s &#39;concordance model&#39; uses six numbers, which are called &#39;free parameters&#39; because they cannot be explained within the model but rather are fitted to the measurements. The standard model of particle physics needs not only six of them, but an impressive 17. Why 17?</i></b><i> The above questions about the six numbers in cosmology are dramatically amplified when it comes to particle physics ...</i><br /><br /><i>In his book The Trouble with Physics, Lee Smolin comments on the 17 free parameters. </i><b><i>&#39;The fact that there are that many freely specifiable constants in what is supposed to be a fundamental theory is a tremendous embarrassment.&#39;</i></b><br /><br /><b><i>If we naively add up 17 and 6, we would need 23 arbitrary numbers in all to understand physics and cosmology together. But it seems that physicists can&#39;t keep track of the count anymore. The British cosmologist Martin Rees and the Nobel laureate Frank Wilczek, in a review article, counted a total of 31 fundamental constants, all of which are, again, unexplained numbers. Interestingly enough, at the same time, Rees published a popular book with the catchy title Just Six Numbers, referring to the parameters in cosmology. As you can see, selling books on cosmology is still possible, but imagine trying to offer a treatis on physics to the public with the title Just Thirty-One Numbers!&quot;</i></b> (<i>Bankrupting Physics: How Today&#39;s Top Scientists Are Gambling Away Their Credibility</i>, Alexander Unzicker and Sheilla Jones, p10-11)<br /><br /><b>Unzicker on Dark Energy</b><br /><br /><i>&quot;Chapter 3: A Speedy Revolution:</i><br /><i>Why Cosmology is Going the Wrong Way</i><br /><br /><i>It was early October in 1923 at the Mount Wilson Observatory in the mountains near Pasadena, California.  Astronomer Edwin Hubble was contemplating a photograph of the Andromeda galaxy.  Eventually, his hand started to twitch.  He crossed out the &#39;N&#39; for nova he&#39;d already written on it and jotted down &#39;VAR!&#39;  This had to be one of the most exciting moments of his life.  He had just identified a peculiar type of star that allowed him to determine the distance to our neighboring galaxy (2 million light years).  He had, for the first time, identified a celestial object outside the Milky Way, thus ending the great debate in the astronomy community about whether fuzzy spots on the sky such as Andromeda couldn&#39;t also be objects in our own galaxy.  Immanuel Kant, who had regarded these nebulae as &#39;world islands&#39; similar to our Milky Way, ended up being right.</i><br /><br /><i>Hubble&#39;s first big discovery was evidence that our galaxy was not the only one in the universe, and he followed it up with a discovery that allows astronomers to talk about the age of the universe.  When an object like a galaxy is moving away from us, the light it emits shifts to the red.  What Hubble discovered is that this redshift of galaxy light was related to their distance, which meant that the faster the remote worlds seemed to run away from us, the more distant they were.  Obviously, the universe itself was expanding.  A neat analogy for this expansion is the surface of an inflating balloon where you have marked dots (each representing a galaxy) with a pen.  As the balloon expands, the dots, viewed from each other, begin to recede.</i><br /><br /><i>To measure the expansion of the universe, you divide the speed by the distance to get an &#39;inverse&#39; time, the so-called Hubble constant.  You can now estimate the time it will take for all other galaxies to double their distance from us, or rewind the &#39;movie&#39; to the point in time at which all galaxies were roughly in the same place: the Big Bang.  This time span of 14 billion years is a crude approximation of our universe&#39;s age, but it is still considered valid today.</i><br /><br /><b><i>&#39;Tremendously Far Away&#39; Becomes Measurable</i></b><br /><br /><i>The distances we encounter in the universe may be measurable by our instruments.  However, they are so large as to be barely imaginable for our minds.  Astronomers have been struggling for centuries to make these length specifications trustworthy by using a neat system of intertwined observations, which they call the &#39;distance ladder.&#39;</i><br /><br /><i>Let&#39;s see how to do the first step: stretch out one of your arms in front of you, look at your thumb, and alternatively close your left and your right eye.  You will notice that the thumb switches between certain angles with respect to the background.  If you now measure this angle and the distance your eyes are from each other, you will be able to calculate the length of your arm to about one meter, which is nothing particularly surprising.  Now look up at a star and replace the &#39;eye-switching&#39; with spring and fall, which on Earth&#39;s orbit corresponds to an eye-distance of 200 million miles.  Over that range, the relatively close stars do indeed change their angular position, revealing to us their distance.</i><br /><br /><i>This is called a parallax measurement.  It was first made up by the German astronomer Friedrich Wilhelm Bessel in 1838, and has been developed to the highest precision by a satellite of the European Space Agency (ESA).  Its name, HIPPARCOS, stands for HIgh Precision PARallax COllecting Satellite, given in honor of the ancient astronomer Hipparchus from Nikaea, who calculated the Moon&#39;s distance from Earth using the parallax method.</i><br /><br /><i>By the way, astronomers use a parsec (pc) as a unit of distance (which sounds alien to physicists); it is equivalent to the parallax of an arc second and amounts to 3.26 light years.  HIPPARCOS&#39; successor, the GAIA spacecraft, will soon provide astronomers with still more accurate measurements of positions for about a billion stars in the Milky Way galaxy.  The maximum distance we can measure using stellar parallax right now lies at about 1,000 light years.  So, how do we go about measuring even farther?</i><br /><br /><i>Nearby Candle or Distant Fire?</i><br /><br /><i>There is a serious and fundamental problem with measuring astronomical distances that go beyond what can be determined using stellar parallax.</i><br /><br /><b><i>A star&#39;s visible relative brightness doesn&#39;t tell us much about how far away it is. How do we know if a star we&#39;re seeing is small and close and glowing weakly, or whether it is really very large, far away, and very bright? To figure that out, we have to know either the distance or the luminosity or the size of the star. We know that an object&#39;s brightness lessens by a factor of 100 if we observe it from ten times the distance. However, this helps to no degree if its luminosity isn&#39;t known. To deal with this problem, astronomers developed a stable of stellar objects with known luminosity, fondly called &#39;standard candles.&#39;</i></b><br /><br /><i>Hubble recognized a &#39;standard candle&#39; called a Cepheid variable star when he scribbled &#39;VAR&#39; for variability on the photographic plate.  Just 20 years earlier, Henrietta Leavitt, a reclusive young woman with an obsessive eye for detail, was examining photographic plates at the Harvard Observatory looking for stars with variable brightness in the Small Magellanic Cloud, a companion galaxy of the Milky Way.  Her painstaking work, for which she was paid all of 30 cents an hour, eventually resulted in one of the cornerstones of astronomic distance measurements.  Leavitt found that the period between two brighness peaks was directly related to the luminosity of Cepheid stars. </i><b><i>Just like a larger drum will oscillate more slowly than a smaller one will, a huge Cepheid will oscillate with a period of up to 30 days, while small Cepheids with low light power may change their luminosity on a daily basis.</i></b><br /><br /><b><i>The important thing is that this oscillating period gives us an independent source for the star&#39;s size; thus, the ambiguity between being really bright or being just close vanishes. The striking correspondence of Cepheids revealed by Leavitt became a yardstick with which astronomers could reliably measure the first extragalactic distances. Everything else depends on this second step of the &#39;distance ladder.&#39;</i></b><br /><br /><i>A Sensation, But Still Pretty Wrong</i><br /><br /><i>Hubble had identified a Cepheid in Andromeda, and because he could calculate its distance based on its period and luminosity, he knew it was not in the Milky Way galaxy but much, much further away.  For objects still more distant than Andromeda, Hubble had to rely on other methods and hence greatly underestimated most distances.</i><br /><br /><b><i>Most mistakes in astronomy tend to occur this way. If you look for a star specimen, you will obviously discover their more brilliant exemplars more easily and arrive at an inequitable selection bias in which smaller stars will have been omitted.</i></b><i> Say, for instance, you were selecting apparently random people from a crowd in order to determine the average body height.  You will likely arrive at a mistaken conclusion, because larger people will have been more noticeable and will have skewed your choices toward taller people.  This effect is known in astronomy circles as the Malmquist bias.  It plagues astronomers to this day and has rendered many statistical analyses void.</i><br /><br /><b><i>Hubble made another rather subtle error. A Cepheid&#39;s oscillating period is not only subject to its magnitude, it also depends on the abundance of the elements heavier than helium. (Astronomers, with an insouciant [carefree, nonchalant] disregard for elementary chemistry, call all of them &#39;metals.&#39;) Heavy elements increase the capacity of a star to store heat. In other words, a star with a lot of &#39;metal&#39; will shine brighter at the same oscillating period. Hubble found such a star in Andromeda and then compared it with low-metal Cepheids in the Small Magellanic Cloud, which led him to underestimate Andromeda&#39;s distance. This was merely bad luck, but should also be a warning of the kind of snares that lurk in astronomy.</i></b><br /><br /><i>Hubble would surely find great pleasure in knowing that our most accurate measurements of the Hubble constant is based upon Cepheids in the Virgo and Fornax galaxy clusters, with the data on them all acquired using the Hubble telescope.  What could be a better memorial?  Hubble died just before the Nobel committee could award the 1953 prize to him.</i><br /><br /><i>In 1944, Walter Baade, a German astronomer, recognized Hubble&#39;s mistake with the Andromeda distance.  Curiously enough, Baade was allowed to use the world&#39;s best telescope at Mount Wilson. </i><b><i>Ultimately one can&#39;t blame Hubble for his faulty assumption. It is, however, remarkable that so many astronomers blindly trusted his authority. For quite some time, researchers seemed to be busy bringing the rest of astrophysics in line with Hubble&#39;s distances, instead of questioning them.</i></b><br /><br /><b><i>Technically careful work on its own isn&#39;t always sufficient for achieving accuracy. Systematic errors, like Hubble&#39;s mistake of ignoring the chemical composition of stars, are especially likely to occur in astronomy. Often you have to arrive at conclusions by indirect evidence, relying on premises that may turn out wrong. If that is the case, your great measuring accuracy is all for nothing.</i></b><br /><br /><b><i>When Baade&#39;s new determination of distances doubled the size of the universe, and people in the astronomy community suddenly convinced themselves of its correctness, the Russian physicist Lev Landau remarked ironically, &#39;Cosmologists are often in error, but never in doubt.&#39; Today, the scientific community again believes that the current picture of the universe is more or less correct. We disregard the fact that we have had to change our worldview numerous times over the course of history.</i></b><br /><br /><i>Resizing the Universe Yet Again</i><br /><br /><i>Baade was not, himself, immune to errors.  He also had to face the embarrassment of one of his students, Alan Sandage, correcting his measurements of the Hubble constant.  In this case, Baade had mistaken hydrogen clouds for stars and hence underestimated all cosmological distances again.  Today&#39;s measurements seem more reliable since they are based on Cepheids, which can be detected more than 50 million light years away.  At the moment, the favored value of the Hubble constant (H0) is, put in the astronomer&#39;s awkward way, 72 kilometers per second per megaparsec (3.26 million light years).  This translates into a universe that is roughly 14 billion years old.  Nevertheless, there is no guarantee that this number is free of all the subtle potential mistakes measurements are subject to.</i><br /><br /><b><i>When we compare the distances of the very close Cepheids we measure using stellar parallax to those in the faraway Magellanic Clouds, we have to make guesses about the concentration of heavy elements (metallicity), which could easily be wrong.</i></b><br /><br /><i>Two years prior to his death, the then 82-year-old Sandage could not make it to the cosmology conference of 2008 in St. Petersberg.  He did, however, write a touching letter to Rudy Schild, who then read excerpts of it to the conference&#39;s participants.</i><br /><br /><i>Is the Universe as Old as Its Stars?</i><br /><br /><i>Now, you&#39;re probably thinking that the idea of 14-billion-year-old ancient stars fits nicely with the value of Hubble&#39;s constant, which also corresponds to 14 billion years.  But sorry, there&#39;s a big problem! </i><b><i>We can&#39;t assume that the universe always expanded at that actual rate, since gravitation inevitably decelerated the expansion. And if the growth was faster in the past, then the universe must be younger, and we could argue at best by how much</i></b><i> (a question that depends on the density and will puzzle us later on). </i><b><i>Cosmology was trapped in a contradiction.</i></b><br /><br /><b><i>But in the 1990s, the supernova hunters made a substantial discovery. They realized that the dim explosions finished more quickly than the bright ones. Once again, the larger ones were acting more slowly, just as it was with the oscillation of the Cepheid stars. The explosion&#39;s duration could then be used as a measure of absolute brightness, which dramatically improved the precision when determining the Hubble constant at great distances.</i></b><br /><br /><i>The discoverers of this new cosmic yardstick, Saul Perlmutter, Brian Schmidt, and Adam Riess, were later awarded the 2011 Nobel Prize in Physics.  Their conclusions had suggested the greatly preferred older age of the universe.  But there was still another conundrum to be solved.</i><br /><br /><b><i>The measurements using Cepheids for the shorter distances didn&#39;t fit with the new supernova method for much longer distances. And since the distance was key to calculating the Hubble constant, the inconsistency was obvious, leading to an embarrassing discrepency by a factor of two between the calculations. The researchers were observing different objects, but no error was found in the measurements of either Cepheids or supernovae. Caught in this predicament, they resolved the issue by deciding that both teams could be right!</i></b><br /><br /><i>That, however, required taking one of the fundamental understandings about the cosmos and flipping it on its head.  Rather than a picture where the universe is expanding but gradually decelerating, </i><b><i>the distance measurements could be harmonized by assuming that expansion of the universe is accelerating. This consequence is surely not easy to cope with since it contradicts our experience. It is as if a rock thrown in the sky doesn&#39;t slow down and eventually return back to Earth, but rather, falling upward, accelerates away from us.</i></b><br /><br /><i>Anyhow, the paradox of the globular clusters seeming older than the universe was hereby vanquished and, more importantly, the conflict between the competing Cepheid and supernova teams was set aside. </i><b><i>Everyone was happy. But it is good to keep in mind that, often enough in history, consensus in the scientific community has been dangerous. It may just whitewash a deeper flaw in our understanding.</i></b><br /><br /><i>&#39;Even when the experts all agree, they may well be mistaken&#39;</i> (Bertrand Russell, British philosopher)<br /><br /><i>New Paradigm or Ptolemaic Crisis?</i><br /><br /><i>The supernova data and the new &#39;standard candles&#39; it established rightly led to the 2011 Nobel Prize.  Astronomers found assuming an accelerated expansion of the universe to be a comfortable solution, and nobody was particularly alarmed by it.</i><br /><br /><i>But of course it raised the question about what the heck could cause this weird acceleration.  The answer?  Some sort of &#39;dark energy,&#39; </i><b><i>whose hypothetical properies were designed just for that purpose. However, the term is nothing more than a helpless labeling of something we simply don&#39;t understand. Even the proponents of dark energy baldly admit this. Still other theorists are pleased to have a new field (however dark it may be) in which they can do more math. But physics is simple. Another unaccounted-for numer -- call it dark energy or whatever -- is a grave methodological defeat.</i></b><br /><br /><i>There is a historical parallel to the Ptolemaic view of the universe, which prevailed from about 150 A.D. to the Middle Ages, assuming that all motion in the heavens was circular.  When astronomers couldn&#39;t explain the observed planetary motions with simple circles around the earth, they posited another circular motion on top of the first circular motion, the so-called epicycles.  As slightly better observations showed another discrepancy, the centers of the main circles were shifted by a tiny figure called an eccentric.  And everything worked smoothly again!  Eventually, the whole system collapsed under the weight of all those circles. </i><b><i>We could be just trapped in a modern version of such a system.</i></b><br /><br /><b><i>The science historian Simon Singh wrote in his excellent book Big Bang that &#39;every flawed model can be saved by such fiddling around with numbers.&#39; The process also recalls a term coined by the English philosopher Owen Barfield, &#39;saving the appearances.&#39; It is sometimes simply too hard to throw overboard a model developed over many generations.</i></b><br /><br /><i>Today, everybody would surely agree that embracing such a poorly understood idea as epicycles could just as easily contribute to the erosion of science, the painted-over rust of a crumbling construction, as to the progress of science.  No doubt Ptolemaic astronomers found it hard to let their established picture of the universe go. </i><b><i>There is no reason to suppose that astronomers and cosmologists today are any less susceptible to wanting to save the appearance of the current model of the universe. Still, given that &#39;dark energy&#39; is really just an ad hoc solution to deal with contradictory measurements, shouldn&#39;t we be at least a little bit cautious?</i></b><br /><br /><b><i>At conferences, however, one notes an irritating euphoria. The accelerated expansion, along with its plethora of fancy theoretical explanations, is brought up wherever astronomers and cosmologists gather. Everybody sees it clearly now! &#39;Otherwise we cannot make sense of the X-ray observations&#39; or &#39;galaxy distributions models require its existence,&#39; as a researcher of the University of Nottingham writes with admirable naivete. However, every scientist should keep in mind that throwing another free parameter or one more number into your model to fix your problems can improve the explanation of any data set.</i></b><br /><br /><i>&#39;Cosmological fashions and reputations are made more by acclamation than by genuine scientific debate.&#39;</i> (Mike Disney, British astronomer)<br /><br /><i>While Fighting the Tiger at the Front Door, the Wolf Enters the Back</i><br /><br /><i>Let&#39;s recapitulate the scenario one gets by analyzing the supernova data at different redshifts.  In the very early universe, the acceleration did not act yet and thus the expansion was slowing down.  Eventually dark energy emerged, growing with the expansion until, at a certain moment, the acceleration and deceleration canceled each other out. </i><b><i>Would you like to guess when the moment of cancellation occurred? It turns out that after 14 billion years, we have arrived at that point exactly today. Are you surprised? Justifiably so.</i></b><br /><br /><b><i>Well, that&#39;s just plain coincidence, cosmologists reassure us. But this coincidence problem highlights the paradox of the current model.</i></b><i> The periods of acceleration and deceleration of the expansion of the universe could just as well be distributed in a very different way, a way that wouldn&#39;t make you think that our momentary expansion rate, the Hubble constant, has anything to do with the age of the universe.  It is as if you are on a road trip driving with greatly varying speeds.  At any given time, you can determine the average speed for a route you have already traveled, but the odds are fairly small that you are, at this moment, driving at exactly that speed.</i><br /><br /><b><i>Dark energy has solved the controversy about the age of the universe, but the coincidence problem ought to make us suspicious that this mending was too easy.</i></b><i> The cosmologist Lawrence Krauss has pointed out another funny consequence of this.  As a result of the accelerated expansion, some day remote objects will disappear from our visible horizon.  For instance, in later eras it will be impossible to observe cosmic microwave background.  Future cosmologists will encounter a rather boring universe, </i><b><i>so we humans are really fortunate that our brilliant minds evolved just in time for making sense of these unique observations before they disappear from view. Any doubts about being that lucky?</i></b><br /><br /><i>&#39;I&#39;ll never make that mistake again, reading the experts&#39; opinions&#39;</i> (Richard Feynman)<br /><br /><i>Political Cosmology: Science by a Majority Decision</i><br /><br /><i>&#39;A conclusion is the place where you got tired thinking.&#39;</i> (Martin H. Fischer, American author)<br /><br /><b><i>As much as most cosmologists seem excited that everything fits and we have a &#39;safe model,&#39; there are few introspective voices</i></b><i>, such as that of Bruno Leibundgut, a member of one of the supernova groups that shared a Nobel Prize.</i><br /><br /><b><i>At a conference about dark energy in Munich in 2008, Leibundgut said that it was a nice game to collect up to 1,500 supernovae a year, but he was concerned about the puzzling scatter in the brightness of the explosions. &#39;Just take the best ones,&#39; an audience member exclaimed. &#39;This is a dangerous game,&#39; Leibundgut replied, &#39;and it&#39;s not enough to plot the brightness variation in some diagram. We ought to understand the mechanisms!&#39; All he received were shrugs.</i></b><br /><br /><b><i>Historical reflections are an unfamiliar virtue to a lot of scientists today. Sometimes the conferences seem like big parties where people enjoy their shared feelings about &#39;consensus cosmology.&#39; But what does this mean?  Lev Landau&#39;s warning that cosmologists are &#39;often in error, but never in doubt&#39; inevitably comes to mind, and some poke fun at the &#39;consensus.&#39; Rudy Schild, whom I talked to in St. Petersburg during a coffee break, said, &#39;Which consensus? Do you know who consented? A bunch of guys at Princeton who drink too much tea together.&#39; He grinned mischievously as he said this, but it is definitely true that the new picture of an accelerating universe forced its way through the science community very rapidly. All it took was two research teams agreeing and an article in Nature ... a speedy revolution.</i></b><br /><br /><i>Today we have a &#39;simple&#39; (at least that&#39;s what you read) &#39;standard model&#39; of cosmology. </i><b><i>Everyone interprets the data using it, although the ingredients such as dark matter and dark energy are nothing more than fancy names for substances we do not understand -- if they exist at all. But throughout the history of science, it has always been much easier to add a new concept than to overturn the existing construction -- simply because it&#39;s easier to find people agreeing with you. Thus, barely anyone is willing to question the entire system of assumptions that has peaked with the introduction of dark energy. But what is considered progress today may turn out as an erroneous complication. All indications are that modern cosmology has not learned a thing from the example of the medieval epicycles.&quot;</i></b> (<i>Bankrupting Physics: How Today&#39;s Top Scientists Are Gambling Away Their Credibility</i>, Alexander Unzicker and Sheilla Jones, p31-42)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"Why Evolution is Not What You Think - The Ebner Effect","summary":"The Controversy Over Darwin Played a Crucial Role in Jolting the American Public Into the World of Science in the mid-1850's / But, there's at Least One More Big Surprise Coming on the Subject / The Next Big Step in Evolutionary Biology Will Explain Junk DNA, Epigenetics and the Cambrian Explosion at Once / It Will Remove Any Need at All for Lengthy Timespans to Explain Dramatic Evolutionary Changes / And it Will Forever Alter Our Conception of the Role of DNA in Evolution / The Experiment Which Demonstrates it is So Simple that It Will Inevitably Become Popular as a Grade School Science Fair Exhibit","image":"https://lh3.googleusercontent.com/-b3ngBXicfhw/VxhWzX3n0fI/AAAAAAAAHvY/ZZGVXCcA8xsv4LOYzJLhAgfkn35X_C8jA/w6250-h8750/the-ebner-effect-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/2FcSPmaT4Zn","publishDate":"2016-04-21T04:29:03.842Z","updateDate":"2017-01-03T02:34:39.603Z","text":"<b>Why Evolution is Not What You Think - The Ebner Effect: The Controversy Over Darwin Played a Crucial Role in Jolting the American Public Into the World of Science in the mid-1850&#39;s / But, there&#39;s at Least One More Big Surprise Coming on the Subject / The Next Big Step in Evolutionary Biology Will Explain Junk DNA, Epigenetics and the Cambrian Explosion at Once / It Will Remove Any Need at All for Lengthy Timespans to Explain Dramatic Evolutionary Changes / And it Will Forever Alter Our Conception of the Role of DNA in Evolution / The Experiment Which Demonstrates it is So Simple that It Will Inevitably Become Popular as a Grade School Science Fair Exhibit</b><br /><br /><b>No Other Scientific Controversy Better Demonstrates a Need for Public Awareness of Scientific Controversies</b><br /><br />Here we have an enormously useful biological tool, but since it does not help to prove the textbook theories, scientists have largely failed to notice; and since it undercuts simpler business models, corporations have abandoned it.<br /><br />To better understand the Ebner Effect (sometimes called the Primeval Code), it&#39;s important to learn a few key concepts in biology ...<br /><br /><b>The Riddle of the Cambrian Explosion</b><br /><br /><i>&quot;The earliest rocks with elaborate fossils are said to belong to the Cambrian age, and the entire 4-billion-year history of our planet that preceded it has been, until recently, dismissed as the &#39;pre-Cambrian age.&#39;  Now that the traces of life have unmistakenly been found in it, the more appropriate name of &#39;Cryptozoic eon&#39; (Greek for &#39;hidden life&#39;) is used, while the last 600 million years make up the &#39;Phanerozoic eon&#39; (&#39;visible life&#39;) ...</i><br /><br /><i>The division between the Cryptozoic eon and the Phanerozoic eon is extraordinarily sharp. </i><b><i>At one moment in time, so to speak, there are no fossils at all above the microscopic level, and at the next there are elaborate organisms of a dozen different basic types.&quot;</i></b> (<i>Asimov&#39;s Guide to Science</i>, Isaac Asimov, p761)<br /><br /><b>Junk DNA - Not So Useless After All</b><br /><br />By Alice Park<br />Sept. 06, 2012<br /><br /><a href=\"http://healthland.time.com/2012/09/06/junk-dna-not-so-useless-after-all/\" class=\"ot-anchor\">http://healthland.time.com/2012/09/06/junk-dna-not-so-useless-after-all/</a><br /><br /><i>&quot;Junk. Barren. Non-functioning. Dark matter. That’s how scientists had described the </i><b><i>98% of human genome</i></b><i> that lies between our 21,000 genes, ever since our DNA was first sequenced about a decade ago. The disappointment in those descriptors was intentional and palpable.</i><br /><br /><i>It had been believed that the human genome — the underpinnings of the blueprint for the talking, empire-building, socially evolved species that we are — would be stuffed with sophisticated genes, coding for critical proteins of unparalleled complexity. But when all was said and done, and the Human Genome Project finally determined the entire sequence of our DNA in 2001, researchers found that the 3 billion base pairs that comprised our mere 21,000 genes made up a paltry 2% of the entire genome. The rest, geneticists acknowledged with unconcealed embarrassment, was an apparent biological wasteland.</i><br /><br /><i>But it turns out they were wrong. In an impressive series of more than 30 papers published in several journals, including Nature, Genome Research, Genome Biology, Science and Cell, </i><b><i>scientists now report that these vast stretches of seeming &#39;junk&#39; DNA are actually the seat of crucial gene-controlling activity — changes that contribute to hundreds of common diseases</i></b><i> ...</i><br /><br /><b><i>Rather than being inert, the portions of DNA that do not code for genes contain about 4 million so-called gene switches, transcription factors that control when our genes turn on and off and how much protein they make, not only affecting all the cells and organs in our body, but doing so at different points in our lifetime.&quot;</i></b><br /><br /><b>Epigenetics is Trait Variations Caused by Environmental Factors</b><br /><br />From an intelligent design site at <a href=\"http://www.uncommondescent.com/darwinism/redwood-trees-genes-differ-from-top-to-bottom/\" class=\"ot-anchor\">http://www.uncommondescent.com/darwinism/redwood-trees-genes-differ-from-top-to-bottom/</a> [with my own commentary ...]<br /><br />Redwood trees’ genes differ from top to bottom<br />July 10, 2011<br /><br /><i>&quot;From &#39;Environs Prompt Advantageous Gene Mutations as Plants Grow; Changes Passed to Progeny&#39; (ScienceDaily, July 5, 2011) we learn:</i><br /><br /><b><i>&#39;If a person were to climb a towering redwood and take a sample from the top and a sample from the bottom of the tree, a comparison would show that the two DNA samples are different.</i></b><i> Christopher A. Cullis, chair of biology at Case Western Reserve University, explains that this is the basis of his controversial research findings.</i><br /><br /><i>Cullis, who has spent over 40 years studying mutations within plants, most recently flax (Linum usitatissimum), has found that the environment not only weeds out harmful and useless mutations through natural selection, but actually influences helpful mutations.</i><br /><br /><i>Cullis published his findings in the International Journal of Genetics and Molecular Biology and repeated them in the Journal of Visualized Experiments, where he challenged other scientists to repeat his experiment themselves.&#39;</i><br /><br /><i>Cullis’s theory was originally dismissed on the assumption that plants pass on their genes the same way animals do, but that may not be correct.</i><br /><br /><i>Factoid: &#39;Young redwoods grow by the tips of the existing branches budding into meristems. </i><b><i>Each new meristem is different from the tree because the environment has affected its genetic makeup. And as the redwood grows, the top becomes more and more genetically different from the bottom ...&#39;</i></b><br /><br /><b><i>If this holds up, the world of life is not what we thought and not what we’ve been taught.&quot;</i></b><br /><br />They are correct about that.<br /><br /><b><i>&quot;Are plant genomes a living template, subject to revision as need arises, rather than a cement cast?&quot;</i></b><br /><br />Seems like it.<br /><br /><b><i>&quot;But what decides when the need arises? Some forward look must be built into the system in advance.&quot;</i></b><br /><br />Yes, and it&#39;s called ...<br /><br /><b>The Ebner Effect</b><br /><br />The story of the Ebner Effect has all of the makings of a movie -- one that is still actually unfolding: There is the great unexpected scientific discovery; there is the company which abandons the investigation, seemingly because the discovery would undermine their profits; then, accordingly, a great lull in public awareness and even a sense of apathy, as the world struggles to realize the implications of what has been discovered.<br /><br />Yet the idea does not completely die because the scientists&#39; children launch a public campaign to raise awareness, seize the patent, and today seek to give the discovery -- free of charge -- back to the people of the world, to help increase crop yields without recourse to genetic engineering.<br /><br />In a world that changes only gradually, Darwin&#39;s version of evolution makes perfect sense.  So, if genetics which are presumed to be millions of years old can be produced at once with a simple electrostatic field, then evolution no longer seems like a linear process.  What the Ebner Effect will do to Evolutionary Biology will somewhat resemble the effect that quantum theory had upon physics: An area which was thought to be understood will suddenly be thrown into disarray.  But, as the public comes to realize that Junk DNA is most logically the genetic memory of our planet&#39;s former states, the world will once again be jolted -- this time into the realization that if we wish to settle the remaining medical mysteries of the human body, then we have no choice but to follow the science wherever it leads us.<br /><br />The Ebner Effect enters the debate over evolution at a stage where many minds have already been made up.  And the great irony for Our &quot;Information Age&quot; will prove to be that much of what stands in the way of an explosion in the medical sciences today is the insistence by those who have most vocally defended evolution that we have already fundamentally understood it.<br /><br />The Ebner Effect conclusively demonstrates that we have not.<br /><br /><a href=\"http://www.urzeitcode.com/english/\" class=\"ot-anchor\">http://www.urzeitcode.com/english/</a><br /><br /><i>&quot;Can the global food problem at last be solved – without using genetic engineering? In his new book the &#39;Primeval Code&#39; (Munich 2007) Swiss journalist Luc Bürgin unveils the secret of a sensational biological discovery at the pharmaceutical giant Ciba (now Novartis), which unfortunately has been ignored by the experts up to the present day. </i><b><i>In laboratory experiments the researchers there Dr. Guido Ebner and Heinz Schürch exposed cereal seeds and fish eggs to an &#39;electrostatic field&#39; – in other words, to a high voltage field, in which no current flows.</i></b><br /><br /><b><i>Unexpectedly primeval organisms grew out of these seeds and eggs: a fern that no botanist was able to identify; primeval corn with up to twelve ears per stalk; wheat that was ready to be harvested in just four to six weeks. And giant trout, extinct in Europe for 130 years, with so-called salmon hooks.</i></b><i> It was as if these organisms accessed their own genetic memories on command in the electric field, a phenomenon, which the English biochemist, Rupert Sheldrake, for instance believes is possible.</i><br /><br /><b><i>The Swiss pharmaceutical group patented the process – and then stopped the research in 1992. Why? Because &#39;primeval cereals&#39; generated by an electric field, in contrast to modern strains of seeds, require hardly any fertilisers or pesticides – i.e. crop protection agents, sold as priority products by Ciba at that time. The discovery was soon forgotten, without the global scientific community taking any notice.</i></b><br /><br /><i>In collaboration with the researchers involved – or rather their sons, in this book, the author has for the first time now disclosed in detail how the principle of this revolutionary bio-experiment works. Previously unpublished research reports, 64 exclusive photos, English-language patent specifications and interviews document the &#39;Primeval Code&#39; in all its aspects.</i><br /><br /><i>Nevertheless many scientists who think along conventional lines may remain sceptical about this inexplicable biological effect. But now for the first time, renowned German expert botanists such as Professor Edgar Wagner of the University of Freiburg and Professor Gunter Rothe from the University of Mainz make positive statements about it in the book. </i><b><i>Rothe even had replicated the Ciba experiments in detail at his university in 2001 – and was successful.</i></b><i> And also for the first time, even a Nobel laureate – the world-famous Swiss microbiologist Werner Arber – is [defending] this controversial discovery. Back then, Arber had given an expert opinion on the Ciba experiments in person in the laboratory. The professor said to the author: &#39;I was impressed!&#39;</i><br /><br /><i>Indeed, </i><b><i>Guido Ebner and Heinz Schürch appear to have discovered a biological method of effectively creating antecedents from progeny – without using genetic engineering. Or as both researchers put it: &#39;Our experiments do not involve a mutation of the organism in question, which in the case of genetic engineering involves channelling an additional gene into the organism. No entirely new organism is created. In the electrostatic field, only the gene expression is altered – the retrieval of the existing gene. That is something different.&#39;</i></b><br /><br /><i>In addition to corn and trout experiments, the results of experiments with wheat were also amazing: thus </i><b><i>the cereal exposed to the electrostatic field developed new proteins, which people looked for in the original wheat to no avail. Furthermore, it surprisingly produced much larger roots than the control group. This had the advantage of allowing the crops to grow far more rapidly. And occasionally in these experiments, a variant emerged which resembled the genetic antecedents of the wheat.</i></b><i> Thus, a meadow grass-like arrangement of ear shoots and small narrow leaves was observed, for example.</i><br /><br /><b><i>&#39;In the case of our &#39;manipulated&#39; wheat, growth was so rapid that it was ripe in four weeks instead of the usual seven months&#39;</i></b><i>, Heinz Schürch recalled. &#39;However one has to say that although the ears and stalks were somewhat smaller, there were more ears per plant. The actual benefit is that we could cultivate this wheat in regions where spring and summer are short – where conventional wheat cannot be grown at all.&#39; In this case, one can also cheerfully refrain from using pesticides and herbicides: &#39;The pests that have adapted to the growth process of normal wheat have not yet developed when we harvest our wheat as early as four to eight weeks after planting.&#39;</i><br /><br /><i>The Third World in particular could benefit from this method of cultivation that is both environmentally friendly and inexpensive. For &#39;primeval cereals&#39; from an electric field yield more than modern genetically engineered varieties and do not require fertilisers. Furthermore, it is completely natural and thus healthier. It can be grown by anybody at minimal cost.</i><br /><br /><b><i>After Guido Ebner and Heinz Schürch both passed away unexpectedly in 2001, their experiments were continued by Guido Ebner&#39;s son, Daniel – himself a biologist – on a private basis.</i></b><i> He now wants to export this electric field technology to Africa as soon as possible, as part of an aid project, with the assistance of the former assistant to the UN Food Representative, Professor Jean Ziegler, in order to make it available to local farmers – free of charge, as an ecological alternative to the controversial genetically engineered seeds of the international agro-multinationals.&quot;</i><br /><br /><b>How to Make Extinct Trout</b><br /><br />From the original patent:<br /><br /><i>&quot;Inventor: EBNER GUIDO [CH] // SCHUERCH HEINZ [CH]</i><br /><i>Applicant: CIBA GEIGY CORP [US]</i><br /><i>EC: A01K61/00G</i><br /><i>IPC: A01K61/00 // (IPC1-7):A01K63/00</i><br /><br /><i>Specifically, in the practice of this invention, tanks (aquaria) filled with freshwater or saltwater and containing the fish or eggs, and consisting preferably of electrically non-conductive material (insulator), are placed between the electrodes of a capacitor.</i><br /><br /><i>A direct current voltage of one to tens of thousands of volts is then applied to said electrodes. Instead of using the non-conductive aquarium material, it is of course also possible to use electrodes which are provided with an insulating layer and to immerse these electrically insulated capacitor plates direct in the tanks.</i><br /><br /><i>Of sole importance is that the electrodes of the capacitor are insulated against the freshwater or saltwater acting as dielectric medium. As no currents flow in this apparatus, no perceptible loss of energy is observed. Hence energy is not a cost factor in this invention.</i><br /><br /><i>Firstly, a substantially higher hatching rate of the treated fish eggs is observed, with frequently an increase of 100 to 300% and even higher values being achieved. Secondly, these juvenile fish make a far more agile and more vital impression than the untreated comparison fish.</i><br /><br /><i>Very significant is their markedly higher survival rate which extends not only to their first few days, but virtually to their entire life cycle. This feature becomes all the more marked if all medication is dispensed with. For it is then observed that, in the first few days and weeks, the untreated control group (not exposed to an electrostatic field) is at least twice as heavily reduced by the non-artifically inhibited and naturally occurring population of pathogens as the fish that have been exposed to treatment with an electric field.</i><br /><br /><i>In addition, the treated fish, while receiving identical feeding, grow in weight and size far faster and reach adulthood significantly earlier, and hence can be transferred to natural waters or are available for marketing as edible or ornamental fish. Further, no deleterious changes in the progeny of these treated fish are observed. On the contrary, a certain element of the vitality seems to be transferable to the progeny.&quot;</i><br /><br /><a href=\"http://www.abovetopsecret.com/forum/thread938249/pg1\" class=\"ot-anchor\">http://www.abovetopsecret.com/forum/thread938249/pg1</a><br /><br /><i>&quot;The Aftermath</i><br /><br /><i>Being heavily invested in the agriculture business, Ciba Geigy reviewed the test results, filed a patent and completely stopped the research in 1992 - cancelled without substitution - prohibiting imitation or replication for any commercial purpose. Previously, all attempts of the two scientists to publish their discovery in scientific journals were unsuccessful, leaving them no other choice to finally announce it in a popular familiy Tv-show in Switzerland.</i><br /><br /><i>With the exception of a few news articles, including a german news magazine that admirably dedicated a section of its documentaion on the conflicting business interest of Ciba Geigy, the announcement was mostly ignored or in some cases ridiculed by the scientific community and soon forgotten by the public.</i><br /><br /><i>The Basel-based chemical giant, sells agricultural products at about 22 billion Swiss Francs annually (late 1980&#39;s) ...</i><br /><br /><i>Statement Press Office, Ciba-Geigy:</i><br /><br /><i>&#39;Ciba has given up by now all these studies, there have been no further scientific work undertaken in this area because they can not be assigned to one of the priority research areas of the company. For this reason, the research industry in many, many interesting approaches in exploratory projects will be dropped.&#39;</i><br /><br /><i>Daniel Ebner, Guido Ebner&#39;s son, continued the experiments on a private basis </i><b><i>and was able to acquire the right for the patent a few years ago and has subsequently released it to the public for any kind of use and purpose (including commercial) free of charge. The electrostatic field-method or Ebner/Schürch Effect is now open-source technology.&quot;</i></b><br /><br /><b>Why Americans Have So Far Largely Failed to Catch On</b><br /><br />There is currently no English translation of the one single book written on this subject ...<br /><br /><a href=\"http://www.amazon.com/Urzeit-Code-Luc-B%C3%BCrgin/dp/3776626399/ref=sr_1_1?ie=UTF8&amp;qid=1461211413&amp;sr=8-1&amp;keywords=der+urzeit+code\" class=\"ot-anchor\">http://www.amazon.com/Urzeit-Code-Luc-B%C3%BCrgin/dp/3776626399/ref=sr_1_1?ie=UTF8&amp;qid=1461211413&amp;sr=8-1&amp;keywords=der+urzeit+code</a><br /><br />As of recently, digital copies of the text have appeared on the Internet -- which means that, for the first time ever, it is finally possible to send the book through Google Translate.  I&#39;ve not found this to be sufficient to understand certain key sections.<br /><br />For this reason, I have a German speaker who is helping me to translate a handful of pages which explain the authors&#39; views on the underlying science.<br /><br />In the coming days, I will post these translated sections, as they become available.<br /><br /><b>Where to Purchase the Device (It&#39;s Not Cheap)</b><br /><br /><a href=\"http://www.fios-greenbox.net/eshop/1/greenboxen\" class=\"ot-anchor\">http://www.fios-greenbox.net/eshop/1/greenboxen</a><br /><br /><b>Instructions on How to Build the Device</b><br /><br />See the video at <a href=\"https://www.youtube.com/watch?v=GMHMuhDy5f0\" class=\"ot-anchor\">https://www.youtube.com/watch?v=GMHMuhDy5f0</a>:<br /><br /><i>&quot;It is very simple to duplicate.  You have sheet metal enclosed in plexiglass, two pieces, with some kind of arrangement to hold them a short distance apart.  They used an applied voltage of 2150 volts with a 3 cm spacing between plates to get a field strength between plates of 716 V/cm.  Any high voltage DC source of the proper voltage will do.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Problematic Weight of the Dinosaurs","summary":"Simple Back-of-the-Envelope Calculations Appear to Confirm a Size Limit for Land-Walking Animals / That Raises Legitimate Questions about How the Dinosaurs Became So Large / There Are 3 Separate Questions Which Must be Answered","image":"https://lh5.googleusercontent.com/-zk2LhNcE_5k/VxRhZckd1GI/AAAAAAAAHs4/Xw034uNcarwT90bVKcfVx20IPiZg6tgBgCL0B/the-problematic-weight-of-the-dinosaurs-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/EdMeE4pV2AP","publishDate":"2016-04-18T04:27:34.294Z","updateDate":"2017-01-03T02:34:57.632Z","text":"<b>The Problematic Weight of the Dinosaurs: Simple Back-of-the-Envelope Calculations Appear to Confirm a Size Limit for Land-Walking Animals / That Raises Legitimate Questions about How the Dinosaurs Became So Large / There Are 3 Separate Questions Which Must be Answered: How Could the Largest Sauropods Have Lifted their Own Weight? / How Did they Support and Pump Blood to their Heads at the End of their 50-60 Foot Necks? / And How Could a 200 lb Teratorn Eagle-Like Dino with a 27-foot Wingspan Fly? / These Questions Force Us to Contemplate a World that Must Have Been Very Different from Our Current World</b><br /><br />There are a variety of explanations online of the problem that the square-cube law creates for the more extreme dinosaur sizes.  Some of these explanations are very well-written, but go ignored due to their length or embedded worldview.  In some instances, the back-of-the-envelope algebra is so poorly explained that audiences will be skeptical.  In other instances, crucial history or examples are left out.  I&#39;ve tried to stitch together here an explanation which does not suffer from any of these problems ...<br /><br />Although I have my own personal preference for an explanation, I will seek to avoid framing a specific hypothesis. <b>What I would personally argue is that the problem is indeed real, and the answers which have been proposed are typically designed to avoid questioning the scientific framework.</b><br /><br />From <a href=\"http://www.dinosaurtheory.com/scaling.html\" class=\"ot-anchor\">http://www.dinosaurtheory.com/scaling.html</a><br /><br /><i>&quot;Does size matter? More specifically, can a rock, a person, a car, a planet, an airplane, or a dinosaur exist at any arbitrary size? </i><b><i>This is a fundamental scientific question, and yet for the most part the science community has sidestepped this simple question.</i></b><i> Since the science community has not clarified why animals cannot be any size, science fiction writers have had fun playing with the idea that animals can be many times larger or many times smaller than their normal size. While many of us may enjoy the entertaining movies showing people or other animals the wrong size, this is not helping to clarify to the public that size really does matter. Galileo’s Square-Cube Law explains why size matters, and in fact Galileo’s Square-Cube Law is such a fundamental scientific truth that science is hardly science without it.&quot;</i><br /><br />From <a href=\"https://www.quora.com/How-could-dinosaurs-get-so-big-despite-Galileos-square-cube-law\" class=\"ot-anchor\">https://www.quora.com/How-could-dinosaurs-get-so-big-despite-Galileos-square-cube-law</a><br /><br /><b><i>&quot;How could dinosaurs get so big despite Galileo&#39;s square cube law?</i></b><br /><br /><i>The law - Square-cube law</i><br /><i>From wikipedia</i><br /><br /><b><i>&#39;The giant monsters seen in horror movies (e.g., Godzilla or King Kong) are also unrealistic, as their sheer size would force them to collapse.&#39;</i></b><br /><br /><b><i>Wouldn&#39;t the law also affect dinosaurs like the Giraffatitan?</i></b><br /><br /><i>1 Answer</i><br /><i>Apala Chaturvedi</i><br /><br /><b><i>Galileo&#39;s Dialogue Concerning Two New Sciences contained what he considered to be one of his most profound insights: the square-cube law. If two cubes are made of the same material then they will have the same density. Yet since the two cubes have different area to volume ratios they will likewise have different stress at the base of each cube.</i></b><i> If too much stress is placed on an object then it will fail, or in this case a large cube has a much greater possibility of collapsing. This is why sandcastles can only be a few feet high.</i><br /><br /><b><i>Galileo applied this to animals, what we now call allometry, and noted that a this implies the diameter of bones should be proportional to their length. It explains why ants can walk around on spindly little legs while lifting 50 times their body weight, compared to elephants with their tree-trunk sized feet who would strain to lift a quarter of their mass.</i></b><br /><br /><b><i>Also because of the Square-Cube Law, larger animals have less relative muscle strength than smaller animals. Both the muscle strength and bone strength are functions of the cross sectional area, while the weight of the animal is a function of volume.</i></b><br /><br /><i>It is because of relative muscle strength that an ant can lift fifty times its weight while a human can lift an amount equal to its own weight, and an Asian elephant can only lift 25% of its own weight. The greater muscle to weight ratio of smaller animals is what allows them to jump higher than several times their own height, while at the other extreme an elephant can not even jump.</i><br /><br /><b><i>While Galileo was successful in convincing the Church and the conservative science community that the world is not flat, the conservative science community has yet to embrace Galileo&#39;s Square-Cube Law even though it is clearly correct and fundamental to understanding every major science discipline.</i></b><br /><br /><a href=\"https://www.thunderbolts.info/tpod/2005/arch05/050623impossible-dinosaur.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2005/arch05/050623impossible-dinosaur.htm</a><br /><br /><i>&quot;Jun 23, 2005</i><br /><b><i>Impossible Dinosaurs</i></b><br /><br /><i>The giant dinosaurs are fascinating. How did they get so big? Why are there none alive today? Their size rivals that of modern whales, which cannot survive without the buoyancy of water. </i><b><i>So early paleontologists postulated that the biggest dinosaurs must have spent most of their lives wading in the shallow seas of the Mesozoic Era.</i></b><br /><br /><b><i>Then dinosaur footprints were discovered. Not just a few, but thousands of footprints. Somehow, even the largest of dinosaurs were walking around on land, not even dragging their enormous tails behind them. So wading in shallow seas was replaced by grazing in herds and the original reason for the wading -- that a dinosaur on land would have been a beached whale -- was forgotten.</i></b><br /><br /><i>Catastrophist Ted Holden has resurrected the controversy by examining the relationship of size, weight, and strength in animals. (His analysis was the basis for a documentary televised in Japan in Feb, 2004. See photo above.) </i><b><i>The strength of muscle tissue is fairly constant among all species. Strength is proportional to the cross section of the muscle: If one muscle is two times the diameter of another, the first will be four times (the square of two) as strong. But weight increases with the volume: A muscle that&#39;s twice as big will weigh eight times (the cube of two) as much.</i></b><br /><br /><i>Holden computed the weight/strength ratio of a well-trained human weightlifter and scaled it up to the size of a dinosaur. The weightlifter soon became too big to lift his own weight. </i><b><i>Strength, in its relationship with weight, imposes a limit on size. Holden&#39;s calculations indicate that the heaviest elephants of today approach that limit.</i></b><br /><br /><i>The largest dinosaurs are many times the size of an elephant. And dinosaur skeletons aren&#39;t as well-designed for bearing weight as elephant skeletons. Dinosaurs are impossibly large for planet Earth, but their bones are proof that they must have existed. How could that be? The limit on size depends on weight, and weight depends on the force of gravity. Most conventional theories assume that gravity throughout the universe has always been and will always be a constant property of matter. But that&#39;s only an assumption, and it must be verified empirically ...</i><br /><br /><i>Earth in the Mesozoic Era may have had less gravity than it has today. </i><b><i>Holden calculates that in order for the largest dinosaurs to function, gravity must have been at least 1/3 (and possibly as low as 1/4) what it is today.</i></b><i> He also postulates that gravity increased suddenly at the close of the age of dinosaurs but not to the present value. Lower-than-present gravity continued into the following ages of giant mammals and possibly even to the days when early humans were building giant monuments like Stonehenge.&quot;</i><br /><br />By Ted Holden, originally at <a href=\"http://www.bibliotecapleyades.net/ciencia/ciencia_dinosaurs01.htm\" class=\"ot-anchor\">http://www.bibliotecapleyades.net/ciencia/ciencia_dinosaurs01.htm</a><br /><br />(I have in places generalized the wording and removed Ted&#39;s conclusion so that the problem can be considered completely separate from any specific interpretation.  Please note that this is not because there is something necessarily wrong with his suggested conclusion; I do this because I generally try to keep these cards as discrete, containered units of information, and we need not insist upon an answer as we are asking the question)<br /><br /><i>&quot;Scientists delight in devising explanations for the great dinosaur extinctions.</i><br /><br /><i>But </i><b><i>there are several questions which they have failed to even ask</i></b><i>, much less tried to answer.</i><br /><br />- <i>Why, for instance, in all of the time claimed to have passed since the dinosaur extinctions, has nothing ever re-evolved to the sizes of the large dinosaurs?</i><br /><br />- <i>If such sizes worked for creatures which ruled the Earth for tens of millions of years, then why would not some species of elephant or rhinoceros have evolved to such a size again?</i><br /><br />- <i>What kinds of problems, if any, would sauropod sizes entail in our world as it is presently constituted?</i><br /><br />- <i>Could it be that some aspect of our environment might have to be massively different for such creatures to exist at all?</i><br /><br />- <b><i>A careful study of the sizes of these [former] creatures, and what it would take to deal with such sizes in our world, has led me to believe that the super animals of Earth&#39;s past could not live in our present world at all.</i></b><br /><br /><i>A look at sauropod dinosaurs as we know them today requires that we relegate the brontosaur, once thought to be one of the largest sauropods, to welterweight or at most middleweight status. Fossils found in the 1970&#39;s now dwarf this creature.</i><br /><br /><i>Both the brachiosaur and the supersaur were larger than the brontosaur, and the ultrasaur appears to have dwarfed them all. [1] </i><b><i>The ultrasaur is now estimated to have weighed 180 tons.</i></b><i> [2]</i><br /><br /><i>A comparison of dinosaur lifting requirements to human lifting capabilities is enlightening, though there might be objections to doing so. One objection that might be raised is that animal muscle tissue was somehow &#39;better&#39; than that of humans. This, however, is known not to be the case.</i><br /><br /><i>According to Knut Schmidt-Nielson, author of Scaling: Why is Animal Size So Important?, </i><b><i>the maximum stress or force that can be exerted by any muscle is independent of body-size and is the same for mouse or elephant muscle.</i></b><i> [3]</i><br /><br /><i>Another objection might be that sauropods were aquatic creatures. But nobody believes that anymore; they had no adaptation for aquatic life, their teeth show wear and tear which does not come from eating soft aquatic vegetation, and trackways show them walking on land with no difficulty.</i><br /><br /><i>A final objection might be that dinosaurs were somehow more &#39;efficient&#39; than top human athletes. This, however, goes against all observed data. As creatures get bulkier, they become less efficient; the layers of thick muscle in limbs begin to get in each other&#39;s way and bind to some extent. For this reason, scaled lifts for the super-heavyweight athletes are somewhat lower than for, say, the 200-pound athletes.</i><br /><br /><i>By &#39;scaled lift&#39; I mean a lift record divided by the two-thirds power of the athlete&#39;s body weight.</i><br /><br /><b><i>As creatures get larger, weight, which is proportional to volume, goes up in proportion to the cube of the increase in dimension. Strength, on the other hand, is known to be roughly proportional to the cross-section of muscle for any particular limb and goes up in proportion to the square of the increase in dimension. This is the familiar &#39;square-cube&#39; problem.&quot;</i></b><i> [4]</i><br /><br /><a href=\"http://www.bibliotecapleyades.net/ciencia/ciencia_dinosaurs01.htm\" class=\"ot-anchor\">http://www.bibliotecapleyades.net/ciencia/ciencia_dinosaurs01.htm</a><br /><br /><i>&quot;Consider the case of Bill Kazmaier, the king of the power lifters in the 1970s and 1980s.</i><br /><br /><i>Power lifters are, in my estimation, the strongest of all athletes; they concentrate on the three most difficult total-body lifts, i.e. bench press, squat, and dead-lift. They work out many hours a day and, it is fairly common knowledge, use food to flavor their anabolic steroids. No animal the same weight as one of these men could be presumed to be as strong.</i><br /><br /><i>Kazmaier was able to do squats and dead lifts with weights between 1,000 and 1,100 pounds on a bar, assuming he was fully warmed up.</i><br /><br /><b><i>Standing Up at 70,000 pounds</i></b><br /><br /><b><i>Any animal has to be able to lift its own weight off the ground</i></b><i>, i.e. stand up, with no more difficulty than Kazmaier experiences doing a 1,000-pound squat.</i><br /><br /><i>Consider, however, what would happen to Mr. Kazmaier, were he to be scaled up to 70,000 pounds, the weight commonly given for the brontosaur. Kazmaier&#39;s maximum effort at standing, fully warmed up, assuming the 1,000 pound squat, was 1,340 pounds (1,000 pounds for the bar and 340 pounds for himself). The scaled maximum lift would be 47,558 pounds (the solution to: 1,340/340.667 = x/70,000.667).</i><br /><br /><i>Clearly, he would not be able to lift his weight off the ground!</i><br /><br /><i>A sauropod dinosaur had four legs you might say; so what happens if Mr. Kazmaier uses arms and legs at 70,000 pounds? The truth is that the squat uses almost every muscle in the athlete&#39;s body very nearly to the limits, but in this case, it does not even matter.</i><br /><br /><i>A near maximum bench press effort for Mr. Kazmaier would fall around 600 pounds. This merely changes the 1,340 pounds to 1,940 pounds in the equation above, and the answer comes out as 68,853 pounds. Even using all muscles, some more than once, the strongest man who we know anything about would not be able to lift his own weight off the ground at 70,000 pounds.</i><br /><br /><i>To believe then, that a brontosaur could stand at 70,000 pounds, one has to believe that </i><b><i>a creature whose weight was mostly gut and the vast digestive mechanism involved in processing huge amounts of low-value foodstuffs</i></b><i> was, somehow, stronger than an almost entirely muscular creature its size, far better trained and conditioned than any grazing animal.</i><br /><br /><i>That is not only ludicrous in the case of the brontosaur, but the calculations only become more absurd when you try to scale up to the supersaur and ultrasaur at their sizes.</i><br /><br /><b><i>How heavy can an animal get to be in our world, then? How heavy would Mr. Kazmaier be at the point at which the square-cube problem made it as difficult for him to stand up as it is for him to do 1,000-pound squats at his present weight of 340 pounds?</i></b><br /> <br /><b><i>The answer is 20,803 pounds (the solution to: 1,340/340.667 = x/x.667). In reality, elephants do not appear to get quite to that point.</i></b><br /><br /><i>Christopher McGowan, curator of vertebrate paleontology at the Royal Ontario Museum, claims that a Toronto Zoo specimen was the largest in North America at </i><b><i>14,300 pounds</i></b><i>, [5] and Smithsonian personnel once informed me that the gigantic bush elephant specimen which appears at their Museum of Natural History weighed around </i><b><i>8 tons [16,000 lbs]</i></b><i>&quot;.</i><br /><br /><a href=\"http://www.bibliotecapleyades.net/ciencia/ciencia_dinosaurs01.htm\" class=\"ot-anchor\">http://www.bibliotecapleyades.net/ciencia/ciencia_dinosaurs01.htm</a> <br /><br /><i>&quot;Sauropod Dinosaurs&#39; Necks</i><br /><br /><i>A study of the sauropod dinosaurs&#39; long neck further underscores the problem these creatures would have living under current gravitational conditions. </i><b><i>Scientists who study sauropod dinosaurs now claim [this was written in 1994] that they held their heads low, because they could not have gotten blood to their brains had they held them high.</i></b><i> [6]</i><br /><br /><i>McGowan mentions the fact that a giraffe&#39;s blood pressure -- which at 200-to-300 mm Hg (millimeters of mercury) </i><b><i>is far higher than that of any other animal -- would probably rupture the vascular system of any other animal. The giraffe&#39;s blood pressure is maintained by thick arterial walls and by a very tight skin that apparently acts like a jet pilot&#39;s pressure suit.</i></b><i> A giraffe&#39;s head might reach to 20 feet.</i><br /><br /><b><i>How a sauropod might have gotten blood to its brain at 50 or 60 feet is the real question.</i></b><br /><br /><i>&#39;Gravity is a pervasive force in the environment and has dramatically shaped the evolution of plants and animals,&#39; notes Harvey Lillywhite, a zoologist at the University of Florida at Gainesville.</i><br /><br /><i>As some land animals evolved large body sizes,</i><br /><br /><i>&#39;cardiovascular specializations were needed to help them withstand the weight of blood in long vertical vessels. </i><b><i>Perhaps nowhere in the history of life were these challenges greater than among the gigantic, long-necked sauropods&#39;</i></b><br /><br /><b><i>For a Barosaurus to hold its head high, Lillywhite has calculated that its heart,</i></b><br /><br /><b><i>&#39;must have generated pressures at least six times greater than those of humans and three to four times greater than those of giraffes.&#39;</i></b><i> [7]</i><br /><br /><i>Faced with the same dilemma, University of Pennsylvania geologist Peter Dodson remarked that while the Brachiosaurus was built like a giraffe and may have fed like one, most sauropods were built quite differently.</i><br /><br /><b><i>&#39;At the base of the neck,&#39; Dodson writes, &#39;a sauropod&#39;s vertebral spines, unlike those of a giraffe, were weak and low and did not provide leverage for the muscles required to elevate the head in a high position.</i></b><br /><br /><i>Furthermore, the blood pressure required to pump blood up to the brain, thirty or more feet in the air, would have placed extraordinary demands on the heart and would </i><b><i>seemingly have placed the animal at severe risk of a stroke, an aneurysm, or some other circulatory disaster.&#39;</i></b><i> [8]</i><br /><br /><i>[...]</i><br /><br /><b><i>The only way to keep the required blood pressure &#39;reasonable,&#39; Dodson goes on to add,</i></b><br /><br /><b><i>is &#39;if sauropods fed with the neck extended just a little above heart level, say from ground level up to fifteen feet...&#39;</i></b><br /><br /><b><i>One problem with this solution is that the good leaves were, in all likelihood, above the 20-foot mark; an ultrasaur that could not raise its head above 20 feet would probably starve.</i></b><br /><br /><i>[...]</i><br /> <br /><i>McGowan and others claim that the head and neck were supported by a dorsal ligament and not muscles, but </i><b><i>we know of no living creature using ligaments to support a body structure which its available musculature cannot sustain.</i></b><br /><br /><i>In all likelihood, sauropods, in our gravity at least, could neither hold their heads up nor out.</i><br /><br /><i>[Former] Flying Creatures</i><br /><br /><i>In the [former] world, 350-pound flying creatures soared in skies which no longer permit flying creatures above 30 pounds or so. Modern birds of prey, like the Argentinian teratorn, weighing 170 to 200 pounds, with 30-foot wingspans, also flew. Within recorded history, </i><b><i>Central Asians have been trying to breed hunting eagles for size and strength, and have not gotten them beyond 25 pounds or thereabouts. Even at that weight they are able to take off only with the greatest difficulty.</i></b><br /><br /><i>Something was vastly different in the [former] world.</i><br /><br /><i>Nothing much larger than 30 pounds or so flies anymore, and those creatures, albatrosses and a few of the largest condors and eagles, are marginal. *Albatrosses, notably, are called &#39;goonie birds&#39; by sailors because of the extreme difficulty they experience taking off and landing - their landings being badly controlled crashes, and this despite long wings made for maximum lift.</i><br /><br /><i>[...]</i><br /><br /><i>No flying creature has since re-evolved into anything of such size, and the one or two birds that have retained this size have forfeited flight, their wings becoming vestigial.</i><br /><br /><b><i>Adrian Desmond, in his book The Hot-Blooded Dinosaurs, has a good deal to say about some of the problems the Pteranodon faced at just 40-to-50 pounds. Scientists once thought this pterosaur was the largest creature that ever flew.</i></b><br /><br /><i>The bird&#39;s great size and negligible weight must have made for a rather fragile creature.</i><br /><br /><b><i>&#39;It is easy to imagine that the paper-thin tubular bones supporting the gigantic wings would have made landing dangerous,&#39; writes Desmond.</i></b><br /><br /><b><i>&#39;How could the creature have alighted without shattering all of its bones? How could it have taken off in the first place? It was obviously unable to flap 12-foot wings strung between straw-thin tubes. Many larger birds have to achieve a certain speed by running and flapping before they can take off and others have to produce a wing beat speed approaching hovering in order to rise.</i></b><br /><br /><b><i>To achieve hovering with a 23-foot wingspread, Pteranodon would have required 220 pounds of flight muscles as efficient as those in humming birds. But it had reduced its musculature to about 8 pounds, so it is inconceivable that Pteranodon could have taken off actively.&#39;</i></b><i> [9]</i><br /><br /><b><i>Since the Pteranodon could not flap its wings, the only flying it could ever do, Desmond concludes, was as a glider.</i></b><br /><br /><i>It was, he says,</i><br /><br /><b><i>&#39;the most advanced glider the animal kingdom has produced.&#39;</i></b><i> [10]</i><br /><br /><i>Desmond notes a fairly reasonably modus operandi for the Pteranodon.</i><br /><br /><b><i>Not only did the bird have a throat pouch like a pelican but its remains were found with fish fossils, which seems to suggest a pelican-like existence, soaring over the waves and snapping up fish without landing.</i></b><br /> <br /><i>[...]</i><br /><br /><i>There is one other problem. The Pteranodon was not the largest bird.</i><br /><br /><i>The giant Teratorn finds of Argentina were not known when Desmond&#39;s book was written. News of this bird&#39;s existence first appeared in the 1980s. </i><b><i>The Terotorn was a 160-to-200 pound eagle with a 27-foot wingspan, a modern bird whose existence involved, among other things, flapping wings and aerial maneuvers.</i></b><br /><br /><b><i>But how so? How could it even have flown?</i></b><br /><br /><b><i>How large can an animal be and still fly?</i></b><br /><br /><i>&#39;With each increase in size, and therefore also weight,&#39; writes Desmond, &#39;a flying animal needs a concomitant increase in power (to beat the wings in a flapper and to hold and maneuver them in a glider), </i><b><i>but power is supplied by muscles which themselves add still more weight to the structure.</i></b><br /><br /><i>The larger a flyer becomes the disproportionately weightier it grows by the addition of its own power supply. </i><b><i>There comes a point when the weight is just too great to permit the machine to remain airborne. Calculations bearing on size and power suggested that the maximum weight that a flying vertebrate can attain is about 50 pounds ...&#39;</i></b><br /><br /><i>It is for this reason that scientists believed Pteranodon and its slightly larger but lesser known Jordanian ally Titanopteryx were the largest flying animals of all time.</i><br /><br /><i>The experience from our present world coincides well with this and, in fact, don&#39;t go quite that high. </i><b><i>The biggest flying creatures which we actually see are albatrosses, geese, and the like, at 30 to 35 pounds.</i></b><br /><br /><i>The Pteranodon&#39;s reign as the largest flying creature of all time actually fell in the early 1970s when Douglas Lawson of the University of California found partial skeletons of three ultra-large pterosaurs in Big Bend National Park in Texas. This discovery forced scientists to rethink their ideas on the maximum size permissible in flying vertebrates.</i><br /><br /><b><i>The immense size of the Big Bend pterosaurs may be gauged by noting that the humerus or upper arm bones of these creatures is fully twice the length of Pteranodon&#39;s. Lawson estimated the wingspan for this living glider at over fifty feet.</i></b><br /><br /><i>The Big Bend pterosaurs were not fishers. Their remains were found in rocks that were formed some 250 miles inland and nowhere near any lake deposits. This led Lawson to suggest that these birds were carrion feeders, gorging themselves on rotting mounds of dismembered dinosaur flesh.</i><br /><br /><i>But this hypothesis raised numerous questions in author Desmond&#39;s mind.</i><br /><br /><b><i>&#39;How they could have taken to the air after gorging themselves is something of a puzzle,&#39;</i></b><i> he wonders.</i><br /> <br /><b><i>&#39;Wings of such an extraordinary size could not have been flapped when the animal was grounded. Since the pterosaurs were unable to run in order to launch themselves they must have taken off vertically.</i></b><br /><br /><i>Pigeons are only able to take-off vertically by reclining their bodies and clapping the wings in front of them; as flappers, the Texas pterosaurs would have needed very tall stilt-like legs to raise the body enough to allow the 24-foot wings to clear the ground.</i><br /><br /><b><i>The main objection, however, still rests in the lack of adequate musculature for such an operation.&#39;</i></b><i> [12]</i><br /><br /><i>The only solution seems to be that they lifted passively off the ground by the wind. But this situation, notes Desmond, would leave these ungainly Brobdignagian pterosaurs vulnerable to attack when grounded.</i><br /><br /><i>While Desmond mentions a number of ancillary problems here, any of which would throw doubt on the pterosaur&#39;s ability to exist as mentioned, he neglects the biggest question of all: the calculations that say 50 pounds are the maximum weight have not been shown to be in error; we have simply discovered larger creatures. Much larger.</i><br /><br /><i>This is what is called a dilemma.</i><br /><br /><i>Those who had estimated a large wingspan for the Big Bend bird were immediately attacked by aeronautical engineers.</i><br /><br /><b><i>&#39;Such dimensions broke all the rules of flight engineering,&#39; wrote Colorado paleontologist Robert T. Bakker, in The Dinosaur Heresies, &#39;a creature that large would have broken its arm bones if it tried to fly ...&#39;</i></b><i> [13]</i><br /><br /><i>Subsequently, the proponents of a large wingspan were forced to back off somewhat, since the complete wing bones had not been discovered.</i><br /><br /><i>But Bakker believes these pterosaurs really did have wingspans of over 60 feet and that </i><b><i>they simply flew despite our not comprehending how. The problem is ours, he says, and he proposes no solution.</i></b><br /><br /><i>So much for the idea of anything re-evolving into the sizes of the flying creatures of the [former] world. What about the possibility of man breeding something like a Teratorn? Could man actively breed even a 50-pound eagle?</i><br /><br /><i>Berkuts are the biggest of eagles.</i><br /><br /><i>And Atlanta, an eagle that Sam Barnes, one of England&#39;s top falconers in the 1970s, brought back to Wales from Kirghiz, Russia, is, at 26 pounds in flying trim, as large as they ever get. [14] </i><b><i>These eagles have been bred specifically for size and ferocity for many centuries. They are the most prized of all possessions amongst nomads, and are the imperial hunting bird of the Turko-Mongol peoples.</i></b><br /><br /><i>The only reason Barnes was allowed to bring her back is that Atlanta had a disease for which no cure was available in Kirghiz and was near to death. A Berkut of Atlanta&#39;s size, Barnes was told, would normally be worth more than a dozen of the most beautiful women in Kirghiz.</i><br /> <br /><i>[...]</i><br /><br /><i>The killing powers of a big eagle are out of proportion to its size. Berkuts are normally flown at wolves, deer, and other large prey. Barnes witnessed Atlanta killing a deer in Kirghiz, and was told that she had killed a black wolf a season earlier. Mongols and other nomads raise sheep and goats, and obviously have no love for wolves.</i><br /><br /><i>A wolf might be little more than a day at the office for Atlanta with her 11-inch talons, however, a wolf is a big deal for an average-sized Berkut at 15-to-20 pounds. Obviously, there would be an advantage to having the birds be bigger, i.e. to having the average Berkut weigh 25 pounds, and for a large one to weigh 40-to-50 pounds. It has never been done, however, despite all the efforts and funds poured into the enterprise since the days of Genghis Khan.</i><br /><br /><i>The breeding of Berkuts has continued apace from that day to this, but the Berkuts have still not gotten any bigger than 25 pounds or so. [15]</i><br /><br /><i>It is worth recalling here the difficulty which increasingly larger birds experience in getting airborne from flat ground. Atlanta was powerful enough in flight, but </i><b><i>she was not easily able to take off from flat ground.</i></b><i> This could spell disaster in the wild. A bird of prey will often land with prey, and if take-off from flat ground to avoid trouble is not possible, the bird&#39;s life becomes imperiled.</i><br /><br /><i>A bird bigger than Atlanta with her 10-foot wingspan, like a Teratorn with a 27-foot wingspan and weighing 170 pounds, would simply not Survive.</i><br /><br /><i>Assorted Other Evidence</i><br /><br /><i>There are other categories of evidence [that something important has changed].</i><br /><br /><b><i>It is well known, for example, that elephant-sized animals cannot sustain falls, and that elephants spend their entire lives avoiding them.</i></b><br /><br /><b><i>For an elephant, the slightest tumble can break bones and/or destroy enough tissue to prove fatal. Predators, however, live by tackling and tumbling with prey. One might think that this consideration would preclude the existence of any predator too large to sustain falls. Weight estimates for the tyrannosaurs, however, include specimens heavier than any elephant.</i></b><br /><br /><i>That appears to be a contradiction.</i><br /><br /><b><i>Moreover, elephants are simply too heavy to run in our world. As is well known, they manage a kind of a fast walk. They cannot jump, and anything resembling a gully stops them cold. Mammoths were as big and bigger than the largest elephants, however, and Pleistocene art clearly shows them galloping.&quot;</i></b><br /><br />(this claim is difficult to support; I am not -- yet -- aware of any imagery which definitively supports it ...)<br /><br /><i>&quot;Finally, there is the Utahraptor. Recently found in Utah, this creature is a 20-foot, 1,500-pound version of a Velociraptor. [16]</i><br /> <br /><i>The creature apparently ran on the balls of its two hind feet, on two toes in fact, the third toe carrying a 12-inch claw for disemboweling prey. This suggests a very active lifestyle. Very few predators appear to be built for attacking prey notably larger than themselves; the Utahraptor appears to be such a case.</i><br /><br /><i>In our world, of course, 1,500-pound toe dancers do not exist. The only example we have of a 1,500-pound land predator is the Kodiak bear, the lumbering gait and mannerisms of which are familiar to us all.</i><br /><br /><i>And so, over and over again, this same kind of dilemma -- things which cannot happen in our world having been the norm in the [former] world.</i><br /><br />From <a href=\"https://www.quora.com/How-could-dinosaurs-get-so-big-despite-Galileos-square-cube-law\" class=\"ot-anchor\">https://www.quora.com/How-could-dinosaurs-get-so-big-despite-Galileos-square-cube-law</a><br /><br /><b><i>&quot;Thus, the problem of how dinosaurs grew so large is a scientific paradox that has been holding back science for literally hundreds of years.</i></b><br /><br /><i>Scientists have identified four specific problem areas regarding the large terrestrial Mesozoic animals:</i><br /><br /><i>1) insignificant bone strength,</i><br /><br /><i>2) insignificant muscle strength,</i><br /><br /><i>3) unacceptably high blood pressure existing in the tallest dinosaurs such as the Brachiosaurus, and</i><br /><br /><i>4) grossly insignificant power for cold-blooded reptiles to fly.</i><br /><br /><i>These are the following answers that I could find from various sources:</i><br /><br /><i>a) Galileo&#39;s square-cube &#39;law&#39; is really an approximation for the moderate spectrum of animal sizes, something that explains a lot but doesn&#39;t generalize like gravitation, which Newton used to explain the fall of an apple and the orbit of the moon.</i><br /><br />_See: Lewin&#39;s Love of Physics [at <a href=\"http://falkenblog.blogspot.in/2011/11/lewins-love-of-physics.html%5D_\" class=\"ot-anchor\">http://falkenblog.blogspot.in/2011/11/lewins-love-of-physics.html]_</a><br /><br /><i>b) Perhaps the answer has to do with high net primary productivity during the dinosaur era: atmospheric CO2 levels then were up to 10 times the present levels and elevated CO2 stimulates productivity of some plants.</i><br /><br />_See: Dinosaurs, dragons, and dwarfs: The evolution of maximal body size [at <a href=\"http://www.pnas.org/content/98/25/14518.full%5D_\" class=\"ot-anchor\">http://www.pnas.org/content/98/25/14518.full]_</a><br /><br /><i>c) Perhaps, the density of the Earth’s air near the Earth’s surface would need to be 2/3’s of the density of water to produce the necessary buoyancy so that the dinosaurs could grow to their exceptional size.</i><br /><br />_See: The Solution to the Big Dinosaur Paradox [at <a href=\"http://www.dinosaurtheory.com/solution.html%5D_\" class=\"ot-anchor\">http://www.dinosaurtheory.com/solution.html]_</a><br /><br /><b><i>As of now it is still an unresolved question.</i></b><br /><br /><i>Written May 29, 2014&quot;</i><br /><br />GRAPHIC REFERENCES:<br /><br />[1] - [3] <a href=\"http://www.bibliotecapleyades.net/ciencia/ciencia_dinosaurs01.htm\" class=\"ot-anchor\">http://www.bibliotecapleyades.net/ciencia/ciencia_dinosaurs01.htm</a><br /><br />[4] <a href=\"https://www.quora.com/How-could-dinosaurs-get-so-big-despite-Galileos-square-cube-law\" class=\"ot-anchor\">https://www.quora.com/How-could-dinosaurs-get-so-big-despite-Galileos-square-cube-law</a><br /><br />[5] <a href=\"http://fivethirtyeight.com/features/what-powerlifting-tells-us-about-the-effects-of-peds/\" class=\"ot-anchor\">http://fivethirtyeight.com/features/what-powerlifting-tells-us-about-the-effects-of-peds/</a><br /><br />[6] <a href=\"http://bearfabrique.org/History/sauropods/sauropods.html\" class=\"ot-anchor\">http://bearfabrique.org/History/sauropods/sauropods.html</a><br /><br />[7] <a href=\"http://www.bibliotecapleyades.net/ciencia/ciencia_dinosaurs01.htm\" class=\"ot-anchor\">http://www.bibliotecapleyades.net/ciencia/ciencia_dinosaurs01.htm</a><br /><br />[8] - [10] <a href=\"http://bearfabrique.org/History/sauropods/sauropods.html\" class=\"ot-anchor\">http://bearfabrique.org/History/sauropods/sauropods.html</a><br /><br />[11] - [13] <a href=\"http://www.bibliotecapleyades.net/ciencia/ciencia_dinosaurs01.htm\" class=\"ot-anchor\">http://www.bibliotecapleyades.net/ciencia/ciencia_dinosaurs01.htm</a><br /><br />[14] <a href=\"http://bearfabrique.org/History/sauropods/sauropods.html\" class=\"ot-anchor\">http://bearfabrique.org/History/sauropods/sauropods.html</a><br /><br />[15] and the one before it: <a href=\"http://www.bibliotecapleyades.net/ciencia/ciencia_dinosaurs01.htm\" class=\"ot-anchor\">http://www.bibliotecapleyades.net/ciencia/ciencia_dinosaurs01.htm</a><br /><br />[16] Graphic has been processed, but still reflective of size ... original comes from <a href=\"http://planetsave.com/2013/06/28/argentavis-magnificens-largest-flying-bird-ever-giant-teratorn-facts-extinction-wingspan-etc/\" class=\"ot-anchor\">http://planetsave.com/2013/06/28/argentavis-magnificens-largest-flying-bird-ever-giant-teratorn-facts-extinction-wingspan-etc/</a><br /><br />[17] <a href=\"http://www.bibliotecapleyades.net/ciencia/ciencia_dinosaurs01.htm\" class=\"ot-anchor\">http://www.bibliotecapleyades.net/ciencia/ciencia_dinosaurs01.htm</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Simulation as a Black Box","summary":"The Rewards that Come with Interpreting Nature through the Lens of Computer-Assisted Analysis Are Not Risk-Free / Decisions About What to Model and What Constitute Valid Results Still Root in the Researcher's Worldview / As Virtual Interaction with Molecules has Become Commonplace in Biology, Students Are Observed to Increasingly Treat the Simulations as Black Boxes / When Machines Mediate Observation, Assumptions and Mistakes Already Embedded Into Our Mental Models Can Become More Entrenched","image":"https://lh5.googleusercontent.com/-Z7XapIa9wJs/VxOz91cTRNI/AAAAAAAAHsM/0NwQ3HL9rCIjR7Zx23uJowlx4ZarTgwUgCL0B/the-simulation-as-a-black-box-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Q5eqzQJX8n2","publishDate":"2016-04-17T16:07:38.821Z","updateDate":"2017-01-03T02:35:44.569Z","text":"<b>The Simulation as a Black Box: The Rewards that Come with Interpreting Nature through the Lens of Computer-Assisted Analysis Are Not Risk-Free / Decisions About What to Model and What Constitute Valid Results Still Root in the Researcher&#39;s Worldview / As Virtual Interaction with Molecules has Become Commonplace in Biology, Students Are Observed to Increasingly Treat the Simulations as Black Boxes / When Machines Mediate Observation, Assumptions and Mistakes Already Embedded Into Our Mental Models Can Become More Entrenched</b><br /><br /><b>Climate Change Critic Tim Ball Tells the Story of the Origin of Computer-Based Climate Modeling</b><br /><br /><i>The Deliberate Corruption of Climate Science</i><br />Tim Ball<br />pages 116 - 118<br /><br /><i>&quot;Computer models came on the scene during my career.  Gradually climate conferences were dominated by modelers and </i><b><i>most did it arrogantly.</i></b><i> They battled amongst themselves, with the person with the largest and fastest computer, dominating. </i><b><i>Very few of them knew anything about the climate but were apparently attracted by the challenge of modeling the complexity that is global climate.</i></b><i> Some mathematicians and statisticians test theories using very long records of natural phenomena.  However as AN Whitehead said:</i><br /><br /><b><i>&#39;There is no more common error than to assume that because prolonged and accurate mathematical calculations have been made, the application of the result to some fact of nature is absolutely certain.&#39;</i></b><br /><br /><i>I witnessed a good example early at a conference in Edmonton on Prairie Climate predictions and the implications for agriculture.  Climate modeler Michael Schlesinger dominated as the keynote speaker.  His presentation compared five major global models and their results. </i><b><i>He claimed that because they all showed warming they were valid. Of course they did because they were programmed to that general result.</i></b><i> The problem is they varied enormously over vast regions.  For example, one showed North America cooling, while another showed warming.  The audience was looking for information adequate for planning and became agitated, especially in the question period.  It peaked when someone asked about the accuracy of his warmer and drier prediction for Alberta.  The answer was </i><b><i>50%</i></b><i>.  The person replied that is useless, my Minister needs 95%.  The shouting intensified.</i><br /><br /><i>Eventually a man threw his shoe on the stage.  When the room went silent he said, &#39;I didn&#39;t have a towel.&#39;  We learned he had a voice box and the shoe was the only way he could get attention.  He asked permission to go on stage where he explained his qualifications and put a formula on the blackboard. </i><b><i>He asked Schlesinger if this was the formula he used as the basis for his model of the atmosphere. Schlesinger said yes. The man then proceeded to eliminate variables asking Schlesinger if they were omitted in his work. After a few eliminations he said one was probably enough, but you have no formula left and you certainly don&#39;t have a model. It has been that way ever since with the computer models.&quot;</i></b><br /><br /><b>Computational Biologists Describe the Introduction of Modeling Software to their Discipline, and the Alarming Trend of Students Who Treat the Software as a Black Box</b><br /><br /><i>Simulation and Its Discontents</i><br />Sherry Turkle<br />pages 58 - 67<br /><br /><i>&quot;Life Sciences: The Tension Between Doing and Doubting</i><br /><br /><i>What does the real want in the life sciences?  There, simulation began with a kind of deception, an aesthetic compact with nature.  Early simulations were qualitative and evocative.  For example, one biophysicist, Stephane Leduc (1853-1922), working at the Nantes Medical School, drew on the persuasive powers of mimicry to simulate the mechanical processes governing life forms.  He used salt crystals and dyes to produce artificial cells and organisms.  These chemical creatures, formed by virtue of osmotic gradients, seemed strangely alive, their growth mimicking those of dividing cells, sporulating mushrooms, blooming plants, and free-swimming algae.  Leduc&#39;s simulations mimicked life without reference to its underlying processes.</i><br /><br /><i>As biology matured and computation became its dominant tool, this kind of simulation was discredited.  These days, a life scientist at MIT, who models protein-protein interactions and trains a new generation of biological engineers, describes a model that has no mathematical precision or predictive capability as just a &#39;cartoon.&#39;  For him, anything less than quantification of physics at a molecular level is &#39;mere philosophy.&#39;  These days, visualization and simulation underpin biology as it manipulates and reengineers life at the molecular and cellular level.  Mathematical simulations animate models that represent proteins and cells over time.  Algorithms predict molecular interactions within cells and the pathways of protein folding.  Scientists have built a second nature within the computer through simulations that are ever more manipulable, ever more easily experimented on.  Some describe the result of such virtual practices as &#39;new forms of life.&#39;</i><br /><br /><i>In today&#39;s biology, the simulation of life is central, </i><b><i>but getting it right has remained elusive. Living systems work on many levels, from atoms to organisms; integrating levels is difficult. And the intricate workings of cells and molecules are hard to see, quantify, and analyze. These challenges encourage some life scientists to approach simulation with what the MIT architects of the 1980s termed a critical stance.</i></b><i> In the life sciences, a critical stance toward simulation enforces modesty.  In the field of protein crystallography, which uses X-rays to investigate molecular structure, some researchers take pains to insist that the models of complex molecules they produce are &#39;just models.&#39;  Microscopists are quick to describe the extent to which their images are only mediated representations of the cells they study. </i><b><i>These life scientists take as given that simulations can deceive and that to assess simulation one must find a vantage point outside of it.</i></b><i> Simulation and visualization have become the everyday workplace of life sciences.  But the programs that scientists use are typically &#39;black boxed.&#39;  In this way, scientists&#39; feelings of mastery become tied to anxiety and uncertainty.</i><br /><br /><i>Computers first came into protein crystallography in the late 1940s.  Their job was to lighten the labor that stood behind crystallographic calculations, labor that had typically been allocated to women, who ironically were known as &#39;computors.&#39;  By 1957, computers had been used in the construction of the first visualization of a protein, a model built by hand out of Plasticene and wooden pegs.  Computer graphics for molecular visualization came later, in the mid-1960s.  From this point on, scientists would build molecular models by interacting with computer graphics; physical models were too cumbersome.</i><br /><br /><i>These days, the X-ray diffraction analyses that protein crystallographers depend on are collected, measured, and calculated by computers.  For the most part, protein crystallographers have welcomed this innovation.  In 1959, it had taken Max Perutz and his team of technicians twenty-two years to complete his Nobel Prize-winning model of hemoglobin.  These days, models of even larger proteins can be built by a single graduate student or postdoctoral researcher in a year.</i><br /><br /><i>Perutz and his colleagues, with limited computing power, built their molecules one amino acide residue at a time.  They relied on tacit knowledge, their &#39;feel&#39; for molecular structure.  Perutz described seeing the molecule emerge as &#39;reaching the top of a mountain after a very hard climb and falling in love at the same time.&#39;  His comment recalls the sensibility of physicist William Malven who spoke of science as a place where &#39;the mundane and profound go together -- like washing dishes and love.&#39;  In this view of scientific practice, science will always be human practice, a labor of love that cannot be fully automated.  Malven was willing to automate only the most laborious calculations, and then only with the most transparent instruments, as transparent as his Swiss Army knife.</i><br /><br /><i>This &#39;human practice&#39; view of science still informs the professional identities of some protein crystallographers.  For example, Professor Diane Griffin, head of an East Coast protein crystallogrpahy laboratory, uses the phrase manual thinking to refer to aspects of protein crystallographic practice that resist full automation, including data gathering, imaging, image analysis, and the calculation of crystallographic maps and models.  Griffin belongs to a generation of protein crystallographers who grew up writing their own programs to calculate electron density maps from X-ray data.  Her science depends on the accuracy of these programs and </i><b><i>she knows how hard it is to get errors out of them.</i></b><i> Like Malven, twenty years before her, who spoke of a black box as the most dangerous instrument in a laboratory, Griffin loses confidence when she cannot see inside the programs she uses.  At the Spring 2005 MIT workshop, she says:</i><br /><br /><b><i>&#39;When I was a graduate student, if you were going to convert some data or something like that, you would write the FORTRAN code to convert the data yourself.  That&#39;s how you would do it.  Now there are these programs.  There are these windows and you click.  I find with my students all the time, they don&#39;t know why something isn&#39;t working.  I&#39;m like, well, did the data convert properly?  Open the file and look at it.  It is so black box and it is going from the time when you knew how the data was converted, because you wrote the code to do it yourself, to you don&#39;t even open the file to see if it is full of zeros or not.  So there is a very big disconnect.&#39;</i></b><br /><br /><i>Like Malven, Griffin is particularly skeptical about the use of proprietary software in science; </i><b><i>manufacturers have a stake in closing the black box, of keeping code a secret.</i></b><br /><br /><b><i>Griffin was mentored by a generation of researchers who taught her that scientists should never abdicate authority to instruments they did not fully understand. For them, the advent of opaque software put the scientist in an unacceptable state of ignorance.</i></b><i> In a spirit of vigilant skepticism, Griffin educates graduate students, both in her laboratory and across her campus, to exercise critical judgment about computer-generated data.</i><br /><br /><i>The field of structural biology includes two distinct groups.  Scientists such as Griffin crystallize proteins, conduct X-ray diffraction experiments, and build onscreen molecular models &#39;by hand.&#39;  A second group of predictive modelers work on complex algorithms to predict protein structure.  Those who work with Griffin&#39;s methods insist that they need transparent software to achieve their ends.  They need to continually adjust and readjust code.  The intensity of their involvement keeps the limitations of their representations constantly before them.  They are not likely to confuse the model with the molecule.  Today, it is these crystallographers who produce the trusted structures against which predictive modelers test their algorithms.  The future, however, is uncertain: predictive modelers put their faith in increasingly powerful computers, increasingly powerful algorithms.</i><br /><br /><i>Griffin does not trust the claim of predictive modelers, that their software can automatically fit molecular structures to X-ray crystallographic data.  She has banned their software from her lab. </i><b><i>When she discovered that one of her students had used predictive software to help build part of a model, she made the student repeat the modeling work by hand. And indeed, the computer program had gotten the structure wrong.  Giffin&#39;s fears had been well founded.</i></b><br /><br /><i>These days, there is intense competition between predictive modelers and crystallographers to be the first to publish protein structures.  In competitive science, speed is always of the essence, and this pushes the field toward greater use of automatic techniques.  But for Griffin, the automation of model building is a kind of futile cheating: it provides a shortcut that might get you to the wrong place.  And even if it brings you to your destination, automation may shortchange you.  It certainly shortchanges students because it does not teach them how to use simulation with vigilance. </i><b><i>It deprives them of some fundamental experiences they need to develop a tacit knowledge of molecular configurations. Griffin thinks that crystallographers learn to &#39;think intelligently about structure&#39; by slowly building onscreen models. To do this, protein modelers must learn to work intimately with the computer, building a new hybrid instrument, a &#39;human-computer lens.&#39;</i></b><br /><br /><i>Materiality in Immateriality</i><br /><br /><i>In today&#39;s biology, computer simulations are ever more manipulable, ever more easily experimented on.  They offer an interactivity that makes screen objects seem &#39;material&#39; to the point that contact with them feels like engagement with something quite real.  Traditionally, scientists rely on &#39;witnessing&#39; and &#39;participation&#39; to make claims for the legitimacy of scientific knowledge.  Familiarity with the behavior of virtual objects can grow into something akin to trusting them, a new kind of witnessing.  It is a different sort of trust than Diane Griffin requires, but it can come to feel sufficient.</i><br /><br /><i>Griffin began her graduate training in the late 1980s when trust in one&#39;s computational tools was associated with familiarity with their underlying code. </i><b><i>Younger scientists are increasingly comfortable with black-boxed simulations.</i></b><i> They grew up with personal computers that did not come with programming languages.  They grew up on computer games that offered interactivity without transparency.  Unlike a previous generation, they did not program their own games. </i><b><i>When these younger scientists work with screen molecules, they are more likely than their elders to give themselves over to feeling in the grip of a new materiality.</i></b><br /><br /><i>In this they share an aesthetic with the architects who &#39;fly through&#39; virtual buildings.  In architecture, models of buildings are rotated on the screen.  In architecture, models of buildings are rotated on the screen.  In biology, molecular models are rotated on their axes.  Through these actions, molecules are kept in motion so that the hidden parts of the structure can be brought into view.  In both cases, the experience of depth is suggested by performances that engage the body.</i><br /><br /><i>Although generational markers are important, in design today, attitudes toward simulation do not neatly sort by generation.  The same is true for science.  Youth does not automatically confer uncritical comfort with what simulation offers.  And age does not automatically lead to resistance to simulation.  Some older scientists, for example, justify their use of opaque software by pointing to the infinite regress of computer representations.  After all, they argue, it doesn&#39;t really mean much to know how your simulation is programmed if all you are looking at is a high-level computer language.  The &#39;real guts&#39; of the program is in assembly language and in all that lies beneath that, and no one wants to go to that level with today&#39;s complex machines.  In the 1980s, Professor Barry Niloff insisted that his students learn the physics of display technology; today, such scruples seem of a different era, practical impossibilities that lead scientists, young and old, to accept opacity.  These days, the problem for the working scientist boils down to a question: What level and language will provide enough understanding for me to compare the simulation before me with what I know of nature?</i><br /><br /><i>Some younger scientists who are not altogether content with their opaque simulations feel they have no way to act on their unease.  One, a physicist at a national research laboratory, admits that when he works with new, elaborate 3D simulation, he misses the algorithmic understanding he enjoyed with earlier models.  An older colleague encourages him to play with immersive virtual realities in the spirit of a tinkerer.  Time and interaction will do their work: &#39;Give yourself a few years to try it out and fiddle with it awhile,&#39; he says.  &#39;You will probably find something you can do that you couldn&#39;t do the other way.&#39;  He is convinced that at some point his younger colleague will feel at one with the technology; he will come to &#39;see in simulation,&#39; despite its opacity.</i><br /><br /><i>Gordon Research Conferences provide an international forum for the presentation and discussion of frontier research in the biological, chemical, and physical sciences.  At a Gordon conference in 1965, the structural biologist Richard Langridge presented an interactive computer graphics workstation for visualizing and manipulating molecular models to an unenthusiastic audience of his peers.  Langridge recalled that the objections had to do with people not having their &#39;hands on something, something physical so that [they] could understand it.&#39;  He was not discouraged.  In contemporary terms, molecular biology did not yet have the right &#39;interaction metaphor.&#39;  He said: &#39;Standing up at a conference and showing 16mm movies, in the early days, was really not a good substitute for sitting in front of the computer and actually using it.&#39;  Even though the early simulations were slow, they made it clear that screen molecules could be compelling.  &#39;When you first got your hands on that crystal ball at Project MAC and moved the thing around in three dimensions it was thrilling.  There was no question.&#39;</i><br /><br /><i>As the virtual became increasingly manipulable, as screen movements seemed to happen in real time, protein crystallographers became willing to make the transition from physical to virtual models.  With the new technology, one had the sense of dealing directly with the molecule, </i><b><i>a feeling that did not depend on the model&#39;s appearance, but on the smoothness of the user&#39;s interaction with the screen representations.</i></b><i> [!!!] In 1977, a molecular graphics system called GRIP (Graphics Interaction with Proteins) reached a turning point in fluidity of use.  GRIP gave its users more than an illusion of smooth connection between modeler and molecule; users experienced the system as a prosthetic extension of themselves into what felt like a tangible world of screen molecules.  It is an effect that is familiar to all who play computer games.</i><br /><br /><i>In a lecture on the role that simulation plays in protein crystallography, Griffin describes the physicality of today&#39;s modeling systems.  The best-designed modeling systems try to give protein researchers the tactility and immediacy they came to expect in molecular modeling work.  A user sits in front of a screen, often wearing stereoglasses to enhance three-dimensional efefcts: &#39;You are physically dragging pieces of protein structure, amino acids, and sticking it in [the screen molecule].  You drag it in and you stick it there.  And then with your dials or your mouse, you are adjusting it, moving the pieces to get it to fit.  So you are physically building with the stereoglasses and the mouse.  Protein crystallographers report that they feel the model in their bodies and that their bodies mirror the models they manipulate onscreen.</i><br /><br /><i>It is not surprising that, in this relationship with the computer, individual scientists express individuality and differences in style.  Some scientists want to use the most up-to-date tools, but many enjoy the comforts of the most familiar well-worn virtual tools.  Griffin says that, in her laboratory, researchers tend to use the programs they built or the programs they learned on.  Griffin herself uses a program she wrote herself: &#39;Because I&#39;m so familiar with it, I can just do things automatically, which with another program I would have to sit there and think ... I&#39;ve connected with the software in a way that I don&#39;t have to think about the direction ... I just kind of know how to move the mouse to do what I want to do without thinking.&#39;</i><br /><br /><b><i>Across the professions, software has become increasingly uniform and black boxed</i></b><i>, even as there is demand for nonstandardized tools that can accommodate users with different intellectual styles.  Since today&#39;s users cannot change fundamental things about their programs, many return to what in the 1980s was called &#39;customization,&#39; small changes that make people feel more at home.</i><br /><br /><i>These days, life scientists do not talk much about moving from physical models to computer screens -- that ship has sailed.&quot;</i><br /><br /><b>Two Pilots Explain the Problems Imposed by Synthetic Vision in Electronic Flight Information Systems (EFIS)</b><br /><br />From Don:<br /><br /><i>&quot;The primary panel display merges information from </i><b><i>GPS</i></b><i>, </i><b><i>system-stored terrain maps</i></b><i> (like Google Earth), </i><b><i>system-stored aviation charts</i></b><i> (&#39;visual&#39; and &#39;instrument&#39;), and often </i><b><i>infrared onboard camera images</i></b><i>. The multilevel data are layered to provide an in-cockpit 3D view of the environment.</i><br /><br /><b><i>The hybrid view of the landscape must be taken as-is to be effective if the pilot is unable to see outside the aircraft; e.g., dark night in the mountains. Otherwise the meta-view is too complex to second-guess. A rather extreme example of what could go wrong is an error or failed update to the terrain maps so that a new cell tower near an airport is absent from the synthetic picture</i></b><i> ...</i><br /><br /><i>So with EFIS we have two stacked meta-layers between us and reality ...&quot;</i><br /><br />From Jim:<br /><br /><i>&quot;In the AF we spent a lot of time learning </i><b><i>instrument flight</i></b><i>, which means </i><b><i>without reference to outside stimuli until you are very near the end of the flight, at which point you learn to &#39;transition&#39; to outside references from relying on the mental picture you create from the instrument sources alone</i></b><i>.  There’s a &#39;long&#39; history (way less than a hundred years is &#39;long&#39; in that vocation). </i><b><i>A cardinal rule is to not believe your internal sensory outputs, particularly from the vestibular system which purports to identify your orientation in space relative to the ground</i></b><i> - i.e., am I upright, tilted in some direction or upside down?  When you can’t combine your ears inputs with absent eye inputs, you don’t know for sure what your body is telling you.  The accelerations of an aircraft can fool your ears every time.  There’s even a name for that particular disorientation - &#39;the leans&#39; - so-called because you have this unconscious tendency to lean away from the direction your ears are telling you that you are leaning toward, in order to regain your &#39;balance&#39;. </i><b><i>Without the visual help of the instruments’ diplay, you might easily make a fatal mistake and roll the aircraft in the direction you think is upright.</i></b><br /><br /><b><i>The cardinal rule is Always Believe The Intruments.</i></b><i> Their failure rate is always less than your own, in the conditions of instrument flight. </i><b><i>The second rule, and which may be why a pilot is one of the best examples of a skeptic I can think of, is &#39;Always Suspect and Crosscheck Your Instruments&#39;.</i></b><i> One constantly keeps checking all the instruments rather than focusing on just one, to see if discrepancies arise, such as an altimeter reading that is holding steady while the vertical velocity instrument is shoing a descent rate of 300 feet per minute, which means something ain’t quite right.</i><br /><br /><i>Modern military aircraft have not only a heading indicator that shows you which direction the aircraft is pointed, it may have a second standby instrument that is supposed to tell you the same thing via a different or independendent, redundant means, and there is always a &#39;whiskey compass&#39; - a compass card in a clear liquid , alcohol in the old days, on needle bearings, that points to magnetic north.  You check its direction display, corrected by the local magnetic variation, to see if your instrument display matches up.  In aircraft with dual sets of instruments, the two sets run off different computers and different pitostatic instruments for redundancy, and a good copilot checks his instruments against the pilot’s frequently. </i><b><i>All of this work allows the construction in a pilot’s mind a &#39;scene&#39; in which she knows where along the planned route the aircraft is, the altitude, the wings’ angle of attack, the attitude of the aircraft (banking L or R or not), climbing, descending and at what rate, or level flight, time and distance to the next waypoint &#39;on the ground&#39;, etc, etc.</i></b><br /><br /><i>Today, the visual interpretation of the instrumentation is presented to the pilot in a variety of formats on electronic computer-generated screens, on which a ground map, an aerial waypoint map, and the aircraft itself can be displayed, along with critical information about speed and direction, etc.  The flight management system can call out altitude milestones if desired, and in case of an emergency, some pre-programmed maneuvers may be selected at the push of a button.  The nature of the emergency, an emergency checklist, the closest usable alternative in case a landing is needed immediately, etc are presented much more quickly than used to be able to be accomplished by manual control inputs and responses to memorized items.</i><br /><br /><i>Pilots in training who have a history of playing computer war games tend to have a headstart on those who have not, today. Not all aircraft are so automated, but most are becoming increasingly so.  Don flies with a GPS and an iPad in addition to the &#39;steam gauges&#39; that use physical inputs to display &#39;what’s going on&#39;.  Such tools are already designed in to more complex, expensive aircraft, although many airlines issue an &#39;electronic flight bag&#39; in the form of an electronic pad or tablet device to their pilots, with which they can obtain weather briefing along their route of flight, in addition to the real weather displays that might be updating on their cockpit displays throughout the flight as it progresses.  It’s a different world, and it continues being improved and refined as more and more is learned about the machine-human interface.&quot;</i><br /><br /><a href=\"https://teamprincipia.wordpress.com/2007/07/12/synthetic-vision-systems/\" class=\"ot-anchor\">https://teamprincipia.wordpress.com/2007/07/12/synthetic-vision-systems/</a><br /><br />Synthetic Vision Systems<br />July 12, 2007<br />by Brian W.<br /><br /><i>&quot;On August 6, 1997, Korean Air flight 801, a Boeing 747, crashed at Nimitz Hill, Guam, with 237 passengers on board. The airplane had been cleared to land at Guam International Airport and crashed into high terrain about 3 miles southwest of the airport. 228 people were killed, and the airplane was destroyed by impact forces. Post-crash analysis revealed no mechanical defects with the aircraft (NTSB, 1997).</i><br /><br /><b><i>The National Transportation Safety Board calls this type of accident Controlled Flight Into Terrain (CFIT), in which a functioning airplane is essentially flown into the ground due simply to the pilots’ lack of a clear picture of where they are (Arthur, 2003). According to a study from the Flight Safety Foundation, nearly 80 percent of all fatal airline accidents can be attributed to CFIT or approach-and-landing accidents (North, 1999).&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"Controversy-First Science Education","summary":"Science Instruction Should be Designed to Suit Need / What the Public Needs is a Better Ability to Judge Scientific Controversies, so that they can Provide Meaningful Input on Public Policy Decisions / It's an Imperfect Solution, but the Alternative is to Place Faith in Complex Scientific Claims, which is Itself Unscientific / If Such a Course or Curriculum was Perfected, its Graduates Could Bring a Much-Needed Perspective to Policy Debates by Speaking with Authority on both Past and Ongoing Scientific Controversies","image":"https://lh3.googleusercontent.com/-ZIQdvcfeGw0/VxKmPOQ4JaI/AAAAAAAAHrg/fDEbucU_t9M0cOC6TrFNNMc5nkqPKXM-w/w3600-h5040/controversy-first-instruction-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/F5KdQtMSeR4","publishDate":"2016-04-16T21:11:04.272Z","updateDate":"2017-01-03T02:36:10.006Z","text":"<b>Controversy-First Science Education: Science Instruction Should be Designed to Suit Need / What the Public Needs is a Better Ability to Judge Scientific Controversies, so that they can Provide Meaningful Input on Public Policy Decisions / It&#39;s an Imperfect Solution, but the Alternative is to Place Faith in Complex Scientific Claims, which is Itself Unscientific / If Such a Course or Curriculum was Perfected, its Graduates Could Bring a Much-Needed Perspective to Policy Debates by Speaking with Authority on both Past and Ongoing Scientific Controversies</b><br /><br />Select passages from <i>Teaching Controversy</i><br />By Lisa Jakubowski and Livy Visano<br /><br /><i>&quot;Teaching itself invites the process of experiencing the connections between oneself and the &#39;other.&#39;  Typically, all teaching is a challenge, but teaching controversy involves a greater degree of challenge.  It represents, first of all, a commitment to critical pedagogy.  For us, the term &#39;teaching controversy&#39; signifies a method of teaching and learning that involves </i><b><i>both content and process</i></b><i>, in which the process embodies pedagogical philosophies and methods that interact with course content. </i><b><i>It means getting students to look at what assumptions they are bringing to a discussion, and at where these seemingly fixed assumptions are coming from -- leading them to recognize their own biases and, possibly, the origins of those biases. Ideally, it means encouraging them to get out of the frames they have been using and perhaps thinking more unconventional thoughts.</i></b><br /><br /><i>Teaching controversy is also about expressions of power and cultural control.  Teachers run the risk of being set apart and relegated to the &#39;margins&#39; whenever they question specific canons of the discipline.  While certain types of differences are permitted, </i><b><i>more radical or critical pedagogies are devalued because they are not easily linked to the social organization of conformity -- control</i></b><i>. Control represents the institutional or patterned responses to the &#39;differences&#39; that are designated as threatening.</i><br /><br /><i>Why are controversial forms of teaching characterized as threatening?  Perhaps because, unlike many conventional strategies, </i><b><i>these approaches are designed to question, rather than reinforce, the moral, political, social, economic and intellectual organization (or regulation) of society</i></b><i> ...</i><br /><br /><b><i>Customarily, institutions of learning -- and in particular those in charge -- produce courses designed to protect and promote particular perspectives, and the meanings of these courses are negotiated among relatively powerful participants. Courses that refuse to grapple with controversial topics affirm a certain privilege to particular cultural interpretations by supplying experiences from which inferences are quickly drawn.</i></b><i> For example, traditional pedagogic canons reflect the primacy of a binary code.  These canons present identities, relations and activities strictly as a matter of &#39;either/or&#39; dichotomies: right or wrong, teacher or student, black or white, guilty or innocent, moral or immoral, good or bad, sinner or saint, ugly or beautiful, sane or made, sacred or secular, cerebral or visceral. </i><b><i>This bifurcation misrepresents multilayered identities and phenomena.</i></b><br /><br /><i>Throughout the process of teaching and learning, all teachers and students need to be appreciated as both acting subjects and subjected actors.  Both groups must be understood as active agents situated within wider, more complex and often overlooked social, political and cultural contexts.  Both teachers and students are cultural subjects within discourses of power, and they are engaged in micropolitical (local) struggles shaped by more macrocultural influences (global).</i><br /><br /><i>... traditional teaching consists of a series of snapshots and captured moments that reflect the preferences of the photographer and the subject under focus.  Some images will remain overexposed or underexposed; others are frequently blurred, discarded or never even developed because of the associated costs.  Traditional models of teaching and learning continue to judge differences, colonize compliance and shackle the imagination.  What is required is a more emancipatory and transformative pedagogy, grounded in struggle. </i><b><i>Through a more critical education, we will uncover our assumptions, learn more about our own learning, self-consciously challenge the dominant ethos and develop oppositional currents.&quot;</i></b> (<i>Teaching Controversy</i>, Lisa Jakubowski and Livy Visano, p11-12)<br /><br /><i>&quot;Traditionally, educational institutions have been viewed as places used to transmit knowledge -- including factual information and occupational skills, as well as cultural values and norms -- to members of a society.  People in these institutions are expected, specifically through the transmission of norms, to learn societal rules and expectations.  More candidly, people, through &#39;schooling&#39; are expected to learn how to think, what to think, and how to behave.  In this sense, &#39;schooling&#39; is intimately tied to the concept and reality of hegemony.</i><br /><br /><b><i>Hegemony is the term used by Gramsci (1971) to describe the process by which one group achieves domination over others using physical and/or ideological means.</i></b><i> Here we are most concerned with the ideological dimension of hegemony.  Conceptualized as &#39;a set of ideas and ideals that provide an explanation for a particular community&#39; (Fleras and Kunz 2001: 190), ideologies facilitate the maintenance of social order or the promotion of social change.  In more explicitly political terms, </i><b><i>ideologies can be defined as a &#39;set of ideas and ideals that justify the prevailing distribution of power, resources and privilege in society&#39;</i></b><i> (190).  In this sense, ideologies are an important persuasive tool for those in powerful positions.  According to marino (1997: 105):</i><br /><br /><b><i>&#39;Gramsci develops the notion of hegemony as the process whereby public consensus about social reality is created by the dominant class; whereby those in power persuade the majority to consent to decisions that are disempowering or not benefiting the majority ... In simple terms, hegemony can be defined as persuasion from above (by the dominant class) and consent from below (by the subordinate class)</i></b><i> ... Raymond Williams [1977] further extends the implications of this concept in cultural terms: &#39;A lived hegemony is always a process.  It is not, except analytically, a system or a structure ... It does not just passively exist as a form of dominance.  It has continually to be renewed, recreated, defended and modified.  It is also continually resisted, limited, altered, challenged by pressures not at all its own.&#39;&#39;</i><br /><br /><i>[...]</i><br /><br /><i>Freire (1970: 58) describes the learning process in traditional settings as analogous to &#39;banking&#39;: &#39;an act of depositing in which students are the depositories and the teacher is the depositor. </i><b><i>Instead of communicating, the teacher issues communiques and makes deposits which the students receive and memorize.&#39; Like a form of indoctrination (Reimer 1972), traditional schooling is a sophisticated way through which the dominant value systems are transmitted intergenerationally (Giroux 1981: 1). Among other things, individuals learn about the value of competition, the &#39;value of being taught, rather than learning for oneself -- what is good and what is true&#39; and the need to conform &#39;in order to get along in the system&#39; (Reimer 1972: 21-23). Schooling teaches individuals what behaviours are &#39;right&#39; and &#39;good.&#39;  Most importantly, children learn the importance of respecting and obeying those in authority (Henslin, Henslin and Keiser 1976: 308-309).</i></b><br /><br /><i>[...]</i><br /><br /><b><i>&#39;If we see hegemony as a system that organizes consent, we have the choice of reorganizing or disorganizing that consent.&#39; For instance, if hegemony involves &#39;framing&#39; -- &#39;the process of imposing a preferred meaning (&#39;framework&#39;) on an event&#39; (Fleras and Kunz 2001: 190) -- we can challenge that framework and work to &#39;re:frame&#39; our existing vision.</i></b><i> A central theoretical concept in marino&#39;s (1997: 108-109) critical pedagogy, &#39;re:framing&#39;</i><br /><br /><i>&#39;occurs when problems or experiences are represented in ways that both retain the realities of existing political relationships and transcend them by opening up new (for those involved) and real opportunities for acting on the inequities of those relationships ... [It] requires both teachers and students to co-construct new and useful frames on their experience -- ones that mobilize and empower them.&#39;&quot;</i> (<i>Teaching Controversy</i>, Lisa Jakubowski and Livy Visano, p30-33)<br /><br /><i>&quot;Brookfield (1987: 7-9) suggests that critical thinking involves: 1) </i><b><i>identifying and challenging assumptions</i></b><i>; 2) </i><b><i>recognizing the importance of context</i></b><i>; 3) </i><b><i>trying to imagine and explore alternatives</i></b><i>; and 4) </i><b><i>imagining and exploring alternatives in a way that promotes &#39;reflective skepticism.&#39;</i></b><i> He reminds us that critical thinking is not confined to the classroom (4).  As it is now, from a practical standpoint </i><b><i>the type of &#39;critical thinking skills&#39; generally taught in educational settings are not particularly useful in life beyond school.</i></b><i> Sternberg (1985: 194) highlights the inconsistency between the requirements for critical thinking in adulthood and what is taught in school programs designed to develop critical thinking.  As he puts it, </i><b><i>&#39;We are preparing students to deal with problems that are in many respects unlike those that they will face as adults.&#39;</i></b><br /><br /><i>[...]</i><br /><br /><i>While Brookfield in no way dismisses the importance of learning to think critically in institutes of higher learning, he does argue that this is only one setting in which critical thinking occurs.  It can also occur, he says, in the everyday interactions of adult lives -- in relationships, in the workplace, in political involvements and in interpreting mass media (12).  In this sense critical thinking is &#39;a lived activity, not an academic pastime&#39; (14) -- and it is an activity ... that is </i><b><i>emancipatory</i></b><i>, </i><b><i>dialectical</i></b><i>, and </i><b><i>reflective</i></b><i> (Jakubowski 2001: 64).</i><br /><br /><b><i>It is emancipatory in that participants become conscious of forces that have had an impact on their current realities and accordingly can take action to change some aspect of the situation. It is dialectical in the sense that it focuses on how participants come to be situated in a world characterized by contradictory relationships and interactions, a world in which life circumstances continue to change and evolve through the struggles of real people (Brookfiled 1987: 12-13). Critical thinking is reflective because it is &#39;the process of internally examining and exploring an issue of concern, triggered by an experience, which creates and clarifies meaning in terms of self, and which results in a changed conceptual perspective&#39;</i></b><i> (Boyd and Fales in Brookfield 1987: 14).  Hutchings and Wutzdorff (1988: 15) define reflection as &#39;the ability to step back and ponder one&#39;s own experience, to abstract from it some meaning or knowledge relevant to some other experiences.&#39;  As they put it, </i><b><i>&#39;The capacity for reflection is what transforms experience into learning.&#39; This form of learning has the potential for generating a new understanding about the surrounding social world</i></b><i>, an understanding that often requires &#39;a corresponding change in one&#39;s behaviour and relationships&#39; (Schlossberg in Brookfield 1987: 14).</i><br /><br /><i>[...]</i><br /><br /><b><i>&#39;Teachers help students to realize that dominant values, &#39;commonsense,&#39; wisdom, generally accepted standards and prevailing social and political arrangements are cultural constructs.</i></b><i> Because [these values] are cultural creations, teachers point out that they can be dismantled and reframed by human agency ... The responsive component [represents] the willingness of teachers to adapt their methods, content and approaches, to the context in which they are working and to the ways in which students are experiencing learning.&#39;&quot;</i> (<i>Teaching Controversy</i>, Lisa Jakubowski and Livy Visano, p34-35)<br /><br /><i>&quot;An authentic education must be committed to an open inquiry that respects the diverse needs and backgrounds of students. </i><b><i>This learning requires that students and teachers recognize the existence of alternative worldviews. It also means understanding the construction of hegemony and, related to that, the silencing of certain voices.&quot;</i></b> (<i>Teaching Controversy</i>, Lisa Jakubowski and Livy Visano, p130)<br /><br /><i>&quot;In addition to calling for a fundamental change in thinking, </i><b><i>critical pedagogy invites a more generalized attitude of risk-taking.</i></b><i> Risk is generally defined as a systematic way of dealing with uncertainties, insecurities or hazards. Since all aspects of our lives involve decisions, human beings are risk calculators, and, interestingly, capitalism rewards risk-taking in business.  There are, however, greater risks in learning and teaching about controversy.</i><br /><br /><i>Controversy as a contested terrain articulates antagonistic or contradictory sets of interrogative possibilities.  Critical pedagogy incorporates the dialectic of collaboration and contestation.  Regardless of the merits of the arguments, </i><b><i>controversial challenges are collisions that often result in scrapes or bruises to personal identity, ego or deeply held perspectives.</i></b><i> In oppositional pedagogies, teachers and students are asked to account for their &#39;opinions&#39; and learn from conceptually challenging thoughts.  The term transformative intellectual, coined by Henry Giroux (1988a), simply means (for our purposes) that teachers possess the knowledge, skills, values and attitudes to question, understand, interrogate and eventually act as agents of change, working to overcome structural inequities in their places of employment (Giroux 1988b). </i><b><i>To educate rather than school is to resist the very base of school value structures</i></b><i>; and creating transformative intellectuals means being critical of all forms of schooling (Giroux 1988a).  Critical thinking thus intersects with a critical praxis [convention/habit] that is understood within the context of the economic, political and cultural (Kanpol and Brady 1998).  Shoshana Felman (1987) writes that </i><b><i>teaching is as much about resistance as it is about knowledge.</i></b><i> Moreover, she maintains that the truly revolutionary insight, which can be discovered from teaching, is that &#39;ignorance itself can teach us something&#39; (Felman 1987: 79).&quot;</i> (<i>Teaching Controversy</i>, Lisa Jakubowski and Livy Visano, p144-145)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Fact and the Ritual of Accuracy","summary":"Our Cultural Love for Fact Inspires a Fear for Error / The Ritual of Accuracy Misdirects Our Attention Away from the Value of Interpretation and Thought / And It Corrupts Judgment by Insuring that Trivialities are Set Right While the Trickier, More Serious Errors of Logic and Sense Pass by Unseen","image":"https://lh3.googleusercontent.com/-s5lV1MtfBr8/Vw8zMyPFNiI/AAAAAAAAHqg/Sf9GhIlm-9gqlbP54RZ93RG74In7mP7_Q/w1800-h2520/the-fact-and-the-ritual-of-accuracy-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/cxugHJrr7E7","publishDate":"2016-04-14T06:07:16.915Z","updateDate":"2017-01-03T02:36:38.047Z","text":"<b>The Fact and the Ritual of Accuracy: Our Cultural Love for Fact Inspires a Fear for Error / The Ritual of Accuracy Misdirects Our Attention Away from the Value of Interpretation and Thought / And It Corrupts Judgment by Insuring that Trivialities are Set Right While the Trickier, More Serious Errors of Logic and Sense Pass by Unseen</b><br /><br /><i>Science: The Glorious Entertainment</i> (1967)<br />Jacques Barzun<br />pages 135-137<br /><br /><i>&quot;Also born of research is the love of excess in supplying &#39;background&#39; -- the history and antecedents of every project, idea, or situation.  This excess, if challenged, would be justified by an appeal to the all-importance of fact, which slips easily into the importance of all facts. </i><b><i>In journalism, government, business, popular literature, conversation, and high art, the use of fact to rout [defeat] indifference or disbelief is preferred to the use of ideas.</i></b><i> With the abundant sources at hand it is easier to write a factual than a philosophical book.  The popular work of travel or biography delivers a barrage of small and justly neglected facts -- about times, places, distances -- like the so-called profiles of living men, composed not of significant lines but of factual dots.  The principal form of fiction, the novel, is of course factual to the limit where the glut of fact destroys coherence, as in the fad Dashiell Hammett started of inventorying the clothes, food, drinks, and ablutions of his characters. </i><b><i>And only in a factual society would it be necessary for writers of palpably made-up narratives to state that the persons and places written about are imaginary.</i></b><br /><br /><i>The paradoxical failure of many fact-finding reports to help the public understand its own difficulties is almost always the result of Factuality.  The committee appointed to &#39;study the facts&#39; is itself enlightened by them, </i><b><i>but in publishing too much of what it finds and not enough of what it thinks, it leaves the interested citizen discouraged and the large tomes untouchable.  Abundance here also brings no pleasure. Specialism is not to blame except as it feeds the lust for fact, which in turn is a cause of boredom.</i></b><i> The connection has been noticed by outside observers as a peculiarly American evil.  It is now growing wherever techne, democracy, and research extend.&quot;</i><br /><br />By <b>techne</b>, he means <i>&quot;the application of knowledge to needs and the foreshadowing of theory through mechanical ingenuity.&quot;</i>  He intends the term to <i>&quot;embrace the mixture of machine and soul at the base of our scientific culture and which will at the same time suggest our ambivalence toward it.&quot;</i>  (Honestly, I will admit it&#39;s an odd, even somewhat ambiguous construct.)<br /><br /><i>&quot;Under this regime, every group, however temporary, keeps minutes, issues reports, and publishes a newsletter; it is ready, if necessary, to stop the activity for which it came into being so that the record may be &#39;kept straight&#39; and &#39;made available.&#39;  Only the criminal classes hold to the ancient reticence and wish the facts to stay unknown.  The citizen-researcher would be shocked if he were told that he may be as vulgarly greedy in amassing facts as the tycoon in amassing dollars. </i><b><i>Yet as John Stuart Mill -- no enemy to science -- foretold, the effect of factuality is a paralyzing clutter, a fatal decline of intellect: &#39;What will soon be the worth of a man, for any human purpose, except for his own infinitesimal fraction of human wants and requirements? His state will be worse than that of simple ignorance.&#39; In other words, the appeal to fact, from which so much good was expected and did result during the hopeful time of enlightenment, is now turning into a blanketing darkness.</i></b><br /><br /><i>The presumption of a technical civilization that all good things can be provided meets in this case an instructive denial.  When he was self-appointed, unstimulated, perhaps unrewarded, the searcher after truth possessed an impetus to do the work, which might imply talent and perseverance. </i><b><i>When, on the contrary, search turns into the subsidized routine of research, all that is looked for and most of what is supplied is a steady income of facts. The ability and the will to interpret diminish.</i></b><br /><br /><i>A scientist attached to the National Science Foundation attributes this enfeeblement to overspecializing.  He calls it &#39;the precipitous hazard of undergeneralizing.&#39; </i><b><i>But since intelligence is as frequent in the population as before, what keeps the trained minds from the desired generalizing? The answer, I think, lies in the repression which the love of fact develops, the fear of Error as absolute evil. That many errors are negligible and should not occupy the mind is forgotten in the ritual of accuracy.</i></b><br /><br /><i>In this ritual fear is proportional to visibility: a date, a middle initial are incontrovertibly right or wrong, and the proof of error is painful. </i><b><i>But the effort to be safe first misdirects attention, then corrupts judgment. This is illustrated by the verifying that goes on in editorial and publishing offices. Trivialities are set right and large errors of logic and sense pass by unseen. This is deplorable even in journalism; at every higher stage of intellect the bias spoils more and more important elements. As Judge Cardozo observed many years ago, &#39;There is an accuracy that defeats itself by the overemphasis of details. I often say that one must permit oneself, and that quite advisedly and deliberately, a certain margin of misstatement.</i></b><br /><br /><i>In truth, the commandments derived from science and techne and clustering in the institution of research boil down to a diffuse, pervasive fear -- fear of fellow professionals, fear of one&#39;s own mind and ego as &#39;subjective,&#39; fear of errors creeping into a text while one&#39;s back is turned. </i><b><i>The resulting temper is not, on the face of it, favorable to good work. Indeed, what fear inspires is work other than that which might spontaneously attract the mind; the job must fit the means and curiosity be redirected to questions (&#39;problems&#39;) that can be managed without running the prohibited risks. The mind, in short, is subdued by the mechanical.&quot;</i></b><br /><br />(That&#39;s a tricky paragraph, but worth re-reading if you didn&#39;t quite get it)<br /><br /><i>&quot;Mechanics may mean lists and facts and &#39;theme analysis&#39; done by hand count; or it may mean reliance on the innumerable little studies produced under the prevailing scheme.  Thus a leading Shakespeare scholar in this country states that he has time and strength to be &#39;only a bibliographer&#39; -- the substance of scholarship is held in escrow for a later people.  Or again, mechanics in research may be the preference for visible facts, usually small, on which large inferences can supposedly rest.  To stay with Shakespeare another moment, some scholars believe the study of his works has been revolutionized by the examination of the type and the way in which his plays were set up in early editions.  From the type the typesetter is inferred, from him his characteristic mistakes, and from those the true text.  Compositors A and B are now figures as familiar as the five majuscules who put together Genesis, and a man&#39;s reputation in research was made when he &#39;brilliantly identified apprentice compositor E.&#39;&quot;</i><br /><br />If somebody has a better critique of the cultish nature of facts, please share it.  But, I am not holding my breath.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"Science - It's Not Just a Process of Elimination","summary":"Sherlock Holmes Famously Said \"Once You Eliminate the Impossible, Whatever Remains, No Matter How Improbable, Must be the Truth\" / This Only Works Within a Specific Framework / In the Real World, Our Knowledge is Incomplete / If You Want to be More Like Sherlock Holmes, then Dedicate Some of Your Time to Learning Alternative Hypotheses","image":"https://lh3.googleusercontent.com/-xbOqpbCfrm0/Vw3JF7ar_uI/AAAAAAAAHpo/mvbrtmhenhwsaS-ovUtpZ0eVzHX0U3RHw/w1800-h2520/not-just-a-process-of-elimination-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/MEBFnq984RD","publishDate":"2016-04-13T04:21:47.011Z","updateDate":"2017-01-03T02:37:11.874Z","text":"<b>Science - It&#39;s Not Just a Process of Elimination: Sherlock Holmes Famously Said &quot;Once You Eliminate the Impossible, Whatever Remains, No Matter How Improbable, Must be the Truth&quot; / This Only Works Within a Specific Framework / In the Real World, Our Knowledge is Incomplete / If You Want to be More Like Sherlock Holmes, then Dedicate Some of Your Time to Learning Alternative Hypotheses</b><br /><br />From <a href=\"http://theness.com/roguesgallery/index.php/logicphilosophy/a-sherlock-holmes-logical-fallacy/\" class=\"ot-anchor\">http://theness.com/roguesgallery/index.php/logicphilosophy/a-sherlock-holmes-logical-fallacy/</a><br /><br /><b><i>&quot;A Sherlock Holmes Logical Fallacy</i></b><br /><i>By Steven Novella, on September 7th, 2008</i><br /><i>We recently received the following question:</i><br /><br /><i>&#39;&#39;If you’ve eliminated all other possibilities whatever remains must be the truth,&#39; is the famous a quote from the brilliant, but fictional, detective Sherlock Holmes. It seems to be an inescapable statement of cold, hard logic but it is often used in movies or on TV as a fig leaf to cover a huge leap of logic on the part of the hero or as justification for why the answer to the mystery at hand must be ghosts, aliens or some other supernatural phenomena. Is the statement a logic fallacy, a practical impossibility or a viable method of investigation that is being misused?&#39;</i><br /><br /><i>Thanks for a consistently great show.</i><br /><br /><i>Stuart</i><br /><br /><i>Gothenburg, Sweden</i><br /><br /><i>Thanks for the great question, Stuart. Like many points of logic there are many subtleties that need to be understood before they can be applied properly.</i><br /><br /><i>The actual quote is this:</i><br /><br /><b><i>&#39;Once you eliminate the impossible, whatever remains, no matter how improbable, must be the truth.&#39;</i></b><br /><br /><i>As stated this is an obvious point of logic – the truth must lie within the set of the possible, which is defined as everything that is not impossible. </i><b><i>But Holmes (by which I me Sir Arthur Conan Doyle, the author of Sherlock Holmes) meant the statement as a practical rule of thumb.</i></b><i> As an investigator you create a mental list of all potential explanations for a situation. You then systematically eliminate those explanations that you can demonstrate are impossible, either through logic or empirical evidence. Whatever you are left with is the solution – even if it may seem extremely improbable.</i><br /><br /><i>Holmes, the hyper-rationalist, was a genius at just this type of endeavor. He could make the connections necessary to eliminate possibilities. </i><b><i>He also had the imagination to consider possible solutions that may at first seem entirely improbable</i></b><i> – but if that is what you are left with then it must be true.</i><br /><br /><b><i>But Sherlock Holmes was working within a specific framework – a materialist, rational, scientific view of the world. Within that framework this process of elimination works well.</i></b><i> In fact, I took a course on Sherlock Holmes in medical school, applying his investigative principles to medical diagnosis, which is a type of investigation.</i><br /><br /><i>For example, physicians will make a list of possible diagnoses – called a differential diagnosis. We then systematically eliminate possibilities until we are left with one diangosis that fits all the signs, symptoms, and laboratory results. Whatever remains must be the proper diagnosis, even if it is a rare disease or a very rare manifestation of a more common disease.</i><br /><br /><b><i>In practice this process does not always work because our knowledge is incomplete.</i></b><i> Also, this process is often trumped by a more important clinical principle – risk vs benefit. Often the risk of doing diagnostic procedures to establish a specific diagnosis is not justified by a corresponding benefit (it won’t lead to a treatment), or sometime the pathway of maximal benefit vs risk involves giving a low-risk treatment to see if it works.</i><br /><br /><i>As an aside, the TV character House is a very Sherlock Holmes type character applied to medical diagnosis. The character is portrayed as a complete egotistical ass, partly because he pursues diagnosis even at the expense of other clinical and ethical principles, such as risk vs benefit or informed consent.</i><br /><br /><i>Getting back to Stuart’s question – </i><b><i>problems arise when this very logical principle of investigation are applied without constraints.</i></b><i> The logic breaks down in a world where one allows for the existence of magic. How, then, does one define possible vs impossible? Holmes clearly assumes magic does not exist, and Doyle places him in a world (the real world) where magic in fact does not exist. Therefore Holmes (very much unlike Scully from The X-files, who lives in a paranormal world) is never &#39;baffled&#39; when his rational explanations do not fit an irrational world he refuses to accept.</i><br /><br /><b><i>There is also a very practical consideration in applying this principle – how complete is your set of alternate explanations?</i></b><i> As I tell my students, if you fail to consider the proper diagnosis you will never make it (this is actualy only true for some diagnoses, for others may be made by doing screening tests, even if you are not looking specifically for the diagnosis you find), </i><b><i>or if you prematurely limit the range of possible diagnoses you have no chance of finding the right diagnosis.</i></b><br /><br /><i>[...]</i><br /><br /><i>So Sherlock Holmes’ principle needs to be clarified, in a way that was simply assumed by Holmes:</i><br /><br /><i>Within the set of known phenomena, once you have eliminated the impossible, whatever remains, no matter how improbable, must be true. </i><b><i>If the entire set of known phenomena are eliminated as impossible, then the solution is simply unknown until a new phenomena that can serve as a solution is positively established ...&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"Over-Specialization","summary":"Even by 1914, It Was Remarked that Scientists had Become so Specialized that Outside of their Narrow Subspecialties they Functioned as Laypersons / The Problem Has Dramatically Worsened Since / Over-Specialization Can Undercut Science's Highly-Regarded Capacity for Self-Correction by Making the Errors Invisible to Both Insiders and Outsiders / Micro-Specialists Oftentimes Cannot Understand Fatal Objections and Comprehensive Refutations of their Standard Paradigms when these Originate from Adjacent Areas of Science / It Allows a Situation to Develop where the Whole of a Vast Area of Science Can Become Bogus Knowledge","image":"https://lh6.googleusercontent.com/-Ns3K6piWR5c/VwqZlutlxXI/AAAAAAAAHnw/6bKuB8kzM94R-cn0ar_BddCZ2FAB-ebzA/over-specialization-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/QtKVXCKte2C","publishDate":"2016-04-10T18:24:02.506Z","updateDate":"2017-01-03T02:37:34.693Z","text":"<b>Over-Specialization: Even by 1914, It Was Remarked that Scientists had Become so Specialized that Outside of their Narrow Subspecialties they Functioned as Laypersons / The Problem Has Dramatically Worsened Since / Over-Specialization Can Undercut Science&#39;s Highly-Regarded Capacity for Self-Correction by Making the Errors Invisible to Both Insiders and Outsiders / Micro-Specialists Oftentimes Cannot Understand Fatal Objections and Comprehensive Refutations of their Standard Paradigms when these Originate from Adjacent Areas of Science / It Allows a Situation to Develop where the Whole of a Vast Area of Science Can Become Bogus Knowledge</b><br /><br />8 Critiques of Over-Specialization, Some Broad and Others Extremely Specific ...<br /><br /><b>Jacques Barzun on Specialization</b><br /><br />From <i>Science: The Glorius Entertainment</i> (1967)<br /><br /><i>&quot;What is new among the conditions of knowledge is the result of what is usually called specialization, but should rather be called </i><b><i>specialism</i></b><i>.  Specialization -- attending to one thing at a time, and for as long as will insure thoroughness -- is obviously desirable and it is not a modern invention. </i><b><i>Specialism is something else: it is a piece of etiquette which decrees that no specialist shall bother with the concerns of another, les he be thought intruding and be shown up as ignorant. Specialism is born of what the philosopher Arthur Balfour called &#39;the pernicious doctrine that superficial knowledge is worse than no knowledge at all.&#39; A little learning is dangerous in one who tries to teach or use that little in professional work; it is not a danger but a source of pleasure to the observer of life as a whole.  Thus does a map, yielding a superficial knowledge of geography, add to the traveler&#39;s enjoyment even though he himself could not survey the ground and draw the map.</i></b><br /><br /><b><i>This failure to distinguish the concern of the maker from that of the observer is a characteristic of the scientific culture</i></b><i> and another tribute to the force of science within it.  Because division of labor in scientific work proved useful, recognizing proprietary rights in specialties became a point of honor everywhere, even where it served no purpose but to curtail pleasure.  Every subject today confronts the tyranny of the professional, with the result to be expected: the reduction of every art and mode of thought to a preoccupation with the details of its making.  In parallel with science, these formerly public goods are being withdrawn one by one from among the objects of contemplation; that is, of direct, unscholarly, unpedantic enjoyment, discussion, and criticism.  True, the so-called specialization is frequently decried, but in the tone of hushed hopelessness, as an enslaved people might speak of the age of heroes.  The rampant specialism, an arbitrary and purely social evil, is not recognized for the crabbed guild spirit that it is, and </i><b><i>few are bold enough to say that carving out a small domain and exhausting its soil affords as much chance for protected irresponsibility as for scientific thoroughness.</i></b><br /><br /><i>[...]</i><br /><br /><i>I have heard more than one young scientist confess a desire to write about his work.  Each was held back by fear of the profession.  The man-eating cliches are ready to pounce: </i><b><i>only the performer knows what he is talking about, because he is &#39;inside&#39; -- which insures that he has never stood off and compared his work with other kinds.</i></b><br /><br /><i>It is true that no outsider, not even the closest student of a science or art, will have the same familiarity with its difficulties as the practitioner. </i><b><i>The outsider is often wrong and sometimes unjust; for lack of perfect fluency in the speech of the guild, he can sound ridiculous even when he is right. But none of this makes him any the less necessary. Science itself would never have made headway against the theological monopoly if the advocates of science had not gone &#39;out of their field&#39; to criticize philosophy and religion as amateurs. It is the very familiarity with his own shop that prevents the professional from being critical of it or contemplative about it</i></b><i> -- let alone the fact that he may be an excellent practitioner without possessing the art and logic of public exposition.  And this alone would make it an error to equate a possible cultivation of science with the daily work of the scientific specialist.&quot;</i><br /><br /><b>Hannes Alfven on Specialization</b><br /><br />From &quot;Double Layers and Circuits in Astrophysics&quot; (1986)<br /><br /><i>&quot;The Roederer Syndrome</i><br /><br /><i>In his article &#39;Tearing Down Disciplinary Barriers&#39; [58], Roederer points out the conflict between the demand for &#39;increased specialization on one hand and the pursuit of an increasingly interdisciplinary approach on the other.&#39;</i><br /><br /><i>This is important. Indeed, in the present state of science, specialization is favored to such an extent that science is split up into a number of increasingly small specialties. </i><b><i>We lack the global view</i></b><i> ...</i><br /><br /><i>We should remember that there once was a discipline called &#39;Natural Philosophy&#39; (&#39;reine Naturwissenschaft&#39;). Unfortunately this discipline seems not to exist today. It has been renamed &#39;science&#39; but science of today is in danger of loosing much of the Natural Philosophy aspect.</i><br /><br /><b><i>Roederer further discusses the psychological and structural causes for the loss of the global view, and points out that one syndrome of causes is the &#39;territorial dominance, greed, and fear of the unknown.&#39; Scientists tend to &#39;resist interdisciplinary inquiries into their own territory ... In many instances, such parochialism is founded on the fear that intrusion from other disciplines would compete unfairly for limited financial resources and thus diminish their own opportunities for research.&#39;</i></b><br /><br /><i>B. Microscale Example</i><br /><br /><i>All this agrees with my own experience. </i><b><i>When running a lab, I found that one of my most important activities was to go from room to room and discuss in depth the problems that a certain scientist or a group of scientists was trying to understand. It often happened that one group reported that in their field they had a special problem which they could not possibly understand. I told them that if they cared to open the door to the next room -- it was not locked! -- just this special problem had been solved half a year ago, and if they injected the solution into their own field, it would take a great leap forward. Often they were not at all happy for this suggestion</i></b><i>, probably because of the syndrome that Roederer has discussed, but when faced with &#39;tearing down the disciplinary barriers&#39; within the laboratory they realized how important such action is for progress (cf. Section II-D). This may be considered a mild case of the Roederer syndrome.</i><br /><br /><i>Such an example from the microscale structure of science supports Roederer&#39;s general views, but examples from the macroscale structure are much more important. A large part of this paper has been a series of examples of the malady that Roederer describes.</i><br /><br /><i>The lack of contact between Birkeland&#39;s and Langmuir&#39;s experimental/theoretical approach on the one hand and the Chapman-Cowling mathematical/theoretical approach on the other has delayed progress in cosmic plasma physics by perhaps half a century. The many new concepts that came with the space age begin to be understood by magnetospheric physicists but have not yet reached the textbooks in astrophysics, a delay of one or two decades, often more, as seen in the preceding section. Very few if any deny that (at least by volume) more than 99 percent of the universe consists of plasma but students in astrophysics are kept ignorant even of the existence of important plasma phenomena like those listed in Table I.</i><br /><br /><i>Dr. Roederer&#39;s prescription for curing this serious disease is &#39;tearing down disciplinary barriers,&#39; indeed &#39;interdisciplinarification&#39; of science. This seems to be wise. However, we must suspect that to many astrophysicists this is bitter medicine. </i><b><i>Can we find ways to sweeten it?&quot;</i></b><br /><br /><b>John Burnham on Specialization</b><br /><br />From <i>How Superstition Won and Science Lost: Popularizing Science and Health in the United States</i> (1987)<br /><br /><b>A Modern Scientist Out of His Specialty is No Different than a Layperson</b><br /><br /><b><i>&quot;Just as the editors of the American Naturalist had perceived, what had happened was that scientists had become so specialized that outside of their narrow subspecialties they functioned as lay persons. &#39;Almost every day,&#39; wrote the editors of Science Conspectus in 1914, &#39;there are new developments in special lines of research, any one of which may lead to fundamental discoveries, but, although these matters would be of general interest if they could be understood, their significance is often obscure, even to scientific workers in not dissimilar lines.&#39;&quot;</i></b> (p205, <i>How Superstition Won and Science Lost: Popularizing Science and Health in the United States</i>, John C Burnham)<br /><br /><b>Science as the Tower of Babel</b><br /><br /><i>&quot;Especially after World War II, then, in science, health, and, slightly later, psychology, as nonscientists preempted public forums, few practitioners stood up to speak for science or the religion of science.  All of this occurred in the face of a very great increase in the number of people in clearly scientific occupations.  As all observers noted at the time, Sumner and Angell were correct: </i><b><i>no one could speak up, because scientists had become so specialized that each was limited to his or her own very narrow area of expertise. As early as 1922 William Bateson described his fellow geneticists in the United States as &#39;pathetic in their simplicity, knowing nothing whatever outside Genetics&#39; and as nothing more than &#39;machines for grinding out genetics.&#39;</i></b><br /><br /><i>Mere specialization and narrowness were not themselves impenetrable barriers to scientists&#39; popularizing.  In addition, the specialists suffered a failure of nerve.  The combination of specialization with the lack of zeal, however, was conclusive.  Already in 1937, R.S. Mulliken of the University of Chicago was complaining, </i><b><i>&#39;Even among scientists themselves, outside their own specialties, the scientific attitude is far too rare and is never fully developed.&#39; A quarter of a century later, philosopher of science Stephen Toulmin paralleled science to Babel: &#39;an assemblage of skills -- practical, technological skills, and theoretical, mathematical skills -- rather than ... &#39;natural philosophy.&#39;&quot;</i></b> (p250-251, <i>How Superstition Won and Science Lost: Popularizing Science and Health in the United States</i>, John C Burnham)<br /><br /><b>Scientists as Technicians</b><br /><br /><b><i>&quot;All of the signs came to point in the same direction: American scientists, as a whole, were behaving like mere technicians -- competent in a narrow, technical field but without the vision or identity with a larger calling in society</i></b><i> such as would move them to make a public profession of naturalism and skepticism.  One of the older school in 1945 had suggested that scientists as scientists ought to take an oath: &#39;I pledge that I will use my knowledge for the good of humanity and against the destructive forces of the world and the ruthless intent of men; and that I will work together with my fellow scientists of whatever nation, creed, or color, for these, our common ends.&#39;  Despite the timely liberal preoccupations of the author, the oath suggests both mission and identity and contrasts with the new style of diploma that Conway Zirkle (a man of science) sarcastically suggested for the young scientist of 1955:</i><br /><br /><i>&#39;The Johns Hopkins University</i><br /><i>certifies that</i><br /><i>John Wentworth Doe</i><br /><i>does NOT know anything but</i><br /><i>Biochemistry.</i><br /><br /><b><i>Please pay no attention to any pronouncement he may make on any other subject, particularly when he joins with others of his kind to save the world from something or other.</i></b><br /><br /><i>However, he worked hard for this degree and is potentially a most valuable citizen.  Please treat him kindly.&#39;</i><br /><br /><b><i>One of the marks of the narrow technician was his or her unwillingness to go beyond facts</i></b><i> -- again reflective of the cultural regard for &#39;information&#39; as well as specialization. </i><b><i>Because of specialization, complained a biologist in 1930, too many science courses perpetuated the worst of old-fashioned teaching and were still taught at the most primitive levels, that is, emphasizing the factual and authoritarian, without the enlightenment of what investigation meant. In succeeding decades, other commentators watched the population of scientists increase and their specialization intensify and remarked that the result was not only narrowness but mediocrity. In a highly fragmented, technical system, people flourished professionally who in another day would have been handicapped by insufficiency of breadth, to say nothing of their lacking the culture and calling of Victorian scientists who argued for science because it was culture.</i></b><i> Some contemporary critics extended the critique to note the scholastic qualities of teachers and students alike in the era of narrow specialization.  When institutionalized science, embodying the technical-technician element, came under attack for contributing to environmental irresponsibility, it was entirely appropriate to lament, as did one observer of his times, &#39;Einstein is dead,&#39; meaning broad, culturally responsible representatives of science.&quot;</i> (p250-252, <i>How Superstition Won and Science Lost: Popularizing Science and Health in the United States</i>, John C Burnham)<br /><br /><b>John Ziman on Specialization</b><br /><br />From <i>Prometheus Bound: Science in a Dynamic Steady State</i> (1994)<br /><br /><i>&quot;The norm of originality is extraordinarily difficult to satisfy.  Criticism is inexorable.  Academic researchers have to strive for years to receive communal recognition for their contributions. In practice, the only obvious way to become internationally competitive is to sepcialize intensely in the field of one&#39;s research.  Extremely narrow subject specialization, often for a whole career, is not just a reprehensible academic tradition: it is the rational response of the individual researcher to the demands of the research claims marketplace.  Only by getting to know a specific subject very thoroughly can a scientist be sure of choosing research problems that other scientists have not yet thought to attack, or have not aquired the skill to solve.</i><br /><br /><i>This has two contradictory implications for scientific careers.  On the one hand, academic researchers actualize themselves through the acknowledged mastery of their respective research specialties.  It means a great deal psychologically to be the world authority on some particular scholarly matter, even if this is very pedantic and insignificant in relation to all the rest of knowledge.  But such a status does not last unless it is supported by the further contributions required to keep up with the moving frontier.  It seems imperative to be free to go on working in &#39;one&#39;s own field&#39; -- with all that such freedom requires by way of research equipment, assistants, dispensation from other duties, etc.</i><br /><br /><i>On the other hand, academic specialization can lead to the pathological condition of &#39;undue persistence&#39;.  Individual researchers often become unproductive and stale in mid-career, through trying to stay too long in the same narrow field.  This condition is not unknown in non-academic R&amp;D organizations, but it is widespread in academic research, and probably wastes a great deal of research talent and experience.&quot;</i> (p190-191, <i>Prometheus Bound: Science in a Dynamic Steady State</i>, John Ziman)<br /><br /><b>Jeff Schmidt on Specialization</b><br /><br />From <i>Disciplined Minds: A Critical Look at Salaried Professionals and the Soul-battering System That Shapes Their Lives</i> (2000)<br /><br /><b>Division of Labor as a Means of Control</b><br /><br /><i>&quot;No matter what the product is, employers divide the work into many parts and assign each employee to one type of activity. Narrowly focused individuals can work in a more machine-like way and get more work done per hour. Moreover, people who exercise fewer skills or simpler skills can be paid less. Hence, employers label the division of labor &#39;efficient.&#39; But it is efficient only if one ignores the social cost of organizing production in a way in which jobs tend to be monotonous and unsatisfying. </i><b><i>Such jobs, instead of allowing individuals to develop their mental and physical faculties by exercising them freely and fully (that is, instead of being fun), numb the mind and the body and retard the personal development of those employed to do them. A system of production that works efficiently toward the goals of employers does not necessarily work efficiently toward the goals of employees or toward the goals of society in general.</i></b><br /><br /><i>More important to employers than the economic benefits, however, are the political benefits of the division of labor -- benefits that help management maintain its authority in the workplace. </i><b><i>Confined to a range of activity that is limited both horizontally and vertically, employees do not gain firsthand knowledge of the overall organization, strategy or goals of the institution that employs them. Those who work within this division of labor see the consequent ignorance in themselves and in their coworkers and feel a need to be directed by people who comprehend the whole operation. Management has the broadest view of what is going on, and this helps make its supreme authority in the workplace seem natural and justified.</i></b><br /><br /><b><i>By making employees easier to replace and by deflating their feeling of accomplishment in their work, the division of labor strips workers of their sense of power in the workplace, discouraging them from challenging management on the way the work is organized. And the division of work into narrow tasks (most of which are the same even when the product is different) denies workers a feeling for what they are producing, thereby discouraging them from challenging management on the nature or design of the product or service. Hence the division of labor, by making self-management seem impossible and by strengthening management&#39;s control over the workforce and over the content of the work, helps make the hierarchical system of production more secure.&quot;</i></b> (p91-92, <i>Disciplined Minds: A Critical Look at Salaried Professionals and the Soul-battering System That Shapes Their Lives</i>, Jeff Schmidt)<br /><br /><b>Wal Thornhill on Specialization</b><br /><br />From <i>The Electric Universe</i> (2007)<br /><br /><i>&quot;[I]n their discussion of plasma phenomena, astrophysicists continue to refer to plasma as a gas, and their descriptions of celestial events draw upon the language of wind and water, an invitation to scientific confusion: </i><b><i>plasma discharge follows different rules from those governing the behavior of either gases or liquids.</i></b><br /><br /><b><i>Astrophysicists are not trained in electrodynamics, circuit theory, or plasma discharge phenomenona. Such things would render their gravitational models obsolete and require practical experiments outside the areas of their expertise. They continue to rely on gas and magnetized-fluid physics that is mathematically well-mannered. They seem not to consider that our insulated home at the bottom of an ionosphere on a small rocky planet presents an illussion of electrical neutrality.</i></b><i> In truth, our Earth is part of a complex electric universe.</i><br /><br /><b><i>As a rule, astrophysicists will not attend conferences having anything to do with electric discharge in plasma.</i></b><i> They have little or no interest in the application of electrical phenomena to unsolved enigmas in space. Published findings, including the work of the leading authorities on plasma cosmology -- a discipline recognized by the Institute of Electrical and Electronics Engineers (IEEE) -- receive little acceptance or acknowledgement in mainstream astronomy and astrophysics journals.&quot;</i> (<i>The Electric Universe</i>, Wal Thornhill)<br /><br /><b>Hilton Ratcliffe on Specialization</b><br /><br />From <i>The Virtue of Heresy: Confessions of a Dissident Astronomer</i> (2008)<br /><br /><b>The Effect of Specialization on Modeling</b><br /><br /><i>&quot;The progress of astrophysics has been hobbled by a myopic inability to see across the fences separating various scientific disciplines.  I firmly believe that most astronomers cling to highly unlikely gas models for the Sun and other stars simply because of their ignorance of the nuances of nuclear chemistry.  The tracks of nuclides leave no doubt in the minds of those with the requisite skills to read them.  The odd chemistry credit obtained along the way to a degree in physics is simply insufficient to equip astronomers to see the detail.  This is pointedly clear also in our lack of depth in electrical field theory and experimental plasma dynamics.  A bit of training in these disciplines and we begin to see the universe in a whole new light&quot;</i> (p250, <i>The Virtue of Heresy: Confessions of a Dissident Astronomer</i>, Hilton Ratcliffe, 2nd Ed)<br /><br /><b>Bruce G Charlton on Specialization</b><br /><br />From chapter titled &quot;Micro-specialization and the infinite perpetuation of error&quot; in <i>Not Even Trying: The Corruption of Real Science</i> (2012)<br /><br /><i>&quot;Science, real science, is itself a specialization of philosophy.  After which science itself specialized -- at first into physical and natural sciences, and then into ever-finer divisions.</i><br /><br /><b><i>Scientific specialization is generally supposed to benefit the precision and validity of knowledge within specializations, but at the cost of these specializations becoming narrower, and loss of integration between specializations.</i></b><br /><br /><b><i>In other words, as specialization proceeds, people supposedly know more and more about less and less -- the benefit being presumed to be more knowledge within each domain; the cost that no single person has a general understanding</i></b><i>.</i><br /><br /><i>However, I think that there is no benefit, but instead harm, from specialization beyond a certain point -- an imprecise but long-since-passed point.</i><br /><br /><i>Nowadays, people do not really know more, even within their specialization -- </i><b><i>often they know nothing valid at all; almost everything they think they know is wrong, because [it is] undercut by fundamental errors intrinsic and yet invisible to that specialty.</i></b><br /><br /><b><i>The clear cut benefits of specialization apply only to the early stages such as the career differentiation in the early 20th century</i></b><i> -- the era when there was a threefold division of university science degrees into Physics, Chemistry and Biology.</i><br /><br /><i>It is much less obvious that real science benefited from subdivision of each of these into two or three (e.g. Physics into Theoretical and Applied, Chemistry into Organic and Inorganic; Biology into Zoology and Botany).</i><br /><br /><i>But since the 1960s, scientific specialization has now gone far, far beyond this point, and the process is now almost wholly disadvantageous.</i><br /><br /><i>We are now in an era of micro-specialization, with dozens of subdivisions within sciences.  Biology, for example, fragmented into biochemistry, molecular biology, genetics, neuroscience, anatomy, physiology, pharmacology, cell biology, marine biology, ecology, ...</i><br /><br /><i>Part of this is simply the low average and peak level of ability, motivation and honesty in most branches of modern science.  The number of scientists has increased by more than an order of magnitude -- clearly this has an effect on quality.</i><br /><br /><i>Scientific training and conditions have become prolonged and dull and collectivist -- deterring creative and self-motivated people.  And these changes have happened in an era when the smartest kids tended not to gravitate to science, as they did in the early 20th century, but instead to professions such as medicine and law, and into the financial sector.</i><br /><br /><i>In round numbers, it seems likely that more than ninety percent of modern &#39;scientists&#39; are worse than the worst scientists of 60 years ago.</i><br /><br /><i>However there is a more basic and insoluble problem about micro-specialization.  This is that </i><b><i>micro-specialization is about micro-validation -- which can neither detect nor correct gross errors in its basic suppositions.</i></b><br /><br /><i>In the world of micro-specialization that is a modern scientific career, each specialist&#39;s attention is focused on technical minutiae and the application of conventional proxy measures and operational definitions.  Most day-to-day research-related discussion (when it is not about fund-raising) is troubleshooting -- getting techniques and machines to work, managing personnel and coordinating projects ...</i><br /><br /><i>Specific micro-specialist fields are built [around] specific methodologies -- for no better ultimate reason than &#39;everybody else&#39; does the same, and (lacking any real validity to their activities) there must be some kind of arbitrary &#39;standard&#39; against which people are judged for career purposes (judging people by real scientific criteria of discovering truths is of course not done).</i><br /><br /><i>(&#39;Everybody else&#39; here means the cartel of dominant Big Science researchers who control peer review -- appointments, promotions, grants, publications etc. -- in that micro-specialty.)</i><br /><br /><b><i>Thus, micro-specialists are ultimately technicians and/or bureaucrats; thus they cannot even understand fatal objections and comprehensive refutations of their standard paradigms when these originate from adjacent areas of science.  So long as their own specific technique has been conducted according to prevailing micro-specialist professional practice, they equate the outcome with &#39;truth&#39; and assume its validity and intrinsic value.</i></b><br /><br /><b><i>In a nutshell, micro-specialization allows a situation to develop where the whole of a vast area of science is bogus knowledge; and for this reality of total bogosity to be intrinsically and permanently invisible and incomprehensible to the participants in that science.</i></b><br /><br /><b><i>If we then combine this situation with the prevalent professional reearch notion that only micro-specialists are competent to evaluate the domain of their micro-specialty -- and add [in] the continual fragmentation of research into ever-smaller micro-specialties -- then we have a recipe for permanent and intractable error.</i></b><br /><br /><i>Vast and exponentially-growing scientific enterprises have consumed vast resources without yielding any substantive progress at the level of in-your-face common sense evaluations; and the phenomenon continues for time-spans of whole generations, and there is no end in sight (short of the collapse of science-as-a-whole).</i><br /><br /><i>According to the analysis of classical science, </i><b><i>science was supposed to be uniquely self-correcting -- in practice, now, thanks in part to micro-specialization, it is not self-correcting at all -- except at the trivial and misleadingly reassuring level of micro-defined technical glitches and slip ups.</i></b><br /><br /><i>Either what we call science nowadays is not &#39;real science&#39; or else real science has mutated into something which is a mechanism for the perpetuation of error.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"Why We Critique Science","summary":"Critique of Science is Not Anti-Science / We Critique Science in Order to Have Two Thoughts Where We Formerly Just Had One / To the Reader, it Should Not Greatly Matter Whether or Not they Agree with the Conclusions / The Point of Offering them is to Spur Thought and Improve the Quality of Our Thinking and Discourse / The Public Should Seek to Become Fluent in Scientific Controversies and Critique in Order to Keep Science Functioning, Bearing Fruit and in the Public's Interest","image":"https://lh3.googleusercontent.com/-uTy_3EfhW3I/Vwm7E7OdU2I/AAAAAAAAHnU/bhlOf_SG8IEBtZvsR35CbovR1zN6eMkGg/w1800-h2520/why-we-critique-science-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/hCWxYGv6KBU","publishDate":"2016-04-10T02:32:59.612Z","updateDate":"2017-01-03T02:37:59.066Z","text":"<b>Why We Critique Science: Critique of Science is Not Anti-Science / We Critique Science in Order to Have Two Thoughts Where We Formerly Just Had One / To the Reader, it Should Not Greatly Matter Whether or Not they Agree with the Conclusions / The Point of Offering them is to Spur Thought and Improve the Quality of Our Thinking and Discourse / The Public Should Seek to Become Fluent in Scientific Controversies and Critique in Order to Keep Science Functioning, Bearing Fruit and in the Public&#39;s Interest</b><br /><br />From Jacques Barzun&#39;s <i>Science: The Glorious Entertainment</i> (p6-8)<br /><br /><i>&quot;[S]ome may hastily infer that I am &#39;against science.&#39;  And indeed this charge was occasionally made by readers of The House of Intelelct, despite the fact that it contained a parallel discussion of art, which led other readers to say that I was &#39;attacking art.&#39;  Such notions of attacking and being against are perhaps native to the age; they are foreign to me.  I do not understand what is meant by being against, or for, wholes -- art, science, education, medicine, the state.  I can as readily imagine being against sunsets and for the tides. </i><b><i>Criticism as I understand it differs entirely from attack or complaint.</i></b><i> Its difference from complaint is especially important here, for I am persuaded that complaints against the machinations of culture today have become as poisonous as the things complained of.  This is not surprising. </i><b><i>Resentment and indignation are feelings dangerous to the possessor and to be sparingly used. They give comfort too cheaply; they rot judgment, and by encouraging passivity they come to require that evil continue for the sake of the grievance to be enjoyed.</i></b><br /><br /><b><i>Criticism, on the contrary, aims at action.</i></b><i> True, not all objects can be acted on at once, and many will not be reshaped according to desire; but thought is plastic and within our control, and </i><b><i>thought is a form of action. To come to see, in the light of criticism, a situation as different from what it seemed to be, is to have accomplished an important act.</i></b><i>  The contemporary world, cluttered with leagues and lobbies and overawed by the zero-weighted look of large numbers, has forgotten that </i><b><i>to redirect fundamental opinion -- including one&#39;s own -- is also to do something. It can give solace or mastery, or at the very least replace a plaintive [mournful] passivity with a stoic [emotionless] impassivity.</i></b><br /><br /><i>That is why, in the present work, which is primarily descriptive, and which, like its predecessors, often uses for illustration not what is merely typical, but what is especially worthy of regard, I sometimes employ the rhetoric of argument -- the form that most naturally incites the internal action called thought. </i><b><i>To the reader it should not greatly matter whether or not he agrees with the conclusions I reach. The point of offering them is to reduce confusion and provide a spur to reasoning. For the aim of a critic, beyond that of saying what he thinks, is to make two thoughts grow where only one grew before.</i></b><br /><br /><b><i>Such an aim implies a reader willing to follow discourse even though the subjects brought together are very diverse, and to lend his mind to attitudes that may seem dubious coming from one who lays no claim to being an expert. But in matters of public interest the public must judge and not ask its guides for certificates and [guarantees], for there are no [guarantees]. It is an error to suppose that when a physicist talks about science he is bound to be more reliable than a so-called layman who has taken the trouble to inform himself and to think. For if scientific specialization means anything, it means that the physicist has final authority only on such questions of physics as fall within his specialty.</i></b><i> Whitehead, whose eminence was first a mathematician, devoted his life to grappling with questions of philosophy, history, literature, and education. </i><b><i>These are public questions, and he served the public interest by bringing to bear upon them his vast intelligence and experience of life. So can anyone serve in the degree of his ability, provided he uses his intellect as a guide in the great regions outside his narrow profession.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The Inevitability of Dark Energy and the Higgs-Boson","summary":"Nothingness Somehow Begat the Black Hole, Which Begat the Big Bang, Which in Turn Begat Inflationary Dark Energy / And, What Few People Appear to Currently Understand is that Dark Energy Begat the Higgs Boson / Modern Cosmology is Essentially a Mathematical Creation Story / What Stands Out is the Necessity of Each New Celebrated \"Discovery\", in Order to Keep the Story Alive","image":"https://lh6.googleusercontent.com/-ktf4RUKQrjI/VwdV_an6aXI/AAAAAAAAHlg/2IssHGWhFo4QYKRLtx7fCCX281OCCIJiQ/dark-energy-higgs-boson-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/2jCSyE4geZd","publishDate":"2016-04-08T06:56:04.464Z","updateDate":"2017-01-03T02:38:16.278Z","text":"<b>The Inevitability of Dark Energy and the Higgs-Boson: Nothingness Somehow Begat the Black Hole, Which Begat the Big Bang, Which in Turn Begat Inflationary Dark Energy / And, What Few People Appear to Currently Understand is that Dark Energy Begat the Higgs Boson / Modern Cosmology is Essentially a Mathematical Creation Story / What Stands Out is the Necessity of Each New Celebrated &quot;Discovery&quot;, in Order to Keep the Story Alive</b><br /><br />This is an excerpt from the very recent (2015) publication which is now available for purchase from <a href=\"http://www.mikamar.biz/thunderbolts-product.htm\" class=\"ot-anchor\">http://www.mikamar.biz/thunderbolts-product.htm</a> ...<br /><br /><i>Newton, Einstein &amp; Velikovsky: Celestial Mechanics, Electromagnetism, Solar System Instability &amp; Cosmology</i><br />By Charles Ginenthal<br />pages 257-266<br /><br /><i>&quot;Space Inflation and the Invention of the Higgs Boson</i><br /><br /><i>The universe supposedly began when a singularity or black hole that had all the mass, electricity and space in it for some unknown reason exploded.  Alan Guth received a Nobel Prize for explaining what happened next, which came to be known as &#39;inflation.&#39;  At 10^-36 seconds after the Big Bang, the universe expanded exponentially beyond the speed of light.  It was not the matter that expanded outward, but the space that contained this matter that expanded; since matter cannot, according to Einstein, travel faster than the speed of light, the way to get the matter to expand throughout the universe was to have the space between it expand.  Science writer, Marcia Bartusiak, outlines the development of Guth&#39;s inflationary concept that was very quickly accepted by the scientific establishment because, as we will see, like Dark Matter, it was needed to make the Big Bang theory work.</i><br /><br /><i>&#39;Guth did not start out to do cosmology.  Working with a Cornell University colleague, Henry Tye, he was trying to determine if most current grand unification theories in physics -- theories that attempt to unify the forces of nature -- might give rise to monopoles (hypothetical particles of magnetic charge).  The two particle physicists concluded that monopoles would be generated and proceeded to see how many might be produced in the Big Bang.  So many would be created that &#39;we began to wonder why the universe was here at all,&#39; said Guth &#39;Their tremendous weight would have closed the universe back up [by causing it to implode back into a black hole] eons ago.&#39;&#39; [41]</i><br /><br /><i>Because what they found would not allow the universe to expand to its present size after about 13.7 billion years -- some way had to be found to get out of this impasse, as Bartusiak further explains:</i><br /><br /><i>&#39;Guth and Tye eventually surmised that monopole production could be curtailed if the early universe &#39;Supercooled&#39; as it expanded -- [Supercooling allowed] the forces of nature in effect staying unified for a while as temperatures plunged, just as water can sometimes supercool and remain liquid below its freezing point.  The notion of [there being an] inflation was encountered when Tye casually reminded Guth to check how this supercooling might affect the infant universe.</i><br /><br /><i>Guth carried out the initial calculations at his home office on the night of December 6, 1979.  He started at about 10^-35 seconds into the universe&#39;s birth ... His equations told him that supercooling would endow the universe with a tremendous potential energy.  A pressure contribution to gravity became so substantial that it reversed the effect of gravity, causing the tiny universe to balloon outward at a superacceleration rate for a miniscule (~10^-35 second or so), stretching space-time [with all the protomatter of the Universe contained in it outward] by a factor of 10^-30 or more.  When this supercooled state came to an end, its latent energy was [then] released as a fireball [of expanding heat that cooled down] that eventually cooled into the matter and radiation that surround us today.&#39; [42]</i><br /><br /><i>The reader is asked to note that all this theoretical work was based on mathematics because there is no way to observe that instance of expansion.  But it was accepted because the beauty of Guth&#39;s theory that seemed to explain certain cosmological problems.</i><br /><br /><i>&#39;His scenario solved two puzzles that had long been troubling cosmologists: why the universe was so uniform (horizon problem [a uniform distribution of matter will not collapse to form stars, galaxies]) and why it was right at the brink between eternal expansion and eternal recollapse (its curvature [of space being] geometrically &#39;flat&#39;).  With general relativity, Einstein showed that matter caused space to warp and bend ... with too little mass in the universe, the curvature of space-time will never curl back up on itself; instead the universe remains &#39;open&#39; destined to expand forever.  But a certain quantity of [just the correct amount] of mass/energy -- just beyond the critical density -- can provide enough gravity to &#39;close&#39; the universe, slowing down the galaxies [moving apart] and eventually drawing them back inward ... Such a condition requires fine tuning.  It was far more probable for the universe to be </i><b><i>very</i></b><i> open or [it would] ... have closed up within 10^-44 seconds after the Big Bang.  A burst of inflation though essentially jump-starts the universe and immediately flattens its curvature, driving it naturally to the very boundary between open and closed.&#39; [43]</i><br /><br /><i>Bartusiak concludes her analysis thus:</i><br /><br /><i>&#39;Inflation also explains the universe&#39;s puzzling uniformity.  Over scales of billions of light years, matter and remnant radiation from the Big Bang are distributed fairly smoothly, which has been a mystery because there is not enough time in the standard Big Bang model to obtain the same temperature and density in all parts of the embryonic universe.  But the preinflationary kernel that gave birth to our universe -- a trillion times smaller than a proton -- did have time to blend its contents well with inflation, then stepping [in to] maintain this uniform mixture throughout the growing bubble of space time ...</i><br /><br /><i>Guth&#39;s initial approach had a fatal flaw, which he pointed out in his paper.  At the end of the hyperaccelerated burst [outward], he was left with a chaotic collection of tiny &#39;bubble universes&#39; none of which could evolve into the [present] universe ... But in the following years other theorists, ... developed other versions of inflation involving other mechanisms [which] allowed any one of Guth&#39;s many bubbles to balloon into a suitable universe.  Inflation soon became an essential feature in standard cosmological models.&#39; [44]</i><br /><br /><i>This all appears to work quite nicely as a valid theory that explains how the universe went from a black hole to that which exists today.  But with all the patches created by other theorists, the problem was, as a matter of fact, never actually resolved, as Scoular sums up this question:</i><br /><br /><b><i>&#39;In spite of two decades of theoretical efforts, there is still no real theory of inflation.</i></b><i> All inflationary predictions depend on the assumed form of the inflation potential [energy], not yet derivable from fundamental theory.  Simple models include an arbitrary potential, which must [then] be fine-tuned to obtain agreement with observation.  Worse, they are quantum mechanically inconsistent (non-renormalizable) beyond leading order when gravity is included. </i><b><i>At best, the models are a &#39;stand in&#39; effect theory for a fundamental approach still to be developed ... The strongest argument for inflation is just that there is no other viable alternative.&#39;</i></b><i> [45]</i><br /><br /><i>That is, if one accepts the Big Bang theory, there is no other alternative to go from the tiny kernel of a universe black hole to the condition of the present universe without inflation. </i><b><i>The theory of the Big Bang requires inflation and inflation requires there was a Big Bang. What we have in reality are two assumptions each demanding that the other works and is true as assumed. Physicist Howard Georgi calls inflation &#39;a wonderful sort of scientific myth, which is at least as good as any other creation myth I&#39;ve ever heard.&#39;</i></b><i> [46]</i><br /><br /><i>However, to explain the way that the protomatter in the early universe evolved to become the forms of matter we experience today containing gravity at the end of the inflationary period, somehow a particle that was a pure gravitation entity united with subatomic particles that became atoms and endowed them with mass gravity.  This particle came to be known as the Higgs boson or as Leon Lederer called it, &#39;The God Particle.&#39;  When many of the Higgs bosons exist in space they are referred to as a &#39;scalar field,&#39; a field filled with Higgs bosons.  In this regard, Lisa Randall admits, &#39;Nevertheless, </i><b><i>we are fairly confident the Higgs mechanism applies to our world, since it is the only way to give Standard Model particles their masses.&#39;</i></b><i> [47]</i><br /><br /><i>Historian of science, David Lindley, put the proposition for why inflation demands a Higgs boson:</i><br /><br /><b><i>&#39;Inflationary cosmology has become a widely accepted piece of theorizing while remaining entirely conjectural.</i></b><i> With the right kind of theory, it can be made to work.  Some cosmologists have found even supersymmetry a bit too restrictive, and have decided that inflation is so important a theory that it deserves its very own Higgs mechanism, divorced from any real or imagined symmetry breaking in particle physics.  Thus arises, in some theories, a particle called, almost as an admission of defeat, an &#39;inflation&#39; belonging to a Higgs mechanism </i><b><i>introduced into physics for no other reason than to make inflation work</i></b><i> ...</i><br /><br /><i>Alan Guth meant to demonstrate that grand unification, which would, in turn, lead to a large, uniform, long-lived universe -- the very thing we inhabit.  But now this logic has been turned inside out: [in order] to get our universe, so the argument goes there must have been the right kind of inflation, at an early time, but </i><b><i>neither grand unification nor (probably) supersymmetry is quite up to the task of making the right kind of inflation happen, so the true theory of particle physics must contain some additional piece [the Higgs boson] that drives inflation.&#39;</i></b><i> [48]</i><br /><br /><i>Therefore, the Big Bang demands inflation and inflation demands the Higgs Bosons unified gravity or mass with subatomic massless particles.  Each phenomenon was dependent on the others.  Marcelo Gleisner asks:</i><br /><br /><i>&#39;Whether effective [sic] description or fundamental entities, scalar [Higgs] fields can produce the positive acceleration needed to fuel the faster-than-light cosmic expansion of [Guth&#39;s] inflationary cosmology?  If the field is away from its equilibrium state and thus under strain, it will behave like the mythic cornucopia that never stops producing fruit, [expanding or contracting], until, that is, it reaches its equilibrium, or lowest energy state.</i><br /><br /><i>In Guth&#39;s original model, the all-powerful scalar [Higgs] field that drove the exponentially fast expansion came from a theory that went beyond the Standard Model [of subatomic physics] and attempted to unify all the atomic and subatomic properties of matter: electromagnetism, and the strong and weak nuclear forces.  Such theoretical constructions [that unite these forces] ... go by the ... name Grand Unification theories ... Full unification would be eventually completed through the incorporation of gravity, the fourth and last force.  Unfortunately ... the main predictions from grand unification haven&#39;t yet panned out.  To make things worse, Guth&#39;s original model didn&#39;t work either ...&#39; [49]</i><br /><br /><i>Both inflation and grand unification in Guth&#39;s model didn&#39;t work and, therefore, something had to be done.  According to Gleisner:</i><br /><br /><i>&#39;A new strategy was developed disconnecting inflation from G[rand] U[nification] T[heories].  The logic goes as follows: &#39;Forget Guth&#39;s original motivation from particle physics; given that we don&#39;t really know what was going on during the very first moment of cosmic existence ... let&#39;s assume that some sort of scalar [Higgs] field was present at the time and was able to generate the negative pressure needed to fuel inflation.  Inflation is too simple and compelling an idea, it solves too many problems to be abandoned.  Whatever, [unknown physical entities] drove it, it played the role of an effective [Higgs] field.&#39; [50]</i><br /><br /><i>Here, the admission is being made that the Higgs boson in the scalar field is necessary to drive inflation, but at the same time it was abandoned because no one knows what happened during the first moment of cosmic inflation.  Nevertheless, some kind of scalar Higgs field must have somehow existed to create inflation because again, inflation is necessary.</i><br /><br /><i>[...]</i><br /><br /><i>as Martin Gardner said, it is the scientific establishment that has invented a force [the scalar Higgs field] capable of doing precisely what they want it to do.  There is no scientific evidence whatever for the power or existence of this force in the first moments of creation.  Higgs bosons serve the function of creating inflation for the scientists ... They explain the unexplainable.  But so convinced are the scientists that they can -- with a straight face -- belabor those who refuse to recognize this imaginary energy!</i><br /><br /><i>In this respect, Nobel laureate, Martinus Veltman, shows, </i><b><i>&#39;The biggest drawbacks of the Higgs mechanism is that so far no evidence of its existence has been found.</i></b><i> Instead, a fair amount of indirect evidence already suggests that the elusive particle does not exist.  Indeed, modern theoretical physics is constantly filling the empty vacuum with so many contraptions, such as the Higgs boson, that it is amazing a person [can see through the vacuum] can see the stars on a clear night!&#39; [51]  Scoular goes on to outline the problems of the scalar Higgs field and there are many of them and they are seriously difficult to overcome:</i><br /><br /><i>&#39;Eugene Samuel, of New Scientist, reported in 2001: &#39;The legendary particle that physicists thought explained why matter had mass probably doesn&#39;t exist.  So say researchers who have spent a year analyzing data ... at the CERN nuclear physics lab near Geneva ... </i><b><i>Now the calculations have been reworked, members of the Electroweak Working Group say there was no sign of a Higgs at energies up to 115 GeV [Giga Volts], well past the 80 GeV [Level] where it would be expected [to show itself].</i></b><i> That leaves around 30 percent of possibilities.  Its existence is looking &#39;less and less likely,&#39; say Steve Reucroft ... of Northeastern University. </i><b><i>&#39;We&#39;ve eliminated most of the hunting areas,&#39; [where Higgs bosons should be found], confirms Heil Calder at CERN.</i></b><br /><br /><i>Second, the Higgs mechanism leads to a fine-tuning problem, as theoretical physicist, Lee Smolin, explains:</i><br /><br /><i>&#39;The problem lies with the unprotected particles, which in the standard model of elementary particles mean the Higgs and Higgs alone. </i><b><i>It turns out to protect the Higgs from being pulled up to the Planck mass; we have to tune the constants of the standard model to the amazing precision of thirty-two decimal places. Any inaccuracy in any of those thirty-two decimal places and the Higgs boson ends up much heavier than it is predicted to be.&#39;</i></b><br /><br /><i>Third, according to the equivalence principle, inertial mass and gravitational mass are identical.  That is, acceleration (which is related to inertial mass) ... can be interpreted as gravity (which is related to passive gravitational mass).  That is, by a change of interpretation it is possible to switch between inertial and gravitational mass. </i><b><i>Therefore, one would expect any mechanism for inertial mass to be related to the mechanism for massive gravitational mass. However, the Higgs mechanism only explains inertial mass, not passive gravitational mass.</i></b><br /><br /><i>Fourth, the Higgs mechanism -- other than providing mathematical consistency for the electroweak theory -- does not provide any additional insights as Nobel laureate Martinus Veltman explains:</i><br /><br /><i>&#39;From a physical point of view, little is to be gained by proposing that the Higgs boson accounts for mass.  It is not known, for example, why the Higgs field should couple more strongly to some particles than it does to others.  Nor do investigators understand how the mass of the Higgs boson itself (which is not known) comes about, although it is generally presumed to be dominantly through a self-interaction with the Higgs field.  In this sense, </i><b><i>ignorance about the origin of particle masses is replaced by ignorance about particle-Higgs coupling, and no real knowledge is gained.&#39;</i></b><br /><br /><i>Fifth, </i><b><i>the Higgs mechanism is introduced in an ad hoc way with its only purpose being to make the Standard model work</i></b><br /><br /><i>[...]</i><br /><br /><i>Sixth, the Higgs particle not only couples to other particles, but also to itself.  Bound Higgs particles have lower energy than unbound Higgs particles.  It so happens that an infinite number of Higgs particles have a more negative energy than the positive energy associated with the mass of all Higgs particles.  As a result, when the Universe began at the Big Bang, energy would have been released by creating this bound state.  However, if this occurred, [then] the strong gravitational effects of the bound would lead to a curved universe the size of a [British] football.  This is not observed.&#39; [52]</i><br /><br /><i>While the Higgs boson only has mass it lacks electrical phenomena.  In this regard, it is also unique in terms of physics, as Wal Thornhill explains:</i><br /><br /><i>&#39;The &#39;God Particle&#39; or Higgs boson was invented to explain why other particles exhibit mass. </i><b><i>He [Peter Higgs, of the University of Edinburgh] starts with assuming the existence of a particle that has only mass and no other characteristic, such as charge. So the Higgs particle is like no other in our experience since all normal matter is composed of electric charges that respond to electromagnetic influences</i></b><i> (Dark Matter falls into this same category).  However, observe that the mass of a charged subatomic particle is affected by the application of electromagnetic forces.  At its simplest ... it indicates that mass is related to the storage of energy which a system of electric charges inside the particle ... So how can a massive particle be constructed without electric charge?  It shows the problem inherent in leaving physics to mathematicians -- there is a disconnect between mathematical concepts and reality.&#39; [53]</i><br /><br /><i>Because of these many problems, Martinus Veltman writes, </i><b><i>&#39;One of the Higgs architects calls it a rug under which we seep our ignorance.  [Sheldon] Glasgow is less kind, calling it a toilet in which we flush away the inconsistencies of our present theories.&#39;</i></b><i> [54]  The seriousness of the situation was not lost on the scientists.  The problems related to the Higgs boson threatened not only grand unification theory but Guth&#39;s inflation theory and, thus, fundamental underpinning of the Big Bang theory.  Therefore, an experiment by the ... Large Hadron Collider at CERN was carried out in 2012 to attempt to find the Higgs boson.  The results were far from clear, but the scientists &#39;felt&#39; they &#39;may have&#39; at least discovered this particle, as Dennis Overby reports.</i><br /><br /><i>&#39;&#39;I think we have it,&#39; Rolf-Dieter Hauer ... director general of CERN, said</i><br /><br /><i>Dr. Hauer and others said it was too soon to know whether the new particle, which weighs in at 125 billion electron volts [GeV], one of the heaviest subatomic particles yet, fits the simplest description given by the Standard Model, the theory that has ruled physics for the last half century, or whether &#39;it is an impostor, a single particle or even the first of many particles to be discovered&#39; ... For now, they are calling it a &#39;Higgs-like&#39; particle.&#39; [55]</i><br /><br /><b><i>The problem with this discovery is that its voltage was calculated not from one experiment but from the combined evidence of two experiments.</i></b><i> This is not the way science operates, as physicist Aiden Randle-Conde explains:</i><br /><br /><i>&#39;Why shouldn&#39;t we combine Higgs searches across experiments?</i><br /><br /><i>If neither experiment gets 5 sigma [the level at which this experiment should be confirmed] and we would like a discovery, what can be done?  The next obvious step would be to combine the results from the two experiments and count the Sigma [value of both together]. </i><b><i>Despite being an obvious next step, this is the worst thing we can do at the moment.</i></b><br /><br /><b><i>The reason we have two experiments ... looking for the Higgs boson is because if one experiment makes a discovery, then the other can confirm or refute the discovery ... so when one experiment sees an effect, but its counterpart doesn&#39;t, then it&#39;s likely due to a problem with analysis.</i></b><i> In an experiment as complicated as [this] ... it&#39;s not surprising that something subtle would get missed.  Everything the hardware and software was telling the physicist was there was a bump [observed] in distribution.  The easiest way to see if this is wrong is to see what [other] ... hardware and software tell us. </i><b><i>It turns out they disagree in this instance and we got the crosscheck needed</i></b><i> ...</i><br /><br /><b><i>If we combine measurements from two different experiments we end up losing the vital crosscheck.</i></b><i> The best way to proceed is to do a few more experiments until both experiments produce a 5 sigma [value] and see if the results [of both also] agree.&#39; [56]</i><br /><br /><i>Nevertheless, the Nobel Prize was awarded jointly to Francois Englert and Peter W. Higgs in physics for 2013, not for the discovery of </i><b><i>the</i></b><i> Higgs boson, but </i><b><i>&#39;for the theoretical discovery of a mechanism</i></b><i> that contributes to our understanding of the origin of subatomic particles, and which recently was confirmed through the discovery of the predicted fundamental particle by the [two] ATLAS and CMS experiments of CERN&#39;s Large Hadron Collider.&#39; [57]</i><br /><br /><i>In essence, they were being awarded Nobel Prizes for a &#39;theoretical discovery.&#39;  Regarding the Higgs discovery &#39;confirmation,&#39; the Nobel Prize Committee writes, &#39;On 4 July 2012 at CERN laboratory for particle physics, the theory was confirmed by the discovery of a Higgs particle ... Two research groups of some 3,000 scientists each ... managed to extract the Higgs particle from billions of particle collisions in the LHC.&#39; [58]  The Nobel Committee is claiming that this particle is &#39;a Higgs particle,&#39; not &#39;*the* Higgs particle.&#39;  CERN, itself, admits, &#39;This particle is consistent with the Higgs boson, but it will take further work to determine whether or not it is THE Higgs boson predicted by the Standard Model.&#39; [59]  Amazingly, a year and a half after the &#39;official announcement of the Higgs boson, all papers in Physical Review Letters never call it THE Higgs boson, but a &#39;Higgs-like&#39; or as a &#39;candidate.&#39;  As we are told elsewhere:</i><br /><br /><b><i>&#39;Just because something looks like the Higgs particle does not mean it is the Higgs particle. If physicists do discover a new particle, they will need to measure its numerous properties before they can determine whether it is the Higgs boson described by the Standard Model of particle physics. Theory predicts in great detail how a Standard Model Higgs particle will interact with other particles. Only after carefully measuring and testing these interactions ... - would scientists be certain that they had indeed found the Standard Model Higgs boson.</i></b><i> A new particle that did not act as expected would give a whole new set of mysteries to explore.&#39; [60]</i><br /><br /><i>Miles Mathis adds this further stunning contradiction to physics and reason that the Higgs boson creates, namely that it is much, much larger, more massive than the particles that it gives mass to:</i><br /><br /><i>&#39;The current articles even admit the proposed particles at 125 GeV is no proof of Higgs&#39; theory or of a God particle or any way to give mass to matter.  Mainstream physicists have simply prepared their audience by telling them ... a large particle [found at CERN] will be proof of their theories of mass creation [after the Big Bang].  But do you think the large particle -- supposing it had been found -- has a sign on it that says &#39;Higgs&#39;?  No.  They have to show more than a large particle.  They have to show some rational mechanism by which a large particle gives mass to smaller particles ... How can the mass of a small particle be explained by the existence of a large particle?  Isn&#39;t that topsy-turvy and inverted?  We are being told that smaller particles </i><b><i>are composed</i></b><i> of larger particles.  And if not, what are we being told?</i><br /><br /><i>In the history of physics, larger particles were always composed of smaller ones: that was the definition of &#39;composed.&#39;  A rock is composed of molecules and the molecules are, by necessity, </i><b><i>smaller</i></b><i> than the rock.  Molecules are composed of atoms, and atoms are, by necessity, </i><b><i>smaller</i></b><i> than the molecules.  But now that is reversed.  If the Higgs theory is accepted, we will have larger particles defining the qualities and quantities of smaller particles ... For centuries, we have been composing larger things from smaller things.  That was physics.  But the Higgs theory reverses this and we now have smaller things created from larger things [not that the Higgs boson breaks up into smaller particles to form them] ... The [bigger] Higgs boson &#39;gives&#39; mass to hadrons and leptons and so on.&#39; [61]</i><br /><br /><i>The point that must be stressed is that </i><b><i>if there is no Higgs boson found that is predicted by the Standard Model of physics, it would mean the collapse of that entire theoretical framework.</i></b><i> In his book, The Higgs Fake: How Particle Physicists Fooled the Nobel Committee, by Alexander Unzicker, page 116 cites David Lindley that </i><b><i>the Higgs boson &#39;may be as well regarded as a mathematical invention.&#39;</i></b><i> [62]  Martinas J.G. Veltman, in this regard says, </i><b><i>&#39;the only legitimate reason for introducing the Higgs boson is to make the Standard Model mathematically consistent.&#39; [63] Like so much else, the Higgs boson is absolutely necessary to the last hundred years of particle physics research.</i></b><i> And, therefore, it is unthinkable that an entire field of Ph.D.s in it will, as Soviet Premier, Mikhail Gorbachev, admitted that communism, as an economic, political experiment, failed.  Therefore, I believe this particle will be salvaged in some way, come hell or high water.  Two citations from Richard Feynman sums up the entire concept: &#39;It&#39;s not because Nature is really similar; it&#39;s because physicists have only been able to think of the same damn thing [to fit their theories] over and over again&#39; and &#39;wherever we encoutner a mess of too many problems, it&#39;s because we stick to established methods.&#39; [64]  Unzicker&#39;s book &#39;is a merciless critique of the Large Hadron Collider at CERN and of the theoretical model on which the world&#39;s most expensive experiment is based.  Moreover, with increasingly intricate techniques, particle physicists are fooling themselves with alleged results while their convictions are based on group-think and parroting.&#39; [65]  Unzicker&#39;s work has not endeared him to those working in the field of particle physics, and Peter Woit has responded to his criticisms by calling Unzicker a &#39;crank.&#39;  In other words, if one challenges the accepted theory of particle physics, one must be ostracized, and name-calling is one way to accomplish this.</i><br /><br /><i>Today, no scientist has given the clear proof that </i><b><i>the</i></b><i> Higgs boson has been proved scientifically.  All announcements I have seen say they &#39;believe,&#39; &#39;are confident,&#39; &#39;almost certain,&#39; &#39;most probably have found,&#39; the Higgs boson exists, etc.  It has </i><b><i>not</i></b><i> been found and the scientific establishment, so desperately needs it that they don&#39;t know what to do.  After supposedly announcing the possibility of having found this particle, CERN has shut down for a few years and, thus far, has failed to replicate in two experiments independently of each other collaboration for and of </i><b><i>the</i></b><i> Higgs particle.  It is simply amazing how scientists can be so willing to give up their scientific understandings for a theory and a particle that is so overburdened with contradictions ...&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The Incoherence of Black Hole Mathematics","summary":"The Idea of the Black Hole is Based Upon What is Known as the Schwarzschild Solution / But Schwarzschild Did Not Write It / In Fact, Swarzschild Did Not Say Anything About Black Holes, Because His True Solution Does Not Allow Them / One of the Most Popular Concepts in Science Today is a Purely Mathematical Conjecture Riddled with Contradictions, Ambiguities and a History of Mathematicians Warning that it Would Not Work","image":"https://lh3.googleusercontent.com/-3p5junOtnd4/VwSABhfVk6I/AAAAAAAAHkA/eOJM79F48V0NYolxDt5NlEuNyAsw5uKdA/w6250-h8750/the-incoherence-of-black-hole-mathematics-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/EKYvSxFaszB","publishDate":"2016-04-06T03:35:04.942Z","updateDate":"2017-01-03T02:38:31.170Z","text":"<b>The Incoherence of Black Hole Mathematics: The Idea of the Black Hole is Based Upon What is Known as the Schwarzschild Solution / But Schwarzschild Did Not Write It / In Fact, Swarzschild Did Not Say Anything About Black Holes, Because His True Solution Does Not Allow Them / One of the Most Popular Concepts in Science Today is a Purely Mathematical Conjecture Riddled with Contradictions, Ambiguities and a History of Mathematicians Warning that it Would Not Work</b><br /><br /><a href=\"http://www.sjcrothers.plasmaresources.com/\" class=\"ot-anchor\">http://www.sjcrothers.plasmaresources.com/</a><br /><br /><i>&quot;The Black Hole, the Big Bang, and Modern Physics</i><br /><i>website by Stephen J. Crothers</i><br /><br /><i>The black hole has no foundation in theory whatsoever. Neither Newton&#39;s theory nor Einstein&#39;s theory predict it. In fact, both theories preclude it, contrary to what the relativists claim.</i><br /><br /><i>The so-called &#39;Schwarzschild&#39; solution is not due to Karl Schwarzschild at all. The experts have either not read Schwarzschild&#39;s 1916 memoir or have otherwise ignored it. Go here [13] to get Schwarzschild&#39;s original paper, in English. The so-called &quot;Schwarzschild&quot; solution is due to David Hilbert, itself a corruption of a solution first derived by Johannes Droste in May 1916, whose paper has also been buried or ignored at the convenience of the experts. It appears that the experts have not read Hilbert either. Go here [12] to get a copy of Hilbert&#39;s erroneous derivation, in English. Hilbert&#39;s mistake spawned the black hole and the community of theoretical physicists continues to elaborate on this falsehood, with a hostile shouting down of any and all voices challenging them. Schwarzschild&#39;s solution has no black hole, and neither does Droste&#39;s solution. And while you&#39;re at it you might as well go here [24] to get a copy of Marcel Brillouin&#39;s 1923 paper, in English, in which he demonstrates that the black hole is nonsense. Brillouin&#39;s paper has also been ignored.</i><br /><br /><i>The &#39;experts&#39; are always quick to conveniently brand anyone who questions the black hole as a crackpot. Unfortunately for the experts that does not alter the facts. The experts must also include Schwarzschild himself as a crank since his paper invalidates the black hole outright, as does Brillouin&#39;s, and Droste&#39;s. They must also label Einstein a crackpot, because Einstein always rejected the idea of the black hole, asserting in his research papers and other writings that it is not physical, and that singularities in his gravitational field nullify the theory of General Relativity.</i><br /><br /><i>It is also commonly held by experts, for example, Hawking and Ellis, Misner, Thorne and Wheeler, S. Chandrasekhar, that the Michell-Laplace dark body is a kind of black hole, and that black holes can be components of binary systems and that black holes can collide and merge. These claims are patently false. Go here [25] for a copy of G. C. McVittie&#39;s conclusive arguments which invalidate these ridiculous claims.&quot;</i><br /><br />[1] <a href=\"http://vixra.org/pdf/1306.0024v2.pdf\" class=\"ot-anchor\">http://vixra.org/pdf/1306.0024v2.pdf</a><br /><br />[2][3] <a href=\"http://vixra.org/pdf/1111.0032v1.pdf\" class=\"ot-anchor\">http://vixra.org/pdf/1111.0032v1.pdf</a><br /><br />[4] - [11] <a href=\"http://www.ptep-online.com/index_files/2006/PP-05-10.PDF\" class=\"ot-anchor\">http://www.ptep-online.com/index_files/2006/PP-05-10.PDF</a><br /><br />[12] <a href=\"http://www.sjcrothers.plasmaresources.com/hilbert.pdf\" class=\"ot-anchor\">http://www.sjcrothers.plasmaresources.com/hilbert.pdf</a><br /><br />[13] <a href=\"http://www.sjcrothers.plasmaresources.com/schwarzschild.pdf\" class=\"ot-anchor\">http://www.sjcrothers.plasmaresources.com/schwarzschild.pdf</a><br /><br />[14] <a href=\"http://vixra.org/pdf/1306.0024v2.pdf\" class=\"ot-anchor\">http://vixra.org/pdf/1306.0024v2.pdf</a><br /><br />[15] <a href=\"https://www.thunderbolts.info/wp/wp-content/uploads/2013/03/Crothers-EU-2013.pdf\" class=\"ot-anchor\">https://www.thunderbolts.info/wp/wp-content/uploads/2013/03/Crothers-EU-2013.pdf</a><br /><br />[16] <a href=\"http://vixra.org/pdf/1306.0024v2.pdf\" class=\"ot-anchor\">http://vixra.org/pdf/1306.0024v2.pdf</a><br /><br />[17] <a href=\"http://vixra.org/pdf/1111.0032v1.pdf\" class=\"ot-anchor\">http://vixra.org/pdf/1111.0032v1.pdf</a><br /><br />[18]-[21] <a href=\"http://vixra.org/pdf/1306.0024v2.pdf\" class=\"ot-anchor\">http://vixra.org/pdf/1306.0024v2.pdf</a><br /><br />[22] I covered this Speed of Gravity issue in depth here: <a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/NaN8qrGovon\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/NaN8qrGovon</a>.  Many people at the time were content to accept the explanation, but I wonder what they will think in light of this other problem.<br /><br />[23] I covered Herbert Dingle&#39;s Paradox here: <a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/fK3ZPDqZbrF\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/fK3ZPDqZbrF</a><br /><br />[24] <a href=\"http://www.sjcrothers.plasmaresources.com/brillouin.pdf\" class=\"ot-anchor\">http://www.sjcrothers.plasmaresources.com/brillouin.pdf</a><br /><br />[25] <a href=\"http://www.sjcrothers.plasmaresources.com/McVittie.pdf\" class=\"ot-anchor\">http://www.sjcrothers.plasmaresources.com/McVittie.pdf</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"Black Hole Babble","summary":"Black Hole Theory is so Flexible and so Unreal that it Can be Altered to Suit Any New and Contradictory Observation / This Lends a Comic Book Aspect to Black Hole Journalism Which Attracts People to Science for the Wrong Reasons / People, Please Stop Taking the Bait / There is Real Science Which this Babble Obscures, and You'll Find it Just as Interesting","image":"https://lh4.googleusercontent.com/-ybtZwJsmhmA/VwCi1l3v_lI/AAAAAAAAHik/ptwLFJquwzIZcP7Jze-LKrAPXzcKi6lag/black-hole-babble-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/6QMdzjYhtyn","publishDate":"2016-04-03T05:02:21.493Z","updateDate":"2017-01-03T02:39:00.791Z","text":"<b>Black Hole Babble: Black Hole Theory is so Flexible and so Unreal that it Can be Altered to Suit Any New and Contradictory Observation / This Lends a Comic Book Aspect to Black Hole Journalism Which Attracts People to Science for the Wrong Reasons / People, Please Stop Taking the Bait / There is Real Science Which this Babble Obscures, and You&#39;ll Find it Just as Interesting</b><br /><br />This is based upon clever insight by Liam Scheff at <a href=\"http://liamscheff.com/2009/07/do-black-holes-really-exist/\" class=\"ot-anchor\">http://liamscheff.com/2009/07/do-black-holes-really-exist/</a><br /><br /><i>&quot;When I was a boy, a Black Hole was something you found in fiction, not in genuine Astronomy. They lived in comic books and sci-fi novels, but nobody took them seriously. Serious-minded researchers considered them &#39;purely theoretical.&#39;</i><br /><br /><i>Why? Because, according to &#39;Black Hole Theory,&#39; these space demons are ... Invisible! They can not be seen, under any circumstance. They &#39;bend light and warp time.&#39; Mischievous devils ...</i><br /><br /><i>You can’t see them in the sky. Only the specially-trained can view them – in their minds, or on their computers. Why?</i><br /><br /><i>Because Black Holes are made of numbers – they are equations – they are not made out of common sense or of plain and simple observation ...</i><br /><br /><i>Being purely theoretical, they can be bent to suit any purpose – they can be made infinitely big or small, strong or weak; the theory is so flexible, and so unreal, that it can be altered to suit any new and contradictory observation.</i><br /><br /><i>Is this the sign of a good theory? Or a good fib? (Or a good myth?)</i><br /><br /><i>When Odysseus sailed around the Aegean Sea, thrown hither and yon by vengeful Gods, he encountered the sucking whirlpool monster, from which no ship could escape – Charybdis – and the multi-headed Scylla, who tore his sailors from their oars, and devoured them – &#39;gobbled them up.&#39;</i><br /><br /><i>Were there actual monsters in the ancient Aegean? Or were these monsters, instead, explanations of the wild and ferocious sea itself, from which many ships did not return?</i><br /><br /><i>And so we ask the same of Black Holes. Are they real monsters, </i><b><i>or are they colorful inventions made to explain the ferocious activity in an outer space that we do not properly understand?</i></b><br /><br /><i>Today, the major science journals publish weekly updates on the many oddities and contradictions of these space monsters – except the science journals assert, without blushing, that they are entirely real, and that anybody who says otherwise is, well, in denial.&quot;</i><br /><br />Bill comments on August 25, 2009 at <a href=\"http://liamscheff.com/2009/07/do-black-holes-really-exist/\" class=\"ot-anchor\">http://liamscheff.com/2009/07/do-black-holes-really-exist/</a> ...<br /><br /><b><i>&quot;[W]e hear Black Holes get shot out, kicked out, pushed out ... If this thing sucks everything inside it, never to be seen again, that would sort of, to me at least, imply that stuff like gravity, momentum, force, energy, jelly beans, circus clowns ... have no affect on it. Thus, how would it be pushed, pulled, stretched or affected by anything? Can’t have it all ways.</i></b><br /><br /><i>... Superman (A character in fiction) is totally invincible. Except we want him to occasionally be in danger, so we’ll make him vulnerable to Kyptonite (an imaginary material) that is super rare, except every bad guy will seem to get some of it, and it can kill Superman, though it never will manage to do so, and it will come from his home planet of Kryton (which we made up to fit out story) which was blown up long ago in another dimension, except somehow this stuff also manages to be here, now, in our dimension, and everyone on that planet was killed except Superman, though now, because it helps our story, fantasy, comic, some have managed to survive, seem to be here, now, in our dimension, for some reason on Earth and all of they generally fight Superman even though he is one of them, their home world was a peaceful place full of enlightened Krytonians, yet they all seem to be violent and bent on global domination, even though they are basically Gods, so what use for planets and humans would they have?</i><br /><br /><i>Superman can leap tall buildings in a single bound. Which is now superfluous to mention since he can fly now, as opposed to before when he could only leap really far. So the fact he can fly assumes he can “appear” to leap tall buildings. It just helped the story, theory, fantasy to have him fly, rather than arrive in a Yellow Cab to stop criminals.</i><br /><br /><i>And he has X-ray vision, but it’s now more useful for him to also have heat vision, so now he has that too. And he is only affected by Kryptonite, but now there is Red, Purple, Black, Orange, Green, Yellow, Fusia, Mauve, Amber, Green Tea Extract, Lavender infused Super Latte Kyrptonite that affects him too. Some good, some bad, just basically whatever we need right now to explain our story.</i><br /><br /><b><i>This is what Liam is explaining about Black Holes.&quot;</i></b><br /><br />GRAPHIC SOURCES:<br /><br />[1] <a href=\"http://cosmology.berkeley.edu/Education/BHfaq.html\" class=\"ot-anchor\">http://cosmology.berkeley.edu/Education/BHfaq.html</a><br /><br />[2] <a href=\"http://www.dailygalaxy.com/my_weblog/2007/07/invisible-super.html\" class=\"ot-anchor\">http://www.dailygalaxy.com/my_weblog/2007/07/invisible-super.html</a><br /><br />[3] <a href=\"http://news.bbc.co.uk/2/hi/science/nature/779707.stm\" class=\"ot-anchor\">http://news.bbc.co.uk/2/hi/science/nature/779707.stm</a><br /><br />[4] <a href=\"http://www.sl4.org/archive/0309/7123.html\" class=\"ot-anchor\">http://www.sl4.org/archive/0309/7123.html</a><br /><br />[5] <a href=\"http://www.wired.com/2008/01/black-holes-sho/\" class=\"ot-anchor\">http://www.wired.com/2008/01/black-holes-sho/</a><br /><br />[6] <a href=\"http://www.universetoday.com/32175/super-size-me-black-hole-bigger-than-previously-thought/\" class=\"ot-anchor\">http://www.universetoday.com/32175/super-size-me-black-hole-bigger-than-previously-thought/</a><br /><br />[7] <a href=\"https://www.washingtonpost.com/archive/politics/2000/01/15/weak-black-hole-mystifies-scientists/f7901e1d-108c-4fa3-b89b-6a274d72a171/\" class=\"ot-anchor\">https://www.washingtonpost.com/archive/politics/2000/01/15/weak-black-hole-mystifies-scientists/f7901e1d-108c-4fa3-b89b-6a274d72a171/</a><br /><br />[8] <a href=\"http://www.dailygalaxy.com/my_weblog/2009/06/6-billion-suns-the-biggest-black-holes-in-the-universe-just-got-bigger.html\" class=\"ot-anchor\">http://www.dailygalaxy.com/my_weblog/2009/06/6-billion-suns-the-biggest-black-holes-in-the-universe-just-got-bigger.html</a><br /><br />[9] <a href=\"https://www.newscientist.com/article/dn13166-biggest-black-hole-in-the-cosmos-discovered/\" class=\"ot-anchor\">https://www.newscientist.com/article/dn13166-biggest-black-hole-in-the-cosmos-discovered/</a><br /><br />[10] <a href=\"http://certificate.ulo.ucl.ac.uk/modules/year_one/NASA_GSFC/goddard/imagine.gsfc.nasa.gov/docs/science/know_l2/black_holes.html\" class=\"ot-anchor\">http://certificate.ulo.ucl.ac.uk/modules/year_one/NASA_GSFC/goddard/imagine.gsfc.nasa.gov/docs/science/know_l2/black_holes.html</a><br /><br />[11] <a href=\"http://cerncourier.com/cws/article/cern/29199\" class=\"ot-anchor\">http://cerncourier.com/cws/article/cern/29199</a> and <a href=\"https://en.wikipedia.org/wiki/Micro_black_hole\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Micro_black_hole</a><br /><br />[12] [13] <a href=\"http://www.pbs.org/wgbh/nova/physics/tiny-black-holes.html\" class=\"ot-anchor\">http://www.pbs.org/wgbh/nova/physics/tiny-black-holes.html</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"Hindsight Bias in Science","summary":"\"We Knew it All Along\" / A Common Pattern in Scientific Press Releases is a Brief, Initial Moment of Honest Surprise or Confusion, Quickly Followed by Claims that it Was Known All Along / What the Public Doesn't Appear to Understand is that Scientists Have Been Presenting Unexpected Results as Expected for a Long Time Now","image":"https://lh3.googleusercontent.com/-4_LNQ1Gcj24/Vv6WgvjslGI/AAAAAAAAHiI/unN4mRQOcsEV2szcyvS_iAbpEQwO6MxhQ/w1800-h2520/hindsight-bias-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/UCdoHBaNRDh","publishDate":"2016-04-01T15:41:32.557Z","updateDate":"2017-01-03T02:39:17.970Z","text":"<b>Hindsight Bias in Science: &quot;We Knew it All Along&quot; / A Common Pattern in Scientific Press Releases is a Brief, Initial Moment of Honest Surprise or Confusion, Quickly Followed by Claims that it Was Known All Along / What the Public Doesn&#39;t Appear to Understand is that Scientists Have Been Presenting Unexpected Results as Expected for a Long Time Now</b><br /><br /><b>The Example of the Black Hole: It&#39;s Whatever Is Observed</b><br /><br />From <i>Asimov&#39;s Guide to Science</i>, Isaac Asimov, first published in 1960, last revised in 1972 ...<br /><br /><i>&quot;When Oppenheimer worked out the properties of the neutron star in 1939, he predicted also that it was possible for a star that was massive enough and cool enough, to collapse altogether to nothingness.  When such collapse proceeded past the neutron-star stage, the gravitational field would become so intense that no matter, no light could escape from it.  Nothing could be seen of it; it would simply be a &#39;black hole&#39; in space.</i><br /><br /><i>Will it be possible at some time in the future to detect such black holes -- surely the ultimate in strange new objects in the universe?  That remains to be seen.&quot;</i><br /><br />Don Scott from <b>Thunderbolts of the Gods</b> at <a href=\"https://youtu.be/5AUA7XS0TvA?t=1544\" class=\"ot-anchor\">https://youtu.be/5AUA7XS0TvA?t=1544</a> ...<br /><br /><i>&quot;The black hole, as Wheeler originally visualized it, sucked everything -- including light -- in.  And so, therefore, it was invisible.  So, he could postulate its existence, and we couldn&#39;t see it.  And because we couldn&#39;t see it, we couldn&#39;t prove that it wasn&#39;t there. </i><b><i>Later on, in order to explain these fantastic emissions of energy from galaxies, it was proposed -- and I believe it was Stephen Hawking who did it -- that there were various types of black holes, some of which actually then spat matter back out again in jets -- because why?  Because we had observed jets.&quot;</i></b><br /><br />Nevermind alternative ideas which can point to successful predictions.  Wal Thornhill next at <a href=\"https://youtu.be/5AUA7XS0TvA?t=1583\" class=\"ot-anchor\">https://youtu.be/5AUA7XS0TvA?t=1583</a> ...<br /><br /><i>&quot;In the electrical model, it&#39;s quite simple.  If you think about it, the power that&#39;s expressed in your own home -- in the heaters, in the electric motors, and the lighting and so on, is actually generated at a vast distance away from your house.  In deep space, the same thing is true.  The energy which we see concentrated in these events which are attributed to black holes and so on can more easily be explained as the confluence of electric energy from different parts of the same galaxy or even from other parts of the universe.  And it is this explosive release of energy which forms the vast jets that we see lit up and exploding from the centers of active galaxies.&quot;</i><br /><br />Don Scott again at <a href=\"https://youtu.be/5AUA7XS0TvA?t=1622\" class=\"ot-anchor\">https://youtu.be/5AUA7XS0TvA?t=1622</a> ...<br /><br /><i>&quot;We also have observed double radio sources -- either side of galaxies -- and </i><b><i>that event was predicted by Hannes Alfven, the founding father of modern electrical cosmology, and is still, to this day, unexplained by standard cosmology.&quot;</i></b><br /><br />And the predictable outcome, observable at <a href=\"http://www.cv.nrao.edu/~abridle/dragnparts.htm\" class=\"ot-anchor\">http://www.cv.nrao.edu/~abridle/dragnparts.htm</a> ...<br /><br /><i>&quot;The term DRAGN is an acronym for &quot;Double Radio Source Associated with a Galactic Nucleus&quot; . It was coined by Patrick Leahy in 1993 (in Jets in Extragalactic Radio Sources, eds. Röser, H.J., &amp; Meisenheimer, K.; Berlin: Springer-Verlag).&quot;</i><br /><br /><i>[...]</i><br /><br /><i>&quot;DRAGNs are formed when an active galactic nucleus produces two persistent, oppositely-directed plasma outflows that contain cosmic ray electrons and magnetic fields. We do not know exactly what these &quot;jet&quot; outflows contain, but they clearly include fast-moving electrons and partially ordered magnetic fields which make them visible at radio (and higher) frequencies by their synchrotron radiation (emission that occurs when electrons move across a magnetic field at almost the velocity of light). To preserve overall charge neutrality, there must also be either protons or positrons in the outflows.</i><br /><br /><b><i>The process that launches the plasma jets is thought to be the winding up of magnetic fields in an accretion disk that forms around a massive black hole in the galactic nucleus. The winding of the fields provides a way to convert gravitational and rotational energy into bulk outflows at high speeds perpendicular to the disk.&quot;</i></b><br /><br />From Evolution of the plasma universe. I - Double radio galaxies, quasars, and extragalactic jets by Anthony Peratt:<br /><br /><b><i>&quot;The existence of double radio galaxies presents a major challenge to cosmological theories.</i></b><i> The discovery of discrete radio sources dates back to the pioneering survey of Reber (1944), who found two areas of enhanced intensity in Cygnus and Cassiopeia [59]-[62].  Cygnus A, the brightest radio source in the constellation Cygnus, has proved to be the &#39;prototype&#39; of double radio galaxies, and models of double radio galaxies are usually based on the characteristics of this source ... </i><b><i>regardless of whatever ingredients are postulated as necessary in models used to &#39;explain&#39; their existence, what is observed from any radio source is synchrotron radiation, which requires only the presence of relativistic electrons in a magnetic field.&quot;</i></b><br /><br />And now a question:<br /><br /><b>If it had been standard practice for these theorists to document their expectations, before they had a chance to fit their conclusions to observations, wouldn&#39;t it be more obvious that the black hole idea is simply whatever is observed?</b><br /><br /><a href=\"http://nautil.us/issue/24/error/the-trouble-with-scientists\" class=\"ot-anchor\">http://nautil.us/issue/24/error/the-trouble-with-scientists</a><br /><br /><b><i>&quot;The Trouble With Scientists</i></b><br /><i>How one psychologist is tackling human biases in science.</i><br /><br /><i>BY Philip Ball</i><br /><br />[this is the second half of the article]<br /><br /><i>Surprisingly, Nosek thinks that one of the most effective solutions to cognitive bias in science could come from the discipline that has weathered some of the heaviest criticism recently for its error-prone and self-deluding ways: pharmacology. </i><b><i>It is precisely because these problems are so manifest in the pharmaceutical industry that this community is, in Nosek’s view, way ahead of the rest of science in dealing with them.</i></b><i> For example, because of the known tendency of drug companies and their collaborators to report positive results of trials and to soft-pedal negative ones, </i><b><i>it is now a legal requirement in the Unites States for all clinical trials to be entered in a registry before they begin. This obliges the researchers to report the results whatever they say.</i></b><br /><br /><b><i>Nosek has instituted a similar pre-registration scheme for research called the Open Science Framework (OSF).</i></b><i> He had planned it for many years, but it really took off when former software developer Jeff Spies joined his lab in 2009-2010 and took it on as a dissertation project. &#39;Lots of people got involved and it became a much bigger thing pretty quickly,&#39; says Nosek. &#39;We started a website for the OSF, and a community—and funders—gathered around it.&#39; Nosek and Spies cofounded the Center for Open Science in Charlottesville in 2013, which now administers the OSF and is able to offer its services for free.</i><br /><br /><b><i>The idea, says Nosek, is that researchers &#39;write down in advance what their study is for and what they think will happen.&#39; Then when they do their experiments, they agree to be bound to analyzing the results strictly within the confines of that original plan. It sounds utterly elementary, like the kind of thing we teach children about how to do science. And indeed it is -- but it is rarely what happens. Instead, as Fiedler testifies, the analysis gets made on the basis of all kinds of unstated and usually unconscious assumptions about what would or wouldn’t be seen. Nosek says that researchers who have used the OSF have often been amazed at how, by the time they come to look at their results, the project has diverged from the original aims they’d stated.</i></b><br /><br /><i>Fiedler has used the service and says that not only does it keep the research honest but it makes it run more smoothly. </i><b><i>&#39;Pre-registration at the OSF forces me to think through all the details upfront, and the project, as well as some of the writing, is already done before I even start collecting the data,&#39; she says. &#39;Having this awareness helps me to separate which results I trust and which ones I trust less.&#39; And not just her: Making the whole process transparent &#39;gives every other researcher the chance to judge if this result is worth their valuable research time.&#39;</i></b><br /><br /><i>Stating your aims is also a good way of checking that you know what they are, says Hartgerink, who is also an OSF user. </i><b><i>&#39;Once we decided to do this, we noticed that explicating the hypotheses was difficult in itself&#39; -- an indication that they hadn’t actually been formulated clearly enough. &#39;Pre-registration is technically a must if you want to test hypotheses,&#39; he concludes. Fiedler says that for the past year she and all of her Ph.D. students have used the OSF scheme. &#39;I have learned so much by doing it that I can only recommend it to everyone in our line of work,&#39; she avers.</i></b><br /><br /><b><i>The distinction between OSF and business as usual is considerable, says Hartgerink. Since most researchers write their manuscripts only after having conducted the study, hypotheses are not written down explicitly before. &#39;This results in more favorable formulations of the hypothesis once results are known.&#39;</i></b><i> Psychologist Ernest O’Boyle of the University of Iowa and his coworkers have dubbed this bias to make the retrospective presentation of results more beautiful the &#39;Chrysalis effect.&#39; One consequence, Hartgerink says, is that </i><b><i>it is common to present unexpected results as expected. &#39;Ask anyone in the general public whether it is OK to do that, and they will say it is not. Yet this has been the common thing to do in science for a long time.&#39;</i></b><br /><br /><b><i>Often, this shift in hypotheses and goals just happens, without intention and even without recognition. &#39;Within the sometimes long process of designing an experiment, collecting the data, analyzing it, and presenting the results to our scientific colleagues, our way of looking at a question and the corresponding results evolves,&#39; says Fiedler. &#39;Along the way we might forget about the original tests that failed, and present our new insights as answering different questions based on the same data.&#39; This approach to science has a lot of value, she says: It’s important to discover unforeseen connections. But not only does this shift the goalposts of the research, it can also lead researchers to &#39;put too much trust in maybe spurious effects.&#39; OSF forces researchers to leave their goalposts where they are.</i></b><br /><br /><i>But if you elect to constrain yourself to a narrow set of objectives before you’ve even done the experiments, don’t you close off potentially fertile avenues that you couldn’t have foreseen? Maybe, says Nosek, but &#39;learning from the data&#39; is not the way to reach reliable conclusions. </i><b><i>&#39;At present we mix up exploratory and confirmatory research,&#39; he says. &#39;One basic fact that is always getting forgotten is that you can’t generate hypotheses and test them with the same data.&#39; If you find an interesting new lead, you should follow that up separately, not somehow tell yourself that this is what the experiment was about all along.</i></b><br /><br /><i>Fiedler disputes the accusation that pre-registration will kill creativity and freedom. &#39;It’s not something everybody always has to do,&#39; she says, and exploratory research that collects data without a definite agenda of hypothesis testing still has an important place. But we need to keep the distinctions in view.</i><br /><br /><i>The major obstacle, Hartgerink thinks, is education: Researchers are simply not advised to do things this way. But they had better be. </i><b><i>&#39;If younger researchers do not start applying these techniques now,&#39; he says, &#39;they might find themselves on the backbenches in 10 years, because it is becoming the norm to do your research in a reproducible, transparent, and open manner.&#39;</i></b><br /><br /><i>Ultimately, Nosek has his eyes on a &#39;scientific utopia,&#39; in which science becomes a much more efficient means of knowledge accumulation. Nobody claims that OSF will be the panacea that gets us there, however. As Oransky says, </i><b><i>&#39;One of the larger issues is getting scientists to stop fooling themselves.</i></b><i> This requires elimination of motivated reasoning and confirmation bias, and I haven’t seen any good solutions for that.&#39; So along with OSF, Nosek believes the necessary restructuring includes open-access publication, and open and continuous peer review. We can’t get rid of our biases, perhaps, but we can soften their siren call. As Nosek and his colleague, psychologist Yoav Bar-Anan of Ben-Gurion University in Israel, have said, </i><b><i>&#39;The critical barriers to change are not technical or financial; they are social. Although scientists guard the status quo, they also have the power to change it.&#39;</i></b><br /><br /><i>Philip Ball is the author of Invisible: The Dangerous Allure of the Unseen and many books on science and art.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"Tribalism in Science","summary":"There are Many Examples in the History of Science of Irrational Beliefs Promoted by Famous Thinkers and Adopted by Loyal Disciples / Anyone Who Questioned the Prevailing Belief Would Upset the Peace of the Community / Real Advances in Science Require a Different Cultural Tradition, with Individuals Who Invent New Tools to Explore Nature and Are Not Afraid to Question Authority","image":"https://lh3.googleusercontent.com/-BAlbzD4DWts/VvytRAiW0YI/AAAAAAAAHhA/jqbENzHTPFoH6k7vTVVLHlroTaWo6iEvg/w1800-h2520/tribalism-in-science-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/ACzvWFMXtcS","publishDate":"2016-03-31T04:54:25.279Z","updateDate":"2017-01-03T02:39:34.356Z","text":"<b>Tribalism in Science: There are Many Examples in the History of Science of Irrational Beliefs Promoted by Famous Thinkers and Adopted by Loyal Disciples / Anyone Who Questioned the Prevailing Belief Would Upset the Peace of the Community / Real Advances in Science Require a Different Cultural Tradition, with Individuals Who Invent New Tools to Explore Nature and Are Not Afraid to Question Authority</b><br /><br />By Freeman Dyson at <a href=\"http://www.thegwpf.org/content/uploads/2015/10/benefits1.pdf\" class=\"ot-anchor\">http://www.thegwpf.org/content/uploads/2015/10/benefits1.pdf</a><br /><br /><i>&quot;There are many examples in the history of science of irrational beliefs promoted by famous thinkers and adopted by loyal disciples. Sometimes, as in the use of bleeding as a treatment for various diseases, irrational belief did harm to a large number of human victims. George Washington was one of the victims. Other irrational beliefs, such as the phlogiston theory of burning or the Aristotelian cosmology of circular celestial motions, only did harm by delaying the careful examination of nature. </i><b><i>In all these cases, we see a community of people happily united in a false belief that brought leaders and followers together. Anyone who questioned the prevailing belief would upset the peace of the community.</i></b><br /><br /><b><i>Real advances in science require a different cultural tradition, with individuals who invent new tools to explore nature and are not afraid to question authority. Science driven by rebels and heretics searching for truth has made great progress in the last three centuries. But the new culture of scientific scepticism is a recent growth and has not yet penetrated deeply into our thinking. The old culture of group loyalty and dogmatic belief is still alive under the surface, guiding the thoughts of scientists as well as the opinions of ordinary citizens.</i></b><br /><br /><i>To understand human behavior, I look at human evolution. About a hundred thousand years ago, our species invented a new kind of evolution. In addition to biological evolution based on genetic changes, we began a cultural evolution based on social and intellectual changes. Biological evolution did not stop, but cultural evolution was much faster and quickly became dominant. Social customs and beliefs change and spread much more rapidly than genes.</i><br /><br /><i>Cultural evolution was enabled by spoken languages and tribal loyalties. Tribe competed with tribe and culture with culture. The cultures that prevailed were those that promoted tribal cohesion. Humans were always social animals, and culture made us even more social. We evolved to feel at home in a group that thinks alike. It was more important for a group of humans to be united than to be right. It was always dangerous and usually undesirable to question authority. When authority was seriously threatened, heretics were burned at the stake.</i><br /><br /><b><i>I am suggesting that the thinking of politicians and scientists about controversial issues today is still tribal. Science and politics are not essentially different from other aspects of human culture. Science and politics are products of cultural evolution. Thinking about scientific questions is still presented to the public as a competitive sport with winners and losers. For players of the sport with public reputations to defend, it is more important to belong to a winning team than to examine the evidence.</i></b><i> Cultural evolution was centered for a hundred thousand years on tales told by elders to children sitting around the cave fire. That cave-fire evolution gave us brains that are wonderfully sensitive to fable and fantasy, but insensitive to facts and figures. To enable a tribe to prevail in the harsh world of predators and prey, it was helpful to have brains with strong emotional bonding to shared songs and stories. It was not helpful to have brains questioning whether the stories were true. Our scientists and politicians of the modern age evolved recently from the cave-children. </i><b><i>They still, as Charles Darwin remarked about human beings in general, bear the indelible stamp of their lowly origin.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The Journal Oligopoly","summary":"The Scientific Journal is the Most Profitable Obsolete Technology in History / Over the Past 50 Years, as Federal Research Funding has Encouraged Specialization, Journal Publishing has Become Commercialized, and Some Parts of the Scientific and Technical Literature are Now Being Monopolized by Multinational Publishing Conglomerates / Young Researchers Need to Publish in Prestigious Journals to Gain Tenure / Older Researchers Need to do the Same in Order to Keep their Grants / In this Environment, Publishing in a High Impact Elsevier or Springer Journal is What 'Counts'","image":"https://lh3.googleusercontent.com/-yrPjNxycwRE/VvqYo71Cv3I/AAAAAAAAHgE/IhGsTaqyV-Ip9x16gi_IRGAmTKL8GDKjQ/w1800-h2520/the-journal-oligopily-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Dxq5nJgeDGo","publishDate":"2016-03-29T15:02:25.719Z","updateDate":"2017-01-03T02:39:58.720Z","text":"<b>The Journal Oligopoly: The Scientific Journal is the Most Profitable Obsolete Technology in History / Over the Past 50 Years, as Federal Research Funding has Encouraged Specialization, Journal Publishing has Become Commercialized, and Some Parts of the Scientific and Technical Literature are Now Being Monopolized by Multinational Publishing Conglomerates / Young Researchers Need to Publish in Prestigious Journals to Gain Tenure / Older Researchers Need to do the Same in Order to Keep their Grants / In this Environment, Publishing in a High Impact Elsevier or Springer Journal is What &#39;Counts&#39;</b><br /><br />From <b>The Oligopoly of Academic Publishers in the Digital Era</b> at <a href=\"http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0127502&amp;representation=PDF\" class=\"ot-anchor\">http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0127502&amp;representation=PDF</a><br /><br /><i>&quot;This year (2015) marks the 350th anniversary of the creation of scientific journals. Indeed, it was in 1665 that the Journal des Sçavans and the Philosophical Transactions of the Royal Society of London were first published, in France and in England respectively. </i><b><i>They were founded with the intent to advance scientific knowledge by building on colleagues’ results and avoid duplication of results, and established both the principles of scientific priority and peer review. They changed the process of scholarly communication fundamentally, from personal correspondence through letters (which had become &#39;too much for one man to cope with in his daily reading and correspondence&#39;) [1], society meetings, and books to a more structured and regular distribution of scientific advancements. This structured form, combined with a regular and wide dissemination, enabled systematic recording and archiving of scientific knowledge [1–4].&quot;</i></b><br /><br />Note: It&#39;s important to note that peer review did not become common until much later.<br /><br /><i>&quot;Since the 17th century, the importance of journals for diffusing the results of scientific research has increased considerably. After coexisting alongside correspondence, monographs and treaties -- which often took several years to be published -- they became, at the beginning of the 19th century, the fastest and most convenient way of disseminating new research results [5–7] and their number grew exponentially [1,8]. During the 20th century they consolidated their position as the main media for diffusing research [6], especially in the natural and medical sciences [9]. </i><b><i>Scholarly journals also contributed to the professionalization of scientific activities by delimiting the frontier between popular science and the research front and, as a consequence, increased the level of specialization of research and the formation of disciplines.</i></b><i> Interestingly, while the majority of periodicals emerged from scientific societies, a significant proportion were published by commercial ventures as early as in the Victorian era. At that time, these commercial publishing houses proved more efficient in diffusing them than scientific societies [10]. However, prior to World War II, most scholarly journals were still published by scientific societies [11].</i><br /><br /><i>[...]</i><br /><br /><i>As Lyman and Chodorow [25] put it:</i><br /><br /><b><i>&#39;University presses and disciplinary associations were founded to disseminate research in the original cycle of scholarly communication. The faculty produced the work to be published; non-profit publishers organized the distribution of knowledge; the university library bought the published work at an artificially high price, as a subsidy for learned societies; and the faculty used this literature as the foundation for further research and teaching. [...] However, over the past fifty years, as federal research funding has encouraged specialization, journal publishing has become commercialized, and some parts of the scientific and technical literature are now being monopolized by multinational publishing conglomerates.&#39; (p. 89)</i></b><br /><br /><i>[...]</i><br /><br /><b><i>The possibility to increase profits in such an extreme fashion lies in the peculiarity of the economics of scholarly publishing. Unlike usual suppliers, authors provide their goods without financial compensation and consumers (i.e. readers) are isolated from the purchase. Because purchase and use are not directly linked, price fluctuations do not influence demand. Academic libraries, contributing 68% to 75% of journal publishing revenues [31], are atypical buyers because their purchases are mainly controlled by budgets. Regardless of their information needs, they have to manage with less as prices increase. Due to the publisher’s oligopoly, libraries are more or less helpless, for in scholarly publishing each product represents a unique value and cannot be replaced [19,20,33,34].</i></b><br /><br /><i>[...]</i><br /><br /><i>Since the creation of scientific journals 350 years ago, large commercial publishing houses have increased their control of the science system. The proportion of the scientific output published in journals under their ownership has risen steadily over the past 40 years, and even more so since the advent of the digital era. The value added, however, has not followed a similar trend. While one could argue that their role of typesetting, printing, and diffusion were central in the print world [20,7], the ease with which these function can be fulfilled -- or are no longer necessary -- in the electronic world makes one wonder: what do we need publishers for? What is it that they provide that is so essential to the scientific community that we collectively agree to devote an increasingly large proportion of our universities budgets to them? Of course, most journals rely on publishers’ systems to handle and review the manuscripts; however, while these systems facilitate the process, it is the researchers as part of the scientific community who perform peer review. Hence, this essential step of quality control is not a value added by the publishers but by the scientific community itself.</i><br /><br /><b><i>Thus, it is up to the scientific community to change the system</i></b><br /><br />[...]<br /><br /><b><i>Unfortunately, researchers are still dependent on one essentially symbolic function of publishers, which is to allocate academic capital, thereby explaining why the scientific community is so dependent on ‘The Most Profitable Obsolete Technology in History’ [48]. Young researchers need to publish in prestigious journals to gain tenure, while older researchers need to do the same in order to keep their grants, and, in this environment, publishing in a high impact Elsevier or Springer journal is what ‘counts’. In this general context, the negative effect of various bibliometric indicators in the evaluation of individual researchers cannot be understated. The counting of papers indexed by large-scale bibliometric databases -— which mainly cover journals published by commercial publishers, as we have seen in this paper -— creates a strong incentive for researchers to publish in these journals, and thus reinforces the control of commercial publishers on the scientific community.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"Electric Discharge Machining","summary":"If There was No Atmosphere, then What Would Stop the Charged Particles that Cause the Aurora from Touching the Ground? / And if they Did, What Would Happen to the Ground? / There is One Potential Explanation that Can Tie Together a Dozen Observations","image":"https://lh3.googleusercontent.com/-EVuLKA-YHp4/VvchQ6trU8I/AAAAAAAAHe8/PqkSkr_V4qIZ8EptPIa4Utw8N5WwsGlew/w6250-h8750/electric-discharge-machining-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/eW8EXQEB4pJ","publishDate":"2016-03-27T01:04:15.203Z","updateDate":"2017-01-03T02:40:20.480Z","text":"<b>Electric Discharge Machining: If There was No Atmosphere, then What Would Stop the Charged Particles that Cause the Aurora from Touching the Ground? / And if they Did, What Would Happen to the Ground? / There is One Potential Explanation that Can Tie Together a Dozen Observations</b><br /><br />GRAPHIC REFERENCES:<br /><br />[1] <a href=\"http://www.esa.int/var/esa/storage/images/esa_multimedia/images/2012/04/pit-chains_at_high_resolution/9975780-2-eng-GB/Pit-chains_at_high_resolution.jpg\" class=\"ot-anchor\">http://www.esa.int/var/esa/storage/images/esa_multimedia/images/2012/04/pit-chains_at_high_resolution/9975780-2-eng-GB/Pit-chains_at_high_resolution.jpg</a><br /><br />[2] <a href=\"http://www.esa.int/Our_Activities/Space_Science/Rosetta/Lutetia_s_dark_side_hosts_hidden_crater\" class=\"ot-anchor\">http://www.esa.int/Our_Activities/Space_Science/Rosetta/Lutetia_s_dark_side_hosts_hidden_crater</a><br /><br />[3] <a href=\"http://www.thunderbolts.info/tpod/2005/arch05/050607arsamons.htm\" class=\"ot-anchor\">http://www.thunderbolts.info/tpod/2005/arch05/050607arsamons.htm</a>, image here - <a href=\"http://www.thunderbolts.info/tpod/2005/images05/050607edm.jpg\" class=\"ot-anchor\">http://www.thunderbolts.info/tpod/2005/images05/050607edm.jpg</a><br /><br />Various quotes here come from ...<br /><br /><a href=\"https://www.thunderbolts.info/tpod/2005/arch05/050607arsamons.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2005/arch05/050607arsamons.htm</a><br /><br />... on the right vs on the left ...<br /><br /><a href=\"http://www.dailymail.co.uk/sciencetech/article-2125980/Volcanic-pits-detected-Mars-offer-save-haven-life-scientists-believe--a.html\" class=\"ot-anchor\">http://www.dailymail.co.uk/sciencetech/article-2125980/Volcanic-pits-detected-Mars-offer-save-haven-life-scientists-believe--a.html</a><br /><br />[4] <a href=\"http://www.funkyscience.net/old/imagebank/images_big/river_baltisvallis.jpg\" class=\"ot-anchor\">http://www.funkyscience.net/old/imagebank/images_big/river_baltisvallis.jpg</a><br /><br />[5] <a href=\"https://www.thunderbolts.info/tpod/2005/arch05/050203venusriver.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2005/arch05/050203venusriver.htm</a><br /><br />[6] Voyager spacecraft, <a href=\"http://pirlwww.lpl.arizona.edu/~hoppa/science.html\" class=\"ot-anchor\">http://pirlwww.lpl.arizona.edu/~hoppa/science.html</a><br /><br />[7] <a href=\"http://commons.wikimedia.org/wiki/File:Cycloid_%28PSF%29.png\" class=\"ot-anchor\">http://commons.wikimedia.org/wiki/File:Cycloid_(PSF).png</a><br /><br />[8] <a href=\"https://www.thunderbolts.info/tpod/2004/arch/041019europa.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2004/arch/041019europa.htm</a>, top - Wal Thornhill, bottom - Galileo Project, JPL, NASA<br /><br />[9] <a href=\"http://photojournal.jpl.nasa.gov/jpeg/PIA01664.jpg\" class=\"ot-anchor\">http://photojournal.jpl.nasa.gov/jpeg/PIA01664.jpg</a><br /><br />[10] NASA/JPL/Galileo imaging team, <a href=\"https://www.thunderbolts.info/tpod/2005/arch05/050314europadynamics.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2005/arch05/050314europadynamics.htm</a><br /><br />[11] Snapshot from <i>Symbols of an Alien Sky 2</i> at <a href=\"https://youtu.be/tRV1e5_tB6Y?t=2191\" class=\"ot-anchor\">https://youtu.be/tRV1e5_tB6Y?t=2191</a><br /><br />[12] EDM experiment from <i>Symbols of an Alien Sky 2</i> at <a href=\"https://youtu.be/tRV1e5_tB6Y?t=2131\" class=\"ot-anchor\">https://youtu.be/tRV1e5_tB6Y?t=2131</a><br /><br />[13] <a href=\"http://www.arranmuseum.co.uk/Geology%20Pages/Virtual%20Field%20Trips/fulgurite.htm\" class=\"ot-anchor\">http://www.arranmuseum.co.uk/Geology%20Pages/Virtual%20Field%20Trips/fulgurite.htm</a><br /><br />[14] <a href=\"http://www.arranmuseum.co.uk/Geology%20Pages/Virtual%20Field%20Trips/fulgurite.htm\" class=\"ot-anchor\">http://www.arranmuseum.co.uk/Geology%20Pages/Virtual%20Field%20Trips/fulgurite.htm</a><br /><br />[15] <a href=\"http://www.thunderbolts.info/tpod/2004/arch/040705olympus-mons.htm\" class=\"ot-anchor\">http://www.thunderbolts.info/tpod/2004/arch/040705olympus-mons.htm</a><br /><br />[16] <a href=\"http://eurjmin.geoscienceworld.org/content/21/4/783/F3.large.jpg\" class=\"ot-anchor\">http://eurjmin.geoscienceworld.org/content/21/4/783/F3.large.jpg</a><br /><br />[17] <a href=\"http://www.thunderbolts.info/tpod/2005/arch05/050721dustdevils.htm\" class=\"ot-anchor\">http://www.thunderbolts.info/tpod/2005/arch05/050721dustdevils.htm</a><br /><br />[18] <a href=\"http://mepag.jpl.nasa.gov/topten.cfm?topten=6\" class=\"ot-anchor\">http://mepag.jpl.nasa.gov/topten.cfm?topten=6</a><br /><br />[19] <a href=\"http://blogs.agu.org/martianchronicles/2012/03/08/spectacular-martian-dust-devilghost-worm/\" class=\"ot-anchor\">http://blogs.agu.org/martianchronicles/2012/03/08/spectacular-martian-dust-devilghost-worm/</a><br /><br />[20] [21] <a href=\"https://www.thunderbolts.info/tpod/2007/arch07/071025dustdevils.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2007/arch07/071025dustdevils.htm</a><br /><br />[22] <a href=\"http://www.sciencedirect.com/science/article/pii/S0019103514005132\" class=\"ot-anchor\">http://www.sciencedirect.com/science/article/pii/S0019103514005132</a><br /><br />[23] [24] Google Earth<br /><br />The quotes here come from ...<br /><br /><a href=\"http://www.nytimes.com/2000/06/06/science/making-sense-of-grand-canyon-s-puzzles.html\" class=\"ot-anchor\">http://www.nytimes.com/2000/06/06/science/making-sense-of-grand-canyon-s-puzzles.html</a><br /><br /><a href=\"http://thunderbolts.info/tpod/2008/arch08/080929grandcanyon.htm\" class=\"ot-anchor\">http://thunderbolts.info/tpod/2008/arch08/080929grandcanyon.htm</a><br /><br />[25] <a href=\"http://geyserofawesome.com/post/98201707502/lightning-is-awesome-and-today-the-department-of\" class=\"ot-anchor\">http://geyserofawesome.com/post/98201707502/lightning-is-awesome-and-today-the-department-of</a><br /><br />[26] <a href=\"http://www.koaa.com/news/rare-atmospheric-electricity-seen-over-new-mexico/\" class=\"ot-anchor\">http://www.koaa.com/news/rare-atmospheric-electricity-seen-over-new-mexico/</a><br /><br />[27] <a href=\"http://images.wikia.com/narutofanon/images/0/0c/Lrg-56-lightning-red-sprites-elves-halo-ionosphere-upper-atmosphere.jpg\" class=\"ot-anchor\">http://images.wikia.com/narutofanon/images/0/0c/Lrg-56-lightning-red-sprites-elves-halo-ionosphere-upper-atmosphere.jpg</a><br /><br />[28] <a href=\"https://directory.eoportal.org/web/eoportal/satellite-missions/c-missions/chibis-m\" class=\"ot-anchor\">https://directory.eoportal.org/web/eoportal/satellite-missions/c-missions/chibis-m</a><br /><br />[29] <a href=\"http://news.psu.edu/story/314975/2014/05/07/research/sprites-form-plasma-irregularities-lower-ionosphere\" class=\"ot-anchor\">http://news.psu.edu/story/314975/2014/05/07/research/sprites-form-plasma-irregularities-lower-ionosphere</a><br /><br />Quotes from ...<br /><br /><a href=\"http://news.psu.edu/story/314975/2014/05/07/research/sprites-form-plasma-irregularities-lower-ionosphere\" class=\"ot-anchor\">http://news.psu.edu/story/314975/2014/05/07/research/sprites-form-plasma-irregularities-lower-ionosphere</a><br /><br /><a href=\"http://www.nasa.gov/home/hqnews/2005/mar/HQ_05070_radiation_belt.html#.VDtrQL789ho\" class=\"ot-anchor\">http://www.nasa.gov/home/hqnews/2005/mar/HQ_05070_radiation_belt.html#.VDtrQL789ho</a><br /><br /><a href=\"http://www.rense.com/general68/gint.htm\" class=\"ot-anchor\">http://www.rense.com/general68/gint.htm</a><br /><br /><a href=\"http://www.iop.org/news/14/may/page_63245.html\" class=\"ot-anchor\">http://www.iop.org/news/14/may/page_63245.html</a><br /><br />[30] <a href=\"https://www.thunderbolts.info/wp/2011/12/03/essential-guide-to-the-eu-chapter-5/saturn-enceladus-flux-tube-dgm/\" class=\"ot-anchor\">https://www.thunderbolts.info/wp/2011/12/03/essential-guide-to-the-eu-chapter-5/saturn-enceladus-flux-tube-dgm/</a><br /><br />[31] <a href=\"http://www.jpl.nasa.gov/images/cassini/20100812-b/cassini20100812-b-full.jpg\" class=\"ot-anchor\">http://www.jpl.nasa.gov/images/cassini/20100812-b/cassini20100812-b-full.jpg</a><br /><br />[32] <a href=\"http://www.holoscience.com/wp/wp-content/uploads/2012/04/Enceladus-temp.jpg\" class=\"ot-anchor\">http://www.holoscience.com/wp/wp-content/uploads/2012/04/Enceladus-temp.jpg</a><br /><br />Quotes from ...<br /><br /><a href=\"http://www.universetoday.com/29580/electrically-charged-particles-found-in-enceladus-plumes/\" class=\"ot-anchor\">http://www.universetoday.com/29580/electrically-charged-particles-found-in-enceladus-plumes/</a><br /><br /><a href=\"http://www.thunderbolts.info/tpod/2006/arch06/060313moonjets.htm\" class=\"ot-anchor\">http://www.thunderbolts.info/tpod/2006/arch06/060313moonjets.htm</a><br /><br /><a href=\"http://www.space.com/1437-puzzling-hot-spot-moon-saturn.html\" class=\"ot-anchor\">http://www.space.com/1437-puzzling-hot-spot-moon-saturn.html</a><br /><br /><a href=\"http://science.nasa.gov/science-news/science-at-nasa/2006/09mar_enceladus/\" class=\"ot-anchor\">http://science.nasa.gov/science-news/science-at-nasa/2006/09mar_enceladus/</a><br /><br /><a href=\"http://news.google.com/newspapers?nid=1309&amp;dat=19970413&amp;id=CM1QAAAAIBAJ&amp;sjid=LxUEAAAAIBAJ&amp;pg=6721,4558401\" class=\"ot-anchor\">http://news.google.com/newspapers?nid=1309&amp;dat=19970413&amp;id=CM1QAAAAIBAJ&amp;sjid=LxUEAAAAIBAJ&amp;pg=6721,4558401</a><br /><br />[33] <a href=\"http://lasp.colorado.edu/JUPITER/CH14/Releases/26Feb2001_i25i27tvashtar.html\" class=\"ot-anchor\">http://lasp.colorado.edu/JUPITER/CH14/Releases/26Feb2001_i25i27tvashtar.html</a><br /><br />[34] <a href=\"http://pirlwww.lpl.arizona.edu/~perry/io_images/25ISGIANTS01.png\" class=\"ot-anchor\">http://pirlwww.lpl.arizona.edu/~perry/io_images/25ISGIANTS01.png</a><br /><br />Quotes from here ...<br /><br /><a href=\"http://science.nasa.gov/science-news/science-at-nasa/1999/ast04oct99_1/\" class=\"ot-anchor\">http://science.nasa.gov/science-news/science-at-nasa/1999/ast04oct99_1/</a><br /><br /><a href=\"http://www.jpl.nasa.gov/releases/2001/release_2001_240.html\" class=\"ot-anchor\">http://www.jpl.nasa.gov/releases/2001/release_2001_240.html</a><br /><br /><a href=\"https://www.thunderbolts.info/tpod/2004/arch/041217io-series-4.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2004/arch/041217io-series-4.htm</a><br /><br /><a href=\"http://solarsystem.nasa.gov/planets/profile.cfm?Object=Jup_Io\" class=\"ot-anchor\">http://solarsystem.nasa.gov/planets/profile.cfm?Object=Jup_Io</a><br /><br /><a href=\"http://photojournal.jpl.nasa.gov/catalog/PIA02545\" class=\"ot-anchor\">http://photojournal.jpl.nasa.gov/catalog/PIA02545</a><br /><br /><a href=\"https://www.thunderbolts.info/wp/2012/07/01/io/\" class=\"ot-anchor\">https://www.thunderbolts.info/wp/2012/07/01/io/</a><br /><br />[35] <a href=\"http://science.nasa.gov/science-news/science-at-nasa/2008/20jun_apollorelic/\" class=\"ot-anchor\">http://science.nasa.gov/science-news/science-at-nasa/2008/20jun_apollorelic/</a><br /><br />[36] <a href=\"http://www.ldsfreedomforum.com/viewtopic.php?t=21475\" class=\"ot-anchor\">http://www.ldsfreedomforum.com/viewtopic.php?t=21475</a><br /><br />[37] <a href=\"http://teslamania.delete.org/frames/longarc.htm\" class=\"ot-anchor\">http://teslamania.delete.org/frames/longarc.htm</a><br /><br />[38] <a href=\"http://www.animatedweb.it/imagestore/images/deserti/silicaglass1.jpg\" class=\"ot-anchor\">http://www.animatedweb.it/imagestore/images/deserti/silicaglass1.jpg</a><br /><br />[39] <a href=\"http://www.lpi.usra.edu/meetings/lpsc2004/pdf/1406.pdf\" class=\"ot-anchor\">http://www.lpi.usra.edu/meetings/lpsc2004/pdf/1406.pdf</a><br /><br />Quotes from here ...<br /><br /><a href=\"http://stardust.jpl.nasa.gov/news/news113.html\" class=\"ot-anchor\">http://stardust.jpl.nasa.gov/news/news113.html</a><br /><br /><a href=\"http://www.world-mysteries.com/pex_6.htm\" class=\"ot-anchor\">http://www.world-mysteries.com/pex_6.htm</a><br /><br /><a href=\"http://www.popsci.com/science/article/2013-07/what-does-space-smell\" class=\"ot-anchor\">http://www.popsci.com/science/article/2013-07/what-does-space-smell</a><br /><br /><a href=\"http://en.wikipedia.org/wiki/Fulgurite\" class=\"ot-anchor\">http://en.wikipedia.org/wiki/Fulgurite</a><br /><br />[40] <a href=\"http://blog.world-mysteries.com/wp-content/uploads/2012/09/stone_ball_lg.jpg\" class=\"ot-anchor\">http://blog.world-mysteries.com/wp-content/uploads/2012/09/stone_ball_lg.jpg</a><br /><br />[41] <a href=\"http://www.in5d.com/out-of-place-artifacts.html\" class=\"ot-anchor\">http://www.in5d.com/out-of-place-artifacts.html</a><br /><br />[42] <a href=\"https://www.thunderbolts.info/tpod/2005/arch05/050325blueberries.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2005/arch05/050325blueberries.htm</a><br /><br />[43] Do a Google search on &quot;martian blueberries&quot; for such images<br /><br />[44] <a href=\"https://www.thunderbolts.info/tpod/2005/arch05/050329domedcraters.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2005/arch05/050329domedcraters.htm</a><br /><br />[45] <a href=\"http://mars.nasa.gov/msl/multimedia/raw/?rawid=0746MR0032080400403454E01_DXXX&amp;s=746\" class=\"ot-anchor\">http://mars.nasa.gov/msl/multimedia/raw/?rawid=0746MR0032080400403454E01_DXXX&amp;s=746</a><br /><br />[46] <a href=\"http://www.wired.com/2013/07/space-environmentalism/all/\" class=\"ot-anchor\">http://www.wired.com/2013/07/space-environmentalism/all/</a><br /><br />[47] <a href=\"http://mars.jpl.nasa.gov/gallery/atlas/valles-marineris.html\" class=\"ot-anchor\">http://mars.jpl.nasa.gov/gallery/atlas/valles-marineris.html</a><br /><br />[48] <a href=\"http://www.pianeta-marte.it/marte_in_cifre/english_guinnes_of_mars.htm\" class=\"ot-anchor\">http://www.pianeta-marte.it/marte_in_cifre/english_guinnes_of_mars.htm</a><br /><br />Quotes from here ...<br /><br /><a href=\"https://www.thunderbolts.info/tpod/2005/arch05/050408marineris.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2005/arch05/050408marineris.htm</a><br /><br /><a href=\"http://www.rense.com/general65/valles.htm\" class=\"ot-anchor\">http://www.rense.com/general65/valles.htm</a><br /><br />[49] <a href=\"http://www.rense.com/general65/valles.htm\" class=\"ot-anchor\">http://www.rense.com/general65/valles.htm</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Lightning to Space","summary":"120 Years of Eyewitness Observations Were Dismissed by Numerous Physicists and Atmospheric Scientists Because their Notions of Lightning Had No Use for these Observations / These Same Scientists Today Tell Us that Whatever the Cause, it's Certainly Not an Electrical Connection to Space / They Hope You Will Not Know that the Same Approach Was Already Tried, and Failed, with the Aurora / What Should Have Raised Perfectly Valid Questions About Earth's Electrical Connection With its Surrounding Space Has Instead Been Transformed Into a Quest to Vindicate the Textbooks, Regardless of the Observations","image":"https://lh3.googleusercontent.com/-EgDqB6Fb408/VvbyIjDR54I/AAAAAAAAHeA/PDY7lI9tsiMcwnAOty6aC1HZfcpPEpj4g/w6250-h8750/lightning-to-space-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/MTP8cEwsnA4","publishDate":"2016-03-26T20:34:07.369Z","updateDate":"2017-01-03T02:40:34.822Z","text":"<b>Lightning to Space: 120 Years of Eyewitness Observations Were Dismissed by Numerous Physicists and Atmospheric Scientists Because their Notions of Lightning Had No Use for these Observations / These Same Scientists Today Tell Us that Whatever the Cause, it&#39;s Certainly Not an Electrical Connection to Space / They Hope You Will Not Know that the Same Approach Was Already Tried, and Failed, with the Aurora / What Should Have Raised Perfectly Valid Questions About Earth&#39;s Electrical Connection With its Surrounding Space Has Instead Been Transformed Into a Quest to Vindicate the Textbooks, Regardless of the Observations</b><br /><br />Most of the attempts to explain upper-atmospheric lightning fail to make any mention at all of the obvious, burning question which Mel Acheson rightly asks at <a href=\"http://www.rense.com/general68/gint.htm\" class=\"ot-anchor\">http://www.rense.com/general68/gint.htm</a>:<br /><br /><b><i>&quot;What are we to think about giant lightning bolts to space? Can we think of space as neutral or empty in the face of these spectacular discharges?</i></b><br /><br /><i>[...]</i><br /><br /><b><i>Investigators finally were able to correlate their observations. They realized that every time there was a sprite above the clouds there was a bolt of positive lightning below the clouds. The sprite and the positive bolt were parts of a single discharge that stretched from space to the Earth&#39;s surface.&quot;</i></b><br /><br />The fact you most need to know about this subject will almost never be mentioned in any of the scientific press releases you read on the subject: The assumption that scientists strictly adhere to today is that the Earth&#39;s electrical activity is self-contained to the Earth.  In this view, the Earth is not electrically connected to its surroundings -- and even though it is simultaneously supposed that the Earth has an iron core and is immersed in a solar wind filled with charged particles!  This core assumption drives every single question that researchers today ask -- or more accurately stated, the ones they refuse to ask.  Nevermind that an exactly identical approach was tried, and already failed, decades ago with the aurora.<br /><br />Kristian Birkeland theorized that the aurora was caused by the Sun.  His biographer, Lucy Jago notes on page 276 of <i>The Northern Lights</i> that <i>&quot;rejection of his theories probably slowed the advance of geomagnetic and auroral physics for nearly half a century.&quot;</i><br /><br />This worldview of an electrically isolated Earth never had any use at all for lightning to space -- which is why they ignored the claims up until the point that they were undeniable.  The pattern which has been observed to date, and which we can expect scientists to continue to follow -- as it is by now an established pattern -- is that they will first attempt to force-fit these awkward, unexpected observations into their electrically isolated Earth worldview.<br /><br />It is of course a tremendous irony that in a world which runs on electricity oftentimes transferred from its source thousands of miles away, that the notion that the Earth might also be electrically connected to its larger electrical environment is somehow controversial.  But, this is definitively the current state of research on upper-atmospheric lightning.<br /><br />Wal Thornhill presents the controversy within this larger context better than any other ...<br /><br /><a href=\"http://www.holoscience.com/wp/the-balloon-goes-up-over-lightning/\" class=\"ot-anchor\">http://www.holoscience.com/wp/the-balloon-goes-up-over-lightning/</a><br /><br /><i>&quot;Few direct measurements have been made of the flashes, dubbed &#39;sprites&#39; by Davis Sentman of the University of Alaska Fairbanks. Sentman chose the name &#39;sprites&#39; for the red flashes because, like woodland elves, they can only be seen out of the corner of your eye. </i><b><i>The thin air of the mesosphere where the sprites appear is too high for research aircraft to fly and too low for orbiting spacecraft to access, so most of what is known comes from low-light video cameras and electromagnetic sensors based in mountain-top labs. Prof. Edgar Bering, a physicist at the University of Houston in Texas, has recently changed that. He heads a team from NASA’s National Scientific Balloon Facility to study sprites by the tricky business of flying a high-altitude balloon above major thunderstorms.</i></b><br /><br /><i>Adapted from Rider on the Storm by Harriet Williams, New Scientist Vol. 172 No. 2321, 15 December 2001</i><br /><br /><i>After more than a decade of disagreement, atmospheric physicists think they are finally close to agreeing on how sprites form. </i><b><i>Bering’s balloon offers one of the first real chances to confirm their theories.</i></b><i> But it seems a storm is brewing that threatens to set Bering against his colleagues. </i><b><i>What little information we have has led to a model of sprite formation that many in this close-knit community of investigators now agree on. It depends on intense but short-lived electric fields created in the atmosphere by lightning discharge.</i></b><br /><br /><i>How a sprite is formed</i><br /><br /><i>Although most lightning originates in the negative charges at the bottom of storm clouds, roughly 1 in every 5 lightning strikes originate in the positive charges near the cloud tops. This results in an energetic positive cloud-to-ground discharge, in which the positive charge is neutralized by an upwards flow of electrons from the ground. </i><b><i>The negative charges left in the lower part of the cloud set up what physicists call a &#39;quasi-electrostatic field&#39; -- an intense electric field that extends high into the atmosphere above the storm.</i></b><br /><br /><i>&#39;There is absolutely no question in my mind,&#39; says Umran Inan, director of the Space, Telecommunications and Radioscience Lab at Stanford University. &#39;Sprites are caused by QE fields.&#39;</i><br /><br /><i>The sprite lies above horizontal (so-called spider) lightning in the lower portion of the upper stratiform cloud. The spider lightning’s large horizontal extent shows the size of the large layer of electric charge that feeds the positive ground flashes. Such lightning flashes are not generally seen in ordinary isolated thunderclouds.</i><br /><br /><i>Other unanswered questions remain. The extraordinarily rapid initial growth of sprites is not well understood. Nor is the pronounced asymmetry between the number of sprites produced by negative lightning and the number produced by positive lightning. Only two sprites have ever been clearly associated with flashes of negative cloud-to-ground lightning, whereas the number of sprites verifiably produced by more energetic positive cloud-to-ground lightning runs to thousands. Obviously, there is still much to learn.</i><br /><br /><i>The critical breakdown limit for air depends on its density. At very high altitudes -- about 75 kilometres up -- where air density is low, the QE field now exceeds the critical breakdown limit for air. Electrical breakdown occurs and molecules such as nitrogen and oxygen are ionised, releasing electrons. Under the influence of the QE field, free electrons are accelerated upwards, while positive ions accelerate down towards the ground.</i><br /><br /><b><i>Edgar Bering’s balloon flights suggest that the currents responsible for sprites may carry far more oomph than anyone had suspected. Previous estimates suggested that the sprite-inducing current carries about 3000 amperes. Bering’s data, on the other hand, puts the figure nearer 12,000 amperes.</i></b><i> Whether this huge current could pose any direct physical danger to anyone is unknown. Airliners don’t fly in the mesosphere, but sprites can reach down into the cloud tops. And it is certainly possible that sprites could affect spacecraft, Bering suggests. Sprites are the prime suspect in the unexplained downing of a high-altitude balloon a few years ago.</i><br /><br /><b><i>Almost as quickly as it appears, the sprite fades away, disappearing completely in just a couple of milliseconds. However, the QE field is believed to last much longer. Researchers on the ground can monitor its presence using radio receivers since the field produces a continuous electromagnetic signal at frequencies from a few hertz to tens of kilohertz. The signal often persists long after the sprite has disappeared, slowly fading as charges in the cloud disperse. This electromagnetic signal, Inan and his colleagues argue, is the signature of the QE field.</i></b><br /><br /><i>But what scientists needed were direct measurements of the electric field. What could be better than information gathered by a balloon flying high above the clouds?</i><br /><br /><b><i>The team scoured the balloon flight results for the signature of a QE field -- the low-frequency radio hum. But they were in for a surprise. The balloon’s instruments did not record it. The researchers realized the favoured model of sprite formation didn’t measure up.</i></b><br /><br /><i>The results from the ground stations suggest that once a positive lightning strike occurs, the intensity of the electric field in the mesosphere builds up over two or three milliseconds until breakdown occurs, and the sprite lights up. This delay may be related to the flow of currents created by the lightning which bring the high-altitude electric field to the level required for breakdown, says Victor Pasko, an atmospheric physicist at Pennsylvania State University in University Park. Then, once the sprite has faded, charges in the clouds begin to disperse or flow away, and the electric field observed from the ground decays slowly over tens of milliseconds.</i><br /><br /><b><i>However, the balloon data paints a very different picture. It implies that sprites are produced by a sudden burst of current and there is no slow build-up of the electric field. Several milliseconds after the positive lightning strike, sensors recorded a sudden upward-flowing current pulse. Just 300 microseconds later, the sprite lit up in the sky. To add to the mystery, the electric field disappeared far more rapidly than ground observations suggest, in just a few milliseconds.</i></b><br /><br /><i>Bering’s results -- some of which he presented at the recent American Geophysical Union meeting in San Francisco -- turns sprite theory on its head. </i><b><i>&#39;The charge that produces sprites is not below in the cloud, it’s in the mesosphere itself,&#39; suggests Bering. So now there are new puzzles: where could this charge be coming from</i></b><i>, and if there’s no QE field, what causes the delay between lightning and sprite? &#39;We have a problem understanding why the sprite takes so long to form,&#39; admits James Benbrook, a colleague of Bering’s in the physics department at the University of Houston.</i><br /><br /><i>And what of the low-frequency hum picked up by labs on the ground? Bering thinks the signal may be caused by the lightning strike itself rather than the mechanism that lights up a sprite. Researchers on the ground face an additional problem, they are close to one electrical contact of the global electric circuit -- the Earth itself. The low-frequency hum could be an artefact and we hear it if we are on the ground when the charges in the clouds flow to earth, Bering suggests.</i><br /><br /><i>Benbrook agrees. The signal received on the ground is more likely due to the rearrangement of charge in the cloud tops, he says, or the flow of current in the lightning channel. &#39;But I don’t see what that has to do necessarily with an excitation mechanism in the mesosphere.&#39;</i><br /><br /><i>Other researchers urge caution in interpreting Bering’s results. &#39;At high altitudes the field can be very small,&#39; says Pasko. Inan suggests that more sensitive instruments on the balloon may have picked up the hum of the QE field. &#39;Whether or not there is a continuing field signature is a matter of how sensitive your measurements are. It could be there but below the noise level of your instrument.&#39;</i><br /><br /><i>Most sprite investigators agree that Bering should have been able to detect the low-frequency hum, and blame his instruments for failing to do so. Bering defends the quality of his experiment and insists his instruments were working. &#39;We wouldn’t have seen the electric signal of the sprite if they weren’t.&#39;</i><br /><br /><i>Can the QE field theory recover from this blow? &#39;My personal guess is no,&#39; says Bering. </i><b><i>&#39;None of the existing models will survive when people finally pay attention to what our data actually says.&#39;</i></b><br /><br /><i>Toward an Electric Universe model of Sprites</i><br /><br /><b><i>The size and color of sprites is simply explained by the very low air pressure at great heights. In the same way that a long spark in a laboratory discharge tube becomes an extended glow as the air is pumped from the tube, so the &#39;sparks&#39; of lightning at ground level become colorful glows and filaments when they occur in the upper atmosphere.</i></b><br /><br /><b><i>In Physics Today, November 2001, Earle R. Williams [1] made the obvious connection in a feature article, Sprites, Elves, and Glow Discharge Tubes. &#39;The venerable field of gaseous electronics underlies the understanding of a lightning-like phenomenon of spectacular extent, shape, and color. Sprites and elves are a grand natural manifestation of ideas and laboratory experiments conceived many decades ago by Rayleigh, Thomson, Wilson, and Langmuir -- all of whom won Nobel prizes -- and by a host of 19th century glow discharge tube spectroscopists.&#39;</i></b><br /><br /><i>[1] Earle Williams is a research scientist at the Massachusetts Institute of Technology in Cambridge. He works at the Parsons Laboratory on the main campus and at Lincoln Laboratory.</i><br /><br /><i>The discharge tube model has been confirmed by many ground-based experiments. </i><b><i>But discharge tubes require a power supply to function. Where is the power supply for sprites? Anyone who says that it is powered by the thunderstorm hasn’t understood the question.</i></b><i> If we don’t understand how a thunderstorm generates lightning then we have much further to go than is generally admitted by researchers.</i><br /><br /><b><i>Bering writes, &#39;from what is known to date, it may be speculated that sprites or jets, or both, are an integral feature of every thunderstorm system of moderate size or larger in the terrestrial system, and may be an essential element of the earth’s global electrical circuit. Further, it seems likely that they have been a part of thunderstorms that have occurred over previous millions of years or longer. One may speculate about the possible occurrences of similar phenomena associated with lightning on other planets where lightning has been detected, most notably Jupiter and Venus.&#39;</i></b><br /><br /><i>Bering’s speculations are well-founded from the Electric Universe point of view but the stumbling block to further understanding is immediately apparent in the use of the words &#39;earth’s global electrical circuit.&#39; </i><b><i>As big as the term &#39;global&#39; sounds, the circuit is too restricted. It is a circuit that assumes heat driven convection in clouds is the global electricity generator. This generator mysteriously separates electric charge in storm clouds to power world-wide currents. However, the circuit is &#39;unplugged.&#39; It is isolated from electrical connection with anything else in the universe. Such a lack of a holistic or cosmic-scale vision is a fundamental constraint on theorists.</i></b><br /><br /><i>In a report on Venusian lightning, following the successful Russian Venera 11 and 12 entry probes, Professor Donald Hunten of the department of Planetary Sciences at the University of Arizona, Tucson, summed up: </i><b><i>&#39;The indications are that lightning is likely to occur in any substantial planetary atmosphere. Theories of electrification are faced with the need to explain its presence under a wide variety of circumstances and atmospheric conditions.&#39; There are no water clouds on Venus.</i></b><br /><br /><i>In July 1993 at the Cambridge, U.K., Conference of the Society for Interdisciplinary Studies I presented a paper about the planet Venus that dealt with reports of lightning. I said:</i><br /><br /><b><i>&#39;The principal difficulty in understanding the origin of lightning is likely to be the assumption that the Earth and Venus are closed electrical systems with no input from the solar plasma environment via the magnetosphere.&#39;</i></b><br /><br /><i>So let us examine the larger picture. There has been one crucial name missing from the earlier list of Nobel Prize winners. He was on the point of being nominated for a Nobel Prize when he died. Kristian Olaf Bernhard Birkeland (1867-1917) was the founder of experimental astrophysics. Note the crucial adjective &#39;experimental&#39; as distinct from modern theoretical astrophysics. He studied under Poincaré and Hertz and was a professor at Oslo University at the age of 31. Wealth and fame accompanied his many achievements in technology and applied physics.</i><br /><br /><b><i>Birkeland was the good guy in a 50-year dispute involving the idea that electrons streaming along magnetic field lines caused the Earth’s auroras. His opponent was the astronomer Sydney Chapman who maintained that the Earth moved through a vacuum. In 1974 space probes found in Birkeland’s favour. Chapman and others then promptly made space plasma superconducting, which relieved them from the complications of dealing with electric fields. Birkeland actually demonstrated his theory long before in an experiment called a &#39;terrella.&#39; It consisted of an electromagnet contained within a sphere and placed in a large vacuum chamber. By initiating an electric discharge in the chamber he was able to reproduce a light show with many of the odd features of auroras. The importance of this simple experiment cannot be overstated because it demonstrates that aurorae and lightning seem to require an electrical power source external to the Earth! That would explain the puzzle raised by Bering: &#39;The charge that produces sprites is not below in the cloud, it’s in the mesosphere itself.&#39;</i></b><br /><br /><i>The Electric Universe model suggests that the Earth plays a cathode role in the Sun’s discharge and therefore is in the business of supplying negative electrons to space and receiving positive ions from the solar wind. It is interesting therefore that the presence of solar wind ions inside the earth’s magnetosphere has puzzled scientists. </i><b><i>Thunderstorms are not electricity generators, they are passive elements in an interplanetary circuit, like a self-repairing leaky condenser. The energy stored in the cloud &#39;condenser&#39; is released as lightning when it short-circuits. The short-circuits can occur either within the cloud or across the external resistive paths to Earth or the ionosphere. The charge across the cloud &#39;condenser&#39; gives rise to violent vertical electrical winds within the cloud, not vice versa. By creating a short-circuit to high altitudes in the storm the lightning effectively &#39;throws the switch&#39; connected to the glow discharge &#39;tube&#39; in the upper atmosphere. It then makes perfect sense that the much taller positive cloud-to-ground discharge will be more effective at providing power to the glow discharge than will low-level negative cloud-to-ground lightning because the circuit resistance is lower. Ultimately, lightning on Earth is driven by electric power focused on the Sun but minutely intercepted by the Earth. So lightning on Earth is a pale imitation of what is happening on the Sun.&quot;</i></b><br /><br />This provides the introduction that I was hoping to present on the subject, but it&#39;s not the end of Wal&#39;s argument.  See <a href=\"http://www.holoscience.com/wp/the-balloon-goes-up-over-lightning/\" class=\"ot-anchor\">http://www.holoscience.com/wp/the-balloon-goes-up-over-lightning/</a> for his explanation of the interplanetary portion of lightning&#39;s larger circuit.<br /><br />GRAPHIC REFERENCES:<br /><br />[1] [2] <a href=\"https://www.nasa.gov/pdf/24888main_SBIR_Final_Rpt.pdf\" class=\"ot-anchor\">https://www.nasa.gov/pdf/24888main_SBIR_Final_Rpt.pdf</a><br /><br />[3] From a text titled <i>Sprites, Elves and Intense Lightning Discharges</i>, in a section titled &quot;A Brief History of TLE Observations&quot;<br /><br />[4] R.C. Franz, R.J. Nemzek, J.R. Winckler, &quot;Television Image of a Large Upward Electrical Discharge Above a Thunderstorm System&quot;, <i>Science Magazine</i>, 1990<br /><br />[5] <a href=\"https://www.nasa.gov/pdf/24888main_SBIR_Final_Rpt.pdf\" class=\"ot-anchor\">https://www.nasa.gov/pdf/24888main_SBIR_Final_Rpt.pdf</a><br /><br />[6] From a text titled <i>Sprites, Elves and Intense Lightning Discharges</i>, in a section titled &quot;A Brief History of TLE Observations&quot;<br /><br />[7] <a href=\"http://starbaseconnection.ning.com/m/group/discussion?id=6606844%3ATopic%3A103500\" class=\"ot-anchor\">http://starbaseconnection.ning.com/m/group/discussion?id=6606844%3ATopic%3A103500</a><br /><br />[8] This is from the May 1995 issue of <i>Sky &amp; Telescope</i> magazine<br /><br />[9] <a href=\"https://www.nasa.gov/pdf/24888main_SBIR_Final_Rpt.pdf\" class=\"ot-anchor\">https://www.nasa.gov/pdf/24888main_SBIR_Final_Rpt.pdf</a><br /><br />[10] From a text titled <i>Sprites, Elves and Intense Lightning Discharges</i>, in a section titled &quot;A Brief History of TLE Observations&quot;<br /><br />[11] <a href=\"https://weather.com/news/news/transient-luminous-events-mysteries-sky-20130731\" class=\"ot-anchor\">https://weather.com/news/news/transient-luminous-events-mysteries-sky-20130731</a><br /><br />[12] The original video is also at <a href=\"http://pasko.ee.psu.edu/Nature/\" class=\"ot-anchor\">http://pasko.ee.psu.edu/Nature/</a><br /><br />[13] From a text titled <i>Sprites, Elves and Intense Lightning Discharges</i>, in a section titled &quot;A Brief History of TLE Observations&quot;<br /><br />[14] <a href=\"http://news.psu.edu/story/314975/2014/05/07/research/sprites-form-plasma-irregularities-lower-ionosphere\" class=\"ot-anchor\">http://news.psu.edu/story/314975/2014/05/07/research/sprites-form-plasma-irregularities-lower-ionosphere</a><br /><br />[15] <a href=\"http://spp.astro.umd.edu/research.php.html\" class=\"ot-anchor\">http://spp.astro.umd.edu/research.php.html</a><br /><br />[16] Very rare shot! From <a href=\"http://yle.fi/uutiset/finnish_astronomer_snaps_high_altitude_lightning/5972450\" class=\"ot-anchor\">http://yle.fi/uutiset/finnish_astronomer_snaps_high_altitude_lightning/5972450</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Arctic Subtropic","summary":"The Arctic Regions Were Once a Sub-tropical Paradise and Scientists Can’t Explain Why / How Could 1,000-year-old Trees which Reached Up to 35 Meters Survive 3-4 Months of Darkness Every Year? / Crocodiles Lived There, Even Though they Tend to Drown in Water Colder than 65 F / Continental Drift Has Been Ruled Out / Tree Stumps Said to be 50 Mil Years are Unexpectedly Well-preserved / Perfectly Preserved Pine Needles are Said to be 40 Mil Years Old / Something Fundamental About the Earth Changed, & the Dates are Questionable","image":"https://lh3.googleusercontent.com/-PZAc2dy_z0M/VvQM3-U-CCI/AAAAAAAAHdE/4LrMKesst5k/w6250-h8750/the-arctic-subtropic-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/MwPrUjCdS5P","publishDate":"2016-03-24T15:51:36.117Z","updateDate":"2017-01-03T02:40:50.243Z","text":"<b>The Arctic Subtropic: The Arctic Regions Were Once a Sub-tropical Paradise and Scientists Can’t Explain Why / How Could 1,000-year-old Trees which Reached Up to 35 Meters Survive 3-4 Months of Darkness Every Year? / Crocodiles Lived There, Even Though they Tend to Drown in Water Colder than 65 F / Continental Drift Has Been Ruled Out / Tree Stumps Said to be 50 Mil Years are Unexpectedly Well-preserved / Perfectly Preserved Pine Needles are Said to be 40 Mil Years Old / Something Fundamental About the Earth Changed, &amp; the Dates are Questionable</b><br /><br />These findings raise questions about one of the most significant assumptions within Earth sciences -- that of uniformitarianism.  There is a lot to talk about on that subject, so I&#39;ve pulled most of that historical and philosophical conversation out of this card so that it can be raised more properly later.<br /><br />They also should inspire at least a little bit of doubt in the claimed ages.  A person need not believe that the Earth is only 6,000 years old in order to question these dates.  There is a tendency today to treat these dates as factual, but very few people have taken the time to learn how the dates are formulated.  We&#39;ll be getting to that as well at a later time.<br /><br /><b><i>God Star</i></b><br />Dwardu Cardona<br /><br /><b><i>&quot;The Canadian High Arctic</i></b><br /><br /><b><i>The Canadian island of Axel Heiberg, in Nunavut, well above the Arctic Circle, well beyond the present tree line, is littered with the remains of ancient forests -- stumps, logs, and remnants of leaves and even fruit. Although the relics of such forests are known from other parts of the world, those on Axel Heiberg are exceptional because, unlike other remains, they have not been petrified. On the contrary, the remains have maintained their original form and even tissue. The retrieved wood still splits and splinters and can be carved with a sharp knife. It burns as good as modern wood. It has even retained the hue of soft lumber. Not only trees have been preserved, but also leaf mats the likes of which one finds on the ground in modern forests. Barren, gaunt, and forbidding as the island now is, its rolling hills bear the traces of more than twenty separate forest layers, stacked on top of each other, all of which are found in situ, testifying to growth on the spot rather than transmission by the forces of nature. The age between each individual forest layer, which consist of sediment a few meters in thickness, has been calculated to be anything from a few hundred to thousands of years.</i></b><br /><br /><i>As James Basinger noted, these vestiges point to </i><b><i>&#39;a lengthy warm spell during the Eocene epoch ... when mean annual polar temperatures ranged from seven to 15 C.&#39;</i></b><i> [36]</i><br /><br /><i>&#39;Tall trees not unlike the towering redwoods of the Pacific Northwest -- and genetically similar to birch, alder and swamp cypress -- grew beside a meandering river delta hundreds of kilometers wide. </i><b><i>Some of these giants were 35 m high, with stumps 2.5 m around, and appear to have lived for as long as 1,000 years.&#39;</i></b><i> [37]</i><br /><br /><b><i>The problem that has been facing paleobotanists is how such forests could have thrived in a latitude which at present would have forced them to &#39;sleep&#39; through the long polar night. As Art Johnson who, independent of Basinger, have been studying these remains, noted: &#39;We have no forests on Earth where the trees are so big and have to sit in the dark for three months.&#39;</i></b><i> [38]</i><br /><br /><i>Axel Heiberg Island is not the only area in Canada&#39;s High Arctic where the remains of ancient forests have been found. The coal-bearing sediments of the Eureka Sound Group scattered throughout most of the Arctic Archipelago also contain such remains. Plants dated to the Paleocene from Fosheim Peninsula of Ellesmere Island resemble similar Paleocene flora from Western Canada&#39;s interior, indication of a cosmopolitan temperate zone. [39] Some of the plants from these Tertiary forests have been described as being akin to those growing in the present cypress swamps of Florida. [40] Others, like oak, do not grow in swamps. </i><b><i>Trees from the middle Eocene in the same area reached up to 50 meters high.</i></b><br /><br /><i>The fossils of animals found buried amid the remains of these forests -- </i><b><i>ancestors of the horse and rhinoceros, giant lizards, land tortoises, salamanders, snakes, alligators, crocodiles, flying lemurs</i></b><i> [41] -- </i><b><i>nall testify to the warmth of the climate at that time, as so does the discovery of fossil palm trees and huge exotic ferns by Soviet paleobotanists in the islands of Spitsbergen in the Svalbard archipelago far within the Arctic Circle. [42] Even fossil tapirs, the descendants of which now live in the equatorial Amazon forest, were found on Ellesmere Island. [43] As Ian Johnson (not to be confused with Art Johnson, cited above) noted, finding the tropics in such high latitudes raises serious implications for paleontologists:</i></b><br /><br /><b><i>&#39;This far from the equator means 4 months of polar darkness ... If the night temperature was always 10 degrees Celsius, in conjunction with 4 months of darkness, plants would die. Mammals found to date are likely middle Eocene creatures. Crocodiles, lizards and turtles are well adapted to forest life but some of the discovered species [the crocodilians] cannot tolerate near freezing temperatures for very long. This implies that there had to be considerable warmth in the Eocene High Arctic all throughout the the year.&#39; [44]</i></b><br /><br /><i>And:</i><br /><br /><b><i>&#39;Crocodilians are a test of the reconstructed polar forest community because they have changed little since the end of the Triassic ... the crocodilians have been consistent throughout their long evolutionary history in their limited tolerance of the cold. Crocodilia have never occupied ecological niches where near-freezing cold persists continuously for months.&#39;</i></b><i> [45] Alligators are more adapted to cold weather than crocodiles. Crocodiles require shallow water, but </i><b><i>crocodiles in water that is colder than 65 F tend to sink to the bottom in lassitude and drown as they do in places even like Florida during severe cold weather.</i></b><br /><br /><i>The scientific establishment, meanwhile, has not been entirely silent when it comes to propositions concerning the solution of the puzzle which the one-time existence of these Arctic forests raise. Thus, for instance, writers in New Scientist proposed that the Arctic Eocene&#39;s subtropical climate can be explained if the oceans transported far more heat to the poles than they do at present. [46] But evidence of the atmospheric circulation required for this, which depends on temperature differential, is lacking. Moreover, the small size of particles retrieved from Eocene deep-sea sidements confirm the lack of wind speeds at that time. [47]</i><br /><br /><b><i>Continental drift must also be ruled out because Axel Heiberg Island is &#39;only a few hundred kilometers closer to the North Pole than it was when the forests flourished&#39; [48] -- which is definitely not enough for it to have been located outside the Arctic Circle at that time.</i></b><i> As Johnson noted: &#39;Modern geological field work has confirmed that the northern lands of the globe have been located in polar latitudes for at least the last 100 million years, despite ongoing continental drift.&#39; [49]</i><br /><br /><i>Leo Hickey&#39;s &#39;observation&#39; that &#39;fossil leaf fragments indicate that the deciduous leaves were enormous&#39; [50] has remained somewhat controversial. [51] He, however, </i><b><i>claims to have returned from his 1979 field season with fossil leaf fragments, one of which measured two feet in width. [52] Gigantic leaves are known to develop when plants are grown under constant lighting conditions. Thus, his conclusion was that these forests &#39;grew under conditions of continuous light.&#39;</i></b><i> [53] Hickey then proposed that a bank of thick fog could have hung over the forests during the Arctic winter, which fog would have retained enough heat to cause a greenhouse effect. [54] What would have caused the fog, or why such a fog is not at present evident in the polar regions, he left unexplained.</i><br /><br /><b><i>Curt Teichert was honest enough to admit that &#39;attempts to explain rapid climatic changes throughout the Tertiary have been &#39;especially vexing and unsatisfactory&#39;.&#39; [55] As D H Campbell wrote: &#39;It is difficult to imagine any possible conditions of climate in which these plants could grow so near the pole, deprived of sunlight for many months of the year.&#39; [56] Or, as David Mech was forced to conclude, the causes behind such a radical different climate &#39;remain a mystery.&#39;&quot;</i></b><i> [57]</i><br /><br />DWARDU&#39;S REFERENCES:<br /><br />[36] J George, &quot;The Forest of the Past,&quot; Macleans&#39;s (September 6, 1999), pp 16-17.<br /><br />[37][38] Ibid, p 17.<br /><br />[39][40] IC Johnson, &quot;Basinger&#39;s Lecture on the Eocene Forests of the Canadian High Arctic,&quot; Chronology &amp; Catastrophism Workshop 1989:2, p 17.<br /><br />[41] &quot;The Eocene Climate Puzzle,&quot; Chronology &amp; Catastrophism Workshop 1989:1, p 27; IC Johnson, &quot;Anomalous Occurrence of Crocodilia in Eocene Polar Forests,&quot; Chronology and Catastrophism Review XIV (1992), p 7.<br /><br />[42] &quot;Fossils Date the Tilt of Earth&#39;s Axis,&quot; Globe &amp; Mail (June 8, 1984). NOTE: For even earlier Triassic remains, which also point to &quot;a warm and humid climate,&quot; see Soviet Weekly (July 21, 1984).<br /><br />[43] IC Johnson, &quot;Basinger&#39;s Lecture on the Eocene Forests of the Canadian High Arctic,&quot; Chronology &amp; Catastrophism Workshop 1989:2, p 11.<br /><br />[44] IC Johnson, &quot;Basinger&#39;s Lecture on the Eocene Forests of the Canadian High Arctic,&quot; Chronology &amp; Catastrophism Workshop 1989:2, p 17. (emphasis added)<br /><br />[45] Idem, &quot;Anomalous Occurrence of Crocodilia in Eocene Polar Forests,&quot; Chronology and Catastrophism Review XIV (1992), p 7; see also ibid, Part Two, Chronology &amp; Catastrophism Review XV (1993), pp 23-26.<br /><br />[46] New Scientist (December 10, 1987), p 28.<br /><br />[47] &quot;The Eocene Climate Puzzle,&quot; Chronology &amp; Catastrophism Workshiop 1988:1, p28.<br /><br />[48] &quot;The Eocene Climate Puzzle,&quot; Chronology &amp; Catastrophism Workshiop 1988:1, p27.<br /><br />[49] IC Johnson, &quot;Basinger&#39;s Lecture on the Eocene Forests of the Canadian High Arctic,&quot; Chronology &amp; Catastrophism Workshop 1989:2, (Part One), p 8.<br /><br />[50] H Thurston, &quot;Icebound Eden,&quot; Equinox (May/June 1986), p 74.<br /><br />[51] IC Johnson, &quot;Basinger&#39;s Lecture on the Eocene Forests of the Canadian High Arctic,&quot; Chronology &amp; Catastrophism Workshop 1989:2, p 13.<br /><br />[52] H Thurston, &quot;Icebound Eden,&quot; Equinox (May/June 1986), p 81.<br /><br />[53][54] &quot;The Eocene Climate Puzzle,&quot; Chronology &amp; Catastrophism Workshiop 1988:1, p28.<br /><br />[55] IC Johnson, &quot;Basinger&#39;s Lecture on the Eocene Forests of the Canadian High Arctic,&quot; Chronology &amp; Catastrophism Workshop 1989:2, p 8.<br /><br />[56] DH Campbell, &quot;Continental Drift and Plant Distribution,&quot; Science (January 16, 1942), as quoted by I. Velikovsky, Earth in Upheaval (NY, 1955), p 45.<br /><br />[57] LD Mech, &quot;Life in the High Arctic,&quot; National Geographic (June 1988), p 757.<br /><br />REFERENCES FROM THE GRAPHIC:<br /><br />[2] <a href=\"http://en.wikipedia.org/wiki/Axel_Heiberg_Island\" class=\"ot-anchor\">http://en.wikipedia.org/wiki/Axel_Heiberg_Island</a><br /><br />[3] <a href=\"http://www.geosociety.org/gsatoday/archive/12/1/pdf/i1052-5173-12-1-4.pdf\" class=\"ot-anchor\">http://www.geosociety.org/gsatoday/archive/12/1/pdf/i1052-5173-12-1-4.pdf</a><br /><br />[4] <a href=\"http://en.wikipedia.org/wiki/Axel_Heiberg_Island\" class=\"ot-anchor\">http://en.wikipedia.org/wiki/Axel_Heiberg_Island</a><br /><br />[5] God Star, Dwardu Cardona, p366 (Note: this book is not a religious text; the name refers to a claim that the ancients used the word &quot;god&quot; to relate to planets and stars)<br /><br />[6] <a href=\"http://www.geosociety.org/gsatoday/archive/12/1/pdf/i1052-5173-12-1-4.pdf\" class=\"ot-anchor\">http://www.geosociety.org/gsatoday/archive/12/1/pdf/i1052-5173-12-1-4.pdf</a><br /><br />[7] <a href=\"http://www.backintimefossils.com/fossil_info.html\" class=\"ot-anchor\">http://www.backintimefossils.com/fossil_info.html</a><br /><br />[8] <a href=\"http://www.geosociety.org/gsatoday/archive/12/1/pdf/i1052-5173-12-1-4.pdf\" class=\"ot-anchor\">http://www.geosociety.org/gsatoday/archive/12/1/pdf/i1052-5173-12-1-4.pdf</a><br /><br />[9] <a href=\"http://www.swisseduc.ch/stromboli/perm-small/axel_heiberg/index-en.html?id=1\" class=\"ot-anchor\">http://www.swisseduc.ch/stromboli/perm-small/axel_heiberg/index-en.html?id=1</a><br /><br />[10] <a href=\"http://www.swisseduc.ch/glaciers/axel_heiberg/axel_south_aerial/index-en.html\" class=\"ot-anchor\">http://www.swisseduc.ch/glaciers/axel_heiberg/axel_south_aerial/index-en.html</a><br /><br />[11] <a href=\"http://polarfield.com/blog/tag/jaelyn-eberle/\" class=\"ot-anchor\">http://polarfield.com/blog/tag/jaelyn-eberle/</a><br /><br />[12] <a href=\"http://researchnews.osu.edu/archive/mumforestpix.htm\" class=\"ot-anchor\">http://researchnews.osu.edu/archive/mumforestpix.htm</a><br /><br />[16] History of the Inductive Sciences, by W Whewell, <a href=\"http://books.googleusercontent.com/books/content?req=AKW5QafTIxciHhzhRtwU3R1K5Y339D03WvB-h1wj_QguE-yJ5hMJmqo2rEoXI0m2gOGRwv9Jsa0BIZ1XcFYOhyr_GKXSXyMCEVOOCPvoJPuHv9jwzal2kIR36avlx99-krvBCdxviQUpNQFDDXqEYoOrNJ4dbA-kSQLWXk1czedy5mWgr7ERQxK-uffzrwqvguaalyyXtpx5hOVNiTUI0l7DdvJYUjqkj2W-nlnNXN-bw3ZU_IrFGY6On9kpeUESDiWGK0fEs0byCl-reA7ALV3yS1DPI5T7Sw\" class=\"ot-anchor\">http://books.googleusercontent.com/books/content?req=AKW5QafTIxciHhzhRtwU3R1K5Y339D03WvB-h1wj_QguE-yJ5hMJmqo2rEoXI0m2gOGRwv9Jsa0BIZ1XcFYOhyr_GKXSXyMCEVOOCPvoJPuHv9jwzal2kIR36avlx99-krvBCdxviQUpNQFDDXqEYoOrNJ4dbA-kSQLWXk1czedy5mWgr7ERQxK-uffzrwqvguaalyyXtpx5hOVNiTUI0l7DdvJYUjqkj2W-nlnNXN-bw3ZU_IrFGY6On9kpeUESDiWGK0fEs0byCl-reA7ALV3yS1DPI5T7Sw</a>, p593<br /><br />[39] <a href=\"http://polarfield.com/blog/tag/jaelyn-eberle/\" class=\"ot-anchor\">http://polarfield.com/blog/tag/jaelyn-eberle/</a><br /><br />[40][41][42] <a href=\"http://researchnews.osu.edu/archive/mumforestpix.htm\" class=\"ot-anchor\">http://researchnews.osu.edu/archive/mumforestpix.htm</a><br /><br />[43] <a href=\"http://commons.wikimedia.org/wiki/File:Metasequoia_occidentalis_Mummified_Forest_1997-08-03.jpg\" class=\"ot-anchor\">http://commons.wikimedia.org/wiki/File:Metasequoia_occidentalis_Mummified_Forest_1997-08-03.jpg</a><br /><br />[44] <a href=\"http://hoopermuseum.earthsci.carleton.ca/forest/eocene14.html\" class=\"ot-anchor\">http://hoopermuseum.earthsci.carleton.ca/forest/eocene14.html</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Science vs Religion","summary":"Science is Incapable of Coexisting with Other Forms of Knowing and Being / Yet, it Only Seeks to Explain Causes / And Since All Facts are Provisional, this Constant Movement Does Not Offer the Individual a Way of Understanding His Life Other than as an Episode in the Flow / The Truths of Science Do Not Require the Wisdom of the Past / Bryan Appleyard Presents the Dilemma of Science's Domination Over Religion in Terms that Even an Athiest Can Agree Are Troubling","image":"https://lh3.googleusercontent.com/-ok7j6pE2IgM/VvK5va9ml1I/AAAAAAAAHb0/wUBWQA9YGk0/w3600-h5040/science-vs-religion-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Wu1oGP3AYRH","publishDate":"2016-03-23T15:48:58.892Z","updateDate":"2017-01-03T02:41:06.844Z","text":"<b>Science vs Religion: Science is Incapable of Coexisting with Other Forms of Knowing and Being / Yet, it Only Seeks to Explain Causes / And Since All Facts are Provisional, this Constant Movement Does Not Offer the Individual a Way of Understanding His Life Other than as an Episode in the Flow / The Truths of Science Do Not Require the Wisdom of the Past / Bryan Appleyard Presents the Dilemma of Science&#39;s Domination Over Religion in Terms that Even an Athiest Can Agree Are Troubling</b><br /><br />Perhaps it does not matter, but I&#39;ll state for the record that I personally have no religious affiliation.  Yet, I have heard this argument made before elsewhere, and I do get the sense that most science vs religion debates are in fact a caricature of this deeper cultural problem which could conceivably manifest into an existential threat ...<br /><br /><i>Understanding the Present: Science and the Soul of Modern Man</i><br />Bryan Appleyard<br />p213-223<br /><br /><i>&quot;Science made us, science broke us; it is time to start making repairs.</i><br /><br /><i>Let me first summarize the picture and the argument I have been trying to present:</i><br /><br /><i>A new and unprecedentedly effective form of knowledge and way of doing things appeared suddenly in Europe about 400 years ago.  This is what we now know as science.</i><br /><br /><i>This science inspired a version of the universe, of the world and of man that was utterly opposed to all preceding versions.  Most importantly, it denied man the possibility of finding an ultimate meaning and purpose for his life within the facts of the world.  If there were such things as meanings and purposes, they must exist outside the universe describable by science.</i><br /><br /><i>[...]</i><br /><br /><b><i>Science now answers questions as if it were a religion and its obvious effectiveness means that these answers are believed to be the Truth -- again as if it were a religion. But it confronts none of the spiritual issues of purpose and meaning. And, meanwhile, its growing power enables it to drive the very systems that did confront those issues to the margins of our concern and, ultimately, out of existence. As I have said before, our science, whatever it may pretend, is incapable of coexistence.</i></b><br /><br /><i>[...]</i><br /><br /><i>A moment&#39;s self-examination will reveal how obviously true that is.  We know in our daily lives that we either feel at one with ourselves or we do not.  We know also that no matter of reasoning can change either state of affairs. </i><b><i>A doctor might come up with some hormonal explanation of your condition; a psychoanalyst might suggest an incident in childhood. But the point is both would be offering only causes, they would not be explaining or describing the feeling itself.</i></b><i> This is the same as the point I made about the color green -- knowing the particular wavelength of light does not tell us directly about the experience of green.  These feelings of greenness or oneness happen and they exist in the world as surely as rivers, mountains and stars.  That we think we can understand them through hormones, analysis or wavelengths says nothing at all about that existence, though it does say much about the kind of explanation we want to believe.</i><br /><br /><i>The whole point I am making is that a hard, irreducible sense of our own self-awareness has been progressively denied us by the inroads of science both as a form of truth and as a creator of our society.  The chains of scientific causality have gradually appeared to explain away our moods and insights as surely as they have evicted God from the immediate neighborhood of the solar system.  In the past the sure sense of the self has always been sustained by religion in a variety of guises, indeed it might be said that this sense is the religious aspect of us all.  But science has taken away our religion and we cannot invent a new one because religions are, by definition, not inventable.  Certainly theologies can be invented, but, to be effective, they must spring from some deeper human well than the mind of a theologian; they must arise from a totality of human experience, they must be ourselves.  And we cannot invent ourselves any more than we can invent the sensation of greenness.</i><br /><br /><i>[...]</i><br /><br /><i>Time, in this context, takes on a specific moral dimension. </i><b><i>Future time is good, past time bad. We move from this inadequate past into this bright future. Since progress is seen to be happening and is regarded as a virtue, the past comes to be understood as an underdeveloped realm, an impoverished Africa of memory and the imagination, useful only as a staging post for the future. Its significance is thinned out until it becomes a mere prologue for the present and the future. History is a dusty archive of doubtful value.</i></b><br /><br /><i>The most obviously inhuman aspect of all this -- both progressive political insistence on forward movement as well as the cultural virtue of going with the flow -- is that it denies the possibility of peace within human life. </i><b><i>If there is only an eternal project of progress and a perpetual morality only of movement, then my life is of no value. If I accept these ideals, then I am destined to live and die for a cause that can never be triumphant. The point is that constant movement does not offer the individual a way of understanding his life other than as an episode in the flow. It does not offer a way of grasping and giving value to time in the duration of our own lives.</i></b><i> Yet, in spite of ourselves, we embrace this fiction of the flux.</i><br /><br /><i>This doctrine of undifferentiated forward movement can be illustrated by the way we understand and are taught about the great figures of our culture.  If we are told about Abraham Lincoln or Isaac Newton, </i><b><i>we are told about them as having done something that helped us to attain our current condition.</i></b><i> Lincoln was instrumental in the creation of the modern American state and Newton led us to our physics and cosmologies. </i><b><i>But, logically, this should mean that later American politicians or later physicists were greater than either. What we need is an ideal of greatness that refers to the quality of these men&#39;s lives, as individual lives, not as just visible landmarks in some interminable landscape of flux. Otherwise greatness will become meaningless. History will become an insignificant landscape of ages that were trying and failing to become our age. And, of course, in time, our age will be reduced to the same condition. Everything is reduced by the idea of progress and change.</i></b><br /><br /><i>Such a climate and such attitudes are the climate and attitudes of science.  Science created the idea of constant forward movement and of the possibility of complete transformation.  Its experimental methods and attitudes are implicit in the revolt of the young.  And, of course, its ruthless rejection of the past is implicit in the impotence of the parents. </i><b><i>For the truths of science do not require the wisdom of the past. A computer scientist need never have heard of Newton, he need only know of a thin film of recent knowledge to be able to master his art. Scientific progress is so radical that, at every stage, it is able to throw away almost all the baggage of its own history.</i></b><br /><br /><i>This has profound and utterly negative spiritual implications for the inheritors of the scientific legacy.  The American academic Allan Bloom has brilliantly and movingly anatomized these implications in his book The Closing of the American Mind.  Bloom was inspired to write by his increasing dismay at the spectacle of American college students.  They appeared to be progressively more lifeless and ignorant.  But, even worse, </i><b><i>they were entirely unable to view their own culture as anything worth defending. A terrible cultural relativism had invaded their lives, denying them the possibility of choosing one point of view as more valuable than another.</i></b><br /><br /><b><i>&#39;The study of history,&#39; Bloom writes, &#39;and of culture teaches that all the world was mad in the past; men always thought they were right, and that led to wars, persecutions, slavery, xenophobia, racism and chavinism. The point is not to correct the mistakes and really be right; rather it is not to think you are right at all.&#39;</i></b><br /><br /><i>Bloom had identified the way in which a key liberal attitude had penetrated and corrupted the student mind: people thought they were right in the past and did terrible things as a result; so we must never believe we are right.</i><br /><br /><i>Science and liberalism will not give us the means to defend what we, in particular, are because it will not acknowledge the possibility that we, in particular, are right.  So the justifications of the parents, the teachers, the entire culture sound ever more hollow.  Apparently there is nothing to teach since any new development may invalidate any old fragment of knowledge.  Certainly there is no cultural core, no body of virtue to be transmitted.  Our souls have become enfeebled.  Knowing nothing and thinking nothing, we wander through life as through a bewildering, undifferentiated freak show.  Why is this saint more important than this bearded lady?  What does this philosopher know that this clown does not?  Who am I to decide?  Who are you?&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The Scientific Attitude","summary":"A Set of Ideals Popularised and then Reified, But in Reality is Not Often Found in Actual Scientific Practices / The General Public Attributes Much of the Success of Science to their Belief that Scientists Adhere to these Ideals / But, When Scientists Very Obviously Deviate from them, there Are Instances Where Nobody Appears to Notice or Care / Effectively, there is Little Compelling Adherence / Why is the Public Not Using these Ideals in Our Own Science Discourse Rating Schemes?","image":"https://lh4.googleusercontent.com/-tlm4jEKegFQ/VvIWkpWuzxI/AAAAAAAAHak/fwHpX9RD3mgRFpZaWx8B_P6bzM-HqX48g/the-scientific-attitude-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Drcac91Aava","publishDate":"2016-03-23T04:10:01.127Z","updateDate":"2017-01-03T02:41:38.377Z","text":"<b>The Scientific Attitude: A Set of Ideals Popularised and then Reified, But in Reality is Not Often Found in Actual Scientific Practices / The General Public Attributes Much of the Success of Science to their Belief that Scientists Adhere to these Ideals / But, When Scientists Very Obviously Deviate from them, there Are Instances Where Nobody Appears to Notice or Care / Effectively, there is Little Compelling Adherence / Why is the Public Not Using these Ideals in Our Own Science Discourse Rating Schemes?</b><br /><br /><a href=\"http://www.infinite-energy.com/images/pdfs/mitcfreport.pdf\" class=\"ot-anchor\">http://www.infinite-energy.com/images/pdfs/mitcfreport.pdf</a><br /><br /><i>&quot;MIT and Cold Fusion: A Special Report</i><br /><i>Introduction by Dr. Eugene F. Mallove</i><br /><i>(MIT Class of 1969, Aero/Astro Engineering, SB 1969, SM 1970)</i><br /><i>Editor-in-Chief, Infinite Energy Magazine</i><br /><i>President, New Energy Foundation, Inc.</i><br /><br /><i>[...]</i><br /><br /><i>&quot;When many people are asked today about cold fusion, if they recall the 1989 announcement at all, they may offer remarks such as, &#39;The experiment couldn’t be reproduced.&#39; Or, &#39;Cold fusion was quickly dismissed by other laboratories as a mistake.&#39; One of the most significant players in establishing in the public mind that thoroughly erroneous view was a team of investigators at MIT at its lavishly funded hot fusion laboratory, then called the </i><b><i>MIT Plasma Fusion Center</i></b><i>. The MIT group rendered a highly negative assessment of the Fleischmann and Pons claims, in part by performing its own attempt to reproduce the heavy-water/palladium excess heat experiment. The announced &#39;failure to confirm&#39; by the MIT group became one of the three top negative reports weighing against cold fusion in those early days. The U.S. Department of Energy (DOE) cited the MIT PFC’s negative conclusion in rendering its rushed, condemning report in the fall of 1989; alphabetically, the MIT group’s report is the first technical reference cited in the DOE Cold Fusion Panel’s report.</i><br /><br /><i>It is therefore of considerable interest to understand what really happened at MIT in 1989, and the several years following, on the matter of cold fusion. The story is most certainly not what is regurgitated in numerous journalistic accounts, which are most often unflattering to Drs. Fleischmann and Pons and those researchers who followed their pioneering path. In fact, the story of cold fusion’s reception at MIT is a story of egregious scientific fraud and the coverup of scientific fraud and other misconduct -- not by Fleischmann and Pons, as is occasionally alleged -- but </i><b><i>by researchers who in 1989 aimed to dismiss cold fusion as quickly as possible and who have received hundreds of millions of DOE research dollars since then for their hot fusion research.&quot;</i></b><br /><br />But, that conversation will have to wait for another day, for the topic of this discussion is the &quot;scientific attitude&quot; ...<br /><br /><i>&quot;By June of 1989, the hot fusion community and the physics establishment were very satisfied that they had debunked cold fusion. Any of the growing numbers of positive reports could readily be dismissed by reporters and other, less involved scientists. After all, the plasma physicist authorities at MIT had spoken. </i><b><i>In fact, so convinced were the PFC people that they had killed off cold fusion, they held a celebratory party -- billed as a &#39;Wake for Cold Fusion&#39; on June 26, 1989.</i></b><i> The humorous poster for the party notes: &#39;Brought to you by the Center for Contrived Fantasies — Black Armbands Optional.&#39;</i><br /><br /><b><i>What is most interesting about this anything-but-funny mockery by the PFC is that at the time the party was held, the data for the Phase-II calorimetry experiments had not yet been analyzed! It was not until mid-July 1989 that the calorimetry data were put in anything like final published form. No formal conclusion had been set into print.</i></b><i> How do we know this? Simple. In the course of my investigations into cold fusion, I would of course regularly ask PFC team members for their latest impressions, data, etc. So I was given many, many documents that piled up on my desk, not all being closely examined when received. But as I was completing Fire from Ice in the spring of 1991, questions about the PFC calorimetry had been brought up by my cold fusion colleague, electrochemist Dr. Vesco Noninski. Was the methodology and analysis of the PFC Phase-II calorimetry reported in the paper published by the PFC in the Journal of Fusion Energy sound? Noninski had many doubts and so did I. We approached a team member for clarification and got no satisfaction -- just continued brush-off. </i><b><i>I then looked through my stacks of papers from the PFC and found to my complete astonishment (and dismay) the two draft reports on the Phase-II calorimetry. One was dated July 10, 1989 and the other July 13, 1989, a clearly more complete version -- the version that was actually published in both a formal PFC report and the Journal of Fusion Energy.</i></b><i> Only a week after this MIT PFC analysis solidified, PFC Director Parker occupied himself dispensing &#39;humorous&#39; cold fusion mugs that were obtained &#39;wholesale&#39; in Utah (see Exhibit F)!&quot;</i><br /><br />Setting aside the discussion of whether or not the cold fusion claims are correct, we&#39;d be wise to separately consider whether or not the behavior of those assigned to judge these claims were behaving as actual scientists.<br /><br />I&#39;ll assign a number in brackets to each violation.<br /><br /><a href=\"http://www.directions.usp.ac.fj/collect/direct/index/assoc/D769861.dir/doc.pdf\" class=\"ot-anchor\">http://www.directions.usp.ac.fj/collect/direct/index/assoc/D769861.dir/doc.pdf</a><br /><br />(published sometime after 1975, not sure where it came from, and no idea who Pravin Singh actually is ... but this is great stuff ...)<br /><br /><b><i>&quot;Science Education And Scientific Attitudes</i></b><br /><i>Pravin Singh</i><br /><br /><i>Introduction</i><br /><br /><i>Science has several dimensions. Traditionally, the overwhelming emphasis in the science curriculum has been on the content dimension. Consequently students obtained a narrow understanding of the scientific culture. The situation has improved somewhat in the recent years as a result of the development of modern science programmes. Greater attention is given to the nature of scientific enquiry through the promotion of active student participation in activity-oriented learning experiences.</i><br /><br /><i>In addition to the knowledge and process dimensions of science some recognition has been given to scientific attitudes and to developing these attitudes in students. </i><b><i>It is generally maintained and accepted unquestionably that scientists uphold a set of common scientific attitudes. It is also pointed out that students by practising science in the manner of scientists will consequently adopt and internalize these attitudes.</i></b><br /><br /><i>The trend in current science programme is to develop attitudes considered to be &#39;scientific&#39; and therefore valuable. Gauld (1973:25) lists such things as </i><b><i>the tendency to be [1] objective, [2] open-minded, [3] unbiassed, sceptical and [4] curious and the possession of a critical, [5] questioning and rational mind.</i></b><i> Many modern science curricula such as the local Basic Science, the New Zealand Science: Infants to Standard Four and the Physical Science, to name a few, have recognized the need to develop scientific attitudes.</i><br /><br /><i>What are scientific attitudes?</i><br /><br /><i>Scientific attitudes can be regarded as a complex of &#39;values and norms which is held to be binding on the man of science. The norms are expressed in the forms of prescriptions, proscriptions, preferences and permissions. They are legitimatized in terms of institutional values&#39; (Barnes and Dolby, 1970:3). The norms and values are supposed to be internalised by the scientist and thereafter they fashion his/her scientific practice.</i><br /><br /><b><i>The current set of scientific attitudes of objectivity, open-mindedness, unbiassedness, curiosity, suspended judgement, critical mindedness, and rationality has evolved from a systematic identification of scientific norms and values. The earliest papers of any importance in the field of scientific attitudes are those of R.K. Merton (1957). He conceptualized the norms or institutional imperatives on the basis of evidence taken mainly from statements by scientists about science and their scientific activity. He then identified four norms. These are universalism, communality, disinterestedness and organized skepticism.</i></b><br /><br /><b><i>Universalism requires that information presented to the scientific community be assessed independently of the character of the scientist who presents the information. The norm of communality requires that scientific knowledge be held in common, in other words, the researcher is expected to share his findings with other scientists freely and without favour. [6] The norm of disinterestedness requires scientists to pursue scientific knowledge without considering their career or their reputation. Scientists are exhorted by the norm of organized skepticism never to take results on trust. They are expected to be consistently critical of knowledge.</i></b><br /><br /><b><i>To this list of institutional imperatives Barber (1962: 122-142) later added two more — rationality and emotional neutrality. Rationality relates essentially to having faith in reason and [7] depending on empirical tests rather than on tradition when substantiating hypotheses. Scientists are encouraged also to conform to the [8] norm of emotional neutrality i.e. to avoid emotional involvement which may colour their judgement.&quot;</i></b><br /><br />MIT&#39;s Plasma Fusion Center group plainly failed on 7 scientific attitude dimensions; and whether or not they violated point [7] is an extremely close call.<br /><br /><i>&quot;These idealistic institutional imperatives or their resulting variants have been adopted by school science. It is argued (Ben-David, 1975:21) that abiding by the Mertonian norms helps in checking emotions and prejudices from marring one&#39;s research work. Science is also seen to be socially neutral (King, 1971) and consequently much of the endeavour of the scientific community is protected from social criticism. </i><b><i>Price (1963) remarks that the scientific community believes that the success of science and technology can to a large extent be attributed to the adherence to the Mertonian norms. Moreover, the general public attributes much of the success of science to the belief that the scientific community must be open, neutral, self-critical, rational, etc.</i></b><br /><br /><b><i>But is it an unquestionable fact that scientific attitudes have been important in the success of the scientific community? Can one accept without exception that open-mindedness, disinterestedness, objectivity etc. are actually inherent or acquired qualities prevalent amongst the members of any scientific community? Is it not possible that these scientific attitudes have been popularised and then reified as a set of ideal attitudes but in reality is not often found in actual scientific practices?</i></b><i> The following studies raise serious doubts about the scientists&#39; adherence to institutional imperatives.</i><br /><br /><i>Price (1963) reveals that science is now controlled, financed and directed by the state and by industry. </i><b><i>Ellis (1969) points out that governmental and industrial support has grown so much that traditional norms are no longer applicable. Science is now &#39;Big Science&#39; and scientists must conform to a new set of rules dictated to a large degree by state policies and industrial priorities. Under such a situation, secrecy and competition take on a more dominant role. External pressures of industrial demands in terms of costs and benefits and other political and economic implications contribute towards a shift in the scientific comunity&#39;s attitudes towards their work (see Rose and Rose, 1971). So bureaucratization and industrialisation of science are external factors that have somewhat diluted the scientist&#39;s adherence to Mertonian norms.</i></b><br /><br /><i>The study of the personal characteristics of scientists has also raised questions about whether the flourishing of science can be entirely attributed to the scientists&#39; unequivocal acceptance of the traditional norms. </i><b><i>Holton and Roller (1958) have found that the actual human characteristics exhibited by scientists are quite distant from the attitudes ascribed to scientists.</i></b><br /><br /><b><i>Anne Roe (1961) reports that personal factors inevitably enter into scientific activity. They influence a scientist&#39;s choice of what observations to make; they influence a scientist&#39;s selective perception when making the observations. They also influence their judgements about when there is sufficient evidence to be conclusive and considerations as to whether discrepancies between experimental and theoretical data are important or unimportant to their pet theories.</i></b><br /><br /><b><i>Mitroff&#39;s study (1974) of the behaviours of Apollo moon scientists shows that scientists are passionate, irrational and strongly committed to their own favoured theories. What this means is that subjective characteristics of the scientists act as norms rather than the widely accepted Mertonian norms.</i></b><br /><br /><b><i>Mitroff (1974) also noted that scientists are seldom objective; there is no such thing as the disinterested observer. As Mitroff sees it, the real process of doing science is much more complicated. It is filled with subjective and even irrational elements that have been generally unacknowledged. Mitroff concludes by suggesting that &#39;to remove commitment and even bias may be to remove one of the strongest sustaining force for both the discovery of scientific ideas and for their subsequent testing.&#39;</i></b><i> (Mitroff, 1973: 765).</i><br /><br /><i>Quite often school science implies or depicts scientists as being rational and critical in their scientific activities. This, however, may not always be the case. Gauld (1973) admits that rationality does play a part in scientific activity but is not always evident and not always practised by all the members of a scientific community. </i><b><i>Kirkut (1960) suggested that rational thinking is certainly exercised in judging the products of these with whom one disagrees although the same case may not be lavished on the arguments of scientists whose views are closer to one&#39;s own.</i></b><i> Writings by Kuhn (1962) also provide an insight into factors and personal characteristics that influence a scientist&#39;s activity.</i><br /><br /><b><i>The degree of resistance, stubbornness, jealousy and rigid commitment witnessed among the members of the scientific community further undermines the total acceptance of scientific attitudes.</i></b><i> Bernard Barber&#39;s (1961) study provides ample evidence of this. For example, he cites Max Planck who had recorded the following complaints concerning the practice of the members of his scientific community.</i><br /><br /><b><i>&#39;I found no interest, let alone approval, even among the very physicists who were very closely connected with the topic. Helmholtz probably did not read my paper at all. Kirchhoff expressly disapproved ... I did not succeed in reaching Clausius ... I carried on a correspondence with Carl Neumann, of Liepzig, but it remained totally fruitless&#39;</i></b><i> (as cited by Barber, 1961, Page 596).</i><br /><br /><i>Barber (1961) presents several examples that reveal the extent of scientists&#39; stubbornness and resistance to refutation of established scientific ideas and to the presentation of counter-arguments and new concepts. Such investigations weaken the argument that scientists are generally open-minded, objective, skeptical, disinterested, rational and neutral.</i><br /><br /><i>Effect on Students</i><br /><br /><b><i>Science textbooks, in their rush to present organised descriptions of structure, function and process, sacrifice human drama and personal characteristics of the members of the scientific community. Much of the textbooks&#39; interpretation of the images of scientists and their attitudes is a consequence of the analysis and acceptance of the end-products of science. This approach has resulted in the acceptance of a stereotyped image of the scientist.</i></b><br /><br /><i>Ahlgren and Walberg (1973) and Randall (1979) in separate studies, have pointed out that students perceive scientists as cold, impersonal data-dealers, and their work as dull, monotonous and tedious. Bereft of common human feelings and compassion, the robot-type images — a consequence of the projection into the common scientific attitudes — has resulted in the promotion of a negative attitude forwards science and a gradual loss of interest in science (see Shallis and Hills, 1975).</i><br /><br /><i>The quality of objectivity in science seems firmly upheld by scientists and nonscientists alike. Consequently, according to Shallis and Hills (1975), those that are attracted to science subscribe to the notion of objectivity, thereby perpetuating the myth. It is of concern to the general public to realise that many of those attracted to science will be adhering to this norm of objectivity. In doing so, there is always the possiblity that future scientists would become more cold, objective and almost robot-like. However, at a time when the impact of science and technology on the society is so critical, there is a need for the scientific community to be more human and compassionate.</i><br /><br /><i>Science, because it appears so cold, loses its appeal for the general public. This is unfortunate especially when the general public needs to be more alert towards scientific activities. As for South Pacific students, the study of science in most cases is seen as a convenient means of acquiring a pass in public examinations. It is doubtful whether the majority of the school leavers continue to maintain interest in science. Indeed, it is increasingly unlikely that they are keen enough to develop their scientific knowledge after completing their formal education.</i><br /><br /><i>Conclusion</i><br /><br /><i>While it is desirable that students of science should be encouraged to develop these attitudes </i><b><i>we need also make them aware of the role that personal characteristics play in the acquisition of scientific knowledge. By revealing the role of personal characteristcs that scientists are normal human beings, fallible, stubborn, emotional and irrational, we can humanise science and thereby develop in the student proper appreciation of science.</i></b><br /><br /><b><i>To do this the student should be given the opportunity to perceive scientists as normal, actively and occasionally fallible human beings, who are different only in the area of their special training. Students should have access to literature that reveals the extent to which the subjective side of the scientist influences his or her work.</i></b><br /><br /><i>Needless to say, classroom teachers must play the major role in this enterprise, and thus help students acquire a better understanding of science and scientists. To be effective, teachers may need to familiarise themselves with current writings dealing with the nature of scientific knowledge and the practice of scientists at work.&quot;</i><br /><br />Commentary:<br /><br />To emphasize Singh&#39;s point, the public attributes much of the success of science to this set of attributes which scientists do not appear to adhere to.<br /><br />I would argue that the public can play a part in rectifying the situation by re-establishing the scientific attitude as a new system for rating online discourse about science.  I don&#39;t want to go into too much detail here as to what the mechanics of this would be.  I simply want to make the point that even if the scientific community is happy to ignore these norms, the public need not mindlessly follow suit.  We can revive these norms without them as rules for our own discourse.<br /><br />Let them follow suit, if they wish.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Map is Not the Territory","summary":"Human Language is Unique in that it Can Refer to Other Spoken Language / This Aspect Permits Us to Make Use of the Experiences of Others, Which Makes Progress Possible / Most of Our Knowledge is Acquired in this Manner / Similarly, the Success of Science is Precisely Captured by the Realization that We Can Map Places Without Ever Visiting Them / This Ability Gives us a Sense that we Can Grasp Everything, Even Things We Cannot See -- But, With One Caveat -- So Long as We Never Forget that the Map is not Actually the Territory","image":"https://lh6.googleusercontent.com/-i1Q9uzGAC08/VvCwJGcIDlI/AAAAAAAAHZo/mDtyBiM0HrEyHs_CAZ4UW6j2QjkiERzLQ/the-map-is-not-the-territory-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/4QxWp2yPBEt","publishDate":"2016-03-22T02:47:19.974Z","updateDate":"2017-01-03T02:42:03.458Z","text":"<b>The Map is Not the Territory: Human Language is Unique in that it Can Refer to Other Spoken Language / This Aspect Permits Us to Make Use of the Experiences of Others, Which Makes Progress Possible / Most of Our Knowledge is Acquired in this Manner / Similarly, the Success of Science is Precisely Captured by the Realization that We Can Map Places Without Ever Visiting Them / This Ability Gives us a Sense that we Can Grasp Everything, Even Things We Cannot See -- But, With One Caveat -- So Long as We Never Forget that the Map is not Actually the Territory</b><br /><br /><i>Language in Thought and Action</i>, S.I. Hayakawa and Alan R. Hayakawa<br /><br /><i>&quot;While animals use only a few limited cries ... human beings use extremely complicated systems of sputtering, hissing, gurgling, clucking, and cooing noises called language, with which they express and report what goes on in their nervous systems.  Language is, in addition to being more complicated, immeasurably more flexible than the animal cries from which it was developed -- so flexible indeed that it can be used not only to report the tremendous variety of things that go on in the human nervous system, but to report those reports.  That is, when an animal yelps, it may cause a second animal to yelp in imitation or in alarm, but the second yelp is not about the first yelp.  But when someone says, &#39;I see a river,&#39; a second person can say, &#39;He says he sees a river&#39; -- which is a statement about a statement.  About this statement-about-a-statement further statements can be made -- and about those, still more. </i><b><i>Language, in short, can be about language. This is a fundamental way in which human noise-making systems differ from the cries of animals.&quot;</i></b> (<i>Language in Thought and Action</i>, S.I. Hayakawa and Alan R. Hayakawa, p6)<br /><br /><i>&quot;Human beings, then, are never dependent for information on direct experience alone.  Even those in a primitive culture can make use of the experience of neighbors, friends, relatives, and ancestors, communicated by means of language.  Therefore, instead of remaining helpless because of the limitations of their own experience and knowledge, instead of having to rediscover what others have already discovered, instead of exploring the false trails others have explored and repeating their errors, they can go on from where others left off. </i><b><i>Language, that is to say, makes progress possible.&quot;</i></b> (<i>Language in Thought and Action</i>, S.I. Hayakawa and Alan R. Hayakawa, p7)<br /><br /><i>&quot;Language is the indispensable mechanism of human life -- of life such as ours that is molded, guided, enriched, and made possible by the accumulation of the past experience of members of our species.  Dogs, cats, or chimpanzees do not, so far as we can tell, increase their wisdom, their information, or their control over their environment from one generation to the next.  Human beings do.  The cultural accomplishments of the ages, the invention of cooking, of weapons, of writing, of printing, of methods of building, of games and amusements, of means of transportation, and the discoveries of all the arts and sciences </i><b><i>come to us as free gifts from the dead. These gifts, which none of us has done anything to earn, offer us not only the opportunity for a richer life than any of our forebears enjoyed but also the opportunity to add to the sum total of human achievement by our own contributions, however small</i></b><i>.</i><br /><br /><i>To be able to read and write, therefore, is to learn to profit by and to take part in the greatest of human achievements -- that which makes all other human achievements possible -- namely, the pooling of our experience in great cooperative stores of knowledge, available (except where special privilege, censorship, or suppression stand in the way) to all.  From the warning cry of the savage to the latest scientific monograph or news bulletin, language is social.  Cultural and intellectual cooperation is, or should be, the great principle of human life.&quot;</i> (<i>Language in Thought and Action</i>, S.I. Hayakawa and Alan R. Hayakawa, p8)<br /><br /><i>&quot;One is often tempted to say that conflict, rather than cooperation, is the great governing principle of human life ... But what such a philosophy overlooks is that despite all the competition at the surface, there is a huge substratum of cooperation taken for granted that keeps the world going.  The coordination of the efforts of actors, writers, engineers, musicians, photographers, utility companies, typists, program directors, advertising agencies, and hundreds of others is required to create a single television program.  Tens of thousands of persons cooperate in the production of automobiles, including suppliers and shippers of raw materials from different parts of the earth.  Any organized business activity is an elaborate act of cooperation in which every individual worker contributes a share.  A lockout or strike is a withdrawal of cooperation: things are regarded as &#39;back to normal&#39; when cooperation is restored.  As individuals we may compete for jobs, but our function in the job, once we get it, is to contribute, at the right time and place, to that innumerable series of cooperative acts that eventually results in cars being manufactured, in cakes appearing in pastry shops, in department stores serving their customers, in trains and airlines running on schedule.</i><br /><br /><i>This network of cooperation is intricate and complex, and it has been relatively effective. </i><b><i>But because it rests so profoundly upon human agreement, it is also fragile</i></b><i> ... What is important for our purposes here is that all this coordination for the functioning of society is of necessity achieved by language, or else it is not achieved at all.&quot;</i> (<i>Language in Thought and Action</i>, S.I. Hayakawa and Alan R. Hayakawa, p9)<br /><br /><i>&quot;The trouble is that, as Susanne K. Langer has said, &#39;The symbol-making function is one of man&#39;s primary activities ... It is the fundamental process of the mind, and goes on all the time.&#39;  One may try to live a simple life with little concern for symbols of affluence, social status, and the like, but one soon discovers that the rejection of symbolism is itself symbolic.  Wearing a necktie is symbolic, but not wearing a necktie is equally symbolic.  Parents and children have had bitter quarrels in recent years over hair styles -- long, short, spiked, shaved.  Such quarrels are not really about hair but about the symbolic meanings involved in how hair is worn.</i><br /><br /><i>Perhaps some of us would like to escape the complexity of human life for the relative simplicity of such lives as dogs and cats lead.  But the symbolic process, which makes possible the absurdities of human conduct, also makes possible language and therefore all the human achievements dependent upon language.  The fact that more things can go wrong with motorcars than with wheelbarrows is no reason for going back to wheelbarrows.  Similarly, the fact that the symbolic process makes complicated follies possible is no reason to return to a cat-and-dog existence. </i><b><i>To understand the symbolic process is to be able to use it to advantage; not to understand it is to remain forever its victim.&quot;</i></b> (<i>Language in Thought and Action</i>, S.I. Hayakawa and Alan R. Hayakawa, p16)<br /><br /><i>&quot;[M]ost societies systematically encourage, concerning certain topics, the habitual confusion of symbols with things symbolized.  For example, if a Japanese schoolhouse caught fire, it used to be obligatory in the days of emperor worship to try to rescue the emperor&#39;s picture (there was one in every schoolhouse), even at the risk of one&#39;s life.  (If you got burned to death, you were posthumously ennobled.)  In our society, we are encouraged to go into debt in order that we may display, as symbols of prosperity, shiny new automobiles.  Strangely enough, the possession of new automobiles even under these conditions makes their &#39;owners&#39; feel prosperous.  In all societies, the symbols of piety, of civic virtue, or of patriotism are often prized above actual piety, civic virtue, or patriotism.  During the 1988 election campaign, George Bush visited a flag-making factory to show that he was a patriot.  In one way or another, we are all like the student who cheats on exams in order to make Phi Beta Kappa: it is often more important to have the symbol than what it stands for.&quot;</i> (<i>Language in Thought and Action</i>, S.I. Hayakawa and Alan R. Hayakawa, p17-18)<br /><br /><i>&quot;The first of the principles governing symbols is this: </i><b><i>The symbol is not the thing symbolized; the word is not the thing; the map is not the territory it stands for.</i></b><br /><br /><b><i>There is a sense in which we all live in two worlds.  First, we live in the world of happenings that we know at first hand.  This is an extremely small world, consisting only of that continuum of things that we have actually seen, felt, or heard</i></b><i> -- the flow of events constantly passing before our senses.  So far as this world of personal experience is concerned, Africa, South America, Asia, Washington, New York, or Los Angeles do not exist if we have never been to these places.  Desmond Tutu is only a name if we have never seen him.  When we ask ourselves how much we know at first hand, we discover that we know very little indeed.</i><br /><br /><b><i>Most of our knowledge, acquired from parents, friends, school, newspapers, books, conversation, speeches, and television, is received verbally -- that is, in words.</i></b><i> Most of our knowledge of history, for example, comes to us only in words.  The only proof that we have that the Battle of Waterloo ever took place is that we have had reports to that effect. </i><b><i>These reports are not provided by people who saw it happen, but are based on other reports: reports of reports of reports, which go back ultimately to the firsthand accounts of people who did see it happening. It is through reports, then, and through reports of reports, that we receive most knowledge:</i></b><i> about government, about what is happening in the Middle East, about what movie is showing at the nearest theater -- about everything, in fact, that we do not know through direct experience.</i> <br /><br /><i>Let us call the world that comes to us through words the verbal world, as opposed to the world we know or are capable of knowing through our own experience, which we shall call the extensional world.  Human beings, like any other creatures, begin to make their acquaintance with the extensional world from infancy.  Unlike other creatures, however, human beings begin to receive, as soon as they can learn to understand, reports, reports of reports, reports of reports of reports.  In addition, they derive or receive inferences -- conclusions drawn from some sort of evidence.  These inferences may be made from reports or from other inferences.  By the time a child is a few years old, has gone to school and to Sunday school, has made a few friends, has spent some hours watching television, he or she has accumulated a considerable amount of second- and third-hand information about morals, language, history, nature, people, games -- all of which information constitutes one&#39;s verbal world.</i><br /><br /><i>Now, to use the famous metaphor introduced by Alfred Korzybski in Science and Sanity, </i><b><i>this verbal world ought to stand in relation to the extensional world as a map does to the territory it is supposed to represent.</i></b><i> If a child grows to adulthood with a verbal world in his head that corresponds fairly closely to the extensional world that he finds around him in his widening experience, he is in relatively small danger of being shocked or hurt by what he finds, because his verbal world has told him what, more or less, to expect.  He is prepared for life. </i><b><i>If, however, he grows up with a false map in his head -- that is, with a head crammed with error and superstition -- he will constantly be running into trouble, wasting his efforts and acting like a fool. He will not be adjusted to the world as it is; this lack of adjustment may have all manner of serious consequences.</i></b><br /><br /><i>Some of the follies we commit because of false maps in our heads are so commonplace that we do not even think of them as remarkable.  There are those who try to protect themselves from harm by carrying a rabbit&#39;s foot.  Some refuse to sleep on the thirteenth floor of hotels -- a situation so common that many hotels, even in our scientific culture, skip &#39;13&#39; in numbering their floors and rooms.  Some hope to make their teeth whiter by changing their brand of toothpaste.  Some plan their lives on the basis of astrological predictions.  All such people are living in verbal worlds that bear little, if any, resemblance to the extensional world.</i><br /><br /><i>No matter how beautiful a map may be, it is useless to a traveler unless it accurately shows the relationship of places to each other, the structure of the territory.  If we draw, for example, a big dent in the outline of a lake for artistic reasons, the map is worthless.  If we are just drawing maps for fun, without paying any attention to the structure of the region, there is nothing in the world to prevent us from putting in all the extra curlicues and twists we want in the lakes, rivers, and roads.  No harm will be done unless someone tries to plan a trip by such a map.</i><br /><br /><i>Similarly, by means of imaginary or false reports or by false inferences from good reports or by mere rhetorical exercises, we can manufacture at will, with language, &#39;maps&#39; that have no reference to the extensional world.  Here again no harm will be done unless someone makes the mistake of regarding such &#39;maps&#39; as representing real territories.</i><br /><br /><b><i>We all inherit a great deal of useless knowledge, and a great deal of misinformation and error, so that there is always a portion of what we have been told that must be discarded.</i></b><i> It should be noticed that there are three ways of getting false maps of the world into our heads: first, by having them given to us; second, by making them up for ourselves by misreading true maps; third, by constructing them ourselves by misreading territories.  But the cultural heritage that is transmitted to us -- our socially pooled knowledge, both scientific and humane, has been valued principally because we believe that it gives us accurate maps of experience.&quot;</i> (<i>Language in Thought and Action</i>, S.I. Hayakawa and Alan R. Hayakawa, p19-21)<br /><br /><i>Understanding the Present: Science and the Soul of Modern Man</i>, Bryan Appleyard:<br /><br /><i>&quot;Take the idea of the map.  We use maps all the time and we think nothing of how they work.  Our modern maps are complete and clear; there is nothing missing and there is nothing we cannot understand.  Old maps show some regions with a reasonable degree of certainty.  But then knowledge fails and the imagination of the mapmaker takes over.  The region of the known shades away into myths and fairy stories -- dragons and giants at the world&#39;s end, a landscape of chaos beyond the limits of order.  There was a line drawn to mark the limits of human knowledge.  There was an outside, a beyond.</i><br /><br /><i>Now our maps are complete, but not because we have been everywhere and seen everything.  Our maps are complete because we have found </i><b><i>a better way of making them</i></b><i> that excludes the need for dragons.  Indeed, the golden key to the success of science is precisely captured by the realization that we can map places without visiting them.  By drawing lines of longitude and latitude and by astronomical observation we can produce an effective picture of the whole world.</i> <br /><br /><i>Imagine you are a traveler looking around an alien landscape.  There are trees, rivers and mountains.  But they are meaningless in themselves.  You cannot say where you are simply because of that mountain or this tree.  You can spend your days finding out everything about what you see, but it will never tell you where you are.  But if I give you an effective map with your mountain and your river marked upon it, the world is transformed.  You can calculate your position relative to all other positions.  From any point you can, therefore, journey to any other point.  This is not simply better knowledge, it is utterly different knowledge.</i><br /><br /><i>And, once we have such a map, the old map with its unknown regions immediately becomes naive.  The new map tells us that nothing can be ultimately unknown.  We may not know precisely what we may find when we arrive anywhere, but we do know that the place exists in this position relative to this sea, island, or mountain or to our home.  The wisdom of the past has become quaint because it is ineffective.  Modern man with his maps is infinitely more powerful than those poor people who thought there were dragons at the world&#39;s end.  He is like a god.</i><br /><br /><i>One way of knowing -- the casting of an invisible net of latitude and longitude over the earth&#39;s surface -- has proved spectacularly effective.  It has convinced us of our power.  Civilized man with his map can travel far from civilization, secure in the conviction that he is tied to his home by an unbreakable chain of knowledge and calculation.  And, once these lines are secure, they are followed by others -- telecommunication cables, radio, and microwave links.  Finally we have our complete modern, mental map of the world: a blue-green globe nestling in an invisible field of voices and crossed by rapid beeps and streams of computerized information.  We have killed the dragons.</i><br /><br /><i>So, the effectiveness of science gives us more than hot water or the facility to hear good music, it gives us a sense that we can grasp everything, even things we cannot see.  Our maps convince us that, on one level at least, our world is now completely known.  We are superior to a primitive community without such effective and complete maps because we can point to the place where they are and how far they are from every other place.  They can only construct myths about the land they cannot see.</i><br /><br /><i>This illustrates an important, higher aspect of science&#39;s effectiveness.  It shows that mere technology is only the most obvious demonstration of our new powers.  For, above the gadgets, there is the sense that science provides us with a way of doing and knowing almost anything we like.  By providing its maps, its nets of explanation, it convinces us that what is unknown can only be a question of detail.  We may not know if there is a tree at this particular grid reference, but it is only a question of finding out, of sketching in the details.  The map has let us control the space in our minds without having to see it with our eyes.</i><br /><br /><i>This higher type of effectiveness gives us an immense confidence in our powers.  We feel that problems always have solutions waiting for them at some point in the future.  We know the techniques, the territories in which these problems are to be found; it is merely a question of a closer reading of the right maps.  For example, we know, almost to an inch, where the planet Mars is to be found; flying there is merely a matter of technological detail.  Conceptually, we have already landed and conquered.&quot;</i> (<i>Understanding the Present: Science and the Soul of Modern Man</i>, Bryan Appleyard, 1993, p5-7)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"Innovation Starvation","summary":"Our Inability to Match the Achievements of the 1960s Space Program Might be Symptomatic of a General Failure in our Society to Get Big Things Done / Innovation Can’t Happen Without Accepting the Risk that it Might Fail / The Best an Audacious Manager can do Today is to Develop Small Improvements to Existing Systems / Any Strategy that Involves Crossing a Valley -- Accepting Short-term Losses to Reach a Higher Hill in the Distance -- Will Soon be Brought to a Halt by the Demands of a System that Celebrates Short-term Gains and Tolerates Stagnation, but Condemns Anything Else as Failure / In Short, a World Where Big Stuff can Never Get Done","image":"https://lh3.googleusercontent.com/-KFmoP1blI0s/Vu99RRYkEvI/AAAAAAAAHZQ/ne1HHZSPiGQ/w1800-h2520/innovation-starvation-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/U6qWs62w9Mo","publishDate":"2016-03-21T04:50:46.420Z","updateDate":"2017-01-03T02:42:26.067Z","text":"<b>Innovation Starvation: Our Inability to Match the Achievements of the 1960s Space Program Might be Symptomatic of a General Failure in our Society to Get Big Things Done / Innovation Can’t Happen Without Accepting the Risk that it Might Fail / The Best an Audacious Manager can do Today is to Develop Small Improvements to Existing Systems / Any Strategy that Involves Crossing a Valley -- Accepting Short-term Losses to Reach a Higher Hill in the Distance -- Will Soon be Brought to a Halt by the Demands of a System that Celebrates Short-term Gains and Tolerates Stagnation, but Condemns Anything Else as Failure / In Short, a World Where Big Stuff can Never Get Done</b><br /><br /><a href=\"http://www.worldpolicy.org/journal/fall2011/innovation-starvation\" class=\"ot-anchor\">http://www.worldpolicy.org/journal/fall2011/innovation-starvation</a><br /><br /><i>&quot;Innovation Starvation</i><br /><i>By Neal Stephenson</i><br /><br /><i>My lifespan encompasses the era when the United States of America was capable of launching human beings into space. Some of my earliest memories are of sitting on a braided rug before a hulking black-and-white television, watching the early Gemini missions. This summer, at the age of 51 -- not even old -- I watched on a flatscreen as the last Space Shuttle lifted off the pad.  I have followed the dwindling of the space program with sadness, even bitterness.  Where’s my donut-shaped space station? Where’s my ticket to Mars? Until recently, though, I have kept my feelings to myself. Space exploration has always had its detractors. To complain about its demise is to expose oneself to attack from those who have no sympathy that an affluent, middle-aged white American has not lived to see his boyhood fantasies fulfilled.</i><br /><br /><i>Still, </i><b><i>I worry that our inability to match the achievements of the 1960s space program might be symptomatic of a general failure of our society to get big things done. My parents and grandparents witnessed the creation of the airplane, the automobile, nuclear energy, and the computer to name only a few. Scientists and engineers who came of age during the first half of the 20th century could look forward to building things that would solve age-old problems, transform the landscape, build the economy, and provide jobs for the burgeoning middle class that was the basis for our stable democracy.</i></b><br /><br /><i>The Deepwater Horizon oil spill of 2010 crystallized my feeling that we have lost our ability to get important things done. The OPEC oil shock was in 1973 -- almost 40 years ago. It was obvious then that it was crazy for the United States to let itself be held economic hostage to the kinds of countries where oil was being produced. It led to Jimmy Carter’s proposal for the development of an enormous synthetic fuels industry on American soil. Whatever one might think of the merits of the Carter presidency or of this particular proposal, it was, at least, a serious effort to come to grips with the problem.</i><br /><br /><i>Little has been heard in that vein since. We’ve been talking about wind farms, tidal power, and solar power for decades. Some progress has been made in those areas, but energy is still all about oil. In my city, Seattle, a 35-year-old plan to run a light rail line across Lake Washington is now being blocked by a citizen initiative. </i><b><i>Thwarted or endlessly delayed in its efforts to build things, the city plods ahead with a project to paint bicycle lanes on the pavement of thoroughfares.</i></b><br /><br /><i>In early 2011, I participated in a conference called Future Tense, where I lamented the decline of the manned space program, then pivoted to energy, indicating that </i><b><i>the real issue isn’t about rockets. It’s our far broader inability as a society to execute on the big stuff.</i></b><i> I had, through some kind of blind luck, struck a nerve. The audience at Future Tense was more confident than I that science fiction [SF] had relevance -- even utility -- in addressing the problem. I heard two theories as to why:</i><br /><br /><i>1. The Inspiration Theory. SF inspires people to choose science and engineering as careers. This much is undoubtedly true, and somewhat obvious.</i><br /><br /><i>2. The Hieroglyph Theory. Good SF supplies a plausible, fully thought-out picture of an alternate reality in which some sort of compelling innovation has taken place. A good SF universe has a coherence and internal logic that makes sense to scientists and engineers. Examples include Isaac Asimov’s robots, Robert Heinlein’s rocket ships, and William Gibson’s cyberspace. As Jim Karkanias of Microsoft Research puts it, such icons serve as hieroglyphs -- simple, recognizable symbols on whose significance everyone agrees.</i><br /><br /><b><i>Researchers and engineers have found themselves concentrating on more and more narrowly focused topics as science and technology have become more complex.</i></b><i> A large technology company or lab might employ hundreds or thousands of persons, each of whom can address only a thin slice of the overall problem. </i><b><i>Communication among them can become a mare’s nest of email threads and Powerpoints.</i></b><i> The fondness that many such people have for SF reflects, in part, the usefulness of an over-arching narrative that supplies them and their colleagues with a shared vision. Coordinating their efforts through a command-and-control management system is a little like trying to run a modern economy out of a Politburo. Letting them work toward an agreed-on goal is something more like a free and largely self-coordinated market of ideas.</i><br /><br /><i>SPANNING THE AGES</i><br /><br /><i>SF has changed over the span of time I am talking about -- from the 1950s (the era of the development of nuclear power, jet airplanes, the space race, and the computer) to now. Speaking broadly, the techno-optimism of the Golden Age of SF has given way to fiction written in a generally darker, more skeptical and ambiguous tone. I myself have tended to write a lot about hackers—trickster archetypes who exploit the arcane capabilities of complex systems devised by faceless others.</i><br /><br /><b><i>Believing we have all the technology we’ll ever need, we seek to draw attention to its destructive side effects.</i></b><i> This seems foolish now that we find ourselves saddled with technologies like Japan’s ramshackle 1960’s-vintage reactors at Fukushima when we have the possibility of clean nuclear fusion on the horizon. </i><b><i>The imperative to develop new technologies and implement them on a heroic scale no longer seems like the childish preoccupation of a few nerds with slide rules. It’s the only way for the human race to escape from its current predicaments. Too bad we’ve forgotten how to do it.</i></b><br /><br /><i>&#39;You’re the ones who’ve been slacking off!&#39; proclaims Michael Crow, president of Arizona State University (and one of the other speakers at Future Tense). He refers, of course, to SF writers. The scientists and engineers, he seems to be saying, are ready and looking for things to do. Time for the SF writers to start pulling their weight and supplying big visions that make sense. Hence the Hieroglyph project, an effort to produce an anthology of new SF that will be in some ways a conscious throwback to the practical techno-optimism of the Golden Age.</i><br /><br /><i>SPACEBORNE CIVILIZATIONS</i><br /><br /><i>China is frequently cited as a country now executing on Big Stuff, and there’s no doubt they are constructing dams, high-speed rail systems, and rockets at an extraordinary clip. </i><b><i>But those are not fundamentally innovative. Their space program, like all other countries’ (including our own), is just parroting work that was done 50 years ago by the Soviets and the Americans. A truly innovative program would involve taking risks (and accepting failures) to pioneer some of the alternative space launch technologies that have been advanced by researchers all over the world during the decades dominated by rockets.</i></b><br /><br /><i>Imagine a factory mass-producing small vehicles, about as big and complicated as refrigerators, which roll off the end of an assembly line, are loaded with space-bound cargo, and topped off with non-polluting liquid hydrogen fuel, then exposed to intense concentrated heat from an array of ground-based lasers or microwave antennas. Heated to temperatures beyond what can be achieved through a chemical reaction, the hydrogen erupts from a nozzle on the base of the device and sends it rocketing into the air. Tracked through its flight by the lasers or microwaves, the vehicle soars into orbit, carrying a larger payload for its size than a chemical rocket could ever manage, but the complexity, expense, and jobs remain grounded. For decades, this has been the vision of such researchers as physicists Jordin Kare and Kevin Parkin. A similar idea, using a pulsed ground-based laser to blast propellant from the backside of a space vehicle, was being talked about by Arthur Kantrowitz, Freeman Dyson, and other eminent physicists in the early 1960s.</i><br /><br /><i>If that sounds too complicated, then consider the 2003 proposal of Geoff Landis and Vincent Denis to construct a 20-kilometer-high tower using simple steel trusses. Conventional rockets launched from its top would be able to carry twice as much payload as comparable ones launched from ground level. There is even abundant research, dating all the way back to Konstantin Tsiolkovsky, the father of astronautics beginning in the late 19th century, to show that a simple tether -- a long rope, tumbling end-over-end while orbiting the earth -- could be used to scoop payloads out of the upper atmosphere and haul them up into orbit without the need for engines of any kind. Energy would be pumped into the system using an electrodynamic process with no moving parts.</i><br /><br /><i>All are promising ideas -- just the sort that used to get an earlier generation of scientists and engineers fired up about actually building something.</i><br /><br /><i>But to grasp just how far our current mindset is from being able to attempt innovation on such a scale, consider the fate of the space shuttle’s external tanks [ETs]. Dwarfing the vehicle itself, the ET was the largest and most prominent feature of the space shuttle as it stood on the pad. It remained attached to the shuttle -- or perhaps it makes as much sense to say that the shuttle remained attached to it -- long after the two strap-on boosters had fallen away. The ET and the shuttle remained connected all the way out of the atmosphere and into space. Only after the system had attained orbital velocity was the tank jettisoned and allowed to fall into the atmosphere, where it was destroyed on re-entry.</i><br /><br /><i>At a modest marginal cost, the ETs could have been kept in orbit indefinitely. The mass of the ET at separation, including residual propellants, was about twice that of the largest possible Shuttle payload. Not destroying them would have roughly tripled the total mass launched into orbit by the Shuttle. ETs could have been connected to build units that would have humbled today’s International Space Station. The residual oxygen and hydrogen sloshing around in them could have been combined to generate electricity and produce tons of water, a commodity that is vastly expensive and desirable in space. But in spite of hard work and passionate advocacy by space experts who wished to see the tanks put to use, NASA -- for reasons both technical and political -- sent each of them to fiery destruction in the atmosphere. Viewed as a parable, it has much to tell us about the difficulties of innovating in other spheres.</i><br /><br /><i>EXECUTING THE BIG STUFF</i><br /><br /><b><i>Innovation can’t happen without accepting the risk that it might fail. The vast and radical innovations of the mid-20th century took place in a world that, in retrospect, looks insanely dangerous and unstable.</i></b><i> Possible outcomes that the modern mind identifies as serious risks might not have been taken seriously -- supposing they were noticed at all -- by people habituated to the Depression, the World Wars, and the Cold War, in times when seat belts, antibiotics, and many vaccines did not exist. Competition between the Western democracies and the communist powers obliged the former to push their scientists and engineers to the limits of what they could imagine and supplied a sort of safety net in the event that their initial efforts did not pay off. A grizzled NASA veteran once told me that the Apollo moon landings were communism’s greatest achievement.</i><br /><br /><i>In his recent book Adapt: Why Success Always Starts with Failure, Tim Harford outlines Charles Darwin’s discovery of a vast array of distinct species in the Galapagos Islands -- a state of affairs that contrasts with the picture seen on large continents, where evolutionary experiments tend to get pulled back toward a sort of ecological consensus by interbreeding. &#39;Galapagan isolation&#39; vs. the &#39;nervous corporate hierarchy&#39; is the contrast staked out by Harford in assessing the ability of an organization to innovate.</i><br /><br /><b><i>Most people who work in corporations or academia have witnessed something like the following: A number of engineers are sitting together in a room, bouncing ideas off each other. Out of the discussion emerges a new concept that seems promising. Then some laptop-wielding person in the corner, having performed a quick Google search, announces that this &#39;new&#39; idea is, in fact, an old one -- or at least vaguely similar -- and has already been tried. Either it failed, or it succeeded. If it failed, then no manager who wants to keep his or her job will approve spending money trying to revive it. If it succeeded, then it’s patented and entry to the market is presumed to be unattainable, since the first people who thought of it will have &#39;first-mover advantage&#39; and will have created &#39;barriers to entry.&#39; The number of seemingly promising ideas that have been crushed in this way must number in the millions.</i></b><br /><br /><i>What if that person in the corner hadn’t been able to do a Google search? It might have required weeks of library research to uncover evidence that the idea wasn’t entirely new -- and after a long and toilsome slog through many books, tracking down many references, some relevant, some not. When the precedent was finally unearthed, it might not have seemed like such a direct precedent after all. There might be reasons why it would be worth taking a second crack at the idea, perhaps hybridizing it with innovations from other fields. Hence the virtues of Galapagan isolation.</i><br /><br /><i>The counterpart to Galapagan isolation is the struggle for survival on a large continent, where firmly established ecosystems tend to blur and swamp new adaptations. Jaron Lanier, a computer scientist, composer, visual artist, and author of the recent book You are Not a Gadget: A Manifesto, has some insights about the unintended consequences of the Internet -- the informational equivalent of a large continent -- on our ability to take risks. </i><b><i>In the pre-net era, managers were forced to make decisions based on what they knew to be limited information. Today, by contrast, data flows to managers in real time from countless sources that could not even be imagined a couple of generations ago, and powerful computers process, organize, and display the data in ways that are as far beyond the hand-drawn graph-paper plots of my youth as modern video games are to tic-tac-toe. In a world where decision-makers are so close to being omniscient, it’s easy to see risk as a quaint artifact of a primitive and dangerous past.</i></b><br /><br /><i>The illusion of eliminating uncertainty from corporate decision-making is not merely a question of management style or personal preference. In the legal environment that has developed around publicly traded corporations, managers are strongly discouraged from shouldering any risks that they know about -- or, in the opinion of some future jury, should have known about -- even if they have a hunch that the gamble might pay off in the long run. There is no such thing as &#39;long run&#39; in industries driven by the next quarterly report. The possibility of some innovation making money is just that -- a mere possibility that will not have time to materialize before the subpoenas from minority shareholder lawsuits begin to roll in.</i><br /><br /><b><i>Today’s belief in ineluctable certainty is the true innovation-killer of our age. In this environment, the best an audacious manager can do is to develop small improvements to existing systems -- climbing the hill, as it were, toward a local maximum, trimming fat, eking out the occasional tiny innovation -- like city planners painting bicycle lanes on the streets as a gesture toward solving our energy problems. Any strategy that involves crossing a valley -- accepting short-term losses to reach a higher hill in the distance -- will soon be brought to a halt by the demands of a system that celebrates short-term gains and tolerates stagnation, but condemns anything else as failure. In short, a world where big stuff can never get done.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The Misconception of the Cell Membrane as a Container","summary":"Cell Biology Textbooks Cast the Cell as a Small Membrane-Bounded Compartment Filled with a Concentrated Aqueous Solution of Chemicals, Like a Balloon Filled with Molasses / Yet, Cell Function Can Persist Even When the Membrane Has Been Compromised / Everything that we Think of as the Biological Foundations of Health and Sickness will be Misinterpreted if There are Fundamental Misconceptions About Physiology","image":"https://lh3.googleusercontent.com/-jUGKBjjOh_M/Vu8VKPdLPnI/AAAAAAAAHYQ/FR9flvD4LMQ/w1800-h2520/the-cell-membrane-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Yj8o1Rpiz8w","publishDate":"2016-03-20T21:25:07.277Z","updateDate":"2017-01-03T02:42:38.174Z","text":"<b>The Misconception of the Cell Membrane as a Container: Cell Biology Textbooks Cast the Cell as a Small Membrane-Bounded Compartment Filled with a Concentrated Aqueous Solution of Chemicals, Like a Balloon Filled with Molasses / Yet, Cell Function Can Persist Even When the Membrane Has Been Compromised / Everything that we Think of as the Biological Foundations of Health and Sickness will be Misinterpreted if There are Fundamental Misconceptions About Physiology</b><br /><br />Image sequence from graphic:<br /><br /><a href=\"http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC2111832&amp;blobtype=pdf\" class=\"ot-anchor\">http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC2111832&amp;blobtype=pdf</a><br /><br />Chart from graphic:<br /><br /><a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2139853/pdf/23092.pdf\" class=\"ot-anchor\">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2139853/pdf/23092.pdf</a><br /><br />Quote from graphic is from the preface to <i>Cells, Gels and the Engines of Life</i> ...<br /><br /><i>Cells, Gels and the Engines of Life: A New Unifying Approach to Cell Function</i><br />Dr. Gerald Pollack<br />From the Preface<br /><br /><i>&quot;The story begins with a curious scene described in the standard cell biology textbook by Alberts et al. (first edition, p. 604). A crawling cell somehow manages to get stuck onto the underlying surface. </i><b><i>The leading edge continues to press on, tears free, and continues its journey onward as though oblivious to the loss of its rear end.</i></b><br /><br /><i>Something about this scenario does not seem right. We have been taught that cell function depends on the integrity of the cell membrane; yet the membrane of this cell has been ripped asunder and the cell continues to function as though none of this matters. It’s business as usual -- even though the cell has effectively been decapitated.</i><br /><br /><i>When first exposed to this scenario, I reacted the way you may be reacting -- the cell must somehow reseal. The membrane must spread over the jagged surfaces, covering the wound and restoring the cell’s integrity. I was inclined to accept this seemingly flimsy account even though I could not understand how a bilayer membrane built of molecules with fixed lateral packing could spread over half again as much area. Nevertheless, it seemed expedient to shelve this anomaly and press on with more immediate matters; someone else could figure out what might be going on.</i><br /><br /><b><i>With time, it became clear that the survival of the decapitated cell was but one example of a set of related anomalies.</i></b><i> Cells survive sundry insults equivalent to being guillotined, drawn and quartered, and shot full of holes with electrical bullets (Chapter 2). If the ruptured membrane does not reseal—and no evidence I’ve seen convinces me that &#39;resealing&#39; is any more than a conveniently invoked expedient -- </i><b><i>the implication is that membrane integrity may be less consequential than presumed.&quot;</i></b><br /><br /><a href=\"http://ac.els-cdn.com/S0092867402007912/1-s2.0-S0092867402007912-main.pdf?_tid=0d4c6954-eed1-11e5-b85b-00000aab0f02&amp;acdnat=1458501914_8061d85d0850cc3e1f60aca748b45d0b\" class=\"ot-anchor\">http://ac.els-cdn.com/S0092867402007912/1-s2.0-S0092867402007912-main.pdf?_tid=0d4c6954-eed1-11e5-b85b-00000aab0f02&amp;acdnat=1458501914_8061d85d0850cc3e1f60aca748b45d0b</a><br /><br />Don Ingber<br />Children&#39;s Hospital/Harvard Medical School,<br />Boston, MA 02115 USA<br /><br /><b><i>&quot;Putting the Cell Biology Establishment on the Stand</i></b><br /><i>Cells, Gels and the Engines of Life</i><br /><i>by G.H. Pollack</i><br /><i>Seattle, WA: Ebner &amp; Sons (2001)</i><br /><i>305 pp. $27.95</i><br /><br /><b><i>When I was a graduate student, I set out to get a handle on cell biology by carrying out an extensive review of the literature, beginning with the earliest citations I could find. By observing how the field progressed over time, I hoped to get a better sense for where the field would move in the future. I read the preface of the first edition of a biochemistry textbook  from the early 1900s which explained that, for simplicity, it was assumed that all reactions took place in a well-stirred solution in a test tube in the section on thermodynamics and kinetics. The authors clearly recognized that life was not a structureless chemistry and thus, they explicitly warned the reader that this limitation must be addressed in the future. However, when I read the preface of the second and third editions of the same textbook, this warning was nowhere to be found. I realized that generations of scientists were being trained without any awareness of this fundamental flaw in their understanding of what governed chemical reactions within a living cell. For this reason, I was not shocked when the cell biology &#39;establishment&#39; was taken aback by the suggestion that structural (cytoskeletal) scaffolds and mechanical forces play critical roles in virtually all aspects of cell regulation; they just never read the preface.</i></b><i> In Cells, Gels and the Engines of Life, Gerald Pollack has done us all a service: he has provided us with a 305 page preface to the future of cell biology which warns us all -- students and establishment alike -- that there will always be a fine line between  understanding and assumption.</i><br /><br /><b><i>While our knowledge of the molecular widgets that comprise living cells has exploded beyond our wildest dream, our understanding of cell architecture and the relation between structure and function still remain rudimentary. For example, one mainstream cell biology textbook defines the cell as &#39;a small membrane-bounded compartment filled with a concentrated aqueous solution of chemicals,&#39; like a balloon filled with molasses. In fact, many biologists who work with molecules in isolation still share this view, as do virtually all lay people,</i></b><i> including the congressmen and women who decide which science  projects the government will invest in.  Pollack views this image as a dragon that must be slain and I cannot agree more.&quot;</i><br /><br />From a page that apparently no longer exists titled &quot;Battle Royale: The Blood &amp; Guts Of Cell Theory&quot;:<br /><br /><i>&quot;The Membrane Theory</i><br /><br /><i>The current mainstream view of cell physiology goes a little something like this:</i><br /><br /><i>The living cell is similar to a bubble</i><br /><i>The cell membrane is composed of a lipid bilayer and serves as a wall</i><br /><i>The cytoplasm (inside the cell) is mostly an aqueous (water-like) solution</i><br /><i>Cell gradients are maintained by a series of channels and pumps on the cell membrane&quot;</i><br /><br /><a href=\"http://www.functionalps.com/blog/2011/10/15/carbon-monoxide-cancer-hormone/\" class=\"ot-anchor\">http://www.functionalps.com/blog/2011/10/15/carbon-monoxide-cancer-hormone/</a><br /><br /><i>&quot;When I started graduate school in biology at the University of Oregon, cell &#39;membrane&#39; research was a thriving business, almost as lucrative as genetics. Years earlier, I had been intrigued by Linus Pauling’s suggestion that anesthetics might act by &#39;structuring&#39; the water in nerve cells, and in trying to understand the physiology of hearing, I had concluded that the &#39;unit discharge&#39; (all-or nothing) idea of nervous transmission left almost everything unexplained. As a result, I concentrated on the organized internal complexity of structure in cells, and found no reason to believe that cells were bags of randomly acting enzymes enclosed in an &#39;amazing membrane&#39; which contained an array of pumps that regulated the composition of the enclosed fluid. </i><b><i>When people produced pictures of &#39;membrane pores,&#39; I thought they should also have to explain why pores of the same size appear when distilled water is frozen and photographed by the same techniques. If a red blood cell, when etched away microscopically looks like a sponge, how can a hypothetical membrane on the surface be all-important? Why did it take so long for electron microscopists to produce images of the membrane, when their microscopes were producing fine images, and why did it take several decades for them to decide how thick these membranes should be? Reading the history of the theory of the &#39;plasma membrane,&#39; I decided that it was almost perfectly irrelevant to biology, because dead cells, as in hair, can demonstrate the same ionic gradients that were the reason for believing in the regulatory membrane.</i></b><br /><br /><i>[...]</i><br /><br /><b><i>When electron microscopes became available, people expected to see the cell membranes they believed in. But in the first pictures, no membranes were visible. After many trials, methods were found to create the appearance of a membrane on cells, but for more than 20 years microscopists were arguing over the thickness of the membrane. The images showed membranes that were as thin as 20 Angstrom units, or as thick as 300 Angstroms. After 30 years, their thickness was decided to be between 60 and 100 Angstrom units. Osmium, which is the standard material used for producing images of cell membranes, was already in medical use for creating &#39;false membranes&#39; on burned or ulcerated tissue. How do you suppose this material came to be used to reveal the membranes which theory required, but ordinary techniques didn’t show?&quot;</i></b><br /><br />An excerpt from <a href=\"http://raypeat.com/articles/articles/physiology-texts-and-the-real-world.shtml\" class=\"ot-anchor\">http://raypeat.com/articles/articles/physiology-texts-and-the-real-world.shtml</a><br /><br /><b><i>Physiology texts and the real world</i></b><br />Ray Peat<br /><br /><b><i>&quot;Immunity, intelligence, appetites, tumor growth, aging, the proper development of organs -- everything that we think of as the biological foundations of health and sickness -- will be misinterpreted if there are fundamental misconceptions about physiology.</i></b><br /><br /><i>Physiology is the study of the vital functions of organisms, but especially when talking about &#39;pathologic physiology,&#39; great emphasis in physiology textbooks is given to the processes that maintain homeostasis of the milieu interieur, or the constancy of composition of the &#39;fluid in which tissue cells are bathed.&#39; </i><b><i>Since cells are embedded in a gel-like matrix, &#39;connective tissue,&#39; the connective tissue should have some serious attention in physiology courses, but in practice its composition is described, and then the rest of physiology treats it as the &#39;extracellular space.&#39; Only specialists in the extracellular matrix are likely to take it seriously as a factor in physiology.</i></b><br /><br /><b><i>If medical physiologists are likely to think of cells as being &#39;bathed in fluid&#39; which fills the empty spaces around the cells, they are also likely to think of the cell’s interior as a watery solution which &#39;fills the space enclosed by the cell membrane.&#39; It is this image of the organism that has made traditional biochemistry possible, since enzymes extracted from cells and dissolved in water had been thought to function the way they function in the living state. But the living cell isn’t like a tiny water-filled test-tube.&quot;</i></b><br /><br /><a href=\"http://bazaarmodel.net/phorum/read.php?3,9364,9364\" class=\"ot-anchor\">http://bazaarmodel.net/phorum/read.php?3,9364,9364</a><br />Ray Peat<br /><br /><i>&quot;In 1968 or ‘69, I had been reading the previous’ 50 years of research on cell physiology, and I saw that for 18 years, Gilbert Ling had been almost alone in offering a view of the cell that was physically possible. He had solved the problems of ionic regulation in ways that physical chemists could accept, </i><b><i>but biologists were [proceeding] as if his solutions to their problems didn’t exist.</i></b><i> I wrote to him, to see if I, as a newcomer to &#39;science,&#39; was missing something. He said that I just didn’t understand what &#39;science&#39; was; it was a matter of money, prestige, and influence, with little concern for what was true.&quot;</i><br /><br />Dr. Gerald Pollack&#39;s work in cell biology rests in part upon the work of Dr. Ling ...<br /><br /><i>Cells, Gels and the Engines of Life: A New Unifying Approach to Cell Function</i><br />Dr. Gerald Pollack<br />pages 25 - 32<br /><br /><i>&quot;Pumps and channels did not emerge in a vacuum.  They arose out of an established conceptual framework and it may be that the original sin lies within the framework -- much like the epicycles.</i><br /><br /><i>The framework in question is the cell membrane. </i><b><i>Pumps came into being when a membrane that was presumed impermeant to solutes was found to be permeant. At first it was a single solute, then another, and then additional ones. Instead of abandoning the notion of impermeance, the passage of each solute was accommodated by presupposing another channel (and pump). Serious consideration was not given to the alternative possibility -- that the continuous barrier framework itself might be erroneous.</i></b><br /><br /><i>When dealing with membranes, I recognize we are treading on hallowed ground. </i><b><i>The continuous phospholipid-barrier concept has become so deeply engrained in modern thinking that merely putting it on the table for discussion seems akin to reconsidering the virtues of motherhood.</i></b><i> Yet, as a major limb of the logic tree, the issue cannot be ignored.</i><br /><br /><i>To approach this issue, we consider what happens when the membrane is disrupted. </i><b><i>If ion partitioning (Table 1.1) requires a continuous barrier, violating the barrier should collapse these critical gradients. The violated cell should also lose enzymes and fuel, metabolic processes should grind to a halt, and the cell should be brought quickly to the edge of death.</i></b><i> On the other hand, if the barrier were not continuous to begin with then membrane disruption could prove relatively inconsequential.</i><br /><br /><i>Wanton Acts</i><br /><br /><i>To disrupt the membrane experimentally, scientists have developed means uncannily similar to those developed to inflict wounds on humans, namely cellular swords and guns</i><br /><br /><i>[...]</i><br /><br />- <i>The microelectrode (or micropipette) is a tapered glass cylinder filled with an electrolyte solution.  The tip is plunged into the cell in order to probe its electrical properties.  The microelectrode tip may seem diminutive by conventional standards, but to the 10-micron cell, invasion by a 1-micron probe is not entirely dissimilar to a human torso being impaled by a fence post.</i><br /><br />- <i>Electroporation is a widely used method of punching multiple holes in the cell membrane.  The holes are created by shotgunning the cell with a barrage of high-voltage pulses, leaving the membrane riddled with orifices large enough to pass genes, proteins and other macromolecules.</i><br /><br />- <i>The patch-clamp method involves the plucking of a 1-micron patch of membrane from the cell for electrophysiological investigation.</i><br /><br /><i>Although these insults may cause fatal injuries in some cells, the surprise, as we see below, is that they are not consistently consequential</i><br /><br /><i>[...]</i><br /><br /><i>Stuart Taylor and colleagues have routinely used micropipettes to micro-inject calcium-sensitive dyes at multiple sites along the isolated muscle cell.  Achieving this with a single micropipette requires repeated withdrawals that leave micron-sized holes. </i><b><i>Yet for up to several days after injection, the cell continues to function normally</i></b><i> (Taylor et al., 1975).</i><br /><br /><i>The results of patch removal are similar.  Here again, the hole in question is roughly a million times the size of the potassium ion.  My colleagues, Guy Vassort and Leslie Tung, who routinely do these kinds of experiments, tell me that </i><b><i>following removal of a 1-micron patch, the 10 micron isolated heart cell will frequently live on and continue beating.</i></b><br /><br /><i>[...]</i><br /><br /><i>... the absence of resealing is confirmed by direct evidence ...</i><br /><br /><i>Long-term orifices are also confirmed during electroporation.  Electroporation is a technique developed principally to transfer foreign molecules into cells (Fig. 2.2).  Although permeability can sometimes be short-lived, this is not necessarily the rule.  When molecules are introduced into the bath well after the end of the electrical barrage, entry into the cell is still possible.  The window of opportunity depends on molecular size.  In the case of huge molecules such as DNA, substantial penetration is observed when the molecules are added from 20 minutes to an hour after the shock terminates (Xie et al., Klenchin et al., 1991).  Albumin, a common extracellular protein can also penetrate for approximately one hour (Prausnitz et al., 1994), </i><b><i>and for smaller substances with molecular mass on the order of 1,000 Daltons, post-electroporation flow can occur for hours to days (Schwister and Deuticke, 1985; Serpersu et al., 1985). Thus, the pores can remain open for long periods during which substances thousands of times the size of an ion can pass.</i></b><br /><br /><i>Clearly, then, wanton acts such as punching holes in the membrane do not necessarily wreak havoc within the cell.  The holes are huge relative to the ion.  Yet, there is no evidence that the cell really cares.  In spite of stringent ion-concentration requirements, the cell can sail through such insults with little sign of any abnormal behavior.</i><br /><br /><i>If the examples above seem too technical, consider the behavior of the common alga Caulerpa, a single cell whose length can grow to several meters. </i><b><i>This giant cell contains stem, roots, and leaves in one cellular unit undivied by internal walls or membranes (Jacobs, 1994). Although battered mercilessly by pounding waves and gnawed on relentlessly by hungry fish, this single cell cares not a whit; such breaches of integrity do not threaten its survival.  In fact, deliberately cut sections of stem or leaf will grow back into entire cells. Severing the membrane is irrelevant.</i></b><br /><br /><i>Yet another example lies within the domain of expermental genetics, where otherwise genteel scientists will brutally slice innocent cells in two in order to monitor the fates of the respective factions. </i><b><i>When cultured epithelial cells are sectioned by a sharp micropipette, the non-nucleated fraction survives for 1-2 days while the nucleated, centrosome-containing fraction survives indefinitely and can go on to produce progeny (Maniotis and Schliwa, 1991). The cell does not seem to mind that it had just been sliced like a tomato. Sectioned muscle and nerve cells survive similarly</i></b><i> (Yawo and Kuno, 1985; Casdemont et al., 1988; Krause et al., 1984).</i><br /><br /><b><i>Then, there is the crawling cell that tears itself apart as it journeys onward ... Leaving behind a trail of cellular fragments is the rule rather than the exception (Chen, 1981). In the case of fibroblast cells, fragments down to 2% of initial volume continue to show standard behaviors</i></b><i> such as ruffling, blebbing, filopodia production, and contact avoidance (Albrecht-Buehler, 1980). </i><b><i>The fragments remain &#39;alive&#39; for up to eight hours.</i></b><br /><br /><b><i>Finally, and perhaps not surprisingly in light of all that has been said, ordinary mammalian cells are continually in a &#39;wounded&#39; state. Cells that suffer mechanical abrasion such as skin cells, gut endothelial cells and muscle cells are particularly prone to wounds</i></b><i> -- as confirmed by entry into the cell of large traces that ordinarily fail to enter, such as serum albumin, horseradish peroxidase, and dextran (mol. wt. 10,000). </i><b><i>Such wounded cells appear otherwise structurally and functionally normal</i></b><i> (McNeil and Ito, 1990; McNeil and Steinhardt, 1998).  The fraction of wounded cells in different tissues is variable (Table 2.1).  In cardiac muscle cells it is ~20%, but under isoproterenol stimulation the fraction rises to 60% (Clarke et al., 1995).  Thus, </i><b><i>tears in the cell membrane are neither exotic nor arcane; they occur as a common event in normal, physiologically functioning tissue.</i></b><br /><br /><i>It appears we are struck on the horns of a dilemma.  If we insist that a continuous barrier envelops the cell, we need to reconcile the aforementioned series of observations and we need to explain why breaching the barrier is not more consequential than it seems to be. </i><b><i>The anticipated collapse of ion gradients should quickly destroy cellular proteins</i></b><i> (Choi, 1988; Berridge, 1994).  On the other hand, if we concede that the barrier may not be continuous so that creating yet another opening makes little difference, we then face an obstacle of a different nature.  We challenge the long-held dogma of the continuous barrier, as well as the evidence on which the continuous barrier concept is presumably based.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Problematic Origin of the Sodium Pump Hypothesis","summary":"At the Heart of Cell Biology is a Glaring Contradiction","image":"https://lh3.googleusercontent.com/-oEdVfOYg3P4/Vu3l-dDwbtI/AAAAAAAAHXI/fHcRNfj5AKw/w6250-h8750/the-problematic-origin-of-the-sodium-pump-hypothesis-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/WxMoLogxwwV","publishDate":"2016-03-19T23:52:04.796Z","updateDate":"2017-01-03T02:43:21.527Z","text":"<b>The Problematic Origin of the Sodium Pump Hypothesis: At the Heart of Cell Biology is a Glaring Contradiction: The Leading Theory for the Cell Membrane is Seemingly Discounted by Calculations of the Energy and Space Required to Support it / As Increasing Numbers of Problems are Identified for the Theory, the Model Becomes Ever More Complex / Dr. Gerald Pollack of the University of Washington Has Presented the Nature of the Problem in Terms Which Anybody Can Understand</b><br /><br /><a href=\"http://www.cell.com/trends/microbiology/abstract/S0966-842X%2802%2902324-7\" class=\"ot-anchor\">http://www.cell.com/trends/microbiology/abstract/S0966-842X(02)02324-7</a><br /><br /><b>A phase transition for cell biology?</b><br /><i>Trends in Microbiology</i><br />Volume 10, Issue 4, p200, 1 April 2002<br /><br /><i>Cells, Gels and the Engines of Life</i><br />by Gerald Pollack<br />Ebner &amp; Sons, 2001.<br />$55 (hbk)/$27.95 (pbk) (xiv + 305 pages)<br />ISBN 0 96268 951 3/0 96268 952 1<br /><br /><i>&quot;Gerald Pollack has written a rollicking romp through cell biology that should rock the science. He has taken the principles of physical chemistry and applied them to the fundamental processes of the cell in a lucid explanation of how these ‘engines of life’ might work. To be fair, the experimental science is mostly not his own, but represents his deep understanding and appreciation of the work of people such as Gilbert Ling and Albert Szent-Györgyi. </i><b><i>If they are collectively correct, this work represents a paradigm shift in cell biology, and Pollack performs a service comparable to the popularization of Copernicus by Galileo.</i></b><i> (The author is not shy about the extreme nature of his views and draws his own analogy to the Copernican revolution in the opening paragraphs.) </i><b><i>Pollack has hit upon water structuring by proteins and phase changes to explain everything</i></b><i> from cell division to muscle contraction, expanding the idea of phase changes to include everything from voltage changes to conformational changes in proteins. His explanation of the sodium–potassium gradient is simple, clear and elegantly chemical. I like the book most of all because it has a protein-centric view.</i><br /><br /><b><i>The first chapter is the most shocking</i></b><i> and could rest on shakier ground than later chapters where Pollack is in his element. </i><b><i>In the first chapter, the very foundations of membrane biology are questioned: the membrane does not form a barrier, pumps are not pumps at all and the cell is not a water balloon. There are plenty of intriguing experiments and illustrations to support these ideas.</i></b><i> What the membrane and pumps do is left for others to determine, but the phenomena Pollack discusses are real and often at odds with the orthodox explanations, whether the new explanations stand or not. In later chapters, he moves into the two foundations of his new paradigm: water structuring and phase transitions. He is on terra firma when he describes muscle cells or any kind of movement; this is where his prose and his ideas shimmer. Pollack writes simply and lucidly about the meaning of the very fundamental processes of order. He explains how the entropy gained from water destructuring might be used, and generalizes as to the purposes of various types of phase transitions. </i><b><i>The analogies to gels are entirely appropriate, insightful and useful.</i></b><br /><br /><i>‘Cells, Gels...’ is a good read in the sense that you can take it to the beach or it can keep you up all night. This is a book my husband, a physician, might enjoy, or any scientist, amateur or professional, but it probably assumes too much for the general public.&quot;</i><br /><br />[don&#39;t believe that for a second; the first half of this book is completely accessible to the layperson, and the generalist will get exactly what they need from these chapters.]<br /><br /><i>&quot;Pollack has gone out on a limb to shake up the status quo of cell biology and for this alone we should be grateful. The illustrations by David Olsen contribute substantively to communicate the ideas in simple and fun ways. The prose is elegant and eloquent and the analogies are colorful, illuminating and memorable. For example, in speaking of the cooperativity of phase transitions, he writes, ‘Once the critical threshold is crossed, the transition proceeds with the inevitability of a sneeze’. </i><b><i>Pollack’s ideas have made me think more deeply about my own science and, I suspect, will affect the thinking of any life scientist with an open mind and a sense of adventure.</i></b><i> Whether they will withstand the passage of time and experiments to come, we shall see.&quot;</i><br /><br />Felicia Etzkorn<br /><b>Dept of Chemistry, Virginia Tech,</b><br />107 Davidson Hall 0212,<br />Blacksburg, VA 24061-0212, USA.<br />e-mail: fetzkorn@vt.edu<br /><br />If your interest is in scientific controversies, and you want to support researchers who are bringing these controversies to a layperson audience, then you will want to purchase Dr. Gerald Pollack&#39;s groundbreaking book.  Even if all you do is pass it on as a gift to a friend who is into science, the way we can support them is to reward them for their courage to stand up to the establishment.<br /><br /><a href=\"http://www.amazon.com/Cells-Gels-Engines-Gerald-Pollack/dp/0962689521/ref=sr_1_1?ie=UTF8&amp;qid=1458415323&amp;sr=8-1&amp;keywords=cells%2C+gels+and+the+engines+of+life\" class=\"ot-anchor\">http://www.amazon.com/Cells-Gels-Engines-Gerald-Pollack/dp/0962689521/ref=sr_1_1?ie=UTF8&amp;qid=1458415323&amp;sr=8-1&amp;keywords=cells%2C+gels+and+the+engines+of+life</a><br /><br /><i>&quot;Revolutionary and courageous</i><br /><br /><i>This book is heretical and courageous, and - if it can escape burning - may become a seminal landmark in our understanding of living systems.&quot;</i><br /><br />[...]<br /><br /><i>&quot;As a non-biologist (nuclear chemist/experimental physicist) I read Dr. Pollack&#39;s book with interest, in 2006, and found it relevant to our work - and plausible.&quot;</i><br /><br />[...]<br /><br /><i>&quot;Changed My View of Science</i><br /><br /><i>During my senior year of bioengineering at ASU, one of my instructors recommended that I read this book. I went on Amazon and purchased a copy. Tragically, I let it sit on my shelf for almost six months without reading it. About a year ago, I picked it up and read it, expecting it to be a labor-intensive read. It was not. Instead, in very simple terms and using simple yet convincing examples, Pollack managed to challenge everything I ever learned in school in two days&quot;</i><br /><br />[...]<br /><br /><i>&quot;A fresh new look into cell function</i><br /><br /><i>This great book fundamentally changed the way I think about cells. It is not one of your typical cell biology textbooks that only show you the a priori views about cells, leaving out the findings not fitting their overall picture. This book takes you from the very basic laws of physics and chemistry and explains the common cell functions step by step. On the way, it challenges many dogmatic views about cells and introduces very logical and simple new mechanisms. It has a great narrative, which makes you think and speculate, almost like a good mystery novel. I strongly recommend this book to any open-minded science-lover with an interest in cell biology.&quot;</i><br /><br />There are in fact many more reviews just like those, but let&#39;s get on with it ...<br /><br /><i>Cells, Gels and the Engines of Life: A Unifying Approach to Cell Function</i><br />Dr. Gerald Pollack<br /><br />the first 17 pages:<br /><br /><i>&quot;Long ago, scientists believed that the center of the universe was the earth: The sun could be seen to traverse the heavens, so it was logical to conclude that the earth must lie at the center point.</i><br /><br /><i>But this view eventually encountered difficulties. As the growth of mathematics increased the power of astronomy, it became possible to compute orbital pathways. The planets’ paths around the earth turned out to be less simple than anticipated; each planet followed an orbit called an epicycle (Figure, opposite), which was sufficiently intricate to imply that something was surely amiss.</i><br /><br /><i>What was amiss is no longer a mystery. Although the persistent notion of an earth-centered universe may gratify our collective egos, Galileo showed that it was the sun that held this honor. With the sun at the solar system’s center, orbital paths no longer required complex epicycles; they became a lot simpler. </i><b><i>What had earlier seemed a reasonable hypothesis supported by seemingly indisputable visual observation, turned out to be dead wrong. A complicated paradigm was replaced by a simpler one.</i></b><br /><br /><b><i>Is Life Really Any Different Now?</i></b><br /><br /><b><i>In the field of cell biology at least, complicated paradigms raise similar concern.</i></b><i> On the surface everything seems to be in order. Virtually all known cellular processes are by now accounted for by well-described mechanisms: ions flow through channels; solutes are transported by pumps; vesicles are moved by motors; etc. For every problem there is a solution. But as we shall see as we probe beneath the surface of these solutions, a bewildering level of complexity hints at a situation that could parallel the epicycles.</i><br /><br /><b><i>I propose to step back and regroup. For genuine progress, foundational concepts must be unquestionably sound; otherwise an edifice of understanding may rise over a crevasse of uncertainty -- no apparent problem until the edifice grows weighty enough to crack the foundation and tumble into the abyss. Firm ground needs to be identified.</i></b><br /><br /><b><i>I begin by considering two elements thought to be fundamental to cell function: membrane pumps and channels. Pumps transport solutes across the cell boundary against their respective concentration gradients. Channels permit the solutes to trickle back in the opposite direction. Through a balance between pump-based transport and channel-based leakage, the characteristic partitioning of solutes and ions is thought to be established.</i></b><br /><br /><i>Thus, potassium concentration is relatively higher inside the cell, and sodium is relatively higher outside (Table 1.1).</i><br /><br /><i>That pumps and channels exist seems beyond doubt -- or to put it more precisely, the existence of proteins with pump-like or channel-like features cannot be doubted. Genes coding for these proteins have been cloned, and the proteins themselves have been exhaustively studied. There can be no reason why their existence might be challenged.</i><br /><br /><i>Where some question could remain is in the functional role of these proteins. What I will be considering in this chapter is whether these proteins really mediate ion partitioning. </i><b><i>Because a &#39;pump&#39; protein inserted into an artificial membrane can translocate an ion from one side of the membrane to the other, can we be certain that ion partitioning in the living cell necessarily occurs by pumping?</i></b><br /><br /><i>This task of checking this presumption may in this case be approached through the portal of historical perspective. Scientists on the frontier often dismiss history as irrelevant but in this particular instance a brief look into the trail of discovery is especially revealing.</i><br /><br /><i>Origins</i><br /><br /><b><i>The emergence of pumps and channels was preceded by the concept of the cell membrane. The latter arose during the era of light microscopy, prior to the time any such membrane could actually be visualized.</i></b><i> Biologists of the early nineteenth century observed that a lump of cytoplasm, described as a &#39;pulpy, homogeneous, </i><b><i>gelatinous</i></b><i> substance&#39; (Dujardin, 1835) </i><b><i>did not mix with the surrounding solution.</i></b><br /><br /><b><i>To explain why this gelatinous substance did not dissolve, the idea arose that it must be enveloped by a water-impermeant film.</i></b><i> This film could prevent the surrounding solution from permeating into the cytoplasm and dissolving it. The nature of the membranous film had two suggested variants. Kühne (1864) envisioned it as a layer of coagulated protein, while Schültze (1863) imagined it as a layer of condensed cytoplasm. </i><b><i>Given the experimental limitations of the era, the nature of the putative film, still not visualizable, remained uncertain.</i></b><br /><br /><b><i>The idea of an invisible film was nevertheless attractive to many of the era’s scientists, and was increasingly conferred with special attributes.</i></b><i> Thus, Theodore Schwann (1839) viewed this film as </i><b><i>&#39;prior in importance to its contents.&#39; The membrane grew in significance to become the presumed seat of much of the cell’s activity. Yet this view was not accepted by all.</i></b><i> Max Schültze, often referred to as the father of modern biology, discounted the evidence for a cytoplasmic film altogether, and instead regarded cells as &#39;membraneless little lumps of protoplasm with a nucleus&#39; (Schültze, 1861). </i><b><i>In spite of Schültze’s prominence, the concept of an enveloping membrane held firm.</i></b><br /><br /><i>The modern idea that the membrane barrier might be semi-permeable came from the plant physiologist Wilhelm Pfeffer. Pfeffer was aware of the ongoing work of Thomas Graham (1861) who had been studying </i><b><i>colloids, which are large molecules suspended indefinitely in a liquid medium —- e.g., milk. According to Graham’s observations, colloids could not pass through dialysis membranes although water could. To Pfeffer, colloids seemed to resemble the cytoplasm. If the dialysis membrane were like the cell membrane, Pfeffer reasoned, the cell interior would not dissipate into the surrounding fluid even though the membrane might still be water-permeable. Thus arose the idea of the semi-permeable membrane.&quot;</i></b><br /><br />[see graphic]<br /><br /><i>&quot;Pfeffer took up the semi-permeable membrane idea and pursued it. He carried out experiments on membrane models made of copper ferrocyanide, which acted much like dialysis membranes in that they could pass water easily but solutes with great difficulty. It was on these experiments that Pfeffer based the modern cell-membrane theory (Pfeffer, 1877). The membrane at this stage was presumed permeable to water, but little else.</i><br /><br /><b><i>Although Pfeffer’s theory held for some time, it suffered serious setbacks when substances presumed unable to cross the membrane turned out to cross. The first and perhaps most significant of these substances was potassium. The recognition, in the early twentieth century, that potassium could flow into and out of the cell (Mond and Amson, 1928; Fenn and Cobb, 1934), prompted a fundamental rethinking of the theory.</i></b><br /><br /><i>Origin of the Channel</i><br /><br /><i>Faced with the need to explain the potassium-permeability issue, Boyle and Conway (1941) proposed an elegant solution: </i><b><i>the potassium channel. Since the hydrated potassium ion was known to be smaller than the hydrated sodium ion, 3.8 Å vs. 5 Å, Boyle and Conway proposed transmembrane channels of critical size -- large enough to pass potassium and its shell of associated water, but small enough to exclude sodium with its shell. The membrane was effectively a sieve that passed small ions, but excluded larger ones.</i></b><br /><br /><i>The Boyle-Conway atomic sieve theory was attractive in that it could also account for several known features of cell behavior without too much difficulty. </i><b><i>It explained the accumulation of potassium inside the cell as an attraction to the cell’s negatively charged proteins (a so-called Donnan effect). It explained the cell potential as arising from a charge separation across the membrane (a capacitive effect). And it accounted for the changes of cell volume that could be induced by changes of external potassium concentration (an osmotic effect). The sieve theory seemed to explain so much in a coherent manner that it was immediately granted an exalted status.</i></b><br /><br /><b><i>But another problem cropped up, perhaps even more serious than the first. The membrane turned out to be permeable also to sodium</i></b><i> (Fig. 1.1). The advent of radioactive sodium made it possible to trace the path of sodium ions, and a cadre of investigators promptly found that sodium did in fact cross the cell boundary (Cohn and Cohn, 1939; Heppel, 1939, 1940; Brooks, 1940; Steinbach, 1940). </i><b><i>This finding created a problem because the hydrated sodium ion was larger than the channels postulated to accommodate potassium; sodium ions should have been excluded, but they were not. Thus, the atomic-sieve theory collapsed.</i></b><br /><br /><i>Collapse notwithstanding, the transmembrane-channel concept remained appealing. One had to begin somewhere. </i><b><i>A channel-based framework could circumvent the sodium problem with separate channels for sodium and potassium: if selectivity were based on some criterion other than size, then distinct channels could suffice. A separate channel for sodium could then account for the observed leakage of sodium ions into the cell.</i></b><br /><br /><b><i>But leakage of sodium introduced yet another dilemma, of a different nature. Sodium could now pass through the channel, flowing down its concentration gradient and accumulating inside the cell. How then could intracellular sodium remain as low as it is?</i></b><br /><br /><i>Origin of the Pump</i><br /><br /><b><i>The solution was to pump it out. In more-or-less the same manner as a sump-pump removes water that has leaked into your basement, a membrane pump was postulated to rid the cell of the sodium that would otherwise have accumulated inside.</i></b><br /><br /><b><i>The idea of a membrane pump actually originated before the sodium problem. It began at the turn of the last century</i></b><i> with Overton, a prominent physiologist who had advanced the idea that the membrane was made of lipid. Realizing that some solutes could cross an otherwise impermeable lipid membrane, Overton postulated a kind of secretory activity to handle these solutes. Through metabolic energy, the membrane could thus secrete, or pump, certain solutes into or out of the cell.</i><br /><br /><i>The pump concept resurfaced some forty years later (Dean, 1941), to respond specifically to the sodium-permeability problem. Dean did not have a particular pumping mechanism in mind; in fact, the sodium-pump was put forth as the least objectionable of alternatives.* Thus, Dean remarked, &#39;It is safer to assume that there is a pump of unknown mechanism which is doing work at a constant rate excreting sodium as fast as it diffuses into the cell.&#39; With this, the sodium pump (later, the Na/K exchange pump) came decidedly into existence.</i><br /><br /><b><i>By the mid-twentieth century, then, the cell had acquired both channels and pumps.</i></b><i> With channels for potassium and sodium, along with pumps to restore ion gradients lost through leakage, the cell’s electrophysiology seemed firmly grounded. Figure 1.2 says it all.</i><br /><br /><i>Reflections</i><br /><br /><i>Why have I dragged you through so lengthy a review? </i><b><i>My purpose was to demonstrate how channels and pumps arose. They came into being not because some alert scientist stumbled upon them during a groundbreaking session at the electron microscope, but as ad hoc hypotheses needed to patch otherwise flagging theories.</i></b><i> The channel arose when a putatively ion-impermeant membrane was found to pass potassium; the channel could pass potassium while properly excluding sodium and other larger hydrated ions. </i><b><i>Then, sodium was found to enter the cell and instead of reconsidering the channel concept, a second channel specific for sodium was postulated. Sodium permeability also implied a persistent leak into the cell. To keep gradients from collapsing, a sodium pump was postulated.</i></b><br /><br /><b><i>Once this hypothetical framework gained a foothold, it expanded boundlessly. For the same reason that a sodium pump was needed, it became evident that pumps for other solutes were needed as well. Virtually all of the cell’s solutes partition far out of electrochemical equilibrium (Stein, 1990) and therefore need to be pumped. Hydrogen-ion pumps, calcium pumps, chloride pumps, and bicarbonate pumps to name a few, soon came into being over and above the postulated sodium/potassium pumps. Yet, even at the height of this intense activity, Glynn and Karlish (1975) in their classic review had to reluctantly admit that notwithstanding an enormous thrust of experimental work on the subject, still no hypothesis existed to explain how pumps pump.</i></b><br /><br /><b><i>The channel field exploded similarly. With the advent of the patch-clamp technique (see below) in the late 1970s, investigators had gained the capacity to study what appeared to be single ion channels. It seemed for a time that new channels were being identified practically monthly, many of them apparently selective for a particular ion or solute. *The number of channels has risen to well over 100.</i></b><i> Even water channels have come into being (Dempster et al., 1992). Elegant work was carried out to try to understand how channels could achieve their vaunted selectivity (Hille, 1984). At least some channels, it appeared, could pass one ion or solute selectively, while excluding most others.</i><br /><br /><i>Given the astonishing expansion of activity in these fields, could there be any conceivable basis for doubt? Mustn’t the concepts of pumping and channeling be as firmly grounded as any biological principle?</i><br /><br /><i>It is tempting to answer by placing side-by-side the key experiments originally adduced to confirm pumping and channeling together with the published challenges of those experiments (Troshin, 1966; Hazlewood, 1979; Ling, 1984, 1992). I hesitate to recapitulate that debate because the challenges are largely technical. Readers willing to invest time in acquiring familiarity with technical details are invited to consult these sources and consider whether the published concerns are valid or not.</i><br /><br /><b><i>Another approach is to consider evidence that could potentially lie in conflict with these concepts. Unlike mathematical theorems, scientific theories cannot be proven. No matter how much evidence can be marshaled in support of a theory, it is always possible that some new piece of evidence will be uncovered that does not fit, and if such evidence is both sound and fundamental, the theory may require reconsideration. As we shall see in the next sections, certain basic questions concerning pumps and channels have not yet been adequately dealt with.</i></b><br /><br /><i>Channels Revisited</i><br /><br /><b><i>The existence of single ion channels appeared to be confirmed by groundbreaking experiments using the patch-clamp technique. In this technique the tip of a micropipette is positioned on the cell surface. Through suction, a patch of membrane is plucked from the cell and remains stuck onto the micropipette orifice (Fig. 1.3A). A steady bias voltage is placed across the patch, and the resulting current flow through the patch is measured. This current is not continuous; it occurs as a train of discrete pulses. Because the pulses appear to be quantal in size, each pulse is assumed to correspond to the opening of a single ion channel.&quot;</i></b><br /><br />[see graphic]<br /><br /><i>&quot;This dazzling result has so revolutionized the field of membrane electrophysiology that the originators of the technique, Erwin Neher and Bert Sakmann, were awarded the Nobel Prize. </i><b><i>The observation of discrete events would seem to confirm beyond doubt that the ions flow through discrete channels.</i></b><br /><br /><b><i>Results from the laboratory of Fred Sachs, on the other hand, make one wonder. Sachs found that when the patch of membrane was replaced by a patch of silicon rubber, the discrete currents did not disappear (Sachs and Qin, 1993); they remained essentially indistinguishable from those measured when the membrane was present (Fig. 1.3B). Even more surprisingly, the silicon rubber sample showed ion-selectivity features essentially the same as the putative membrane channel.</i></b><br /><br /><b><i>A similarly troubling observation was made on polymer samples (Lev et al., 1993). Current flow through synthetic polymer filters was found to be discrete, just as in silicon rubber (Fig. 1.3C). The filters also showed features commonly ascribed to biological channels such as ion selectivity, reversal potential, and gating. Yet, the sample was devoid of any protein or lipid.&quot;</i></b><br /><br />Oops.<br /><br /><i>&quot;In yet another set of experiments, channel-like behavior was observed in pure lipid-bilayer membranes (Woodbury, 1989). Following brief exposure to large concentrations of lipid vesicles ejected from a pipette tip approximately 0.5 mm distant, these membranes showed typical channel-like fluctuations (Fig. 1.3D). </i><b><i>Conductance changed in ways usually considered to be indicative of reconstituted protein channels—including step conductance changes, flickering, ion selectivity, and inactivation. But no channels were present; the membranes contained only lipid.</i></b><br /><br /><b><i>What are we to do with such observations? It is clear from these three studies that the discrete currents previously taken to confirm the existence of single biological channels seem to be general features of current flow through small samples.</i></b><i> The currents presumably arise from some common feature of these specimens that is yet to be determined, but evidently not from single channels since they are absent. The channels may exist—but the prime evidence on which their existence is based is less than conclusive.</i><br /><br /><i>Ironically, the silicon-rubber test had actually been carried out as a control in the original patch clamp studies (Neher et al., 1978). The authors did sometimes note &#39;behavior contrary&#39; to what was expected (p. 223); but such behavior was dismissed as having arisen from irregularities of the pipette tip. </i><b><i>The possibility that small samples in general might give rise to channel-like behavior was apparently not considered.</i></b><br /><br /><i>Setting aside the above-mentioned concern, a second point to consider is the manner in which the channel achieves its specificity. Channels exist for each one of the cell’s ions; additional channels exist for amino acids, peptides, toxins, and sugars, most of these being otherwise unable to cross the lipid bilayer; and as I mentioned, there are also channels for water. Thus, a plethora of channels exists, most engineered to be solute-specific. How is such specificity achieved?</i><br /><br /><b><i>To explain such exquisite specificity, models of some complexity have evolved (Hille, 1984; 1992). One model contains 16 different transition states, plus additional sub-states. Another contains 27 states. Most models are sufficiently complex that the solution requires numerical methods. Indeed, calculating the trajectory of a molecule diffusing through a channel during a 100-picosecond time window is the work of a supercomputer. Thus, models of daunting complexity are required for understanding the channels’ apparent selectivity. It is not a simple process.</i></b><br /><br /><i>The naive question nevertheless lingers: How is it that small solutes do not pass through large channels? To imagine how one or two small ions might be excluded from a large channel as a result of a distinctive electric field distribution is not too difficult to envision. But the theory implies that smaller solutes should be excluded as a class -- otherwise, independent channels for these solutes would not be required. This enigma harks of the dog-door analogy (Fig. 1.4). </i><b><i>Why bother adding a cat door, a ferret door, a hamster door and a gerbil door when these smaller animals could slip readily through the dog door? Must some kind of repellent be added for each smaller species?</i></b><br /><br /><i>The seriousness of the size problem is illustrated by considering the hydrogen ion. </i><b><i>The hydrogen ion is only about 0.5 Å in diameter (the hydrated ion somewhat larger). The sodium-channel orifice is at least 3 - 5 Å across, and channels specific for some of the larger solutes must have a minimum orifice on the order of 10 - 20 Å. How is it possible that a 20 Å channel could exclude a diminutive hydrogen ion? It is as though a two-foot sewer pipe that easily passes a beach ball could at the same time exclude golf balls, as well as tennis balls, billiard balls, etc.</i></b><br /><br /><i>Arguably, the situation is not so black and white. Textbook depictions of the channel as a hollow tube oversimplify the contemporary view of the channel as a convoluted pathway; and the process of selectivity is thought to rest not on size per se but on some complex interaction between the solute’s electric field and structural features of the channel’s filter (e.g., Doyle et al., 1998). Also, channel selectivity is not absolute (Hille, 1972). Nevertheless, the issue of passing only one or a few among a field of numerous possible solutes including many smaller ones remains to be dealt with in a systematic manner. And the issue of non-biological samples producing single-channel currents certainly needs to be evaluated as well. What could all this imply?</i><br /><br /><i>Pumps Revisited</i><br /><br /><i>Like channels, pumps come in many varieties and most are solute-specific. </i><b><i>The number easily exceeds 50. The need for multiple pumps has already been dealt with: unless partitioning between the inside and outside of the cell is in electrochemical equilibrium, pumping is required. Because so few solutes are in equilibrium, one or more pumps are necessary for each solute.</i></b><br /><br /><b><i>A question that arises is how the cell might pump a solute it has never seen. Antibiotics, for example, remain in high concentration outside the bacterial cell but in low concentration inside. Maintaining the low intracellular concentration implies the need for a pump, and in fact, a tetracycline pump for E. coli has been formally proposed (Hutchings, 1969). A similar situation applies for curare, the exotic arrow poison used experimentally by biophysicists.  Because curare partitioning in the muscle cell does not conform to the Donnan equilibrium, a curare pump has been proposed (Ehrenpries, 1967). To cope with substances it has never seen, the cell appears to require pumps over and above those used on a regular basis—on reserve.</i></b><br /><br /><i>How is this possible? One option is for existing pumps to adapt themselves to these new substances. But this seems illogical, for </i><b><i>if they could adapt so easily why would they have been selective to begin with?</i></b><i> An alternative is for the cell to synthesize a new pump each time it encounters a foreign substance. But this option faces the problem of limited space: Like the university parking lot, </i><b><i>the membrane has just so many spaces available for new pumps (and channels).</i></b><i> Given chemists’ ability to synthesize an endless variety of substances -- </i><b><i>10 million</i></b><i> new chemical substances have been added to the American Chemical Society’s list of molecules during the last quarter century alone (N. Y. Times, Feb 22, 2000) -- </i><b><i>how could a membrane already crowded with pumps and channels accommodate all that might eventually be required?</i></b><i> Could a membrane of finite dimension accommodate a potentially infinite number of pumps?</i><br /><br /><b><i>A second question is how the cell musters the energy required to power all of its pumps. Where might all the ATP come from? Since ions and other solutes cross the membrane continually even in the resting state (in theory because of sporadic channel openings), pumps must run continuously to counteract these leaks. Pumping does not come free. The sodium pump alone has been estimated, on the basis of oxygen-consumption measurements, to consume 45 - 50% of all the cell’s energy supply (Whittam, 1961). Current textbooks estimate a range of 30 - 35%.</i></b><br /><br /><i>To test whether sufficient energy is available to power pumping, a well known experiment was carried out long ago by Ling (1962). Ling focussed on the sodium pump. The idea was to expose the cell to a cocktail of metabolic poisons including iodoacetate and cyanide, and to deprive it of oxygen -- </i><b><i>all of which would deplete the cell of its energy supply and effectively pull the pump’s plug. If these pumps had been responsible for maintaining sodium and potassium gradients, the gradients should soon have collapsed. But they did not. After some eight hours of poison exposure and oxygen deprivation, little or no change in cellular potassium or sodium was measurable.</i></b><br /><br /><i>Ling went on to quantitate the problem. He computed the residual energy -- the maximum that could conceivably have been available to the cell following poisoning. This residual was compared to the energy required to sustain the ion gradient, the latter calculable from the known sodium-leak rate. </i><b><i>Using the most generous of assumptions, a conservative estimate gave an energy shortfall of 15 to 30 times (Ling, 1962). The pump energy needed to sustain the observed gradient, in other words, was concluded to be at least 15 to 30 times larger than the available energy supply.</i></b><br /><br /><i>This conclusion stirred a good deal of debate. The debate was highlighted in a Science piece written by the now well-known science writer Gina Kolata (1976), a seemingly balanced treatment that gave credence to the arguments on both sides. Kolata cited the work of Jeffrey Freedman and Christopher Miller who had challenged Ling’s conclusion about the magnitude of the energy shortfall. Ling’s late-coming rebuttal (1997) is a compelling &#39;must-read&#39; that considers not only this specific issue but also the process of science. </i><b><i>The energy-shortfall claim was nevertheless left to gather dust with the advent of pump-protein isolation -- forgotten by all but a modest cadre of researchers who have remained steadfastly impressed by the arguments (cf. Tigyi et al., 1991).</i></b><br /><br /><i>In retrospect, </i><b><i>any such niggling debate about the magnitude of the sodium-pump-energy shortfall seems academic, for it is now known that numerous other pumps also require power.</i></b><i> Over and above sodium and potassium, the cell membrane contains pumps for calcium, chloride, magnesium, hydrogen, bicarbonate, as well as for amino acids, sugars, and other solutes. Still more pumps are contained in organelle membranes inside the cell: In order to sustain intra-organelle ion partitioning, organelles such as the mitochondrion and endoplasmic reticulum contain pumps similar to those contained in the surface membrane. Given leak rates that are characteristically proportional to surface area, we are not speaking here of trivial numbers of pumps: Liver cell mitochondria contain 20 times the surface area of the liver cell membrane (Lehninger, 1964), and the area of the muscle’s sarcoplasmic reticulum is roughly 50 times that of the muscle cell membrane (Peachey, 1965). </i><b><i>Membranes of such organelles must therefore contain pumps in numbers far higher than those of the cell membrane -— all requiring energy.</i></b><br /><br /><i>In sum, pumping faces obstacles of space and energy. The membrane’s size is fixed but the number of pumps will inevitably continue to grow. At some stage the demand for space could exceed the supply, and what then? Pumping also requires energy. The Na/K pump alone is estimated to consume an appreciable fraction of the cell’s energy supply, and that pump is one of very many, including those in internal membranes. How is the cell to cope with the associated energy requirement?&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Cancer as a Fungus","summary":"Fungi is the Most Powerful and Most Organized Micro-organism Known / They Exhibit an Unlimited Pathogenic Potential Due to their Variety of Reproductive Manifestations and Ability to Morph Features to Suit their Environment / Cancer and Fungus Share Numerous Key Attributes, and Can Be Confused for One Another / Can Explain Why Cannabis Extractions Can Exhibit Anti-Cancer Properties / The Theory Provides a Simple Theoretical Foundation to Prevent and Combat the Disease","image":"https://lh4.googleusercontent.com/-d6HzD7eSs2Y/Vuuw3gZK0DI/AAAAAAAAHUc/PHAJf1rnxMwBxjP1ZjciLQ6ipUExziAZQ/cancer-as-a-fungus-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/TJ45cMZTXJK","publishDate":"2016-03-18T07:42:56.038Z","updateDate":"2017-01-03T02:43:37.546Z","text":"<b>Cancer as a Fungus: Fungi is the Most Powerful and Most Organized Micro-organism Known / They Exhibit an Unlimited Pathogenic Potential Due to their Variety of Reproductive Manifestations and Ability to Morph Features to Suit their Environment / Cancer and Fungus Share Numerous Key Attributes, and Can Be Confused for One Another / Can Explain Why Cannabis Extractions Can Exhibit Anti-Cancer Properties / The Theory Provides a Simple Theoretical Foundation to Prevent and Combat the Disease</b><br /><br />Let&#39;s start out by reviewing the paradigm change said to be occurring right now in cancer research:<br /><br /><b>The Pharmaceutical Giants Are &quot;Scanning the [Microbiome] Space&quot; for a Cancer Cure</b><br /><br /><i>&quot;How Gut Bacteria Are Shaking Up Cancer Research</i><br /><i>Makiko Kitamura</i><br /><i>March 13, 2016</i><br /><br /><i>Top scientists at Roche Holding AG and AstraZeneca Plc are sizing up potential allies in the fight against cancer: the trillions of bacteria that live in the human body.</i><br /><br /><b><i>&#39;Five years ago, if you had asked me about bacteria in your gut playing an important role in your systemic immune response, I probably would have laughed it off,&#39; Daniel Chen, head of cancer immunotherapy research at Roche’s Genentech division, said in a phone interview. &#39;Most of us immunologists now believe that there really is an important interaction there.&#39;</i></b><br /><br /><i>Two recent studies published in the journal Science have intrigued Chen and others who are developing medicines called immunotherapies that stimulate the body’s ability to fight tumors.</i><br /><br /><i>In November, University of Chicago researchers wrote that giving mice Bifidobacterium, which normally resides in the gastrointestinal tract, was as effective as an immunotherapy in controlling the growth of skin cancer. Combining the two practically eliminated tumor growth. In the second study, scientists in France found that some bacterial species activated a response to immunotherapy, which didn’t occur without the microbes.</i><br /><br /><i>Human Microbiome</i><br /><br /><i>That’s increased drugmakers’ interest in the human microbiome -- the universe of roughly 100 trillion good and bad bacteria, fungi and viruses that live on and inside the body. </i><b><i>Roche is already undertaking basic research in the field and plans to investigate the microbiome’s potential for cancer treatment, Chen said.</i></b><br /><br /><i>*&#39;Certainly, we are already scanning the space for interesting opportunities as the science continues to emerge,&#39; he said. &#39;We are very interested in testing these in a controlled setting.&#39;</i><br /><br /><i>Some experienced investors are skeptical and see the possibility of an approved product for cancer to be at least five years away.</i><br /><br /><i>&#39;To therapeutically influence the microbiome long-term in humans is a big hurdle,&#39; said Sander van Deventer, managing partner at venture-capital firm Forbion Capital Partners. &#39;The microbiome is very stubborn. Everything we’ve done so far has only had a temporary effect.&#39;</i><br /><br /><b>Cancer as a Fungus</b><br /><br />It can oftentimes become difficult to track down the best arguments of a debate.  To my knowledge, the strongest argument on this particular claim is here.  And it&#39;s definitely worth listening to before dismissing the idea:<br /><br /><i>Cancer is a Fungus: A Revolution in Tumor Therapy</i>, Dr. T. Simoncini, pages 99-115<br /><br /><i>&quot;it is useful to remember the example of heliobacter pylori, a micro-organism whose pathogenic value has always been neglected, and which turned out to be the causal agent for gastric ulcers.</i><br /><br /><i>In view of the data available, the answer for neoplastic disease may be hidden in one of the elements of microbiology, that is, bacteria, viruses, or mycetes.&quot;</i><br /><br /><b>DEFINITION:</b> <b><i>&quot;Neoplastic disease</i></b><i> is the excessive division of cells, due to a variety of causes, that results in the formation of atypical bodies of tissue called neoplasms. A neoplasm can form virtually anywhere in the body and is commonly referred to as a tumor.&quot;</i> (<a href=\"http://www.wisegeekhealth.com/what-is-neoplastic-disease.htm\" class=\"ot-anchor\">http://www.wisegeekhealth.com/what-is-neoplastic-disease.htm</a>)<br /><br /><b>DEFINITION:</b> <b><i>&quot;Mycetes:</i></b><i> a collective term for fungi, esp microscopic pathogenic fungi&quot;</i> (<a href=\"http://www.collinsdictionary.com/dictionary/english/mycetes\" class=\"ot-anchor\">http://www.collinsdictionary.com/dictionary/english/mycetes</a>)<br /><br /><i>&quot;The solution emerges as soon as we turn to clinical experience which, combined with a wealth of descriptive elements, can only suggest that the mycetes or fungus is the sole agent responsible. </i><b><i>It is the only one that can explain the incubation, rooting, course, and symptomatology for tumors.</i></b><i> There are also several clinical considerations in favor of this theory.</i><br /><br /><i>1) Generally speaking, </i><b><i>a bacteriological infection with a non-occasional pathogenic character causes elevated levels of exhaustion and the regression of exhaustion in a short time.</i></b><br /><br /><b><i>It is accompanied by an elevated fever, it generally produces a pathology of the organs with related symptomatology which is globally stereotyped. Its action, therefore, takes place in a determined organ and in a precise temporal space.</i></b><br /><br /><i>2) A viral infection exclusively of the endocellular type is characterized by such rapidity, and ensuing hyper-pyretic climax (high fever), as to be </i><b><i>sometimes instantaneous because of the viruses&#39; small dimensions, which allow for immediate diffusion.</i></b><br /><br /><b><i>Its action takes place and terminates in an extremely short time, with cellular effects that are exclusively necrotizing, but not in the direction of a metaplastic induction power.&quot;</i></b><br /><br /><b>DEFINITION:</b> <b><i>&quot;Necrotic tissue</i></b><i> is dead tissue, which usually results from an inadequate local blood supply. Necrotic tissue contains dead cells and debris that are a consequence of the fragmentation of dying cells. Necrotic tissue changes color from red to brown or black/purple, as it becomes more dehydrated.&quot;</i> (<a href=\"http://www.medscape.com/viewarticle/459733_5\" class=\"ot-anchor\">http://www.medscape.com/viewarticle/459733_5</a>)<br /><br /><b>DEFINITION:</b> <b><i>&quot;Metaplasia</i></b><i> ... is the reversible replacement of one differentiated cell type with another mature differentiated cell type. The change from one type of cell to another may generally be a part of normal maturation process or caused by some sort of abnormal stimulus. </i><b><i>In simplistic terms, it is as if the original cells are not robust enough to withstand the new environment, and so they change into another type more suited to the new environment.</i></b><i> If the stimulus that caused metaplasia is removed or ceases, tissues return to their normal pattern of differentiation.&quot;</i> (<a href=\"https://en.wikipedia.org/wiki/Metaplasia\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Metaplasia</a>)<br /><br /><i>&quot;In the cases where, instead, chronic pathology is produced, such as cirrhotic hepatitis, the causal agent can be found easily.</i><br /><br /><i>3) The morbid manifestations of bacterial origin with a more marked tendency to become chronic belong to that species of germs that show as mycetes, or, rather, their spores do, with a high content of lipids (fatty acids) in their structure.</i><br /><br /><i>It is for this reason that the bacillus of a chronic bacterial disease such as tuberculosis is called a mycobacterium.</i><br /><br /><i>4) Even real oncogenic viruses (actually rare) for which an actual malignancy in animals has been observed show a high lipid content in their structure.</i><br /><br /><i>From everything we have considered so far, the most logical and real cause of neoplastic proliferation would seem to be a fungus, and most likely one of those somehow pathogenic to humans.</i><br /><br /><b><i>The World of Fungi</i></b><br /><br /><i>As early as the nineteenth century, the German biologist Ernest Haeckele (1834-1919), when using the Linnean concept [37] that makes two great kingdoms out of all living things -- that of vegetables and that of animals -- reported difficulties of classification for all those microscopic organisms that because of the properties and characteristics could not be put into either the animal or the vegetable kingdom. He, therefore, proposed a third kingdom, that he called Protist.</i><br /><br /><i>O. Verona says:</i><br /><br /><i>&#39;This vast and complex world ranges from entities with a subcellular structure -- and here we are at the limit of life -- such as viruses and viroidals, to get -- through microplasms -- to other organisms of higher organization: bacteria, actingomycetes, myxomycetes, fungi, protozoa, and if we want, even some microscopic algae.&#39; [38]</i><br /><br /><b><i>The common element of all these organisms is the feeding system, which, save for a few exceptions, takes place with direct absorption of soluble organic compounds. That differentiates them from both animals, which nourish themselves by ingesting solid organic materials that are transformed with digestive processes, and from vegetables, which synthesize the organic substance from mineral compounds through light energy.</i></b><br /><br /><i>Although perfected, the current tendency of biologists is to adopt the concept of the Third Kingdom. </i><b><i>Some go even further, arguing how fungi must be allocated to a different classification.</i></b><br /><br /><i>Again, O. Verona:</i><br /><br /><i>&#39;If we put in the first kingdom pluri-cellular organisms provided with photosynthetic abilities (plants) and in the second the organisms not provided with photosynthetic pigmentation (animals) -- both constructed of cells with distinct nuclei (eukarytotes), and additionally we put in another kingdom, finally, to possess a distinct nucleus.&#39; [39]</i><br /><br /><b><i>Furthermore, unlike all other micro-organisms, they possess a curious property, that of having a basic microscopic structure ... and at the same time the tendency to reach remarkable dimensions (even several kilograms) while they keep unchanged their ability to adapt and to reproduce at any size.</i></b><br /><br /><b><i>In this way, therefore, they cannot be properly considered as organisms but as aggregates of cells of their own kind with an organismic behavior, since each cell keeps its potential for survival and reproduction intact and independent of the structure to which it belongs. Therefore, it is strikingly clear how very difficult it becomes to identify such complex living realities in all their biological processes. In fact, even today, there are huge gaps and approximations in the taxonomic characteristics used in mycology.&quot;</i></b><br /><br />Image Caption / Aside:<br /><br /><i>&quot;Typically, fungi start from spores, growing as filaments called hyphae, with a diameter of about 5-10 microns thick </i><b><i>(it isn&#39;t that hyphae grow from a body; the fungus itself is a hypha).</i></b><br /><br /><i>As hyphae grow, they continuously produce new ramifications.</i><br /><br /><b><i>As hyphae of a single fungus come in contact with those of other fungine cellular units, they form an organism of bigger dimensions called a mycelium. The layer of mold on bread can give an example of this aggregation. All its mass is a single fungus but, if it is subdivided in smaller parts, still these form living, autonomous units.</i></b><br /><br /><b><i>Hyphae grow at their apexes ... transporting their protoplasm (the internal, cellular substance) into the space where they grow.</i></b><br /><br /><b><i>This mechanism allows a steady growth towards new nutritional areas, even through the penetration of solid surfaces, such as are the cell walls of plant and of living organisms.&quot;</i></b><br /><br />Book continues ...<br /><br /><i>&quot;It is worth the trouble to stop and examine in greater depth this strange world with its peculiar characteristics, while attempting to highlight those elements that somehow pertain to the problems of oncology.</i><br /><br /><i>1) Fungi are heterotrophic organisms </i><b><i>(they depend on ready-made food)</i></b><i> and therefore </i><b><i>they need pre-formed compounds to obtain carbon and nitrogen. The simple carbohydrates of these elements (for example monosaccharide glucose, fructose, and mannose) are the sugars that are the most utilized.</i></b><br /><br /><b><i>This means that in their life cycle they depend for nutrition on other living beings which have to be exploited in various ways, both in a saprophytic (nourishment through organic waste) and in a parasitic manner (attaching themselves directly to the tissues of the host).</i></b><br /><br /><i>2) They are classified as eumycetes with sexual reproduction (that is, perfect fungi) and as deuteromycetes with asexual reproduction that does not stem from any fecundation process.</i><br /><br /><i>In both cases, the reproductive cycle takes place through unicellular or pluricellular spores. [40]&quot;</i><br /><br /><b>DEFINITION:</b> <b><i>&quot;Polymorphism</i></b><i> [1] in biology and zoology is the occurrence of two or more clearly different morphs or forms, also referred to as alternative phenotypes, in the population of a species. In order to be classified as such, morphs must occupy the same habitat at the same time and belong to a panmictic population (one with random mating).[2]&quot;</i> (<a href=\"https://en.wikipedia.org/wiki/Polymorphism_%28biology%29\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Polymorphism_(biology)</a>)<br /><br /><b><i>&quot;The extreme complexity of the reproductive panorama of fungi is exceedingly interesting, because their very polymorphism highlights such biological variety that we can infer an unlimited adaptability and therefore an unlimited pathogenic potentiality. In this way, the great variety of reproductive manifestations (sexual, asexual, gemmation, which can often to be observed in a single mycetes) combined with great morph structural variety of the related organs, is geared to the creation of spores to which the continuity and spreading of the species is entrusted.</i></b><br /><br /><i>3) It is often possible to observe a particular phenomenon called heterocariosis in mycology, which is characterized by the coexistence of normal and mutated nuclei in cells that have undergone a hyphal fusion. Today there is great concern by the phytopathologists about the formation of individuals that are genetically quite different from the parents and which takes place through reproductive cycles termed parasexual.&quot;</i><br /><br /><b>DEFINITION:</b> <b><i>&quot;Phytotherapy</i></b><i> is the study of the use of extracts of natural origin as medicines or health-promoting agents. Phytotherapy medicines differ from plant-derived medicines in standard pharmacology. Where standard pharmacology isolates an active compound from a given plant, phytotherapy aims to preserve the complexity of substances from a given plant with relatively less processing.&quot;</i><br /><br /><i>&quot;The vast use of phyto drugs, in fact, has often caused mutations in the nuclei of many parasite fungi with the consequential formation of heterocarion. Sometimes these fungi are particularly virulent in their pathogenicity. [41]&quot;</i><br /><br />[heterocarion???]<br /><br /><i>&quot;4) As a parasite, fungi can develop some specialized structures shaped like a roster of variable size from hyphas (the base implant). [42] These roster structures make penetration into the host possible.</i><br /><br /><i>5) The production of spores can be so abundant that each cycle always includes tens, hundreds, and even thousands of millions of elements that can be dispersed at a remarkable distance from their starting point. A small movement, for example, is sufficient to set off their immediate dispersal.</i><br /><br /><i>6) </i><b><i>The spores possess a very strong resistance to external aggression, as they are capable of staying dormant for many years if the environmental conditions dictate it while preserving their regenerative potential unchanged.</i></b><br /><br /><i>7) </i><b><i>The development coefficient of the hyphal apexes (the tips) after germination is extremely fast (100 microns per minute in an ideal environment), with a branching ability, and thus with the appearance of a new apex region, which in some cases takes only around 40-60 seconds.</i></b><i> [43]</i><br /><br /><i>8) </i><b><i>The shape of the fungus is never defined, as it is imposed by the environment in which the fungus develops.</i></b><br /><br /><b><i>It is possible to observe, for example, the very same mycelium existing in a status of simple isolated hyphas in a liquid environment or existing in aggregations ever more solid and compact</i></b><i> up to the formation of pseudo-parenchymas (stromas or supporting structures) and mycelial filaments and strings (rhizomorphs). [44]</i><br /><br /><b><i>By the same token, it is possible to observe the same shape in different fungi where they must conform to the same environment</i></b><i> (the phenomenon is usually called dimorphism).</i><br /><br /><i>9) </i><b><i>The partial or total substitution of nourishing substances induces frequent mutations in fungi and that testifies to their marked adaptability to all substrata.</i></b><br /><br /><i>10) </i><b><i>When precarious nutritional conditions exist, many fungi respond with hyphal fusion (between neighboring fungi), which allows them to explore the available material more easily and with more complete physiological processes. This property substitutes competition with cooperation and sets fungi aside from any other micro-organism. For this reason, some biologists call them social organisms.</i></b><i> [45]</i><br /><br /><i>11) </i><b><i>When a cell gets old or is damaged (for example, by toxic substances or drugs) many fungi whose intercellular septi are provided with pores react with the implementation of a defense process called protoplasmatic flux through which they transfer both the nucleus and the cytoplasm of the damaged cell into a healthy one while preserving all their biological potential unchanged.</i></b><br /><br /><i>12) How the development of hyphal ramification is regulated is unknown. [46] </i><b><i>It consists of either a rhythmic development, or in the appearance of sectors that, while originating from the hyphal system, are nevertheless self-regulating, [47] that is, they are independent from the regulation and the behavior of the rest of the colony.</i></b><br /><br /><i>13) Fungi are able to implement an endless number of modifications to their own metabolism to overcome the defense mechanism of the host. These consist of plasmatic and biochemical actions as well as volumetric increments (hypertrophia) and numerical increments (hyperplasia) of the affected cells. [48]</i><br /><br /><i>14) </i><b><i>They have exceptional aggressiveness. They attack not only plants but also animal tissue, food supplies, and even other fungi as well as protozoa, amoebas and nematodes.</i></b><br /><br /><i>The hunt for nematodes, for example, takes place with special hyphal modifications that constitute veritable </i><b><i>mycelial traps, which may be criss-crossed, viscous, or ring-shaped.</i></b><br /><br /><b><i>These traps immobilize the worms which are subsequently invaded with hyphas. In some cases, the aggressive power of fungi is so high as to allow a cellular ring made of only three units to surround, imprison, and kill a prey in a short time despite its desperate wiggling.</i></b><br /><br /><b><i>From the short notes given above, it would therefore seem appropriate to pay more attention to the world of fungi</i></b><i>, which we have seen are living objects representing the twilight layer between plants and animals. </i><b><i>Special attention is to be paid if we consider that both biologists and microbiologists, when called to describe or interpret the form, the physiology and the reproduction of a fungus, always show large knowledge gaps.</i></b><br /><br /><i>It seems therefore very logical to assume that a cause of neoplastic proliferation could be a fungus -- </i><b><i>the most powerful and most organized micro-organism known. Furthermore, the cause might be found in those imperfect fungi (so named because of our lack of knowledge about their biological processes) whose essential prerogative is in their ability to ferment.&quot;</i></b><br /><br /><b>DEFINITION:</b> <i>&quot;Most of the so-called </i><b><i>imperfect fungi</i></b><i>, a group also called deuteromycetes, are </i><b><i>those in which the sexual reproductive stages have not been observed.</i></b><i> Most of these appear to be related to ascomycetes although some have clear affinities to the other phyla. The group of fungi from which a particular nonsexual strain has been derived usually can be determined by the features of its hyphae and asexual reproduction. It cannot, however, be classified by the standards of that group because the classification systems are based on the features related to sexual reproduction. One consequence of this system is that as sexual reproduction is discovered in an imperfect fungus, it may have two names assigned to different stages of its life cycle. </i><b><i>There are some 17,000 described species of imperfect fungi</i></b><i> ...&quot;</i> (<a href=\"http://www.mhhe.com/biosci/genbio/tlw3/eBridge/Chp15/15_6.pdf\" class=\"ot-anchor\">http://www.mhhe.com/biosci/genbio/tlw3/eBridge/Chp15/15_6.pdf</a>)<br /><br /><i>&quot;The gravest disease of humanity may be hidden within the small group of pathogenic fungi. Perhaps the cause of cancer can finally be located with some simple deductions that may lead us to a solution to the problem.</i><br /><br /><b><i>Pathogenic Mycetes in Humans</i></b><br /><br /><i>When compared with the whole universe of fungi forms mycetes that are pathogenic for humans are not very numerous.</i><br /><br /><i>They usually cause diseases called mycosis, which are commonly divided into superficial (when the infection is limited to the cutis, body, hair, hair of the head, and nails) and deep (when the infection attacks internal organs such as lung, intestine encephalus, bones, and others). The fungi are generally classified as:</i><br /><br /><i>1. Dermatorphytes, causing afflictions that are typical of the epidermis (tinea).</i><br /><br /><i>2. sporotrichum schenckii, which are also almost exclusively located on the epidermis.</i><br /><br /><i>3. Criptococcus neoformans, responsible for a diffused infection of the lung (the organisms are inhaled with dust) as well as chronic meningitis.</i><br /><br /><i>4. Histoplasma capsulatum, which in humans produces the nodular cutaneous form, mucous form, the pulmonary form, and the systemic form.</i><br /><br /><i>5. Actinomycetes, with pathogenic action on the cutis, lungs, and intestine.</i><br /><br /><i>6. Chrysosporium parvum (causal agent of the adiasphyromycosis), a cosmopolitan disease where the respiratory tract constitutes the primary and only localization of the infection.</i><br /><br /><i>7. Aspergillus fumigatus, cause of the Aspergillosis, whose most frequent location is in the lungs, followed by a secondary location in the cerebrum and in the kidneys.</i><br /><br /><i>8. Paracocci dioides brasiliensis, which causes the paracoccidioidomycosis, a primary pulmonary infection that can become diffused in immuno-depressed patients.</i><br /><br /><i>9. In recent years, Pheoiphomycosis, ialiphmycosis, pennicilinosis (marneffei), zigomicosis and other rare mycotic infections are acquiring more and more importance since they can be responsible for pathological scenarios that are sometimes very serious because of the compromised conditions of immunocompromised patients.</i><br /><br /><i>10. Candida, both as Albicans and as any other pathogenic stock which afflicts the cutis, nails, internal mucus membranes (oral cavity, vulvar vaginitis, urethritis, balanitis, perianal infection), bronchi and lungs.</i><br /><br /><i>Candida is also responsible for causing generalized forms of septicemia of remarkable gravity.&quot;</i><br /><br /><b>DEFINITION:</b> <b><i>&quot;Septicemia</i></b><i> is an infection of the blood, also known as bacteremia or blood poisoning ... It must be treated in the hospital. If left untreated, septicemia can progress into sepsis.&quot;</i> (<a href=\"http://www.healthline.com/health/septicemia\" class=\"ot-anchor\">http://www.healthline.com/health/septicemia</a>)<br /><br /><b>DEFINITION:</b> <b><i>&quot;Sepsis</i></b><i> is a life-threatening condition that arises when the body&#39;s response to infection injures its own tissues and organs.[1] Common signs and symptoms include fever, increased heart rate, increased breathing rate, and confusion.[2] There may also be symptoms related to a specific infection, such as a cough with pneumonia, or painful urination with a kidney infection.&quot;</i><br /><br /><b><i>&quot;The gravest disease of humanity is, therefore, hidden within this grouping of fungi.</i></b><i> Some further analysis will make it easier to identify the cause.</i><br /><br /><i>Dermatorphytes and sporotrichum are responsible for a morbidity that is too specific. We know from experience that Actinomycetes, Criptococcus, Hystoplasm, Chrysosporium, Paracoccidioides and other causal agents of Pheoiphomycosis ialiphmycosis, pennicilinosis, zigomicosis are very rarely part of a pathological context. Finally, Aspergillus can be considered a variation of Candida. Only one of the six kinds described above remains as the sole responsible agent for tumors: Candida.</i><br /><br /><i>To that end, it is useful to call the reader&#39;s attention to the fact that in recent years the infections spread by the Candida species (Albicans, Glabrata, Krusi, Parapsilosis, Tropicalis, and others) -- that is, systemic candidosis -- have been raised to great importance in oncology. </i><b><i>These infections today represent, according to the majority of scholars, the main cause of morbidity and mortality in patients afflicted by neoplasias. It is sufficient to consider that Candida-related sepsis alone has increased 400 times in the last few years in American hospitals.</i></b><br /><br /><b><i>Why is there such pathological parallelism in the evolution of cancer and Candida? Isn&#39;t it perhaps simpler to assume that the two converge up to the point of being considered the very same disease? Actually, if we stop and reflect for a moment on Candida&#39;s characteristics we observe many analogies with neoplastic disease. The most obvious of these are:</i></b><br /><br /><i>a. ubiquitous rooting. No organ or tissue is spared</i><br /><i>b. constant lack of hyperpyrexia (high fever)</i><br /><i>c. sporadic involvement of muscles and nervous tissue</i><br /><i>d. invasiveness of the almost exclusive focal type</i><br /><i>e. progressive debilitation</i><br /><i>f. refractoriness to any treatment</i><br /><i>g. proliferation favored by a multiplicity of undifferentiated concomitant causes</i><br /><i>h. basic symptomatological configuration with structure tending to become chronic</i><br /><i>i. frequent formation of parparenchymal masses that are morphologically similar to neoplastic masses.</i><br /><br /><b><i>Why Candida is Cancer</i></b><br /><br /><i>Generally, mycetes have a recognized and almost unlimited ability to adapt, made possible by their seemingly endless ways of reacting to biological substrata, and ranging from simple metabolic variations to radical morphological changes.</i><br /><br /><i>In the human organism, other than their presence in various forms, we can observe that mycetes have a variability of biological expression depending on the function of the tissue or organ they are interested in. The current classification into superficial and deep forms stems from this variability.</i><br /><br /><i>The characteristics described above are the prerogatives of the entire fungin spectrum. However, they have particular relevance in Candida, as it is no doubt the most significant representative of the kind.</i><br /><br /><i>It is enough to consider this fact: </i><b><i>about 70 species are recognized as a saprophytic, that is, a micro-organism that gets its nourishment from decomposing substances and 20 as a pathogen.</i></b><br /><br /><i>As far as the interaction of fungi with humans is concerned, should we be content only with the usual classification proposed, superficial and deep mycosis, </i><b><i>which is exclusively founded on what can be directly observed?</i></b><i> In other words, can we be sure that this wonder of adaptation called Candida cannot find a way to survive in more infinitesimal biological dimensions?</i><br /><br /><i>Obviously not.</i><br /><br /><i>Unfortunately. the technology available to medical science may not be able to see it in all its vital excursions and as a result may relegate it to more or less a dimension of semisaphrophytism, which may be very false and dangerous.</i><br /><br /><b>DEFINITION:</b> <b><i>&quot;Saprophytism:</i></b><i> the ability of certain plants to live in dead or decaying organic matter.&quot;</i> (<a href=\"http://www.thefreedictionary.com/saprophytism\" class=\"ot-anchor\">http://www.thefreedictionary.com/saprophytism</a>)<br /><br /><b><i>Indeed, there is an exceptionally high pathogenic potential in this mycetes with the size of a few microns, which, although it cannot be found with the current tools of investigation, cannot be dismissed from the clinical point of view.</i></b><br /><br /><b><i>In fact, we frequently observe recurrent candidoses that are so obstinate -- and for which the only evolution is markedly in an irreversible and chronic direction -- that they are insensitive to any medical treatment.</i></b><br /><br /><b><i>This is because Candida, the moment it is attacked by the immune system of the host or by a conventional anti-mycotic treatment, defends itself by transforming into elements that are ever smaller and undifferentiated, while staying fully fecund up to the point of almost hiding its presence from both the hosting organism and diagnostic investigations.</i></b><br /><br /><b><i>Its behavior also shows an &#39;elastic&#39; nature: when rooting conditions are favorable it thrives on an epithelium (a vital surface), but as soon as the tissue reaction is engaged, it transforms itself massively into a less productive form but one which cannot be attacked, the spore. If sub-epithelial continuity solutions arise that are conjugated with a hypodermic areactivity [???], the spore is able to wedge in and then sink into the underlying connective tissue and hide in such a way as to be invulnerable and irreversible. In short, Candida takes advantage of a structural interchangeability that it utilizes according to the difficulties it encounters in its biological niche.</i></b><br /><br /><b><i>In the soil, in the air, in the water and in vegetation -- that is, where there is no antibody reaction -- Candida is free to grow to a mature vegetative form. In epitheliums it takes on a mixed form which is reduced solely to the form of spores (at least in the initial phases when it penetrates to deeper levels).</i></b><br /><br /><i>Again, Candida has an unlimited pathogenic potential which is underestimated, perhaps because of the way it is commonly described and understood, although </i><b><i>there are already many studies that testify to its carcinogenic power.</i></b><br /><br /><i>Why, one could ask, should we assume a different and enhanced activity for Candida Albicans, since it has been extensively described in these pathological manifestations?</i><br /><br /><i>The answer is that it has been studied only within a pathogenic context, that is, only in relationship to the tissues that cover a diseased organism. </i><b><i>In reality, Candida possesses an aggressive valence which is diversified as a function of the tissue it is interested in.</i></b><i> It is only in the connective or in the connective environment -- and not in differentiated tissues -- that Candida finds the conditions for unlimited expansion.</i><br /><br /><i>Over 50 years ago, Wilhelm Reich wrote:</i><br /><br /><b><i>&#39;If in the connective tissue which is directly in contact with the tumor the specific structures are disintegrated, the physiological barrier which normally exists between epithelial and connective tissue is destroyed and the cancer cells have free rein.&#39;</i></b><i> [49]</i><br /><br /><i>This becomes even clearer if we stop for a moment to consider what the main function of the connective tissue is: </i><b><i>that of carrying and supplying the cells of the entire organism with nourishing substances.</i></b><br /><br /><b>DEFINITION:</b> <b><i>&quot;Sui generis</i></b><i> ... is a Latin phrase, meaning &quot;of its (his, her, or their) own kind; in a class by itself; unique&quot;</i><br /><br /><i>In this context, in fact, it can be considered as an external environment sui generis next to the more differentiated cells such as those of the nerves and muscles; </i><b><i>it is here where the competition for nourishment takes place.</i></b><br /><br /><b><i>On the one hand, the cellular elements of the organism try to defeat all forms of invasion while on the other, the fungin cells try to absorb ever-growing quantities of nourishing substances</i></b><i>, as they must obey the biological needs of the species which is that of tending to the formation of ever larger and spreading masses and colonies. This is a biological thrust sometimes capable of producing aggregates of unusual dimensions, the most striking example of all being the fungal colonies in the United States which cover 44 hectares of land.</i><br /><br /><i>[...]</i><br /><br /><b><i>Candida is Always Present in Cancer</i></b><br /><br /><i>There are a large number of works that document the constant presence of the mycetes in the tissues of cancer patients, especially in terminal patients.</i><br /><br /><i>In recent years, we have observed a crescendo of voices addressing this terrible fungus to the point of defining it as &#39;the most important and most urgent problem that oncology has to solve.&#39; The following figures concerning the coexistence of Candida and cancer have been collected by several authors: [53]</i><br /><br /><i>R.L. Hopfer: 79%</i><br /><i>U. Kaben: 80%</i><br /><i>W. T. Hughes: 91%</i><br /><i>T.E. Kiehn: 97%&quot;</i><br /><br />[These figures are disputed at <a href=\"http://www.123hjemmeside.dk/cancer_is_not_a_fungus/21160737\" class=\"ot-anchor\">http://www.123hjemmeside.dk/cancer_is_not_a_fungus/21160737</a>.  I&#39;m not sure it really matters that much, in light of the meat of the argument.]<br /><br /><i>&quot;The percentages observed are truly impressive, especially when considering the difficulty of seeing Candida in the organic materials to be examined. This was also reported by R.S. Escuro, Z.O. Karaev, and T.J. Walsh. [54]</i><br /><br /><i>The positive results quoted allow us to confirm that Candida is always present in the tissues of cancer patients. Not only that, but Candida species represent today, according to several scholars, the first cause of morbidity and mortality in patients affected by neoplasias of the hemolinphopoietic system. [55]</i><br /><br /><i>O. Uzun even analyzed all data from l974 to 1999 concerning the presence of candidosis in patients and the prognostic factors including predictable elements of mortality and came to the conclusion that the global rate of mortality in cancer patients varies between 33% and 75% and that this is independent of the type of infecting Candida. [56]</i><br /><br /><b><i>The phenomenon is usually interpreted as a consequence of the weakening and of the exhaustion of the organism because of neoplastic lesions.</i></b><i> Conversely, we have to believe that the aggression of Candida takes place in the carcinogenic sense after the superficial pathogenic phases -- that is, the classic epithelial candidosis -- in several stages:</i><br /><br /><i>a) rooting in the deep connective tissue (in the various organs).</i><br /><i>b) expansion with evoking of an organic reaction that attempts to encyst the fungin colonies, with the outcome being the formation of neoplasias.</i><br /><i>c) growth both in the surrounding tissue and remotely (metastasis).</i><br /><i>d) progressive exhaustion of the organism with consequential global organism invasion. This is the stage that is most commonly observed and that is considered &#39;opportunistic&#39;.</i><br /><i>e) exitus.</i><br /><br /><i>In summary, Candida is not a post hoc but an ante hoc cause. Several works support what has been stated here concerning a causal link between Candida and cancer: [57]</i><br /><br />- <i>A. Pedersen and J. Trotoux: The Formation of Epidermoidal Tongue Carcinoma.</i><br />- <i>K.V. Zhang and O&#39;Grady: Oral Cavity&#39;s Neoplasias.</i><br />- <i>J.N. Hicks: Larynx&#39;s Neoplasia.</i><br />- <i>E.A. Field and F.R. Wang: Lung Cancer.</i><br />- <i>P. Joseph: Streaked Mixoma.</i><br />- <i>A. Rumi et al: Esophagus Cancer.</i><br />- <i>T. Taguchi: Intestinal Carcinoma.</i><br />- <i>V. Raina: Hodgkin&#39;s Disease.</i><br />- <i>M. Piazzi: Kaposi&#39;s Disease.</i><br />- <i>A. Mannell: Pancreas&#39;s Tumor.</i><br /><br /><b>DEFINITION:</b> <b><i>&quot;Carcinogenetic:</i></b><i> of or pertaining to carcinogenesis; forming cancer cells.&quot;</i> (<a href=\"https://en.wiktionary.org/wiki/carcinogenetic\" class=\"ot-anchor\">https://en.wiktionary.org/wiki/carcinogenetic</a>)<br /><br /><i>The considerations and the works cited above testify that Candida possesses a great carcinogenetic ability, and confirms that today it is no longer acceptable to conceive solely a pathogenic role for Candida that is consequential to a state of post-tumoral exhaustion.</i><br /><br /><b>DEFINITION:</b> <b><i>&quot;Prepose:</i></b><i> to place before or in front of something&quot;</i> (<a href=\"http://www.merriam-webster.com/dictionary/prepose\" class=\"ot-anchor\">http://www.merriam-webster.com/dictionary/prepose</a>)<br /><br /><i>Many amongst the authors we have cited and others that we have not cited [58] admit without hesitation to a direct causal role of the mycetes in the genesis of cancer, but </i><b><i>the error they often make is that of believing Candida solely responsible for the production of substances that modify the genes that are preposed the functionality of the cell. This evaluation prevents them from attributing a direct responsibility to mycetes, and this is the aspect that would finally plough the way to the conclusive discovery of the cause of cancer.</i></b><br /><br /><i>Because of the evidence that currently exists, it is not at all logical therefore to insist in seeing Candida as a micro-organism in the twilight zone between pathogenicity and harmlessness. Rather, it should be investigated as the sole, terrible causal generator of neoplasias.&quot;</i><br /><br /><b>A Very Unlikely Coincidence: A Corroboration of the Claim Through Alternative Means</b><br /><br /><a href=\"http://www.bibliotecapleyades.net/salud/salud_defeatcancer30.htm\" class=\"ot-anchor\">http://www.bibliotecapleyades.net/salud/salud_defeatcancer30.htm</a><br /><br /><b><i>The ancient Egyptians knew about the healing properties of anti-fungal substances and Indian books going back a thousand years actually recommend &#39;alkaline of strong potency&#39; for treating cancer.</i></b><br /><br /><i>In 1983, Simoncini treated an Italian man, Gennaro Sangermano, who had been given months to live with lung cancer. A few months later he wasn&#39;t dead, he was back to health and the cancer was gone.</i><br /><br /><i>More success followed and Simoncini presented his findings to the Italian Department of Health in the hope that they would begin scientifically-approved trials to show that [his treatment] worked.</i><br /><br /><i>But he was to learn the true scale of medical manipulation and deceit.</i><br /><br /><b><i>The authorities not only ignored his documentation, he was disbarred from the Italian Medical Order for prescribing cures that had not been approved.</i></b><br /><br /><i>[...]</i><br /><br /><i>Tullio Simoncini is, thankfully, no quitter and he has continued to circulate his work on the Internet and in public talks. I heard of him through Mike Lambert at the Shen Clinic and Simoncini spoke there while I was away in the United States.</i><br /><br /><i>I know that he is having remarkable success in dramatically reducing and removing altogether even some real late stage cancers using sodium bicarbonate. This can take months in some cases, but in others, like breast cancer where the tumor is easily accessible, it can be days before it is no more.</i><br /><br /><i>[...]</i><br /><br /><i>I wrote a newsletter last April about the fact that cancer is a fungus in an article about the findings of two British scientists and researchers, Professor Gerry Potter of the Cancer Drug Discovery Group and Professor Dan Burke.</i><br /><br /><i>Their combined findings reveal the following ...&quot;</i><br /><br />I recommend that if you do not understand this on the first try, read it over a few times until you get it.  It&#39;s worth understanding ...<br /><br /><b>[1]</b> <b><i>&quot;Cancer cells have a unique &#39;biomarker&#39; that normal cells do not, an enzyme called CYP1B1 (pronounced sip-one-bee-one).&quot;</i></b><br /><br />Confirmed.<br /><br />From <a href=\"http://jpet.aspetjournals.org/content/296/2/537.full\" class=\"ot-anchor\">http://jpet.aspetjournals.org/content/296/2/537.full</a>:<br /><br /><i>&quot;The cytochrome P450 1B1 (CYP1B1) is involved in the metabolism of procarcinogens and xenobiotics. </i><b><i>Human CYP1B1 protein has been detected in a variety of tumors but is not detected in adjacent normal tissues</i></b><i> or in liver. This suggests that CYP1B1 could biotransform anticancer agents specifically in the target cells.&quot;</i><br /><br /><b>[2]</b> <b><i>&quot;Enzymes are proteins that &#39;catalyze&#39; (increase the rate of) chemical reactions.&quot;</i></b><br /><br />This is true, of course.<br /><br /><b>[3]</b> <b><i>&quot;The CYP1B1 alters the chemical structure of something called salvestrols that are found naturally in many fruit and vegetables.&quot;</i></b><br /><br />Traced back to a paper with additional sources:<br /><br /><a href=\"http://truemedmd.com/wp-content/uploads/2013/06/Savestrol-Cases-Studies_JOM_2007_v22_n04_p177.pdf\" class=\"ot-anchor\">http://truemedmd.com/wp-content/uploads/2013/06/Savestrol-Cases-Studies_JOM_2007_v22_n04_p177.pdf</a><br /><br /><i>Salvestrols are a class of phytonutrients that, in humans, are metabolized by the tumour-specific CYP1B1 enzyme in cancer cells [1]-[8] to initiate a cascade of processes, including apoptosis, that result in the arrest or decline of the cancer. In this way Salvestrols are natural prodrugs as their activity hinges on activation by CYP1B1. [9][10]</i><br /><br /><i>In their natural source Salvestrols form part of a plant’s defence mechanism. Some are hydrophilic while others are lipophilic but all are phytoalexins that are elicited by invading pathogens. For example, when a ripe fruit comes under attack by fungus the synthesis of a pathogen-specific Salvestrol is induced. The Salvestrol is synthesized at the site of the attack, typically the skin of the fruit or the root of the plant, and enters the pathogen.</i><br /><br />Even more sources can be found here:<br /><br /><a href=\"http://truemedmd.com/2013/06/salvestrols-for-cancer-survivors-jeffrey-dach/\" class=\"ot-anchor\">http://truemedmd.com/2013/06/salvestrols-for-cancer-survivors-jeffrey-dach/</a><br /><br /><b>[4]</b> <b><i>&quot;This chemical change turns the salvestrols into an agent that kills cancer cells, but does no harm to healthy cells ...&quot;</i></b><br /><br />There is a graph of this here ...<br /><br /><a href=\"http://www.salvestrol.ca/salvestrolinfohcp.asp\" class=\"ot-anchor\">http://www.salvestrol.ca/salvestrolinfohcp.asp</a><br /><br />[Where does this graph come from?  No idea.]<br /><br /><b>[5]</b> <b><i>&quot;The CYP1B1 enzyme appears only in cancer cells and it reacts with salvestrols in fruit and vegetables to create a chemical substance that kills only cancer cells.&quot;</i></b><br /><br />There are many sites which repeat this same claim.  Here&#39;s one of the more detailed examples ...<br /><br /><a href=\"http://www.salvestrol.ca/salvestrolinfohcp.asp\" class=\"ot-anchor\">http://www.salvestrol.ca/salvestrolinfohcp.asp</a><br /><br /><i>&quot;Salvestrols are essential phytonutrients supplied through our diet. They are otherwise chemically unrelated substances except for their reaction with a particular enzyme (CYP1B1) that has been established as a universal marker for cancer cells. When a salvestrol encounters the enzyme it is metabolized into a new compound which initiates cell apoptosis, a huge benefit because the enzyme is not present in healthy cells as can be seen in the graph to the right. The red line shows the devastating effect of salvestrols on cancer cells and the green line shows absolutely no effect upon healthy cells. The blue line shows a similar absence of an effect upon cells in which expression of CYP1B1 has been inhibited.</i><br /><br /><i>The discovery of Salvestrols resulted from the convergence of two research paths: (1) Dan Burke&#39;s work trying to identify universal cancer markers, and (2) Gerry Potter&#39;s work designing prodrugs utilizing CYP enzymes to target cancer cells. While he was at the Institute for Cancer Research in London, Gerry, now a Professor of Medicinal Chemistry, had designed a prodrug that targeted CYP17. Aberaterone, as it is now known, has been producing such good results with prostate cancer that Johnson &amp; Johnson recently bought the company that had licensed the rights for more than a billion dollars. Dan, a Professor of Pharmaceutical Metabolism and head of the School of Pharmacy, had demonstrated that another CYP enzyme (CYP1B1) was a universal marker for cancer cells, a result later confirmed by Dana-Farber at Harvard. When Gerry joined Dan at the University of de Montefort, Gerry designed a prodrug to target CYP1B1. That drug was also licensed to an American biotechnology company and is currently in clinical trials.</i><br /><br /><i>Given that the drugs they had designed would not be available for many years, Gerry and Dan set about identifying natural compounds that could do the same job. Gerry had noticed that the drug he had designed was similar to natural compounds present in food. If suitable compounds could be identified, then the foods containing them could be promoted for a beneficial diet. Such compounds were found but, unfortunately, foods that should have had held high concentrations didn&#39;t due to modern farming and food production techniques which for various reasons removed them. Dan and Gerry therefore devised a dietary supplement that includes four of these compounds, now known as Salvestrols.&quot;</i><br /><br /><b>[6]</b> <b><i>&quot;Salvestrols are the natural defense system in fruit and vegetables against fungal attacks and that&#39;s why you only find them in those species subject to fungus damage, like strawberries, blueberries, raspberries, grapes, blackcurrants, redcurrants, blackberries, cranberries, apples, pears, green vegetables (especially broccoli and the cabbage family), artichokes, red and yellow peppers, avocados, watercress, asparagus and aubergines.&quot;</i></b><br /><br />Confirmed:<br /><br /><a href=\"http://news.bbc.co.uk/2/hi/health/6987200.stm\" class=\"ot-anchor\">http://news.bbc.co.uk/2/hi/health/6987200.stm</a><br /><br /><i>&quot;Tangerine peel &#39;kills cancer&#39; Tangerines</i><br /><br /><i>A compound extracted from tangerine peel can kill certain human cancer cells, research shows.</i><br /><br /><i>A team from Leicester School of Pharmacy found Salvestrol Q40 was turned into a toxic compound in cancer cells, destroying them.</i><br /><br /><i>Salvestrol Q40 is found at higher concentrations in tangerine peel, than in the flesh of the fruit.</i><br /><br /><i>The researchers suggest the modern trend to throw away peel may have contributed to a rise in some cancers.</i><br /><br /><i>Lead researcher Dr Hoon Tan said his work was still at an early stage, but together with his colleagues he has formed a company to investigate further the potential to develop natural anti-cancer therapies.</i><br /><br /><i>He said: &#39;It is very exciting to find a compound in food that can target cancers specifically.&#39;</i><br /><br /><i>Plant immune system</i><br /><br /><i>Salvestrol 40 is a type of phytoalexin - a chemical produced by plants to repel attackers, such as insects or fungi.</i><br /><br /><i>It is converted into a toxic compound by the P450 CYP1B1 enzyme, found in much higher levels in cancer cells.</i><br /><br /><i>As a result, the researchers found, it proved to be 20 times more toxic to cancer cells than their healthy equivalents.</i><br /><br /><i>Dr Tan said Salvestrol was found in other fruit and vegetables, such as the brassica family, which includes broccoli and brussels sprouts.</i><br /><br /><i>However, the compound tends to be produced at higher levels when infection levels among crops are high.</i><br /><br /><i>Therefore, the use of modern pesticides and fungicides, which have cut the risk of infections, have also led to a drop in Salvestrol levels in food.</i><br /><br /><i>Dr Julie Sharp, Cancer Research UK&#39;s science information manager, said: &#39;Many naturally occurring substances have anticancer properties, but while this research shows that salvestrols have an effect on cells in the laboratory, there is no evidence that they have a similar effect in patients.&#39;</i><br /><br /><i>&#39;Clinical trials would be needed to tell us if these substances could be developed into a cancer treatment.&#39;&quot;</i><br /><br /><b>Additional Coincidences</b><br /><br /><a href=\"https://www.libertariannews.org/2015/02/02/cancer-a-yeast-fungus-gone-out-of-control/\" class=\"ot-anchor\">https://www.libertariannews.org/2015/02/02/cancer-a-yeast-fungus-gone-out-of-control/</a><br /><br /><i>&quot;If the assumption is made that cancer is a fungal infection, or at least that some cancers are the result of a fungal infection, all sorts of things begin to make sense.</i><br /><br /><b><i>Cannabis has anti-fungal properties, particularly against candida albicans.</i></b><br /><br /><i>If you look up natural treatments for candidemia, it reads like a natural cures list for cancer. Caprylic acid (unrefined coconut oil), Vitamin C, Vitamin D, Laetrile, cannabis oil, sodium bicarbonate, green tea, DMSO, even bee venom and certain snake venoms, they have all been touted as cancer cures, and all of them have antifungal properties.</i><br /><br /><i>[...]</i><br /><br /><i>Oh, in case you were wondering, albicans grows on tobacco agar.  As one study notes:</i><br /><br /><b><i>&#39;The predisposition of cigarette smokers for development of respiratory and oral bacterial infections is well documented. Cigarette smoke can also contribute to yeast infection.&#39;&quot;</i></b><br /><br />&quot;Most yeast infections are caused by a type of yeast called Candida albicans.&quot; (<a href=\"http://www.webmd.com/women/tc/vaginal-yeast-infections-topic-overview\" class=\"ot-anchor\">http://www.webmd.com/women/tc/vaginal-yeast-infections-topic-overview</a>)<br /><br />_&quot;Boy, it sure is an odd coincidence that things which promote cancer also seem to promote fungal infections, no? <b>Did you know that a standardized way of inducing cancer in a laboratory rat is to inoculate the rat with aflatoxin b1, a fungal mycotoxin. Let me repeat that. We induce mycotoxicoses in rats by injecting them with fungal mycotoxin, and then we call the resulting condition of that cancer.</b><br /><br /><i>An article in the New York Times notes:</i><br /><br /><b><i>&#39;The old view is that cancer is a linear process,&#39; said Dr. Barnett Kramer, associate director for disease prevention at the National Institutes of Health. &#39;A cell acquired a mutation, and little by little it acquired more and more mutations. Mutations are not supposed to revert spontaneously.&#39;</i></b><br /><br /><b><i>Indeed, they are not. Of course, fugal infections can easily account for this phenomena of cancer.</i></b><i> The article talks about how routine screening for cancer found many cancers that, &#39;were destined to stop growing on their own or shrink, or even, at least in the case of some breast cancers, disappear.&#39;*</i><br /><br /><i>The American Thoratic Society, a completely mainstream entity, has this to say about Aspergillosis:</i><br /><br /><i>&#39;Aspergillosis generally starts out as a spot in your lung called a lung &#39;nodule&#39;. (This nodule can be mistaken for lung cancer or tuberculosis).&quot;</i><br /><br /><b>DEFINITION:</b> <i>&quot;Aspergillosis is the name given to a wide variety of diseases caused by infection by fungi of the genus Aspergillus. The majority of cases occur in people with underlying illnesses such as tuberculosis or chronic obstructive pulmonary disease (COPD), but with otherwise healthy immune systems.&quot;</i> (<a href=\"https://en.wikipedia.org/wiki/Aspergillosis\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Aspergillosis</a>)<br /><br /><i>&quot;The symptoms of fungal infection and cancer are virtually indistinguishable. That’s because there is no difference. A fungal infection presents in an identical way to cancer, because it is &#39;cancer.&#39; </i><b><i>In fact, the biopsy markers that are used to diagnose cancer are often the same as those for diagnosing a fungal infection.</i></b><br /><br /><b><i>So here we have a situation where a disease (with no known cause) acts so much like another disease (one that has a known cause) that it can be mistaken for it. It responds to exactly the same treatment as the disease with a known cause. We also frequently find the disease with a known cause inside the tumors supposedly caused by the disease with no known cause.</i></b><br /><br /><i>Perhaps if it walks like a duck, it looks like a duck and it quacks like a duck.&quot;</i><br /><br />For a sampling of arguments against these claims, try this site:<br /><br /><a href=\"http://www.123hjemmeside.dk/cancer_is_not_a_fungus/21160727\" class=\"ot-anchor\">http://www.123hjemmeside.dk/cancer_is_not_a_fungus/21160727</a><br /><br /><i>&quot;Listed here are claims that Tullio Simoncini make in various videos. Just follow the links to see why these claims are wrong.</i><br /><br /><i># 1: Cancers are white.</i><br /><i>#2:  The white in cancers are fungal colonies.</i><br /><i>#3: On the surface cancers are covered with reactive cells, and they are red.</i><br /><i>#4: A pathologist only gets to examine the superficial reactive cells ( the red ones) - Not the white mass (fungus)</i><br /><i>#5: Biopsy of cancers lead to [release] of fungi into the blood thus creating metastasis.</i><br /><i>#6: The reason that it is not known to mainstream doctors that cancers are fungi is, that the information is [suppressed] by the pharmaceutical companies.</i><br /><i>#7: The idea is backed up by conventional scientific data</i><br /><i>#8: Treating cancer as if it is a fungus is safe and effective&quot;</i><br /><br />In the view of this critic, that is what is required to rebut Simoncini&#39;s hypothesis.  What is remarkable about this set of counter-claims is that it should be apparent that they do not appear to address the core of the claims.<br /><br />I am struck by the number of coincidences in this cancer-fungus case.  I am also appreciating the detail of his critique (he even explains the nature of the mistake).  With this approach, it seems to me that the total is more than the sum of just the individual parts.  What he&#39;s arguing for forms a cohesive, possibly verifiable fabric that spans a large number of concepts and observations.  It brings together a lot of disparate knowledge and it spans multiple domains (a very tricky problem in a world of specialists).<br /><br />The reason that I&#39;m not bothered by the controversy over the numbers (here: <a href=\"http://www.123hjemmeside.dk/cancer_is_not_a_fungus/21160737\" class=\"ot-anchor\">http://www.123hjemmeside.dk/cancer_is_not_a_fungus/21160737</a>) is that his arguments don&#39;t lend particular importance to those numbers.  He&#39;s saying that fungi can reproduce in a variety of ways, some of which remain unknown.  He&#39;s pointing to how difficult it would be to identify a single spore in a human body (this is hard for me to argue against).<br /><br />Once a promising debate is located, the next step is typically to map the back-and-forth claims out in detail.  But, this is not something that a single person could probably accomplish; it would take a community of people and a tool that is designed for this specific purpose.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Panspermia and the Endospore","summary":"Panspermia is the Hypothesis that Some Microscopic Life Forms Can Survive the Effects of Space, And Have Accordingly Already Spread / Idea Dates to 5th Century B.C. / The Endospore is a Dormant, Tough, and Non-Reproductive Structure Produced by Certain Extremophile Micro-organisms / Endospores Have Been Captured in the Upper Troposphere / In 2001, in the Kerala District of Western India, a Loud Sonic Boom Was Followed by an Unusually Long Red Rain / Scientists Have Been Surprised by this Exotic Biological Material's Properties","image":"https://lh3.googleusercontent.com/-WSmBff4SXkc/VupLf7hC11I/AAAAAAAAHUM/jLRhkaidFaw/w1800-h2520/panspermia-and-the-endospore-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/C9SarGvn3LH","publishDate":"2016-03-17T06:15:46.156Z","updateDate":"2017-01-03T02:43:52.481Z","text":"<b>Panspermia and the Endospore: Panspermia is the Hypothesis that Some Microscopic Life Forms Can Survive the Effects of Space, And Have Accordingly Already Spread / Idea Dates to 5th Century B.C. / The Endospore is a Dormant, Tough, and Non-Reproductive Structure Produced by Certain Extremophile Micro-organisms / Endospores Have Been Captured in the Upper Troposphere / In 2001, in the Kerala District of Western India, a Loud Sonic Boom Was Followed by an Unusually Long Red Rain / Scientists Have Been Surprised by this Exotic Biological Material&#39;s Properties</b><br /><br /><b><i>&quot;On 25 July, 2001, blood-red rain fell over the Kerala district of western India. And these rain bursts continued for the next two months.</i></b><i> All along the coast it rained crimson, turning local people&#39;s clothes pink, burning leaves on trees and falling as scarlet sheets at some points.</i><br /><br /><i>Investigations suggested the rain was red because winds had swept up dust from Arabia and dumped it on Kerala. But Godfrey Louis, a physicist at Mahatma Gandhi University in Kottayam, after gathering samples left over from the rains, concluded this was nonsense. </i><b><i>&#39;If you look at these particles under a microscope, you can see they are not dust, they have a clear biological appearance.&#39; Instead Louis decided that the rain was made up of bacteria-like material&quot;</i></b><br /><br />The images in the graphic are claimed to be of this material.  To formulate a more meaningful opinion of the claim, we need to back up a little bit and review some background claims, concepts and scientific facts.  Some of this material is intended to be skimmed.  I have included multiple sources where it might help to add some credibility.<br /><br /><b>A Few Interesting Claims By Terence McKenna</b><br /><br /><a href=\"http://www.lycaeum.org/~sputnik/McKenna/Evolution/theory.html\" class=\"ot-anchor\">http://www.lycaeum.org/~sputnik/McKenna/Evolution/theory.html</a><br /><br /><i>&quot;I think in a hundred years if people do biology they will think it quite silly that people once thought that spores could not be blown from one star system to another by cosmic radiation pressure.&quot;</i> (Terence McKenna, in an interview with <i>High Times</i>)<br /><br /><a href=\"http://www.lycaeum.org/~sputnik/McKenna/Evolution/theory.html\" class=\"ot-anchor\">http://www.lycaeum.org/~sputnik/McKenna/Evolution/theory.html</a><br /><br /><i>&quot;....What the mushroom says about itself is this: that it is an extraterrestrial organism, that spores can survive the conditions of interstellar space. They are deep, deep purple -- the color that they would have to be to absorb the deep ultraviolet end of the spectrum. The casing of a spore is one of the hardest organic substances known. The electron density approaches that of a metal.</i><br /><br /><i>Is it possible that these mushrooms never evolved on earth? That is what the Stropharia cubensis itself suggests. Global currents may form on the outside of the spore. </i><b><i>The spores are very light and by Brownian motion are capable of percolation to the edge if the planet&#39;s atmosphere.</i></b><i> Then, through interaction with energetic particles, some small number could actually escape into space. Understand that this is an evolutionary strategy where only one in many billions of spores actually makes the transition between the stars -- a biological strategy for radiating throughout the galaxy without a technology. Of course this happens over very long periods of time. But if you think that the galaxy is roughly 100,000 light-years from edge to edge, if something were moving only one one-hundredth the speed of light -- now that&#39;s not a tremendous speed that presents problems to any advanced technology -- it could cross the galaxy in one hundred million years. There&#39;s life on this planet 1.8 billion years old; that&#39;s eighteen times longer than one hundred million years. So, looking at the galaxy on those time scales, one sees that the percolation of spores between the stars is a perfectly viable strategy for biology. It might take millions of years, but it&#39;s the same principle by which plants migrate into a desert or across an ocean.</i><br /><br /><b><i>There are no fungi in the fossil record older than forty million years. The orthodox explaination is that fungi are soft-bodied and do not fossilize well, but on the other hand we have fossilized soft-bodied worms and other benthic marine invertebrates from South African gunflint chert that is dated to over a billion years</i></b><i> ...&quot;</i> (Terence McKenna, from a transcript of a talk given at the Esalen Institute)<br /><br /><b>Immanuel Velikovsky&#39;s Thesis</b><br /><br /><a href=\"https://books.google.com/books?id=Yf2NWgNhEecC&amp;pg=PA114&amp;lpg=PA114&amp;dq=Using+the+Bible,+myths,+and+legends+as+his+primary+sources,+Velikovsky&amp;source=bl&amp;ots=1o2ht4iBMT&amp;sig=Mhzg2Moj82lsm71qoG9guoGZ7uA&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwi3z9bw9sbLAhUBFmMKHaE_BnoQ6AEIHDAA#v=onepage&amp;q=Using%20the%20Bible%2C%20myths%2C%20and%20legends%20as%20his%20primary%20sources%2C%20Velikovsky&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=Yf2NWgNhEecC&amp;pg=PA114&amp;lpg=PA114&amp;dq=Using+the+Bible,+myths,+and+legends+as+his+primary+sources,+Velikovsky&amp;source=bl&amp;ots=1o2ht4iBMT&amp;sig=Mhzg2Moj82lsm71qoG9guoGZ7uA&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwi3z9bw9sbLAhUBFmMKHaE_BnoQ6AEIHDAA#v=onepage&amp;q=Using%20the%20Bible%2C%20myths%2C%20and%20legends%20as%20his%20primary%20sources%2C%20Velikovsky&amp;f=false</a><br /><br /><i>&quot;Using the Bible, myths, and legends as his primary sources, Velikovsky argued that about 1450 B.C. a comet that bad been ejected from the planet Jupiter came very near the Earth. As the Earth passed through the comet&#39;s tail, a rain of red meteorite dust made rivers, lakes, and seas appear to turn to blood.  Fiery meteorites and ash fell from the skies like hail.  Petroleum rained down also, causing fires to rage all over tbe world.  The dust from the comet&#39;s tail produced a pall of darkness over the earth that lasted for days.  Earthquakes and hurricane-force winds rocked the globe, producing tsunamis that destroyed many areas and uncovered some former seabeds (like the Red Sea) for a time.  The Earth&#39;s rotation was slowed and the heat thus produced melted rocks, boiled seas, and caused a furious propagation of frogs, flies, and other vermin.&quot;</i><br /><br />In <i>Worlds in Collision</i>, he states:<br /><br /><a href=\"http://www.bibliotecapleyades.net/cosmic_tree/velikovsky.html\" class=\"ot-anchor\">http://www.bibliotecapleyades.net/cosmic_tree/velikovsky.html</a><br /><br /><i>&quot;THE RED WORLD</i><br /><br /><i>In the middle of the second millennium before the present era, as I intend to show, the earth underwent one of the greatest catastrophes in its history. A celestial body that only shortly before had become a member of the solar system -- a new comet -- came very close to the earth. The account of this catastrophe can be reconstructed from evidence supplied by a large number of documents.</i><br /><br /><i>The comet was on its way from its perihelion and touched all earth first with its gaseous tail. Later in this book I shall show that it was about this comet that Servius wrote: &#39;Non igneo sed sanguineo rubore fuisse&#39; </i><b><i>(It was not of a flaming but of a bloody redness).</i></b><br /><br /><b><i>One of the first visible signs of this encounter was the reddening of the earth&#39;s surface by a fine dust of rusty pigment. In sea, lake, and river this pigment gave a bloody coloring to the water. Because of these particles of ferruginous or other soluble pigment, the world turned red.</i></b><br /><br /><b><i>The MANUSCRIPT QUICHÉ of the Mayas tells that in the Western Hemisphere, in the days of a great cataclysm, when the earth quaked and the sun&#39;s motion was interrupted, the water in the rivers turned to blood.</i></b><br /><br /><b><i>Ipuwer, the Egyptian eyewitness of the catastrophe, wrote his lament on papyrus: &#39;The river is blood,&#39; and this corresponds with the Book of Exodus (7:20): &#39;All the waters that were in the river were turned to blood.&#39; The author of the papyrus also wrote: &#39;Plague is throughout the land. Blood is everywhere,&#39; and this, too, corresponds with the Book of Exodus (7:21): &#39;There was blood throughout all the land of Egypt.&#39;</i></b><br /><br /><b><i>The presence of the hematoid pigment in the rivers caused the death of fish followed by decomposition and smell. &#39;And the river stank&#39; (Exodus 7:21). &#39;And all the Egyptians digged round about the river for water to drink; for they could not drink of the water of the river&#39; (Exodus 7:24). The papyrus relates: &#39;Men shrink from tasting; human beings thirst after water,&#39; and &#39;That is our water! That is our happiness! What shall we do in respect thereof? All is ruin.&#39;</i></b><br /><br /><b><i>The skin of men and of animals was irritated by the dust, which caused boils, sickness, and the death of cattle -- &#39;a very grievous murrain&#39;.</i></b><i> Wild animals, frightened by the portents in the sky, came close to the villages and cities.</i><br /><br /><i>The summit of mountainous Thrace received the name &#39;Haemus&#39;, and Apollodorus related the tradition of the Thracians that the summit was so named </i><b><i>because of the &#39;stream of blood which gushed out on the mountain&#39; when the heavenly battle was fought between Zeus and Typhon, and Typhon was struck by a thunderbolt. It is said that a city in Egypt received the same name for the same reason.</i></b><br /><br /><i>The mythology which personified the forces of the cosmic drama described the world as colored red. In one Egyptian myth the bloody hue of the world is ascribed to the blood of Osiris, the mortally wounded planet god; in another myth it is the blood of Seth or Apopi; in the Babylonian myth the world was colored red by the blood of the slain Tiamat, the heavenly monster.</i><br /><br /><b><i>The Finnish epos of KALEVALA describes how, in the days of the cosmic upheaval, the world was sprinkled with red milk. The Altai Tatars tell of a catastrophe when &#39;blood turns the whole world red&#39;, and a world conflagration follows. The Orphic hymns refer to the time when the heavenly vault, &#39;mighty Olympus, trembled fearfully ... and the earth around shrieked fearfully, and the sea was stirred [heaped], troubled with its purple waves&#39;.</i></b><br /><br /><i>An old subject for debate is: Why is the Red Sea so named? If a sea is called Black or White, that may be due to the dark coloring of the water or to the brightness of the ice and snow. The Red Sea has a deep blue color. As no better reason was found, a few coral formations or some red birds on its shores were proposed as explanations of its name.</i><br /><br /><i>[H.S. Palmer, SINAI (1892). Probably at that time the mountainous land of Seir, upon which the Israelites wandered, received the name Edom (Red), and Erythrea (erythraios -- red in Greek) its name; Erythrean Sea was in antiquity the name of the Arabian Gulf of the Indian Ocean, applied also to the Red Sea.]</i><br /><br /><i>Like all the water in Egypt, the water on the surface of the Sea of the Passage was of a red tint. It appears that Raphael was not mistaken when, in painting the scene of the passage, he colored the water red.</i><br /><br /><i>It was, of course, not this mountain or that river or that sea exclusively that was reddened, thus earning the name Red or Bloody, as distinguished from other mountains and seas. But crowds of men, wherever they were, who witnessed the cosmic upheaval and escaped with their lives, ascribed the name Haemus or Red to particular places.</i><br /><br /><i>The phenomenon of &#39;blood&#39; raining from the sky has also been observed in limited areas and on a small scale in more recent times. One of these occasions, according to Pliny, was during the consulship of Manius Acilius and Gaius Forcius.</i><br /><br /><i>[Another instance, according to Plutarch, occurred in the reign of Romulus. (It must be mentioned here that Romulus and Remus founded Rome at the time of the Trojan War, that is, around the date of the so-called &#39;Great Eclipse&#39; and &#39;Great Earthquake&#39; of 15 June 762 BCE, a date which marked the beginning of the departure sequence of this &#39;comet&#39; Hyperborea, as is discussed elsewhere in this series of essays. RS)]</i><br /><br /><b><i>Babylonians, too, recorded red dust and rain falling from the sky; instances of &#39;bloody rain&#39; have been recorded in diverse countries.</i></b><i> The red dust, soluble in water, falling from the sky in water drops, does not originate in clouds, but must come from volcanic eruptions or from cosmic spaces. The fall of meteorite dust is a phenomenon generally known to take place mainly after the passage of meteorites; this dust is found on the snow of mountains and in polar regions.</i><br /><br /><i>[It is estimated that approximately one ton of meteorite dust falls daily on the globe.].&quot;</i><br /><br /><b>Extremophiles Basics</b><br /><br />Worth skimming through, as preparation for the upcoming claims ...<br /><br /><i>&quot;Extremophiles are organisms that are adapted to grow optimally at or near to the extreme ranges of environmental variables. Most extremophiles are microorganisms that thrive under conditions that, from a human perspective, are clearly hostile. RD MacElroy first coined the term &#39;extremophile&#39; in a 1974 paper entitled &#39;Some comments on the evolution of extremophiles,&#39; but definitions of extreme and extremophile are of course anthropocentric; from the point of view of the organism per se, its environment is that to which it is adapted and thence is completely normal. </i><b><i>A much larger diversity of organisms are known that can tolerate extreme conditions and grow, but not necessarily optimally in extreme habitats;</i></b><i> these organisms are defined as extremotrophs (Mueller et al. 2005).&quot;</i> (<i>Extremophiles Handbook</i>, Koki Horikoshi, p4)<br /><br /><i>&quot;Acidophile: an organism with a pH optimum for growth at, or below 3–4</i><br /><br /><i>Alkaliphile: an organism with optimal growth at pH values above 10</i><br /><br /><i>Endolith: an organism that lives inside rocks</i><br /><br /><i>Halophile: an organism requiring at least 1 M salt for growth</i><br /><br /><i>Hyperthermophile: an organism having a growth temperature optimum of 80 deg C or higher</i><br /><br /><i>Hypolith: an organism that lives inside rocks in cold deserts</i><br /><br /><i>Metalotolerant: organisms capable of tolerating high levels of heavy metals, such as copper, cadmium, arsenic, and zinc</i><br /><br /><i>Oligotroph: an organism capable of growth in nutritionally deplete habitats Piezophile: an organism that lives optimally at hydrostatic pressures of 40 MPa or higher</i><br /><br /><i>Psychrophile: an organism having a growth temperature optimum of 10 deg C or lower, and a maximum temperature of 20deg C</i><br /><br /><i>Radioresistant: organisms resistant to high levels of ionizing radiation Thermophile: an organism that can thrive at temperatures between 60deg C and 85 deg C</i><br /><br /><i>Toxitolerant: organisms able to withstand high levels of damaging agents, such as organic solvents</i><br /><br /><i>Xerophile: an organism capable of growth at low water activity and resistant to high desiccation&quot;</i> (<i>Extremophiles Handbook</i>, Koki Horikoshi, p5)<br /><br /><i>&quot;the last two decades have shown a surge in the interest in the world of microorganisms that live in extreme conditions. More and more extreme environments that could not be accessed earlier because of technical limitations are now being explored, and this has led to the isolation of a wealth of new organisms. Their study not only shows us how life functions at what we consider environmental extremes, but also teaches us much about the nature of life itself and about the possible properties of the first organisms that colonized planet Earth at a time when conditions were far more extreme than those tolerated by most life forms that presently inhabit our planet. </i><b><i>A highly relevant question is whether the existing extremophiles have only recently adapted to the unusual environments in which they live, or whether they may be vestiges of ancient types of organisms that evolved under extreme conditions on the early Earth.&quot;</i></b> (<i>Extremophiles</i>, Fred A Rainey and Aharon Oren, p2)<br /><br /><i>&quot;As temperature increases, we find progressively fewer groups of organisms that can cope with the stress of high temperatures. </i><b><i>The upper limit for eukaryotic life (fungi, algae, protozoa) is about 60 deg C.</i></b><i> The existence of truly thermophilic bacteria was firmly established during Tom Brock’s pioneering studies of the hot springs of Yellowstone National Park in the 1960s. A wide variety of microorganisms adapted to life at high temperatures was discovered, including phototrophs (up to about 72–73 deg C) and heterotrophs (up to 91 deg C, the temperature at which water boils at the elevation of Yellowstone) (Brock, 1978). At the time, Brock postulated that life may exist as long as there is liquid water. Indeed, the search for thermophiles in undersea volcanic areas in which the boiling point of water exceeds 100 deg C has shown a wealth of organisms, all belonging to the domain Archaea, that can grow at temperatures higher than those of the boiling point of water at normal atmospheric pressure (Stetter, 1996). Even more challenging environments to search for hyperthermophiles are the deep-sea hydrothermal vents: the anoxic, sulfide and mineral-loaded waters emitted by the so-called &#39;black smoker&#39; chimneys can reach temperatures up to about 350 deg C.</i><br /><br /><b><i>Thus far the most heat-tolerant, and also most heat-requiring of all documented species of prokaryotes is probably Pyrolobus fumarii, a crenarchaeote isolated from the wall of a deep-sea black smoker, which does not grow at temperatures below 90 deg C, has its optimum at 106 deg C, and will still grow, albeit at a low rate, at 113 deg C</i></b><i> (Blochl et al., 1997). There are even reports of an iron-reducing archaeon, not yet named and still incompletely characterized and known only as &#39;Strain 121&#39;, that reportedly grows at 121 deg C, the temperature within an autoclave (Kashefi and Lovley, 2003). It is still unknown what the true upper temperature of life is; it is probably dictated by the limited stability of DNA, RNA, proteins, and other biological molecules.&quot;*</i> (<i>Extremophiles</i>, Fred A Rainey and Aharon Oren, p8)<br /><br /><i>&quot;Psychrophilic (cold-loving and even cold-requiring) and psychrotolerant microorganisms have been isolated from all these environments. True psychrophiles are generally defined as such organisms that have their optimum temperature below 15–20 deg C. </i><b><i>The most psychrophilic microorganism described in the literature is probably Polaromonas vacuolata, a member of the g-Proteobacteria isolated from Antarctic marine waters: it grows optimally at 4 deg C, with a minimum at 0 deg C and a maximum of 12 deg C</i></b><i> (Irgens et al., 1996).&quot;</i> (<i>Extremophiles</i>, Fred A Rainey and Aharon Oren, p9)<br /><br /><i>&quot;Some microorganisms can tolerate surprisingly high levels of gamma ionizing radiation ... </i><b><i>Why these prokaryotes are resistant to elevated doses of ionizing radiation is unclear, considering that natural sources of radiation on Earth emit at very low levels.</i></b><i> Studies on the desiccation resistance of ionizing radiation resistant organisms suggest that the ability of microorganisms to repair their DNA might be a response to DNA damage caused by prolonged desiccation rather than ionizing radiation (Mattimore and Battista, 1996). Environmental evidence would seem to support this idea, considering that higher proportions of ionizing radiation resistant bacteria have been found in arid soils as compared to soils from less arid regions (Rainey et al., 2005). Not only are higher proportions of ionizing radiation resistant bacteria found in these arid soils but the genetic diversity of these organisms is also extensive, as demonstrated by the isolation of nine novel species of the genus Deinococcus from a single desert soil sample (Rainey et al., 2005).</i><br /><br /><i>The mechanisms that enable Deinococcus to survive these extremely high radiation levels are only now becoming clear. A number of ideas to explain the high level of ionizing radiation resistance have been put forward. These include the ability of deinococci to rapidly repair damaged DNA using the normal DNA repair enzymes found in all prokaryotes, the possession of a novel set of DNA repair systems, and use of the multiple chromosomes for homologous recombination (Battista et al., 1999). Recently a special way of packaging of the DNA, that facilitates repair not only of single-strand breaks but of double-strand breaks as well, has been described, and this appears to be one of the basic features in which Deinococcus differs from all other cells (Englander et al., 2004). </i><b><i>It is interesting to note that this unusual mode of DNA packaging seen in Deinococcus is also present in dormant endospores of Bacillus, which are extremely desiccation-resistant as well, providing addition support for the link between ionizing radiation and desiccation resistance</i></b><i> (Frenkiel-Krispin et al., 2004).&quot;</i> (<i>Extremophiles</i>, Fred A Rainey and Aharon Oren, p13)<br /><br /><b>Panspermia</b><br /><br />Wikipedia:<br /><br /><i>&quot;Panspermia ... is the hypothesis that life exists throughout the Universe, distributed by meteoroids, asteroids, comets,[1][2] planetoids,[3] and, also, by spacecraft in the form of unintended contamination by microorganisms.[4][5]</i><br /><br /><i>Panspermia is a hypothesis proposing that microscopic life forms that can survive the effects of space, such as extremophiles, become trapped in debris that is ejected into space after collisions between planets and small Solar System bodies that harbor life. Some organisms may travel dormant for an extended amount of time before colliding randomly with other planets or intermingling with protoplanetary disks. If met with ideal conditions on a new planet&#39;s surfaces, the organisms become active and the process of evolution begins. </i><b><i>Panspermia is not meant to address how life began, just the method that may cause its distribution in the Universe.</i></b><i> [6][7][8]&quot;</i><br /><br /><a href=\"https://www.researchgate.net/publication/226344407_Panspermia_according_to_Hoyle\" class=\"ot-anchor\">https://www.researchgate.net/publication/226344407_Panspermia_according_to_Hoyle</a><br /><br /><i>&quot;In the mid-1970’s panspermia theories were thought by many to be on the fringes of science. The idea that life-seeds are distributed widely in the cosmos has ancient roots. It was discussed in some form in the writings of the Greek philosopher Anaxoragas in the 5th Century BC. It was also a widely held belief amongst Vedhic and Buddhist philosophers of the Orient at even earlier times. The revival of panspermia in a purely scientific context came about in the early 20th Century with the writings of Svante Arrhenius (1903). Arrhenius’ ideas soon fell into disfavour, but for reasons that later turned out to be wrong. Becquerel (1924) and others used laboratory experiments to argue that bacteria would not survive the radiation environment of space, particulary ultraviolet radiation. It was later shown that microorganisms are not too easily destroyed by UV, but are mostly rendered inactive and that this process can be relatively easily reversed. Hoyle and I also argued much later that for bacteria in interstellar molecular clouds ultraviolet radiation poses no problem. Only a thin coating of condensed carbonaceous material around a bacterium would shield it from potentially damaging radiation. So Becquerel’s original objections to panspermia turned out to be flawed.</i><br /><br /><i>Moreover, it has been amply demonstrated that that bacteria are resistant to a wide range of extreme conditions that would prevail in space, including their ability to withstand high doses of ionising radiation and extremely low temperatures. Despite all these developments </i><b><i>the stigma against panspermia continued to linger well into the 1970’s</i></b><i>, the time when Fred and I began to work on these ideas.&quot;</i> (&#39;Panspermia According to Hoyle&#39;, Chandra Wickramasinghe)<br /><br /><i>&quot;we argued that there is no logic to demand an origin of life on the Earth, and that the odds against such a purely terrestrial origin are superastronomical (Hoyle and Wickramasinghe, 1981).&quot;</i> (&#39;Panspermia According to Hoyle&#39;, Chandra Wickramasinghe)<br /><br /><b>Introduction to the Endospore</b><br /><br />Wikipedia:<br /><br /><b><i>&quot;An endospore is a dormant, tough, and non-reproductive structure produced by certain bacteria from the Firmicute phylum.[1][2] The name &#39;endospore&#39; is suggestive of a spore or seed-like form (endo means within), but it is not a true spore (i.e., not an offspring). It is a stripped-down, dormant form to which the bacterium can reduce itself. Endospore formation is usually triggered by a lack of nutrients, and usually occurs in gram-positive bacteria. In endospore formation, the bacterium divides within its cell wall. One side then engulfs the other. Endospores enable bacteria to lie dormant for extended periods, even centuries. Revival of spores millions of years old has been claimed.[3] When the environment becomes more favorable, the endospore can reactivate itself to the vegetative state. Most types of bacteria cannot change to the endospore form. Examples of bacteria that can form endospores include Bacillus and Clostridium.[4]</i></b><br /><br /><i>The endospore consists of the bacterium&#39;s DNA, ribosomes and large amounts of dipicolinic acid. Dipicolinic acid is a spore-specific chemical that appears to help in the ability for endospores to maintain dormancy. This chemical comprises up to 10% of the spore&#39;s dry weight.[5]</i><br /><br /><i>Endospores can survive without nutrients. They are resistant to ultraviolet radiation, desiccation, high temperature, extreme freezing and chemical disinfectants. Thermo-resistant endospores were first hypothesized by Ferdinand Cohn after studying Bacillus subtilis (pictured to the right) growth on cheese after boiling the cheese. His notion of spores being the reproductive mechanism for the growth was a large blow to the previous suggestions of spontaneous generation. Astrophysicist Steinn Sigurdsson said &#39;There are viable bacterial spores that have been found that are 40 million years old on Earth – and we know they&#39;re very hardened to radiation.&#39;[6] Common anti-bacterial agents that work by destroying vegetative cell walls do not affect endospores. Endospores are commonly found in soil and water, where they may survive for long periods of time. A variety of different microorganisms form &#39;spores&#39; or &#39;cysts,&#39; but the endospores of low G+C gram-positive bacteria are by far the most resistant to harsh conditions.[7]</i><br /><br /><i>Some classes of bacteria can turn into exospores, also known as microbial cysts, instead of endospores. Exospores and endospores are two kinds of &quot;hibernating&quot; or dormant stages seen in some classes of microorganisms.&quot;</i><br /><br /><b>Microbes in the HAA (High Altitude Atmosphere)</b><br /><br /><a href=\"https://www.cospar-assembly.org/abstractcd/OLD/COSPAR-10/abstracts/data/pdf/abstracts/F35-0009-10.pdf\" class=\"ot-anchor\">https://www.cospar-assembly.org/abstractcd/OLD/COSPAR-10/abstracts/data/pdf/abstracts/F35-0009-10.pdf</a><br /><br /><i>&quot;Terrestrial microbes may be transported into the upper atmosphere via various means. Due to the environmental similarity of the upper atmosphere to outer space, knowledge of microbes in the upper atmosphere would be valuable for assessing the chances and limits of microbial transfer from the earth to extraterrestrial bodies (i.e., Panspermia of terrestrial microbes). We collected air dust samples in the upper troposphere and the stratosphere over Japan by using aircrafts or balloons. </i><b><i>Microbial isolates from the samples were endospore-forming species (Bacillus, Paenibacillus, Streptomyces) and non-spore-forming Deinococci.</i></b><i> Besides the evidence of microbial presence in the upper atmosphere, we show the possible presence of terrestrial microbes in space by extrapolated height-dependent distribution of microbes. High resistance to radiation and desiccation was common for our upper-atmospheric isolates and likely the most important feature enabled their survival in the environment of elevated radiation and desiccation. In this regard, Panspermia of viable Deinococci and endospores would be more likely than other terrestrial microbes. Specifically, the Deinococcus isolates exhibited extreme resistance to radiation (several times higher than bacterial endospores), the principle threat for microbial survival during interplanetary transfer.&quot;</i> (&#39;Microbial Isolates from the Upper Atmosphere Support Panspermia Hypothesis&#39;)<br /><br /><a href=\"http://www.ls.toyaku.ac.jp/~lcb-7/yamagishi/yang2009.pdf\" class=\"ot-anchor\">http://www.ls.toyaku.ac.jp/~lcb-7/yamagishi/yang2009.pdf</a><br /><br />&#39;Assessing Panspermia Hypothesis by Microorganisms Collected from The High Altitude Atmosphere&#39;, Biological Sciences in Space, Vol.23 No.3, 151-163, 2009<br /><br />Yinjie Yang, Shin-ichi Yokobori and Akihiko Yamagishi<br />Department of Molecular Biology, Tokyo University of Pharmacy and Life Science, 1432-1, Horinouchi, Hachioji, Tokyo 192-0392, Japan<br /><br /><i>[...]</i><br /><br /><i>&quot;Transport mechanisms of microbes to the HAA It is certain that there are microbes in the HAA. Earth’s surface atmosphere contains various microbiota such as viruses, bacteria, algae, fungi and protozoa from a variety of sources (Brown et al., 1964; McGovern et al., 1965; Schlichting, 1969; Broady, 1979; Pósfai et al., 2003). The microbiota in the low altitude atmosphere, within troposphere, can get upward transportation via vertical atmospheric convection. Radar studies showed that some birds such as passerines fly at heights up to 3 km and wader up to 7 km during long migration flights (Bruderer, 1997). It is very likely that birds traveling at high altitude carry microbes and release them at that height. Since the late 1800’s and early 1900’s, human activities started to contribute the distribution of microbes into the HAA by balloons, aircrafts and rockets. Human skydivers might have also brought microbes up to the middle stratosphere (McCrory, 2004). Other mechanisms of transporting microbes into the HAA include wind storms (Kellogg and Griffin, 2006), volcanic ejection (Robock, 2002; Antuña et al., 2003; Oman et al., 2005) and meteorite impacts (Alvarez et al., 1980). </i><b><i>Simulation study showed that, electric fileds found during thunderstorms could accelerate charged micro-sized particles rapidly up to the upper stratosphere against the forces of drag of neutral atmosphere and gravity (Dehel et al., 2008).</i></b><br /><br /><b><i>The presence of microbes in the HAA has been known for more than a century.</i></b><i> However, the HAA remains a poorly explored area of microbiology. There is only fragmentary information about microbial populations in the HAA. A few samples have been collected in the HAA and the microbes in these samples were studied (Table 1). </i><b><i>Fungi and pigmented endospore-forming bacteria from the high atmosphere were first reported in Europe (Cristiani, 1893; Harz, 1904). Later studies in the USA, Soviet Union, and India reported the recovery of fungi, and endospore-forming bacteria such as Bacillus (Rogers and Meier, 1936; Soffen, 1965; Bruch, 1967; Wainwright et al., 2003; Griffin, 2005), and non-sporeforming Micrococci and Mycobacterium (Fulton, 1966; Bruch, 1967; Imshenetsky et al., 1978; Griffin, 2008). We isolated Deinococci and endospore-forming bacteria from samples collected in the upper troposphere and stratosphere in Japan (Yang et al., 2008a, 2008b). Algae, viruses and protozoans have not been reported to date. Spore-forming bacteria and fungi were the predominant species of reported HAA isolates.</i></b><i> The predominant nonspore-forming isolates reported were Micrococci, a group to which the Deinococci previously belonged (Murray, 1992).</i><br /><br /><i>Due to their high resistance to desiccation and radiation, Bacillus spores and Deinococcus radiodurans cells have been frequently used in ground-based space research (Dose and Gill, 1995; Dose et al., 1996, 2001; Dose and Klein, 1996; Mancinelli and Klovstad, 2000; Horneck et al., 2001; Saffary et al., 2002; FajardoCavazos et al., 2005; Nicholson and Schuerger, 2005; Diaz and Schulze-Makuch, 2006; Tauscher et al., 2006; de la Vega et al., 2007; Stöffler et al., 2007; Osman et al., 2008) or exposure experiments in near-earth orbits (Horneck et al., 1995; Reitz et al., 1995; Brack et al., 2001; Rettberg et al., 2002a; Rabbow et al., 2003; Sancho et al., 2007). Since the microbes isolated from the HAA had high chance to escape from the earth and survive under outer space environment, they can be representatives of terrestrial microbes for panspermia. In particular, Bacillus and Deinococcus isolates from the HAA that exhibited high levels of radiation- and desiccation resistance (Shivaji et al., 2006; Yang et al., 2008b, 2009a, 2009b) would be good candidates for future tests of interplanetary transfer of life.</i><br /><br /><i>[...]</i><br /><br /><i>Microbial concentrations in the HAA It is generally thought that population density of microbiota decreases with increasing altitude in the troposphere. However, the height dependence of microbial population density has not well been studied.</i><br /><br /><i>[...]</i><br /><br /><i>The altitude-dependence of microbial density in Fig. 2 must be taken to be a rough estimate, however, since microbial density data at specific altitudes from independent investigations may have ambiguity due to various factors such as variable atmospheric conditions, sampling methods, and even the isolation media used. More data, particularly in the atmosphere higher than 25 km, is needed to obtain a better understanding of the microbial distribution in the HAA. It is worth emphasizing that the data in Fig. 2 is limited to cultivated microbes, although generally less than 1 % of the microbes in the natural environment can be cultivated. Wainwright et al. (2004) found non-cultivable bacterial clumps in HAA samples using viable fluorescent staining. The actual density of microbes at various altitudes might be two orders of magnitude higher than that shown in Fig. 2.</i><br /><br /><i>[...]</i><br /><br /><i>Most of the physical conditions of outer space resemble those of the upper atmosphere. The radiation of outer space, which is different from that of the HAA in spectra and density, is considered to be a most serious threat for microbial survival during interplanetary transfer. Extraterrestrial UV radiation can kill unprotected B. subtilis spores within seconds (Horneck and Brack, 1992). Since unprotected microbes rarely survive a long trip in space, lithopanspermia (interplanetary transfer of life inside meteorites) is thought to be more likely (Clark et al., 1999; Mileikowsky et al., 2000)</i><br /><br /><i>[...]</i><br /><br /><b><i>Endospores of many bacterial species can sustain heat treatment higher than 80 ˚C. E. coli in desiccated state exposed to short heating pulses under vacuum conditions survived up to 250 ˚C, while the same heating under atmospheric pressure led to complete sterilization</i></b><i> (Pavlov et al., 2007). It is unknown that up to which high temperature desiccated Deinococci and spores can tolerate under vacuum condition. Microbes traveling opposite to the sun would suffer less solar heat. If microbes are ejected from Earth and travel at the high speeds as suggested by Dehel (2006), the traveling time to Mars or Titan could be less than 1 year.&quot;</i><br /><br /><b>Endospores in Space</b><br /><br /><a href=\"http://www.nasa.gov/mission_pages/station/research/news/eu_tef/#.Vuouq2SANBc\" class=\"ot-anchor\">http://www.nasa.gov/mission_pages/station/research/news/eu_tef/#.Vuouq2SANBc</a><br /><br /><i>&quot;Spore-forming bacteria are of particular concern because spores can withstand certain sterilization procedures and may best be able to survive the harsh environments of outer space or planetary surfaces. Spores of Bacillus pumilus SAFR-032 have shown especially high resistance to techniques used to clean spacecraft, such as ultraviolet (UV) radiation and peroxide treatment. </i><b><i>When researchers exposed this hardy organism to a simulated Mars environment that kills standard spores in 30 seconds, it survived 30 minutes. For one of the recent experiments, Bacillus pumilus SAFR-032 spores were exposed for 18 months on the European Technology Exposure Facility (EuTEF), a test facility mounted outside the space station.</i></b><br /><br /><b><i>&#39;After testing exposure to the simulated Mars environment, we wanted to see what would happen in real space, and EuTEF gave us the chance,&#39; says Venkateswaran. &#39;To our surprise, some of the spores survived for 18 months.&#39;</i></b><i> These surviving spores had higher concentrations of proteins associated with UV radiation resistance and, in fact, showed elevated UV resistance when revived and re-exposed on Earth.&quot;</i><br /><br /><i>[...]</i><br /><br /><i>&quot;In another investigation, spores of Bacillus pumilus SAFR-032 and another spore-forming bacteria, Bacillus subtilis 168, were dried on pieces of spacecraft-quality aluminum and subjected for 1.5 years to the vacuum of space, cosmic and extraterrestrial solar radiation and temperature fluctuations on EuTEF. These samples also were subjected to a simulated Martian atmosphere using EuTEF. Most of the organisms exposed to solar UV radiation in space and in the Mars spectrum were killed, but </i><b><i>when UV rays were filtered out and samples were kept in the dark, about 50 percent or more of those subjected to other space- and Mars-like conditions survived.</i></b><i> That makes it likely that spores could survive a trip on a spacecraft to Mars if they are sheltered against solar radiation, perhaps in a tiny pocket of the spacecraft surface or underneath a layer of other spores.&quot;</i><br /><br /><b>Red Rain</b><br /><br /><a href=\"https://www.theguardian.com/science/2006/mar/05/spaceexploration.theobserver\" class=\"ot-anchor\">https://www.theguardian.com/science/2006/mar/05/spaceexploration.theobserver</a><br /><br /><b><i>&quot;Red rain could prove that aliens have landed</i></b><br /><br /><i>The following correction was printed in the Observer&#39;s For the record column, Sunday March 12 2006</i><br /><br /><i>In the article below, Dr Milton Wainwright was quoted as saying that red rain lacked DNA. Dr Wainwright has asked us to make clear that currently he has no view on whether red rain contains DNA and that it is physicist Godfrey Louis who is of that view.</i><br /><br /><i>[...]</i><br /><br /><i>There is a small bottle containing a red fluid on a shelf in Sheffield University&#39;s microbiology laboratory. The liquid looks cloudy and uninteresting. Yet, if one group of scientists is correct, the phial contains the first samples of extraterrestrial life isolated by researchers.</i><br /><br /><i>Inside the bottle are samples left over from one of the strangest incidents in recent meteorological history. </i><b><i>On 25 July, 2001, blood-red rain fell over the Kerala district of western India. And these rain bursts continued for the next two months.</i></b><i> All along the coast it rained crimson, turning local people&#39;s clothes pink, burning leaves on trees and falling as scarlet sheets at some points.</i><br /><br /><i>Investigations suggested the rain was red because winds had swept up dust from Arabia and dumped it on Kerala. But Godfrey Louis, a physicist at Mahatma Gandhi University in Kottayam, after gathering samples left over from the rains, concluded this was nonsense. </i><b><i>&#39;If you look at these particles under a microscope, you can see they are not dust, they have a clear biological appearance.&#39; Instead Louis decided that the rain was made up of bacteria-like material</i></b><i> that had been swept to Earth from a passing comet. In short, it rained aliens over India during the summer of 2001.</i><br /><br /><i>Not everyone is convinced by the idea, of course. Indeed most researchers think it is highly dubious. One scientist who posted a message on Louis&#39;s website described it as &#39;bullshit&#39;.</i><br /><br /><i>But a few researchers believe Louis may be on to something and are following up his work. </i><b><i>Milton Wainwright, a microbiologist at Sheffield, is now testing samples of Kerala&#39;s red rain.</i></b><i> &#39;It is too early to say what&#39;s in the phial,&#39; he said. &#39;But it is certainly not dust. Nor is there any DNA there, but then alien bacteria would not necessarily contain DNA.&#39;</i><br /><br /><b><i>Critical to Louis&#39;s theory is the length of time the red rain fell on Kerala. Two months is too long for it to have been wind-borne dust, he says. In addition, one analysis showed the particles were 50 per cent carbon, 45 per cent oxygen with traces of sodium and iron: consistent with biological material. Louis also discovered that, hours before the first red rain fell, there was a loud sonic boom that shook houses in Kerala. Only an incoming meteorite could have triggered such a blast, he claims.</i></b><i> This had broken from a passing comet and shot towards the coast, shedding microbes as it travelled. These then mixed with clouds and fell with the rain. Many scientists accept that comets may be rich in organic chemicals and a few, such as the late Fred Hoyle, the UK theorist, argued that life on Earth evolved from microbes that had been brought here on comets. But most researchers say that Louis is making too great a leap in connecting his rain with microbes from a comet.</i><br /><br /><i>For his part, Louis is unrepentant. &#39;If anybody hears a theory like this, that it is from a comet, they dismiss it as an unbelievable kind of conclusion. Unless people understand our arguments - people will just rule it out as an impossible thing, that extra-terrestrial biology is responsible for this red rain.&#39;</i><br /><br /><a href=\"http://thewatchers.adorraeli.com/2012/11/18/sri-lanka-red-rain-mistery-solved/\" class=\"ot-anchor\">http://thewatchers.adorraeli.com/2012/11/18/sri-lanka-red-rain-mistery-solved/</a><br /><br /><i>&quot;Incidents where water bodies all over the world have been turning red have been reported for some time now. While most of these occurrences, including the ‘red rain’ of Sri Lanka, have been given sufficient explanations by scientists, </i><b><i>some isolated incidents remain a mystery.</i></b><i> The DNA-less structures observed by them had allegedly displayed a number of remarkable characteristics, atypical of those displayed by any known micro-organisms. </i><b><i>These properties had included the ability to replicate and multiply at extreme temperatures of up to 300 degrees Celsius, and the ability to be cultured in unconventional substrate materials.</i></b><br /><br /><b><i>&#39;As far as the Kerala red rain is concerned, there is a mysterious microorganism that has defied identification so far. We have not been able to convincingly extract any DNA from them and Prof Louis has maintained that there is no DNA, but it can multiply at very high temperatures under high pressure conditions.&#39;&quot;</i></b><br /><br /><a href=\"http://www.marsanomalyresearch.com/evidence-reports/2009/159/red-rain-organism.htm\" class=\"ot-anchor\">http://www.marsanomalyresearch.com/evidence-reports/2009/159/red-rain-organism.htm</a><br /><br /><i>&quot;Some of you may be aware that for several months in 2001 a primarily red color rain fell in the south western state of Kerala in India. It began on July 25, 2001 and the locals reported that it was preceded by a loud thunderclap and a flash of light. It appears that the color of the rainwater was dictated by solid particles in suspension in the water. There were also other colors but mainly a brownish-red color. There were many of these red rain occurrences finally diminishing in September, 2001. Since then there have been others but sporadically.</i><br /><br />[...]<br /><br /><b><i>The Indian government sponsored initial research into this phenomena. The consensus initial conclusion reached was that the particulate in suspension in the rain water was producing the red color and in fact it is biological life in origin and not inorganic material.</i></b><i> However, this did not get much world wide attention no doubt because the initial and conventional explanation was mostly that this living organism was likely lichen originating from the surrounding countryside.</i><br /><br /><i>That is until 2003. At that time Dr&#39;s. Godfrey Louis an Santhosh Kumar then physicists at Mahatma Gandhi University in Kottayam, Kerala posted their conclusions. This was that the biological cell material came from a comet that disintegrated over Kerala (explaining the thunderclap and flash of light) releasing the red particles into the atmosphere to fall to the ground in rain water.</i><br /><br /><i>That extraterrestrial origin concept was of course enough to crank up controversy but </i><b><i>it didn&#39;t go very far because there was little supporting data</i></b><i> and so it was of speculative interest mostly in certain areas of the science community. Then Dr&#39;s. Louis and Kumar subsequently published a paper titled &#39;The Red Fain Phenomenon of Kerala and Its Possible Extraterrestrial Origin&#39; in the July 4, 2006 journal Astrophysics and Space Science. This level of perceived legitimacy in conjunction with the extraterrestrial origin claim aspect caught the media&#39;s attention producing a lot of sensational popularized stories. If you&#39;ve heard of this before, it is probably via this time period even though the occurrence itself was several years earlier.</i><br /><br /><i>Since then, </i><b><i>subsequent analysis by Louis and Kumar indicates that the microorganism demonstrates the ability to consume and metabolize a wide range of both organic an inorganic materials giving it tremendous survival potential. Further, it also generates autofluoresence in multiple colors which is something that no known biological life on Earth is known to do. This evidence again leans in favor of the extraterrestrial origin.</i></b><br /><br /><i>However, it doesn&#39;t stop there. It appears tests confirm that </i><b><i>this microorganism grows more active and will replicate itself at, ... get this, up around +300 deg C or +572 deg F and under tremendous pressures. That incredible high temperature is long after any known Earth based life form will have fried and died. Incredibly it appears that this organism is just getting into its comfort zone replicating and proliferating like this at such high temperatures. It appears that the cell replicates by developing one or more white or colorless offspring within its cell body and then when they turn a red or darker color (maturity?) they can extrude through the cell wall forming a new additional cell.</i></b><br /><br /><b><i>Not only that but the organism has been tested surviving down to –200 deg C or –328 deg F and immersed in liquid nitrogen without damage. Also add to that survival in very high pressure that Earth based life could not have survived. Obviously survival in such extreme temperatures and pressures quickly rules out the initial identification of this organism as a lichen of any known kind. It is also the same with the autoflouresence feature.</i></b><br /><br /><i>However, one of the strongest factors that also got the media and science community attention was Louis and Kumar claims that the extraterrestrial origin tends to support the scientific &#39;Panspermia&#39; hypothesis. In basic this hypothesis says that the basic seeds of life exist all over the Universe and that life for example here on Earth may have originated through these seeds through dispersal by comets, etc. The red rain organism could be one type of this &#39;seed.&#39; In other words, a seed like this planted in the earlier much hotter and turbulent age of Earth&#39;s planetary development would have flourished and then later that life here evolved to fit the changing environment as the Earth cooled and stabilized over so many millions or even billions of years.</i><br /><br /><i>Since this in its present form is a only a hypothesis originating from within the science community with some supporters, plenty of detractors, and a great deal more careful fence straddlers as is the case with so many such hypothesis and theories that are subject to interpretation, it is a guarantee of controversy and argument. That means that passions can run high and reputations can hang in the balance on such issues as sides align. As you know, in the field of science and academics, reputations are everything.</i><br /><br /><i>If that isn&#39;t enough to guarantee conflict, it appears that </i><b><i>so far in the last 8 years or so, no DNA has been found to conclusively exist in the red rain organism and scientist have been looking for it.</i></b><i> Now all life that the science community can even conceive of has DNA with no exceptions and anything else is just impossible. Further, even the primary proponents of the Panspermia hypothesis presume the presence of DNA in those &#39;seeds&#39; of life. So if this red rain organism evidence can be confirmed to NOT have DNA, that will go against so much that is accepted as foundation fact in science.</i><br /><br /><i>That puts Dr&#39;s. Louis and Kumar out of a very long lonely limb bucking the science consensus and reputation tide with Dr. Louis the more publicly visible of the two. Dr. Louis is now head of the Department of Physics at Cochin University of Science and Technology located in Kochi, Kerala in India. </i><b><i>Louis and Kumar have shared the red rain samples openly with prominent scientists around the world for testing.</i></b><i> It remains to be seen if DNA will or will not be confirmed by all this testing and also if it will be reported honestly.</i><br /><br /><i>[...]</i><br /><br /><i>already some scientists with no consensus to back them up and protect them are characterizing Louis and Kumar&#39;s claims as &#39;wild&#39; even though the claims are just driven by the to date evidence, even though those making such statements are themselves so far unable to produce findings of DNA in the samples, and Louis and Kumar acknowledge that more confirmation investigation needs to be done. So some serious foot dragging can no doubt be expected on this issue of DNA or no DNA.</i><br /><br /><i>[...]</i><br /><br /><b><i>One must ask oneself how this life form came by an adaptation to such temperature (and pressure) extremes spanning a plus to minus range of at least 500 deg C or 900 deg F. An organism does not accidentally develop such a capability. There must first be a need for it at some initial point in its existence.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Microbiome","summary":"Both Ulcers and Stomach Cancer Were Traced in the Early 80s to a Bacteria / The Internist Physician Who Discovered It Struggled to Be Heard / It Wasn't Until He Infected, then Cured, Himself that People Listened / Since then, this Line of Investigation Has Exploded","image":"https://lh6.googleusercontent.com/-f-g41JNZoF4/VujvYLAqVHI/AAAAAAAAHSk/RUMTUTSj1Qwfl2_ChSF36AYzhCMxr8ZYg/the-microbiome-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/MeQAt54JGZj","publishDate":"2016-03-16T05:32:49.957Z","updateDate":"2017-01-03T02:44:13.253Z","text":"<b>The Microbiome: Both Ulcers and Stomach Cancer Were Traced in the Early 80s to a Bacteria / The Internist Physician Who Discovered It Struggled to Be Heard / It Wasn&#39;t Until He Infected, then Cured, Himself that People Listened / Since then, this Line of Investigation Has Exploded</b><br /><br /><a href=\"http://discovermagazine.com/2010/mar/07-dr-drank-broth-gave-ulcer-solved-medical-mystery\" class=\"ot-anchor\">http://discovermagazine.com/2010/mar/07-dr-drank-broth-gave-ulcer-solved-medical-mystery</a><br /><br /><b><i>&quot;The Dr. Who Drank Infectious Broth, Gave Himself an Ulcer, and Solved a Medical Mystery</i></b><br /><br /><b><i>The medical elite thought they knew what caused ulcers and stomach cancer. But they were wrong -- and did not want to hear the answer that was right.</i></b><br /><br /><i>By Pamela Weintraub</i><br /><i>Thursday, April 08, 2010</i><br /><br /><i>For years an obscure doctor hailing from Australia’s hardscrabble west coast watched in horror as ulcer patients fell so ill that many had their stomach removed or bled until they died. That physician, an internist named Barry Marshall, was tormented because he knew there was a simple treatment for ulcers, which at that time afflicted 10 percent of all adults. In 1981 Marshall began working with Robin Warren, the Royal Perth Hospital pathologist who, two years earlier, discovered the gut could be overrun by hardy, corkscrew-shaped bacteria called Helicobacter pylori. </i><b><i>Biopsying ulcer patients and culturing the organisms in the lab, Marshall traced not just ulcers but also stomach cancer to this gut infection. The cure, he realized, was readily available: anti­biotics. But mainstream gastroenterologists were dismissive, holding on to the old idea that ulcers were caused by stress.</i></b><br /><br /><b><i>Unable to make his case in studies with lab mice (because H. pylori affects only primates) and prohibited from experimenting on people, Marshall grew desperate. Finally he ran an experiment on the only human patient he could ethically recruit: himself. He took some H. pylori from the gut of an ailing patient, stirred it into a broth, and drank it. As the days passed, he developed gastritis, the precursor to an ulcer: He started vomiting, his breath began to stink, and he felt sick and exhausted. Back in the lab, he biopsied his own gut, culturing H. pylori and proving unequivocally that bacteria were the underlying cause of ulcers.</i></b><br /><br /><i>Marshall recently sat down with DISCOVER senior editor Pam Weintraub in a Chicago hotel, wearing blue jeans and drinking bottled water without a trace of Helicobacter. The man The Star once called &#39;the guinea-pig doctor&#39; can now talk about his work with the humor and passion of an outsider who has been vindicated. </i><b><i>For their work on H. pylori, Marshall and Warren shared a 2005 Nobel Prize. Today the standard of care for an ulcer is treatment with an antibiotic. And stomach cancer—once one of the most common forms of malignancy—is almost gone from the Western world.</i></b><br /><br /><i>[...]</i><br /><br /><i>Before the 20th century, the ulcer was not a respectable disease. Doctors would say, “You’re under a lot of stress.” Nineteenth-century Europe and America had all these crazy health spas and quack treatments. By the 1880s doctors had developed surgery for ulcers, in which they cut off the bottom of the stomach and reconnected the intestine. </i><b><i>We’re pretty certain now that by the start of the 20th century, 100 percent of mankind was infected with Helicobacter pylori, but you can go through your whole life and never have any symptoms.</i></b><br /><br /><i>[...]</i><br /><br /><i>How did you get the word out about your discovery?</i><br /><br /><i>I presented that work at the annual meeting of the Royal Australasian College of Physicians in Perth. </i><b><i>That was my first experience of people being totally skeptical. To gastroenterologists, the concept of a germ causing ulcers was like saying that the Earth is flat. After that I realized my paper was going to have difficulty being accepted. You think, &#39;It’s science; it’s got to be accepted.&#39; But it’s not an absolute given. The idea was too weird.</i></b><br /><br /><i>[...]</i><br /><br /><i>That letter must have provoked an uproar.</i><br /><br /><b><i>It didn’t. In fact, our letters were so weird that they almost didn’t get published.</i></b><i> By then I was working at a hospital in Fremantle, biopsying every patient who came through the door. I was getting all these patients and couldn’t keep tabs on them, so I tapped all the drug companies to request research funding for a computer. They all wrote back saying how difficult times were and they didn’t have any research money. But they were making a billion dollars a year for the antacid drug Zantac and another billion for Tagamet. You could make a patient feel better by removing the acid. Treated, most patients didn’t die from their ulcer and didn’t need surgery, </i><b><i>so it was worth $100 a month per patient, a hell of a lot of money in those days. In America in the 1980s, 2 to 4 percent of the population had Tagamet tablets in their pocket. There was no incentive to find a cure.</i></b><br /><br /><i>[...]</i><br /><br /><i>But one drug company did provide useful information, right?</i><br /><br /><i>I got an interesting letter from a company that made an ulcer product called Denel, which contained bismuth—much like Pepto-Bismol in the United States. The company had shown that it healed ulcers just as quickly as Tagamet, even though the acid remained. The weird thing was that if they treated 100 patients with this drug, 30 of them never got their ulcer back, whereas if you stopped Tagamet, 100 percent would get their ulcer back in the next 12 months. So the company said: &#39;This must heal ulcers better than just removing the acid. It must do something to the underlying problem, whatever that is.&#39; They sent me their brochure with &#39;before&#39; and &#39;after&#39; photographs. On the &#39;before&#39; photograph they had Helicobacter in the picture, and in the &#39;after&#39; picture there was none. So I put their drug on Helicobacter and it killed them like you wouldn’t believe. They helped me present at an international microbiology conference in Brussels.</i><br /><br /><i>The microbiologists in Brussels loved it, and by March of 1983 I was incredibly confident. During that year Robin and I wrote the full paper. </i><b><i>But everything was rejected. Whenever we presented our stuff to gastroenterologists, we got the same campaign of negativism. I had this discovery that could undermine a $3 billion industry, not just the drugs but the entire field of endoscopy. Every gastroenterologist was doing 20 or 30 patients a week who might have ulcers, and 25 percent of them would. Because it was a recurring disease that you could never cure, the patients kept coming back. And here I was handing it on a platter to the infectious-disease guys.</i></b><br /><br /><i>Didn’t infectious-disease researchers support you, at least?</i><br /><br /><i>They said: &#39;This is important. This is great. We are going to be the new ulcer doctors.&#39; There were lots of people doing the microbiology part. </i><b><i>But those papers were diluted by the hundreds of papers on ulcers and acid.</i></b><i> It used to drive me crazy.</i><br /><br /><i>[...]</i><br /><br /><i>Your personal experience convinced you that Helicobacter infection starts in childhood. Can you explain?</i><br /><br /><i>At first I thought it must have been a silent infection, but after I had it, I said, &#39;No, it’s actually an infection that causes vomiting.&#39; And when do you catch such infections? When you’re toddling around, eating dirty things and playing with your dirty little brothers and sisters. </i><b><i>The reason you didn’t remember catching Helicobacter is that you caught it before you could talk.</i></b><br /><br /><i>[...]</i><br /><br /><i>You published a synthesis of this work in The Medical Journal of Australia in 1985. Then did people change their thinking?</i><br /><br /><b><i>No, it sat there as a hypothesis for another 10 years. Some patients heard about it, but gastroenterologists still would not treat them with antibiotics. Instead, they would focus on the possible complications of antibiotics. By 1985 I could cure just about everybody, and patients were coming to me in secret</i></b><i> -- for instance, airline pilots who didn’t want to let anyone know that they had an ulcer.</i><br /><br /><i>So how did you finally convince the medical community?</i><br /><br /><i>I didn’t understand it at the time, but Procter &amp; Gamble [the maker of Pepto-Bismol] was the largest client of Hill &amp; Knowlton, the public relations company. After I came to work in the States, publicity would come out. Stories had titles like &#39;Guinea-Pig Doctor Experiments on Self and Cures Ulcer,&#39; and Reader’s Digest and the National Enquirer covered it. Our credibility might have dropped a bit, but interest in our work built. Whenever someone said, &#39;Oh, Dr. Marshall, it’s not proven,&#39; I’d say: &#39;Well, there’s a lot at stake here. People are dying from peptic ulcers. We need to accelerate the process.&#39; And ultimately, the NIH and FDA did that. They fast-tracked a lot of this knowledge into the United States and said to the journals: &#39;We can’t wait for you guys to conduct these wonderful, perfect studies. We’re going to move forward and get the news out.&#39; That happened quite quickly in the end. Between 1993 and 1996, the whole country changed color.&quot;</i><br /><br />Graphic comes from <a href=\"http://big.assets.huffingtonpost.com/2015_GutBacteriaA.png\" class=\"ot-anchor\">http://big.assets.huffingtonpost.com/2015_GutBacteriaA.png</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"The Generalist","summary":"If Historians of Science Are Right, Science is Today Over-Specialized / This is Only a Problem If We Also Decide to Specialize, Ourselves / If We Become Generalists Instead, Then It's as If the Specialists Work for Us / The Value of Learning Critiques of Textbook Science is Because It Enables Us to Be Better Generalists","image":"https://lh3.googleusercontent.com/-u4XUu_rkHT8/VuerLDfqRuI/AAAAAAAAHSA/J8nloUL4N9o/w1800-h2520/the-generalist-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Cp7MKcMN3dr","publishDate":"2016-03-15T06:36:12.575Z","updateDate":"2017-01-03T02:44:42.182Z","text":"<b>The Generalist: If Historians of Science Are Right, Science is Today Over-Specialized / This is Only a Problem If We Also Decide to Specialize, Ourselves / If We Become Generalists Instead, Then It&#39;s as If the Specialists Work for Us / The Value of Learning Critiques of Textbook Science is Because It Enables Us to Be Better Generalists</b><br /><br />If the historians of science are right, then science has become overly-specialized.  This is not a minor thing.  That knowledge is power; we can actually do something with it.  We can use use this knowledge as a tool for altering our approach to problem-solving:<br /><br /><b>We can decide to become generalists.</b><br /><br />From wikipedia ...<br /><br /><i>&quot;The story of the blind men and an elephant originated in the Indian subcontinent from where it has widely diffused. It is a story of a group of blind men (or men in the dark) who touch an elephant to learn what it is like. Each one feels a different part, but only one part, such as the side or the tusk. They then compare notes and learn that they are in complete disagreement.&quot;</i><br /><br />A second reaction worth considering is to <b>seek out ideas which appear to exhibit broad explanatory powers.</b> How many scientific disciplines are affected by this single idea?  It&#39;s certainly not a perfect metric, but it&#39;s a good rule of thumb that in a world of specialist scientists, there are probably ideas of worth whose appeal only becomes apparent once a person takes the broad view.<br /><br />Third, <b>listen to critics. In fact, seek them out, and avoid the temptation to reject everything they say due to a single possibly discordant claim.</b><br /><br />Fourth: Be prepared to change your mind. <b>Science is the belief in the ignorance of experts.</b> Treat it like a tool for changing, better.<br /><br />And another thing: Don&#39;t take claims on faith.  If you don&#39;t understand something, then don&#39;t commit to it.  This is enormously important. <b>Seek to feel comfortable with not feeling certain in light of scientific controversies.</b><br /><br />I&#39;m not saying question everything; I&#39;m saying <b>when you see claims being made which run counter to textbook theory, keep an open mind about them.</b> Some of these against-the-mainstream claims will, without a doubt, become recognized as true in just a generation.<br /><br />And even one step beyond that, seek out good questions. <b>The questions we ask are a function of our worldviews. What that means is that it is possible that certain questions will never produce any useful answer at all -- because in some situations we are operating from the wrong worldview, to begin with. In those cases, we are not even asking the right questions.</b><br /><br />The notion of becoming a generalist is not meant as an absolute.  Think of it more as a flip of a continuum: Rather than predominantly thinking as a specialist, we can let strong, controversial claims over time lead us to new domains of knowledge.  When something looks interesting, we can dive a bit deeper to answer a question that&#39;s bugging us.  But, at any point, if the area starts to feel &#39;stale&#39; for some reason, we can quickly back out and move on to other things.<br /><br />Our enemies in the generalist game are the layers of interpretation which separate us from raw data (which, technically speaking, does not even exist).  The way we keep our superficial exposure to numerous domains from creating a superficial comprehension is to pay careful attention to whistleblowers and controversial claims.  Once you know the patterns that these layers of interpretation tend to adhere to, you will become better at judging claims, more broadly.<br /><br />This is why mistakes in science are like gold; people tend to focus upon the implication that if science is wrong, then that means that the future will not be what we hoped for.  What they should be doing is seeking to identify the lesson in the story.  It&#39;s the difference between being a spectator and a participant.<br /><br />It&#39;s not just a matter of understanding logical fallacies; there is a complex interplay of sociology and of course your own psychology at play as well.  Confusingly, all of the layers occur at once.<br /><br />Now, not everybody will do this -- and that&#39;s okay -- but what I want to caution is that if you do this for long enough, eventually you will start to see interesting, unexpected connections, <b>and you will wish that you had more carefully taken notes.</b><br /><br />That said, none of it will happen unless you want it to happen.  The thing about people is that we have an innate desire to learn which we are all born with.  Each person wants to learn <b>something</b>; the real trick is to take that specialist urge and convince yourself that you will be much better at that <b>thing</b> if you decide to approach that subject in a more generalist, interdisciplinary manner.<br /><br />When you are a generalist in a world of specialists, you are making a conscious decision to take this amazing specialist knowledge that surrounds you, <b>and actually do something amazing with it.</b><br /><br />You are in a sense now the <b>boss</b> of your own mind, and the scientific community now works for <b>you.</b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"Plato's Reveal","summary":"With Just a Few Paragraphs of Text, Plato Helped Inspire Profound Questions About the Origin of Religion, and Altered the Meaning of the First 3,000+ Years of Human Story-Telling / This Would Set the Stage for a Scientific Re-Examination of Those First Stories Which Would Not Become Possible for 2,400 More Years","image":"https://lh5.googleusercontent.com/-Hz5XGO6f24M/VuXlPoP4G5I/AAAAAAAAHQU/OPxpx2dDSuk8nwZB3u4T_LPZ18cuT54vw/platos-reveal-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Dm93UMBrkr9","publishDate":"2016-03-13T22:10:53.002Z","updateDate":"2017-01-03T02:45:10.876Z","text":"<b>Plato&#39;s Reveal: With Just a Few Paragraphs of Text, Plato Helped Inspire Profound Questions About the Origin of Religion, and Altered the Meaning of the First 3,000+ Years of Human Story-Telling / This Would Set the Stage for a Scientific Re-Examination of Those First Stories Which Would Not Become Possible for 2,400 More Years</b><br /><br />Attempts to decode meaning within the first stories told by mankind -- today referred to as <i>mythology</i> -- date back to 6th century B.C.  There have been a variety of approaches which can essentially be broken down into three categories.<br /><br /><b>The Three Approaches to Mythology</b><br /><br />Mel Acheson writes at <a href=\"https://www.thunderbolts.info/wp/2011/11/07/that-one-story-again/\" class=\"ot-anchor\">https://www.thunderbolts.info/wp/2011/11/07/that-one-story-again/</a><br /><br /><b><i>&quot;Petroglyphs, myths, and rituals around the world are composed of the same motifs: the ladder to heaven, the great star, and the thunderbolt</i></b><i> -- to name only three. The motifs are linked within each ancient society’s oeuvre and therefore are implicitly defined: the thunderbolt springs from the eye of the hero as he descends the ladder from heaven. Furthermore, </i><b><i>the linkages are the same around the world.</i></b><br /><br /><i>This is not a new insight. Many mythologists have remarked on the one story told around the world. The difficulty lies in explaining that pattern of recurrence. There would seem to be only three possibilities.</i><br /><br /><b><i>One Story:</i></b><i> Some physical event with a global occurrence inspired the anthropomorphic narrative. If the story were about sunrises and thunderstorms, there would be no difficulty. However, the story is about planetary gods hurling hammers and fiery wheels from a celestial column that is fixed along the axis of heaven. If those planetary gods are the same planets that we see today, the story violates the Law of Gravity and is impossible.</i><br /><br /><b><i>One Storyteller:</i></b><i> This was the first story told by the handful of first humans as they huddled around the first campfire in Africa. Their descendants took it with them as they spread around the world. They changed the names of the characters as they invented different languages, but they kept the motifs and plots and even the specific details of the images and interrelationships.</i><br /><br /><i>Thus, a primitive tribe in the Amazon today tells of the same ladder to heaven as the ancient Babylonians told of because their ancestors carried it unchanged across the Bering land bridge during the Ice Age. This explanation trades impossibility for incredibility.</i><br /><br /><b><i>One Storytelling:</i></b><i> The human brain is hardwired to generate this myth in all its details. Then why did it generate those myths and glyphs only during the Age of Mythmaking, not before, not now? Today’s comparable creation myth, the Big Bang, retains the explosive initial event but puts it far in the past beyond human witnessing and devotes most of the narrative to slowly changing uniformity.&quot;</i><br /><br /><b>The Astronomical Case</b><br /><br />Of all such attempts to explain myth, the analysis put forth by Plato demands special recognition.<br /><br />From Plato&#39;s Dialogues at <a href=\"https://books.google.com/books?id=6IJEAAAAIAAJ&amp;pg=PA367&amp;lpg=PA367\" class=\"ot-anchor\">https://books.google.com/books?id=6IJEAAAAIAAJ&amp;pg=PA367&amp;lpg=PA367</a><br /><br /><i>&quot;Phaethon, the son of Helios, having yoked the steeds in his father&#39;s chariot, because he was not able to drive them in the path of his father, </i><b><i>burned up all that was upon the earth, and was himself destroyed by a thunderbolt. Now, this has the form of a myth, but really signifies a declination of the bodies moving around the earth and in the heavens, and a great conflagration of things upon the earth recurring at long intervals of time&quot;</i></b><br /><br />... then further on ...<br /><br /><b><i>&quot;All of these stories, and ten thousand others which are still more wonderful, have a common origin; many of them have been lost in the lapse of ages, or exist only as fragments; but the origin of them is what no one has told&quot;</i></b><br /><br />What is peculiar about this claim is that Plato is unwittingly describing a debris field that would regularly return to the earth after an initial catastrophe.  Plato lived around 400 BC -- a full 2,200 years before the cause of debris fields would become understood. <b>He obviously has no idea what gravity or a debris field actually is, and yet he has accurately described a key aspect.</b><br /><br />But, he does not stop there.  He also appears to make the important claim that <b>all of the myths have a common -- untold -- origin.</b> Plato is arguing for the One Story explanation.<br /><br />All combined, Plato is pointing us to some sort of cosmic event which people collectively experienced as the reason why strikingly similar myths appeared over the entire world at around the same time.  The argument brings with it enormous explanatory potential, but it also directly clashes with the proposition by now firmly entrenched within the scientific community that Earth&#39;s recent history is devoid of any events at all.<br /><br />Modern attempts to follow up on Plato&#39;s tip have, not surprisingly, destroyed careers.  From Dwardu Cardona&#39;s <i>God Star</i> (p23):<br /><br /><b><i>&quot;The casual reader of popular mythology may not realize to what extent the subject is infused with astronomical content.  This is partly due to the aversion displayed by most modern mythologists to anything in their studies that smacks of astronomical connections. This is so true that when Girogio de Santillana and Hertha von Dechend dared to go against the common trend in their monumental work, Hamlet&#39;s Mill, they found the doors of academia closed shut in their face.</i></b><i> As Martin Bernal found reason to state:</i><br /><br /><i>&#39;Having written a major book on Galileo, de Santillana [whom Bernal qualifies as possibly the greatest historian of Renaissance science] became interested in the Hermetic Egyptian tradition; then, late in life, he read Dupuis&#39;s Origine de tous les cultes, </i><b><i>and was convinced by its argument that much of ancient mythology was indeed allegory for scientific astronomy.</i></b><br /><br /><i>Despite de Santillana&#39;s enormous reputation, Hamlet&#39;s Mill -- the book in which he and a younger German colleague [Hertha von Dechend] set out [ such a mytho-astronomical] scheme -- was not accepted by any university press and was published commercially.  This means that respectable scholars are not obliged to take such work into account.  In addition, de Santillana&#39;s having stuck his neck out so far lessened his effectiveness as a [historian].  Furthermore, his work ... could be lumped together with a more or less &#39;lunatic fringe&#39;; this allowed or even compelled orthodox scholars to ignore it.&#39;</i><br /><br /><i>Earlier in this century, mythologists were quite aware of the astronomical nature of ancient myths, but, of lately, the general assumption has been that these astral connections are of late derivation, having been foisted on an already existing mythology.  So much can be said against this that volumes, of which [God Star] is one, can be written.</i><br /><br /><i>The problem, however, goes deeper than that because even those earlier mythologists who had accepted the astronomical nature of the myths were themselves often guilty of tampering with their sources so that, today, it is not easy to discern what these ancient sources really had to say about the cosmic powers our ancestors venerated. </i><b><i>This is not to be wondered at because, if one had to pursue the astronomical motifs inherent in most of mythology, one would be forced to assume that the ancients&#39; perception of the celestial sphere was entirely different from that of modern man. Thus, when modern mythologists pursue these astral myths, they tend to look up at the sky, if only figuratively, and realize that what the ancient records state about the planets, and even the Sun and Moon, bears absolutely no resemblance to the present Solar System. And yet one must keep in mind that these self-same ancients ... were quite sophisticated when it came to observational astronomy.</i></b><i>&quot;</i><br /><br /><b>The Comparative Mythology Approach</b><br /><br />These claims lend themselves to a particular avenue of investigation known as <i>comparative mythology</i>.  From wikipedia:<br /><br /><i>&quot;The anthropologist C. Scott Littleton defined comparative mythology as </i><b><i>&#39;the systematic comparison of myths and mythic themes drawn from a wide variety of cultures&#39;.[1] By comparing different cultures&#39; mythologies, scholars try to identify underlying similarities and/or to reconstruct a &#39;protomythology&#39; from which those mythologies developed.</i></b><i> [1] To an extent, all theories about mythology follow a comparative approach: as the scholar of religion Robert Segal notes, &#39;by definition, all theorists [of myth] seek similarities among myths&#39;.[2] However, scholars of mythology can be roughly divided into particularists, who emphasize the differences between myths, and comparativists, who emphasize the similarities. Particularists tend to &#39;maintain that the similarities deciphered by comparativists are vague and superficial&#39;, while comparativists tend to &#39;contend that the differences etched by particularists are trivial and incidental&#39;.[3]</i><br /><br /><b><i>Comparative approaches to mythology held great popularity among eighteenth- and nineteenth-century scholars. Many of these scholars believed that all myths showed signs of having evolved from a single myth or mythical theme.&quot;</i></b><br /><br /><b>The Role of History and the Specialization of Science</b><br /><br />As a culture, we tend to take for granted the fact that much of our current astronomical knowledge has only recently been acquired.  Many significant questions remain completely unanswered -- like, somewhat importantly, <i>where is the missing matter which holds this gravitational framework together?</i>.  The question plainly leaves completely unsettled the universe&#39;s dominant force.  We tend to ignore the possibility that we might have strayed towards a cosmological dead-end.  In such a case, much of what we think we know about space would turn out to be wrong.<br /><br />Yet, when Santillana and Dechend dared to publish <i>Hamlet&#39;s Mill</i>, their critics plainly behaved as though all of the big questions about the universe had already been settled.<br /><br />As observers of modern science, the tendency is to adopt the specialist approach to all subject matters in our attempts to evaluate arguments.  A claim in mythology, it is presumed, can be judged without any need to consider that which we still do not understand in cosmology or astrophysics.  The two, we assume, are completely unrelated, containered.  And even if the assumption turns out to be wrong, we&#39;ll nevertheless continue to assume it all the way up to the point that it&#39;s definitively understood we were wrong.<br /><br />Unfortunately, in this particular case, that would involve some form of catastrophe which we had actively refused to acknowledge and prepare for.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"The Post-Idea World","summary":"A World in Which Big, Thought-Provoking Ideas that Can’t Instantly be Monetized Are of So Little Intrinsic Value that Fewer People are Generating Them and Fewer Outlets are Disseminating Them / In the Past, We Collected Information Not Simply to Know Things / We Sought Not Just to Apprehend the World But to Truly Comprehend It, Which is the Primary Function of Ideas / Today, We Are So Uninterested in Anything Outside Ourselves and Our Friendship Circles that if a Marx or a Nietzsche Were Suddenly to Appear, Blasting His Ideas, No One Would Pay the Slightest Attention","image":"https://lh3.googleusercontent.com/-2XQiPAZJhdI/VuSiUtlXegI/AAAAAAAAHPY/oX0ix1efGX4/w3600-h5040/the-post-idea-world-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/U6R8e12icZa","publishDate":"2016-03-12T23:13:49.932Z","updateDate":"2017-01-03T02:45:26.058Z","text":"<b>The Post-Idea World: A World in Which Big, Thought-Provoking Ideas that Can’t Instantly be Monetized Are of So Little Intrinsic Value that Fewer People are Generating Them and Fewer Outlets are Disseminating Them / In the Past, We Collected Information Not Simply to Know Things / We Sought Not Just to Apprehend the World But to Truly Comprehend It, Which is the Primary Function of Ideas / Today, We Are So Uninterested in Anything Outside Ourselves and Our Friendship Circles that if a Marx or a Nietzsche Were Suddenly to Appear, Blasting His Ideas, No One Would Pay the Slightest Attention</b><br /><br /><a href=\"http://www.nytimes.com/2011/08/14/opinion/sunday/the-elusive-big-idea.html?pagewanted=all&amp;_r=1\" class=\"ot-anchor\">http://www.nytimes.com/2011/08/14/opinion/sunday/the-elusive-big-idea.html?pagewanted=all&amp;_r=1</a><br /><br /><b><i>&quot;The Elusive Big Idea</i></b><br /><i>By Neal Gabler</i><br /><i>Aug. 13, 2011</i><br /><br /><i>The July/August issue of The Atlantic trumpets the &#39;14 Biggest Ideas of the Year.&#39; Take a deep breath. The ideas include &#39;The Players Own the Game&#39; (No. 12), &#39;Wall Street: Same as it Ever Was&#39; (No. 6), &#39;Nothing Stays Secret&#39; (No. 2), and the very biggest idea of the year, &#39;The Rise of the Middle Class — Just Not Ours,&#39; which refers to growing economies in Brazil, Russia, India and China.</i><br /><br /><i>Now exhale. It may strike you that none of these ideas seem particularly breathtaking. </i><b><i>In fact, none of them are ideas.</i></b><i> They are more on the order of observations. But one can’t really fault The Atlantic for mistaking commonplaces for intellectual vision. </i><b><i>Ideas just aren’t what they used to be. Once upon a time, they could ignite fires of debate, stimulate other thoughts, incite revolutions and fundamentally change the ways we look at and think about the world.</i></b><br /><br /><i>They could penetrate the general culture and make celebrities out of thinkers — notably Albert Einstein, but also Reinhold Niebuhr, Daniel Bell, Betty Friedan, Carl Sagan and Stephen Jay Gould, to name a few. The ideas themselves could even be made famous: for instance, for &#39;the end of ideology,&#39; &#39;the medium is the message,&#39; &#39;the feminine mystique,&#39; &#39;the Big Bang theory,&#39; &#39;the end of history.&#39; A big idea could capture the cover of Time — &#39;Is God Dead?&#39; — and intellectuals like Norman Mailer, William F. Buckley Jr. and Gore Vidal would even occasionally be invited to the couches of late-night talk shows. How long ago that was.</i><br /><br /><b><i>If our ideas seem smaller nowadays, it’s not because we are dumber than our forebears but because we just don’t care as much about ideas as they did. In effect, we are living in an increasingly post-idea world — a world in which big, thought-provoking ideas that can’t instantly be monetized are of so little intrinsic value that fewer people are generating them and fewer outlets are disseminating them, the Internet notwithstanding. Bold ideas are almost passé.</i></b><br /><br /><i>It is no secret, especially here in America, that we live in a post-Enlightenment age in which rationality, science, evidence, logical argument and debate have lost the battle in many sectors, and perhaps even in society generally, to superstition, faith, opinion and orthodoxy. While we continue to make giant technological advances, we may be the first generation to have turned back the epochal clock — to have gone backward intellectually from advanced modes of thinking into old modes of belief. But post-Enlightenment and post-idea, while related, are not exactly the same.</i><br /><br /><i>Post-Enlightenment refers to a style of thinking that no longer deploys the techniques of rational thought. </i><b><i>Post-idea refers to thinking that is no longer done, regardless of the style.</i></b><br /><br /><i>The post-idea world has been a long time coming, and many factors have contributed to it. There is the retreat in universities from the real world, and </i><b><i>an encouragement of and reward for the narrowest specialization rather than for daring — for tending potted plants rather than planting forests.</i></b><br /><br /><i>There is the eclipse of the public intellectual in the general media by the pundit who substitutes outrageousness for thoughtfulness, and the concomitant decline of the essay in general-interest magazines. And there is the rise of an increasingly visual culture, especially among the young — a form in which ideas are more difficult to express.</i><br /><br /><i>But these factors, which began decades ago, were more likely harbingers of an approaching post-idea world than the chief causes of it. </i><b><i>The real cause may be information itself. It may seem counterintuitive that at a time when we know more than we have ever known, we think about it less.</i></b><br /><br /><i>We live in the much vaunted Age of Information. Courtesy of the Internet, we seem to have immediate access to anything that anyone could ever want to know. We are certainly the most informed generation in history, at least quantitatively. There are trillions upon trillions of bytes out there in the ether — so much to gather and to think about.</i><br /><br /><i>And that’s just the point. </i><b><i>In the past, we collected information not simply to know things. That was only the beginning. We also collected information to convert it into something larger than facts and ultimately more useful — into ideas that made sense of the information. We sought not just to apprehend the world but to truly comprehend it, which is the primary function of ideas. Great ideas explain the world and one another to us.</i></b><br /><br /><i>Marx pointed out the relationship between the means of production and our social and political systems. Freud taught us to explore our minds as a way of understanding our emotions and behaviors. Einstein rewrote physics. More recently, McLuhan theorized about the nature of modern communication and its effect on modern life. </i><b><i>These ideas enabled us to get our minds around our existence and attempt to answer the big, daunting questions of our lives.</i></b><br /><br /><b><i>But if information was once grist for ideas, over the last decade it has become competition for them. We are like the farmer who has too much wheat to make flour. We are inundated with so much information that we wouldn’t have time to process it even if we wanted to, and most of us don’t want to.</i></b><br /><br /><i>The collection itself is exhausting: what each of our friends is doing at that particular moment and then the next moment and the next one; who Jennifer Aniston is dating right now; which video is going viral on YouTube this hour; what Princess Letizia or Kate Middleton is wearing that day. In effect, we are living within the nimbus of an informational Gresham’s law in which trivial information pushes out significant information, but it is also an ideational Gresham’s law in which information, trivial or not, pushes out ideas.</i><br /><br /><b><i>We prefer knowing to thinking because knowing has more immediate value. It keeps us in the loop, keeps us connected to our friends and our cohort. Ideas are too airy, too impractical, too much work for too little reward. Few talk ideas. Everyone talks information, usually personal information. Where are you going? What are you doing? Whom are you seeing? These are today’s big questions.</i></b><br /><br /><i>It is certainly no accident that the post-idea world has sprung up alongside the social networking world. Even though there are sites and blogs dedicated to ideas, Twitter, Facebook, Myspace, Flickr, etc., the most popular sites on the Web, are basically information exchanges, designed to feed the insatiable information hunger, </i><b><i>though this is hardly the kind of information that generates ideas.</i></b><i> It is largely useless except insofar as it makes the possessor of the information feel, well, informed. Of course, one could argue that these sites are no different than conversation was for previous generations, and that conversation seldom generated big ideas either, and one would be right.</i><br /><br /><i>BUT the analogy isn’t perfect. For one thing, social networking sites are the primary form of communication among young people, and they are supplanting print, which is where ideas have typically gestated. For another, social networking sites engender habits of mind that are inimical to the kind of deliberate discourse that gives rise to ideas. Instead of theories, hypotheses and grand arguments, we get instant 140-character tweets about eating a sandwich or watching a TV show. While social networking may enlarge one’s circle and even introduce one to strangers, this is not the same thing as enlarging one’s intellectual universe. Indeed, the gab of social networking tends to shrink one’s universe to oneself and one’s friends, while thoughts organized in words, whether online or on the page, enlarge one’s focus.</i><br /><br /><i>To paraphrase the famous dictum, often attributed to Yogi Berra, that you can’t think and hit at the same time, you can’t think and tweet at the same time either, not because it is impossible to multitask but because tweeting, which is largely a burst of either brief, unsupported opinions or brief descriptions of your own prosaic activities, is a form of distraction or anti-thinking.</i><br /><br /><b><i>The implications of a society that no longer thinks big are enormous. Ideas aren’t just intellectual playthings. They have practical effects.</i></b><br /><br /><i>[...]</i><br /><br /><i>No doubt there will be those who say that the big ideas have migrated to the marketplace, but there is a vast difference between profit-making inventions and intellectually challenging thoughts. Entrepreneurs have plenty of ideas, and some, like Steven P. Jobs of Apple, have come up with some brilliant ideas in the &#39;inventional&#39; sense of the word.</i><br /><br /><i>Still, while these ideas may change the way we live, they rarely transform the way we think. They are material, not ideational. It is thinkers who are in short supply, and the situation probably isn’t going to change anytime soon.</i><br /><br /><b><i>We have become information narcissists, so uninterested in anything outside ourselves and our friendship circles or in any tidbit we cannot share with those friends that if a Marx or a Nietzsche were suddenly to appear, blasting his ideas, no one would pay the slightest attention, certainly not the general media, which have learned to service our narcissism.</i></b><br /><br /><b><i>What the future portends is more and more information — Everests of it. There won’t be anything we won’t know. But there will be no one thinking about it.</i></b><br /><br /><b><i>Think about that.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"Quasi-neutrality","summary":"Charge-neutrality is Very Different in Solids and Plasmas / To Avoid Confusion, We Use the Term Quasi-neutrality to Refer to the Plasma Case / A Plasma Can be Quasi-neutral and Yet Still Highly Conductive, Because Conductivity is a Function of Charge Mobility / Equal Numbers of Plus and Minus Charges in a Solid, by Contrast, Suggests that Charge Mobility is Constrained -- Hence Conduction is Low","image":"https://lh3.googleusercontent.com/-dGqSLZZDuBI/VuLrYKPZI-I/AAAAAAAAHN4/PsbALZEFaRMbpXkplZrEajXxHVTdGY_CA/quasi-neutrality-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/h1aZukKa5nG","publishDate":"2016-03-11T16:00:57.307Z","updateDate":"2017-01-03T02:45:44.300Z","text":"<b>Quasi-neutrality: Charge-neutrality is Very Different in Solids and Plasmas / To Avoid Confusion, We Use the Term Quasi-neutrality to Refer to the Plasma Case / A Plasma Can be Quasi-neutral and Yet Still Highly Conductive, Because Conductivity is a Function of Charge Mobility / Equal Numbers of Plus and Minus Charges in a Solid, by Contrast, Suggests that Charge Mobility is Constrained -- Hence Conduction is Low</b><br /><br />The image is of the Van Allen Probes, whose observations should have by now altered the textbook definition of quasi-neutrality.<br /><br />Modern plasma physics textbooks are teaching physics graduate students claims about cosmic plasmas as though they are based upon observation, when in fact they are actually ad hoc extensions of gravitational theory added in as an antidote to electrical cosmology notions.  The treatment of quasi-neutral plasmas is a case in point.<br /><br />Ad hoc modeling is a widespread practice whereby a model is extended and made more complex in order to accommodate some sort of discrepant observation.  Our notions of space physics pre-dated the unexpected discovery in the late 50&#39;s that interplanetary gases exhibit some percentage of ionization which can cause them to behave as a plasma.<br /><br />For a review of that history, see <b>The Empty Vacuum of Space Mistake</b> here:<br /><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/QofRTvS6B5b\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/QofRTvS6B5b</a><br /><br />When the opportunity arose to thoughtfully reconsider the role of electricity in space, the approach which instead dominated was to extend the existing set of ideas by seeking to theoretically contain the electrodynamic effects of charged particles in the context of space plasmas.  Graduate students are today not explained the history of these events; the typical textbook treatment of quasi-neutrality is sufficiently mathematical to focus their attention elsewhere; and since these mathematically elegant ideas dealt with phenomena which can be difficult to observe, the notions could persist for many years before evidence might accumulate against them.<br /><br />Here&#39;s one of the more conceptual (and hence, understandable) examples of how quasi-neutrality is typically taught, from &quot;Plasma Physics and Numerical Simulations&quot; at <a href=\"http://www.uio.no/studier/emner/matnat/math/MEK4470/h14/plasma.pdf\" class=\"ot-anchor\">http://www.uio.no/studier/emner/matnat/math/MEK4470/h14/plasma.pdf</a><br /><br /><i>&quot;An important property of plasma is quasineutrality. Quasineutrality means that at large scales there is the same number of negative and positive charges and the net charge is zero, </i><b><i>and that small-scale deviations from neutrality will be quickly restored by plasma.</i></b><i> The plasma particles (i.e., electrons and positive and negative ions) are mobile, and their relative motion gives rise to charge separation setting up electric fields which can set other plasma particles into motion. </i><b><i>Even though there can be a local charge imbalance resulting in, for instance, plasma waves and oscillations, at large scales the system remains neutral.</i></b><br /><br /><i>Quasineutrality is related to another basic property of plasma which is the Debye shielding. </i><b><i>The electric charge of individual plasma particle is collectively screened by other plasma particles. A positive ion will attract electrons as well as negative ions which will form a &#39;cloud&#39; around it, so that the electric field due to the positive ion is reduced as compared to the vacuum.</i></b><i> Similarly, the negative charges will be shielded by positive ions (and will repel other electrons). This collective behavior is one of the fundamental properties of plasma. The characteristic screening length is called the Debye length ...&quot;</i><br /><br />The explanation fails to convey the fact that we have seen plasmas conducting electric currents over enormous distances.  I&#39;ve covered a few obviously discordant examples here:<br /><br /><b>Debye Screening</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/SLm2ki1TTqi\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/SLm2ki1TTqi</a><br /><br /><b>What the explanation focuses on foremost is this notion that the electrodynamics can be contained into a box.</b> They are willing to concede that the size of this box (the Debye radius) can vary, but what they are using the concept of quasi-neutrality to teach is the proposition that <b>space is not a circuit</b>.<br /><br />The problem with the Debye length which this document makes no mention of -- at all -- is the issue of double layers.  Double layers plainly violate this &quot;elegant&quot; logic.  So, the real test of whether or not this thought experiment holds in reality would require careful observation of various space conditions specifically designed to identify double layers. <b>It should be apparent that the explanation is not waiting for such observations</b>; the graduate students are instructed through this education to assume that space is not a circuit, regardless of any future attempts to actually observe it.<br /><br />It is only recently that scientists have attempted to take such measurements.  The Van Allen probes were launched on August 30, 2012.  This paper is dated 2013 ...<br /><br /><a href=\"http://physics.aps.org/articles/v6/131\" class=\"ot-anchor\">http://physics.aps.org/articles/v6/131</a><br /><br /><i>&quot;A double layer is a perturbation that occurs in a &#39;collisionless&#39; plasma, traveling along the magnetic field and consisting of two layers of charge. As in a parallel-plate capacitor, the electric field between the layers gives rise to a corresponding potential drop. In space plasmas, the two charged layers grow out of plasma instabilities, and they do not recombine because they are moving, so, for instance, an electron density enhancement leads the ion density enhancement. The Van Allen probes are equipped with measuring spheres tethered on 50-meter-long cables that allow determination of the three-dimensional electric field at 512 samples per second. For occasional short periods, data can be taken much faster, at 16,384 samples per second. Such a high time resolution is needed to characterize double layers, since their transit past the spacecraft typically lasts less than a millisecond.</i><br /><br /><b><i>The remarkable result reported by Mozer et al. is that streams of 7000 double layers were observed in one minute</i></b><i>: it was like ascending a stairway of 7000 steps very quickly. Each double layer is detected as an electric field spike of a typical duration of 0.45 milliseconds and a potential step of about 30 volts. The train of double layers propagates along the magnetic field lines at 3100 kilometers per second, which is the speed of an unusual mode called electron acoustic waves that can exist in a plasma containing both hot and cold electron populations [8] ...&quot;</i><br /><br />The observation of double layers in some regions of space renders the former definition of quasi-neutrality completely meaningless, because in a double layer the collective shielding of charge actually <b>facilitates</b> the transfer of electric currents over filamentary structures which can extend beyond our theoretical box, to limitless distances.  If we define quasi-neutrality in terms of Debye shielding, but then discover that Debye shielding does not always apply to space plasmas, then <b>we are left with many situations where we have no definition of quasi-neutrality at all.</b><br /><br />So, we need to back up a little bit and re-define quasi-neutrality more broadly, and then treat the Debye case as the special case.  And we&#39;d be wise to build our mathematics upon a strong conceptual foundation which is not so overtly motivated by the needs of the scientific framework:<br /><br /><b>An Alternative Definition for Quasi-Neutrality</b><br /><br /><a href=\"http://www.thunderbolts.info/faq/quasineutral.htm\" class=\"ot-anchor\">http://www.thunderbolts.info/faq/quasineutral.htm</a><br /><br /><b><i>&quot;We&#39;ve heard that space plasma is &#39;neutral.&#39; Doesn&#39;t that mean it can&#39;t conduct currents?</i></b><br /><br /><i>Short Answer</i><br /><br /><i>No. Plasma is </i><b><i>quasi-neutral</i></b><i>, meaning it has approximately the same number of positive charges (protons or +ions) in a given volume as it has unbound negative charges (electrons) in the same volume. Since its charges are free to move independently, </i><b><i>unlike in neutral atoms</i></b><i>, plasma is referred to as quasi-neutral </i><b><i>to reduce confusion.</i></b><br /><br /><b><i>However, quasi-neutral and nonconductive are not synonymous. Neutrality has to do with the relative proportion of positive and negative charges in a given volume. Conductivity has to do with the freedom of the charge carriers in a medium and the ease with which an electric current can flow through it.</i></b><br /><br /><i>In a plasma, </i><b><i>the charge carriers are able to move freely</i></b><i> and thus the conductivity of plasmas can be extremely high.</i><br /><br /><i>Observations</i><br /><br /><i>When it comes to a discussion of plasmas, the language of high school electrostatics can create confusion. To properly differentiate the behaviors of gases and plasmas within the cosmos, it&#39;s essential that we first clarify the language used to describe electric charge and electric currents in plasmas and other materials. </i><b><i>It&#39;s particularly important to understand that, unlike most solids and gases that we are familiar with, a neutral plasma can conduct electricity.</i></b><br /><br /><i>Since conductivity and relative charge are independent variables, there can be four scenarios where they co-exist:</i><br /><br /><i>Objects that:</i><br /><br />- <i>are </i><b><i>neutral</i></b><i> and </i><b><i>don&#39;t conduct</i></b><i> a current;</i><br />- <i>are </i><b><i>neutral</i></b><i> but </i><b><i>do conduct</i></b><i> a current;</i><br />- <i>are </i><b><i>non-neutral</i></b><i> but </i><b><i>don&#39;t conduct</i></b><i> a current;</i><br />- <i>are </i><b><i>non-neutral</i></b><i> and </i><b><i>do conduct</i></b><i> a current;</i><br /><br /><i>Within the context of high school electrostatics, one might use the example of a wooden pith ball hanging on a string from the ceiling. If we add several electrons to the pith ball (causing it to become charged), then it&#39;s no longer electrically neutral. That is to say, it has  more charged particles of one sign (negative charges) than particles of the opposite sign (positive charges). Additionally, since it is made of wood, it is an insulator and an electric current cannot flow through it. A pith ball, therefore, is non-neutral and does not conduct electricity.</i><br /><br /><i>However, if an object with a sufficient positive charge were brought into close proximity, a discharge (spark) might occur in the space between the oppositely charged objects. Such a discharge may redistribute charges between the objects, sufficient to return them to  a state close to charge neutrality (approximately equal proportions of positive and negative charges in and on each object). This is the traditional context in which non-neutrality is discussed in high school physics classes.</i><br /><br /><i>A plasma may be considered &#39;quasi-neutral&#39; if it possesses an equivalent number of (positively charged) ions and (negatively charged) electrons. Plasma is also oftentimes described as an &#39;ionized gas&#39;, which is to say that some number of its ions and/or outer valence electrons are freed from bondage (dissociated). </i><b><i>This freedom of the charge carriers makes plasma highly conductive. Therefore plasma is quasi-neutral and does conduct an electric current.</i></b><br /><br /><i>A plasma can exist in different degrees of ionization. For example, we can clearly observe the Sun&#39;s corona. At the temperatures achieved in the corona, we know that the plasma there will be 100% dissociated. In other words, there are no &#39;gases&#39; (composed of neutral atoms) within the Sun&#39;s corona.</i><br /><br /><i>Within the interplanetary medium (the plasma contained within the heliosphere) the degree of dissociation is still around 100%, but the density is very low. Nevertheless, where it exists, the plasma&#39;s temperature is very high (~100,000K) and the conductivity is still very high </i><b><i>(on the order of metals).</i></b><br /><br /><i>In the interstellar medium (the plasma pervading the space between neighboring stars), there are different regions whose degree of dissociation varies from less than 1% to nearly 70%. </i><b><i>Even &#39;gases&#39; in which the degree of ionization is as low as 0.00001%, may still be influenced so much by the tiny ionized proportion that the partially ionized gas as a whole still behaves as a plasma and may still be significantly electrically conductive.</i></b><br /><br /><i>Now, suppose that the excitation (from heat, current, etc.) is removed from the plasma and the positive and negative charges get back together (they &#39;recombine&#39; into atoms). We would then have a non-ionized cloud of gas rather than a plasma. This cloud of gas is also well described by the word neutral and it does not conduct an electric current.</i><br /><br /><i>A non-neutral plasma, on the other hand, is one that contains an unequal number of ions and electrons. </i><b><i>But, it is still a plasma because it is still completely or partially ionized. Therefore, it&#39;s non-neutral but does conduct electricity.</i></b><br /><br /><i>We should now understand, from the above examples, that </i><b><i>conductivity is related to the freedom of the charge carriers and not to the relative proportions of positive and negative charges in a given volume.</i></b><br /><br />- <i>Atmospheric gases usually act as an insulator, that is to say they are </i><b><i>neutral</i></b><i> and </i><b><i>non-conductive</i></b><i>.</i><br /><br />- <i>A pith ball is a piece of wood (also an insulator) to which excess charges can be added, making it </i><b><i>non-neutral</i></b><i> but still </i><b><i>non-conductive</i></b><i>.</i><br /><br />- <i>A non-neutral plasma is an &#39;ionized gas&#39; with an uneven number of positive and negative charges that are free to move independently, thus it is </i><b><i>non-neutral</i></b><i> and </i><b><i>does conduct</i></b><i>.</i><br /><br /><i>A typical plasma has approximately equal numbers of positive and negative charges and those charges are likewise free to move, thus it is </i><b><i>quasi-neutral</i></b><i> and </i><b><i>does conduct.</i></b><br /><br /><i>The outer layer(s) of the sun are composed largely of plasma. Interplanetary space is filled with low-density plasma. Interstellar and intergalactic space is filled with extremely low-density plasma. In fact, up to 99.999% of the visible universe is composed of matter in the plasma state.</i><br /><br /><b><i>To argue that the quasi-neutrality of space plasma precludes that plasma from conducting currents would be erroneous. In light of this revelation, we suggest that astronomers and astrophysicists [should] re-evaluate many current quandaries and prior assumptions</i></b><i> in light of over a century of low-density plasma discharge experiments by such pioneering individuals as Kristian Birkeland, Irving Langmuir and Hannes Alfvén (among others).&quot;</i><br /><br />For more information on the Van Allen probes, go to <a href=\"http://www.nasa.gov/mission_pages/rbsp/mission/index.html\" class=\"ot-anchor\">http://www.nasa.gov/mission_pages/rbsp/mission/index.html</a>.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Scientific Research as a Truth Machine","summary":"Replaces Actual Truth-Seeking With a Formulaic Quest for Technical Accuracy / It's the Idea that If You Perform Observations and Experiments According to the Approved Principles, Then This Will Lead to Facts / And If You Feed these Facts Into the Correct Analytical and Statistical Procedures, Then What Comes Out of the Machine Will Be Objective Truth / This Approach Rejects Authentic Alternative Scientific Approaches as Being Insufficiently Rigorous Due to Lower Precision in the Approved Technical Domain / Yet, the Same Scientists Can Choose to Elaborate Those Alternative Approaches Just the Same","image":"https://lh6.googleusercontent.com/-FmiWoKIcnhs/VuGdBQekPwI/AAAAAAAAHMk/BruUUbSPUuY/scientific-research-as-a-truth-machine-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/gLDN13jAXXq","publishDate":"2016-03-10T16:13:48.885Z","updateDate":"2017-01-03T02:46:01.503Z","text":"<b>Scientific Research as a Truth Machine: Replaces Actual Truth-Seeking With a Formulaic Quest for Technical Accuracy / It&#39;s the Idea that If You Perform Observations and Experiments According to the Approved Principles, Then This Will Lead to Facts / And If You Feed these Facts Into the Correct Analytical and Statistical Procedures, Then What Comes Out of the Machine Will Be Objective Truth / This Approach Rejects Authentic Alternative Scientific Approaches as Being Insufficiently Rigorous Due to Lower Precision in the Approved Technical Domain / Yet, the Same Scientists Can Choose to Elaborate Those Alternative Approaches Just the Same</b><br /><br />Press release in graphic comes from ...<br /><br /><a href=\"http://www.sciencedaily.com/releases/2008/02/080229140415.htm\" class=\"ot-anchor\">http://www.sciencedaily.com/releases/2008/02/080229140415.htm</a><br /><br /><i>Not Even Trying</i><br />Bruce G Charlton<br />pages 93 - 97:<br /><br /><i>&quot;As I survey the wondrous corruption of scientists, it seems that </i><b><i>most</i></b><i> of them fall pretty soon into the obvious dishonesty of hype and spin, selection and exaggeration.</i><br /><br /><i>But not all do so; and among those decent scientists who strive to be honest while pursuing a successful career I perceive an alternative pattern which is only indirectly, and as it were accidentally dishonest; a pseudo-solution which unintentionally makes matters worse, by camouflaging flagrant dishonesty and rejecting real science.</i><br /><br /><i>It is a strategy which is often pursued with high ideals, a clear conscience and in a spirit of modesty, although it is in operation anti-scientific in spirit and effect.</i><br /><br /><i>This strategy is to replace honesty with precision, to replace truth-seeking with a quest for technical accuracy.</i><br /><br /><i>Perhaps the root of this error is the notion that there is such a thing as &#39;scientific method&#39; (detachable from the individuals who practice science); and that </i><b><i>if</i></b><i> this scientific method is strictly adhered-to, </i><b><i>then</i></b><i> the result will be valid science.</i><br /><br /><i>In parody, this is the terribly mistaken view that </i><b><i>science is a truth-machine</i></b><i>; the idea that </i><b><i>if</i></b><i> you do science properly </i><b><i>then</i></b><i> you will manufacture &#39;truth&#39; reliably and cumulatively.</i><br /><br /><i>The idea that </i><b><i>if</i></b><i> you perform observations and experiments according to the approved principles, </i><b><i>then</i></b><i> this will lead to &#39;facts&#39;.  And </i><b><i>if</i></b><i> you feed these &#39;facts&#39; into the correct analytical and statistical procedures (&#39;scientific methodology&#39;) </i><b><i>then</i></b><i> what comes out of the machine will be objective truth.</i><br /><br /><i>The idea that although the exact output may not precisely be known in advance, the </i><b><i>process</i></b><i> by which valid out puts are generated is understood to be </i><b><i>controllable</i></b><i>, and </i><b><i>therefore</i></b><i> it can confidently be predicted that the result of this process will be valuable knowledge.</i><br /><br /><i>In sum, this is the mainstream modern view that research </i><b><i>input</i></b><i> reliably leads to real scientific </i><b><i>output</i></b><i> (albeit with varying degrees of efficiency).</i><br /><br /><i>(This reasoning justifies the usual practice of measuring science by measuring </i><b><i>inputs</i></b><i> -- that is, measuring science by measuring how many resources -- grants, personnel, capital -- are expended on supposedly-scientific </i><b><i>goals</i></b><i>.  The inputs are simply </i><b><i>assumed</i></b><i> to result in valid and relevant outputs of knowledge so long as approved procedures are strictly followed.)</i><br /><br /><i>This is indeed the basic underlying &#39;model&#39; for modern science, especially Big Science -- and it leads to the mainstream assumption that the constraint on science is resources.  The model assumes that -- if you have research managers who are deploying resources (manpower, machines etc) doing the right things -- then resources will be transformed into knowledge.</i><br /><br /><i>There may be disagreement about the </i><b><i>efficiency</i></b><i> of this process, but the assumption is very widely held that spending a lot of money on a problem </i><b><i>will</i></b><i> accumulate knowledge towards its solution -- so long as the researchers are competent and rigorous (and that competence and rigour are themselves defined as products of resources -- i.e. educational and training resources).</i><br /><br /><i>Indeed, </i><b><i>rigour</i></b><i> is a key word here -- because rigour is defined in term of exact adherence to predetermined method, technique, procedure -- and this implies that science ideally ought to be made wholly explicit, planned down to its finest detail, and done in accordance with these plans.</i><br /><br /><i>And this is, indeed, the way that research funding is managed -- &#39;scientists&#39; are compelled to submit detailed plans, which are approved or disapproved.  And research performance is monitored against these plans.</i><br /><br /><i>Science is seen as a process of </i><b><i>implementation</i></b><i>, the process is seen as something explicit and managed, and the role of the individual researcher is -- in a nutshell -- </i><b><i>obedience</i></b><i>.</i><br /><br /><i>It is obvious that this typically modern way of doing professional research -- based on the concept of research as a &#39;truth-machine&#39; -- bears zero relationship to how real science was done in the past, during the golden age of science -- when science was small scale, individualistic, cheap, efficient and led to many breakthroughs; and it is also obvious that this resource- and organization-oriented way of doing research is derived not from science but is instead the characteristic </i><b><i>modus operandi</i></b><i> of bureaucracies.</i><br /><br /><i>And it is worth asking what evidence there is or was that scientific research would be done better, valid knowledge better generated, in this bureaucratic fashion than in the effective mode of the past?</i><br /><br /><i>And the answer is equally obvious that there is </i><b><i>no such evidence</i></b><i>; but, on the contrary, conclusive evidence that scientific research is done </i><b><i>much worse or not at all</i></b><i> with this bureaucratic mode -- less efficiently and less effectively, indeed mostly done with zero or negative real world outcomes -- than when research was done as real science.</i><br /><br /><i>The deep problem with a technical focus on rigour is that </i><b><i>method is a means not an end.</i></b><br /><br /><i>A scientific problem does not dictate a specific method; indeed the solution to a problem often comes from a new, non-obvious and unanticipated method; and the solution to a problem is often best known exactly by the convergence of several methods.</i><br /><br /><i>Furthermore, methods are substantially constrained by manpower and technology (especially by the development of machines, including computers), and a focus on method becomes a race to assemble the largest teams and be the first to deploy the new and &#39;improved&#39; technology.  Linked is the assumption that old technology and methods are intrinsically </i><b><i>unable</i></b><i> to answer the questions.  Whereas old technologies and methods may well be able to answer the questions </i><b><i>if</i></b><i> creative scientific genius is added to the mix -- or even just sincere truth-seeking.</i><br /><br /><i>Yet, as often as not, modern scientific &#39;fields&#39; (research groups, appointments, journals, conferences) are defined by their technologies.  Presumably this is helpful in terms of proximate goals -- trouble-shooting methods; but destroys the possibility of real science.</i><br /><br /><i>Real science does not happen unless scientists are truth seekers and truth speakers, and truth seeking is an end not a means -- truth is not confined by methods; and truth is a whole, not a part -- an excess focus on one aspect is equivalent to the gross exaggeration of one virtue at the expense of virtuous-ness.</i><br /><br /><i>So factual technical rigour, being measurable, on the one hand obscures all other forms of dishonesty -- so that people who are </i><b><i>not even trying</i></b><i> to discover or tell the truth but instead pursuing full-on careerism can nonetheless feel themselves to be self-denying paragons of virtue due to their slavish and uncritical submission to fashionable but arbitrary technical demands; while on the other hand it also rejects real science as being insufficiently rigorous in terms of its having lower precision in the approved technical domain.</i><br /><br /><i>And, because science has been fragmented into micro-specialties, these errors are </i><b><i>ineradicable</i></b><i> -- there is no progress through self-correction, merely the fashion-driven progression of new techniques.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The Texas Sharpshooter Fallacy","summary":"The Joke of the Texas Sharpshooter is that He Fires His Gun Many Times Into a Barn Door, Then Draws a Target Over the Bullet Holes / Science Critic Bruce G Charlton Suggests the Problem is Widespread in Modern Research & Development / In this View, Science is Defined as Whatever Scientists Do, And What Scientists Do Is Defined as Generating Truth","image":"https://lh4.googleusercontent.com/-1JRke3EJan4/VuECPPRtI9I/AAAAAAAAHLE/hXeDMEzvrzw/the-texas-sharpshooter-fallacy-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/joCwyPyqKxk","publishDate":"2016-03-10T05:14:38.223Z","updateDate":"2017-01-03T02:46:17.474Z","text":"<b>The Texas Sharpshooter Fallacy: The Joke of the Texas Sharpshooter is that He Fires His Gun Many Times Into a Barn Door, Then Draws a Target Over the Bullet Holes / Science Critic Bruce G Charlton Suggests the Problem is Widespread in Modern Research &amp; Development / In this View, Science is Defined as Whatever Scientists Do, And What Scientists Do Is Defined as Generating Truth</b><br /><br /><i>Not Even Trying</i><br />Bruce G Charlton<br />pages 73-77:<br /><br /><b><i>&quot;Measuring Human Capability: Moonshot Versus &#39;Texas Sharpshooter&#39;</i></b><br /><br /><i>But is the Moonshot really a valid measure of human capability?</i><br /><br /><i>Yes.  The reason that the Moonshot is a valid measure of human capability is that the problem was difficult and was not chosen but imposed.</i><br /><br /><i>The objective of landing men on the moon (and bringing them safely back) was not chosen by scientists and engineers as being something already within their capability -- but was a problem imposed on them by politicians ...</i><br /><br /><i>Other examples of imposed problems include the Manhattan Project for devising an atomic bomb -- although in this instance the project was embarked upon precisely because senior scientists judged that the problem could possibly, maybe probably, be solved; and therefore that the US ought to solve it first before Germany did so.</i><br /><br /><i>But, either way, the problem of building an atomic bomb was also successfully solved ...</i><br /><br /><i>Since the Moonshot, there have been several difficult problems imposed by politicians on scientists that </i><b><i>have not been solved</i></b><i>: such as finding a &#39;cure for cancer&#39; (or the common cold) and &#39;understanding the brain&#39;.</i><br /><br /><i>These two problems had vastly more monetary and manpower resources (although vastly less talent and creativity) thrown at them than was the case for either the Moonshot or Manhattan Project.</i><br /><br /><i>But modern technological advances are </i><b><i>not</i></b><i> imposed problems; they are instead examples of the Texas Sharpshooter fallacy.</i><br /><br /><i>The joke of the Texas Sharpshooter is that he fires his gun many times into a barn door, then draws a target over the bullet holes, with the bulls-eye over the closest cluster of bullet holes.</i><br /><br /><i>In other words, the Texas Sharpshooter makes it look as if he had been aiming at the bulls-eye and had hit it, when in fact he drew the bulls-eye </i><b><i>only after he took the shots.</i></b><br /><br /><i>Modern science and engineering is like that.  People do research and development, and then proclaim triumphantly that whatever they have done is a breakthrough.  They have achieved whatever-happens-to-come-out-of-R&amp;D; and then they spin, hype and market whatever-happens-to-come-out-of-R&amp;D as if it were a major breakthrough.</i><br /><br /><i>In other words, modern R&amp;D triumphantly solves a </i><b><i>retrospectively designated problem</i></b><i>, the problem being generated to validate whatever-happens-to-come-out-of-R&amp;D.</i><br /><br /><i>The Human Genome Project was an example of Texas Sharpshooting masquerading as human capability.</i><br /><br /><i>Sequencing the human genome was not a matter of solving an imposed problem, nor any other kind of real world problem, but was merely droing </i><b><i>a bit faster</i></b><i> what was already happening.</i><br /><br /><i>Personally, I am no fan of Big Science, indeed I regard the success of the Manhattan Project as the beginning of the end for real science.</i><br /><br /><i>But those who are keen that humanity solve big problems and who boast about our ability to do so need to acknowledge that humanity has apparently become much worse, not better, at solving big problems over the past 40 years -- so long as we judge success only in terms of solving imposed problems which we do not already know how to solve, and so long as we ignore the trickery of the many Texas Sharpshooters among modern scientists and engineers.</i><br /><br /><b><i>The Texas Sharpshooter Society of Secular Modernity</i></b><br /><br /><i>As I said, the Texas Sharpshooter fallacy is a joke which suggests that the TS fires his gun many times into a barn door, then afterwards draws a target over the bullet holes.</i><br /><br /><i>But the sharpshooter fallacy is nowadays unavoidable and everywhere, it characterizes secular modern society throughout, because secular modern society </i><b><i>had no aim</i></b><i> but instead idealizes process and </i><b><i>retrofits aim to outcome.</i></b><br /><br /><i>Indeed, the Texas Sharpshooter strategy is the master theory of our phase of late modernity -- the persuasion of people that whatever has happened is what they wanted and what was intended.</i><br /><br /><i>Secular moderns -- in public discourse -- &#39;believe in&#39; things like freedom, or democracy, or equality, or progress -- but these are processes, not aims.</i><br /><br /><i>Aims are not prescribed in advance and progress checked-against them -- instead, </i><b><i>aims are retrospectively ascribed to whatever emerges from process.</i></b><br /><br /><i>In this respect professional science is merely a typical aspect of modern life -- real science has been assimilated into mainstream contemporary life.</i><br /><br /><i>It happens all the time: abolition of slavery emerged from the American Civil War therefore people retrospectively ascribe liberation as its purpose.  Destruction of the death camps emerged from the second world war, so the liberation of the Jews is ascribed as its purpose.</i><br /><br /><i>Libertarians &#39;believe in&#39; freedom not as a means to some end, but as a process which by definition leads to the best ends; so that they &#39;believe in&#39; whatever comes out of the process.</i><br /><br /><i>The modern attitude is that the best thing is for science to be well funded and to do what science does, and whatever comes out of the process of science is retrospectively defined as &#39;truth&#39;.</i><br /><br /><i>In practice, science is defined as </i><b><i>whatever scientists do</i></b><i>, and what scientists do is defined as generating truth.</i><br /><br /><i>Texas Sharpshooter Fallacy ...&quot;</i><br /><br />(the graphics come from a documentary on this subject which I have not yet seen, trailer here: <a href=\"https://www.youtube.com/watch?v=y_3CsKoXwfA\" class=\"ot-anchor\">https://www.youtube.com/watch?v=y_3CsKoXwfA</a>)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"Zombie Science","summary":"The Funding of Research Long After It Has Expired as a Viable Line of Investigation / It is Science That is Dead, But Will Not Lie Down / Lacks Coherence, Therefore Cannot Be Tested / Comes Across in the Sound Bite World of the Mass Media as Being More Plausible Than Simpler Rival Theories (aka Real Science)","image":"https://lh3.googleusercontent.com/-HjCdvvwfQmk/VuBNUMani2I/AAAAAAAAHKY/rM_4DApE2GQ/w3600-h5040/zombie-science-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/9zqBpnVAKwt","publishDate":"2016-03-09T16:27:57.968Z","updateDate":"2017-01-03T02:46:30.107Z","text":"<b>Zombie Science: The Funding of Research Long After It Has Expired as a Viable Line of Investigation / It is Science That is Dead, But Will Not Lie Down / Lacks Coherence, Therefore Cannot Be Tested / Comes Across in the Sound Bite World of the Mass Media as Being More Plausible Than Simpler Rival Theories (aka Real Science)</b><br /><br /><i>Not Even Trying</i><br />Bruce G Charlton<br />pages 98-102<br /><br /><i>&quot;Since modern researchers have abandoned the core ethic of truth seeking, most &#39;scientists&#39; are quite willing to pursue wrong ideas so long as they are rewarded for doing so with sufficient career incentives.</i><br /><br /><i>The primary criterion of the &#39;validity&#39; of a modern research field is therefore, in practice, the probability that working in it will be likely to benefit your career.</i><br /><br /><i>Nowadays, when a new idea is launched, it is unlikely to win converts unless early-adopters are rewarded in an upfront and obvious fashion -- typically with incentives such as research funding, the opportunity to publish in prestigious journals, and the promise of increased status exemplified by interest, admiration and respect from other researchers.</i><br /><br /><i>This is the currency of science -- the tokens used to exchange for status, jobs, salary, promotions, prizes ...</i><br /><br /><i>Therefore new research fields and theories may </i><b><i>with extraordinary rapidity</i></b><i> become popular and even dominant purely and simply because adoption is reinforced by career incentives.</i><br /><br /><i>Scientific strengths or scientific weaknesses are nowadays strictly irrelevant.</i><br /><br /><i>In terms of the classical theory of science, worthless theories (e.g. theories that are incoherent or fail to predict observations) should be demolished by sceptical (or jealous) competitor scientists, who will denounce the weaknesses of merely-fashionable theories in person, in conferences and (especially) in print -- in the scientific record, the &#39;literature&#39;.</i><br /><br /><i>However, in practice it seems that even the most conclusive &#39;hatchet jobs&#39; done on phoney theories will fail to kill, or even weaken, them -- when the phoney theories are backed-up with sufficient career incentives.  Scientists gravitate to where the money is; and the paraphernalia of specialist conferences (to present results at), journals (to publish in) and academic jobs (to work in) will follow the money as night follows day; so long as the funding stream is sufficiently strong, deep and sustained.</i><br /><br /><i>Classical scientific theory has it that a wrong hypothesis will be rejected when it fails to predict &#39;reality&#39; as determined by controlled observations and experiments.  But such a catastrophe can be deferred almost indefinitely by the elaboration of secondary hypotheses to explain why failing to fit the facts is not -- after all -- fatal to the theory; but instead merely implies the need for a more complex theory -- which then requires further testing, and extra funding, and generates more paid work for the bogus believers.</i><br /><br /><i>Furthermore, since the new version of the bogus theory, with its many auxiliary secondary hypotheses, is so complex -- this complexity makes it that much harder to test; indeed conclusive tests may become impossible, even in principle, since there are no precise predictions.  All of which has the effect of putting-off indefinitely the time when a bogus theory needs to be abandoned.</i><br /><br /><i>(Meanwhile, a much simpler rival theory -- i.e. that the first theory is phoney, and always was phoney, and this is why it so singularly fails to predict reality -- is regarded as simplistic, crass, merely a sign of lack of sophistication ...)</i><br /><br /><i>After a while, lavish funding creates powerful interest groups associated with the phoney theory -- including the reputations of numerous scientists who are now successful and powerful on the back of the phoney theory, and who by-now control the peer review process (including allocation of grants, publications and jobs) so there is a powerful disincentive against upsetting the apple cart.  Indeed, the system of peer review sustains the phoney theory on the basis that turkeys do not vote for Christmas.</i><br /><br /><b><i>What is the Function of Zombie Science?</i></b><br /><br /><i>When a branch of science based on incoherent, false or phoney theories is serving a useful </i><b><i>but non-scientific</i></b><i> purpose it may be kept going by continuous transfusions of cash from those whose non-scientific interests it serves.</i><br /><br /><i>Such an interest group moves yet is not truly alive, acts with intent yet cannot be killed; it is an un-dead creature without sentience, externally animated by funding: a Zombie science.</i><br /><br /><i>For example, if a branch of pseudo-science based on a phoney theory is nonetheless valuable for political purposes (e.g. to justify a government intervention such as a new tax or subsidy) or for marketing purposes (to provide the rationale for a sales campaign) then real science expires and a &#39;zombie science&#39; evolves.</i><br /><br /><i>Zombie science is science that is </i><b><i>dead but will not lie down.</i></b><i> It keeps twitching and lumbering around so that (from a distance, and with your eyes half-closed) zombie science looks much like real science.  But in fact the zombie has no life of its own; it is animated and moved only by the incessant pumping of funds.</i><br /><br /><i>Real science is coherent -- and testable (testing being a matter of checking coherence with the result of past and future observations).</i><br /><br /><i>Real science finds its use, and gets its validation, from common sense evaluation and being deployed in technology.</i><br /><br /><i>Real science is validated (contingently) insofar as it leads to precise predictions that later come true; and leads to new ways of solving pressing problems and making useful changes in the world.</i><br /><br /><i>But zombie science is not coherent, therefore cannot be tested; its predictions are vague or in fact retrospective summaries rather than predictions.</i><br /><br /><i>In a nutshell, zombie science is supported because it is </i><b><i>useful propaganda</i></b><i>; trading on the prestige which real science used to have and which zombie science falsely claims for itself.</i><br /><br /><i>Zombie science is deployed in areas such as political rhetoric, public administration, management, public relations, marketing and the mass media generally.  It persuades, it constructs taboos, it buttresses rhetorical attempts to shape opinion.</i><br /><br /><i>Furthermore, most zombie sciences are supported by moral imperatives -- to doubt the zombie science is therefore labelled as wicked, reckless, a tool of sinister and destructive forces.</i><br /><br /><i>To challenge zombie science is not merely to attack the livelihoods of zombie scientists (which, considering their consensus-based power, is itself dangerous) -- but opens the attacker to being labelled a luddite, demagogue, anti-science, a </i><b><i>denialist!</i></b><br /><br /><i>For all its incoherence and scientific worthlessness, zombie science therefore often comes across in the sound bite world of the mass media as being </i><b><i>more plausible than real science</i></b><i>; and it is precisely the superficial face-plausibility which in actuality is the sole and sufficient purpose of zombie science.&quot;</i><br /><br />The icon depicts the narrative of the woolly mammoth&#39;s demise by human hands, which lives on in press releases to this day despite seven recent observations by the Firestone group of meteorites embedded within mammoth tusks, as well a mammoth carcass partially draped by a black radioactive mat with exotic isotopes.  The idea that man killed the mammoths lives on because it supports the narrative that man is destroying the planet.  The alternative view -- that we are occasionally subject to the whims of the universe -- is by contrast a far more difficult claim to sell, and lacks the moral imperative.  The stakes could not be higher, for if the public was better informed of the debate, they&#39;d probably invest more heavily in the colonization of space.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"Big Science","summary":"A Term Used by Scientists and Historians of Science to Describe a Change in the Way We Do Science Towards Larger-scale Projects / Occurred in Industrial Nations During and After WWII / The Idea Rests Upon an Assumption that Spending More Money Will Automatically Generate New Technology / Bruce G Charlton Has Something To Tell You About This","image":"https://lh3.googleusercontent.com/-GExxt7AQy6w/Vt-zV0UTPUI/AAAAAAAAHIg/TuLRbA-X07M/big-science-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/aUmVYLsD2km","publishDate":"2016-03-09T05:26:18.448Z","updateDate":"2017-01-03T02:46:42.035Z","text":"<b>Big Science: A Term Used by Scientists and Historians of Science to Describe a Change in the Way We Do Science Towards Larger-scale Projects / Occurred in Industrial Nations During and After WWII / The Idea Rests Upon an Assumption that Spending More Money Will Automatically Generate New Technology / Bruce G Charlton Has Something To Tell You About This</b><br /><br />Pages 4-13:<br /><br /><i>&quot;Not Even Trying: The Corruption of Real Science</i><br /><i>Bruce G. Charlton</i><br /><i>First Published in Great Britain in 2012</i><br /><br /><i>Since the later 19th century, science has, with each generation, broken-up into smaller and smaller specializations, and become more and more career focused.</i><br /><br /><i>For a while this specialization led to greater achievement, since it allowed the devotion of more time and effort to solving more manageable problems.  Yet each new-generation specialist had been educated in a more generalist tradition -- which acted as a drag on the tendency to fragmentation and incoherence.</i><br /><br /><i>However, specialization continued past this optimal point, and into less-and-less functional fragmentation -- such that science lost unity and specialism lost the ability to serve as mutual checks.</i><br /><br /><i>Science gradually became </i><b><i>nothing but</i></b><i> isolated and irrefutable micro-specialisms.</i><br /><br /><i>Apparently, therefore, specialization was a </i><b><i>slippery slope</i></b><i> for science: such that once science had stepped-onto the slippery slope of specialization, </i><b><i>it could not stop the process,</i></b><i> even when the science had slid far beyond the point at which specialization was helpful.</i><br /><br /><b><i>From Real Science to Generic Bureaucracy</i></b><br /><br /><i>At some point over the past several decades, science stopped being real and evolved into its current state of being merely a research-based variant of generic bureaucracy.</i><br /><br /><i>This was increasingly clear to aware observers from the 1960s, and indeed to the most astute observers (such as Erwin Chargaff) from several decades earlier.  But now it is so obvious that only ignorance or dishonesty prevents it being universally acknowledged.</i><br /><br /><i>However, bureaucracies are systematically ignorant, and dishonesty is now institutional and compulsory, therefore the disappearance of real science is </i><b><i>not</i></b><i> acknowledged but instead vehemently denied, and steady, incremental progress is claimed!</i><br /><br /><i>Science presumably always was done among humans -- albeit at a very low prevalence; technological breakthroughs have tended to accumulate -- albeit with interruptions and local reversals -- throughout recorded history; but modernity happened because real scientific breakthroughs came so </i><b><i>think-and-fast</i></b><i> that increasing efficiency out-ran increasing population -- and humanity escaped what Gregory Clark has called the Malthusian Trap.</i><br /><br /><i>So far, the thesis is relatively uncontroversial.  But if modernity depends on the take-off of real science, upon what does the take-off of real science depend?</i><br /><br /><i>My answer is </i><b><i>creative genius</i></b><i>.</i><br /><br /><i>My understanding is that real science grew fast -- especially in the populations of Northern Europe -- by recruiting from an increased pool of &#39;creative geniuses&#39; who were motivated to do science.  This I regard as the essential underpinning of modernity.</i><br /><br /><i>The take-off of science therefore depended on two main things: 1. a sufficient concentration of creative genius focused on scientific problems plus 2. a </i><b><i>modest</i></b><i> degree of cognitive specialization.</i><br /><br /><i>That is to say, </i><b><i>smart and creative people working cooperatively on relatively-specific &#39;scientific problems.&#39;</i></b><br /><br /><i>And </i><b><i>that</i></b><i>, more or less, is my </i><b><i>definition of science.</i></b><i>.</i><br /><br /><i>Merely that.</i><br /><br /><i>So, real science is smart and creative people working cooperatively on scientific problems.</i><br /><br /><i>But science proved so useful that it became professionalized, and initially this seemed to accelerate progress considerably.  The first few generations of professional scientists from the later 1800s into the twentieth century were immensely productive of significant scientific breakthroughs.</i><br /><br /><i>Science seemed very obviously useful -- the presumption was that even-more science would be even-more useful ...</i><br /><br /><i>And so the growth of professional science continued, and continued ...</i><br /><br /><i>Until it out-grew the supply of creative geniuses and had to recruit from uncreative but very smart people -- but continued growing ...</i><br /><br /><i>Until it then out-grew the supply of uncreative but very smart people, then it had to recruit from uncreative, only moderately smart but hard-working people -- but continued growing ...</i><br /><br /><i>And so on and on, until &#39;science&#39; consisted of whomsoever who would do specific narrow technical and managerial jobs at the wage and conditions on offer.</i><br /><br /><i>That&#39;s where we are now ...</i><br /><br /><i>More importantly, professional science initially recruited only those who regarded </i><b><i>the pursuit of truth as an iron law</i></b><i> (and dishonesty was punished by expulsion from science).</i><br /><br /><i>Yet, due to professionalization, science increasingly attracted </i><b><i>careerists</i></b><i> rather than truth-seekers.</i><br /><br /><i>(Truth-seekers are typically resistant to bureaucratic organization; and bureaucratic organization is intrinsically hostile to truth-seekers.)</i><br /><br /><i>The professionalization of science having eliminated those who were internally-motivated to seek truth; various formal mechanisms and procedures were introduced to try and deal with purely careerist motivations.  These mostly amount to peer review mechanisms (peer review = the opinion of a group of senior colleagues).</i><br /><br /><i>So, instead of truth-seeking, a filter of </i><b><i>committee evaluations</i></b><i> was applied to ever-more-blatantly careerist individual behavior.</i><br /><br /><i>And science continued to grow -- recruiting less-and-less honest personnel </i><b><i>until</i></b><i> ...</i><br /><br /><i>... until untalented, unmotivated and dishonest career-oriented professional scientists became </i><b><i>a large majority</i></b><i> within science and included most of </i><b><i>the most successful researchers</i></b><i>; thus careerists took over the peer review evaluation procedures such as to impose their values; and &#39;science&#39; became nothing but a &#39;professional research bureaucracy&#39;.</i><br /><br /><b><i>I Wasn&#39;t Actually Doing Science</i></b><br /><br /><i>Looking back on 25 years in professional research -- I am forced to admit that, although I certainly tried, I wasn&#39;t actually doing science.</i><br /><br /><i>I began professional science in 1984 -- or, at least, that&#39;s what I thought I was doing.</i><br /><br /><i>Since then I worked in and across a variety of fields: neuro-endocrinology (brain transmitters and blood hormones) in relation to psychiatry; the anatomy and physiology of the adrenal gland (especially from 1989), epidemiology (statistics of health and disease, from about 1991); evolutionary psychology (evolutionary aspects of human behaviour including psychiatric illness and the psycho-active drugs, from 1994); systems theory (understanding complex biological organization, from about 2001); and from 2003-10 I edited an international journal of ideas publishing work from the whole of medicine -- and sometimes beyond.</i><br /><br /><i>In all of these areas and some others I found </i><b><i>serious problems</i></b><i> with the existing scientific literature: errors, inconsistencies, wrong framing of problems.</i><br /><br /><i>(I don&#39;t mean serious problems </i><b><i>in-my-opinion</i></b><i>; I mean that problems objectively, undeniably serious to any honest, informed and competent observer prepared to think for more than five consecutive minutes or two steps of logic -- whichever comes first.</i><br /><br /><i>I was not shocked -- after all, this is what science is supposedly about, most of the time -- providing the negative feedback to correct the wrong stuff.</i><br /><br /><i>After all, science is not at any time-point supposedly to be wholly-correct, rather it is conceptualized as a system of intrinsic </i><b><i>self-correction.</i></b><br /><br /><i>(Generating distinctive new lines of true and useful scientific work is what we would all </i><b><i>prefer</i></b><i> to do, in other words to be </i><b><i>original</i></b><i> -- but only a few who are both very lucky and very able are able to achieve this.)</i><br /><br /><i>My assumption was that -- as the years rolled by -- I would have the satisfaction of seeing the wrong things tested, discredited, discarded and replaced with more-correct knowledge.  Error would be eliminated; truth built-upon.  So that overall, and in the long term, science would progress.</i><br /><br /><i>This is what was supposed to happen.</i><br /><br /><i>Well, it hasn&#39;t happened.</i><br /><br /><i>It hasn&#39;t happened in </i><b><i>any</i></b><i> of the scientific fields with which I am familiar or of which I have any knowledge.  Indeed, instead, much that was true and useful has been lost while much that is utterly worthless -- dishonest, incoherent, useless -- has thriven.</i><br /><br /><i>A few decades ago one could assume that published work was honest and competent (except in specific cases); now one must assume that published work is dishonest and incompetent (except in specific cases).</i><br /><br /><i>A few decades ago one could assume that high status (&#39;successful&#39;) scientists were honest and competent (except in specific cases); now one must assume that famous and powerful scientists are dishonest and incompetent (except in specific cases).</i><br /><br /><i>Overall it seems that things have gone </i><b><i>backwards</i></b><i>, and not just slightly.</i><br /><br /><i>Yet research activity (personnel, funding, publishing, communicating) have all increased exponentially -- doubling in volume every 15 or so years (doubling </i><b><i>every decade</i></b><i> in medical research.  And China has </i><b><i>exploded</i></b><i> with research activity in the past 10 years).</i><br /><br /><i>So there has been massive expansion of inputs with first stagnation then decline of outputs.  Something has gone </i><b><i>terribly</i></b><i> wrong: not just slightly wrong, but terribly wrong.</i><br /><br /><i>So, I must conclude that although I </i><b><i>believed</i></b><i> I was participating in something called science, something that I thought I understood from the writings of Jacob Bronowski and Karl Popper and from reading the great genius scientists of the past -- it turns out that I wasn&#39;t really doing science at all.</i><br /><br /><i>I was &#39;going through the motions&#39; of doing science, true; but the machinery of science was broken, and the work I was trying to do, and the work of those whom I respected, was like a free-spinning-cog -- disconnected from mainstream activity.</i><br /><br /><i>If real science is that done from truth seeking motives and communicated truthfully, then this kind of science had zero impact on the mainstream.</i><br /><br /><i>Get this -- real science had become detached from professional research, technology and policy; and (most important) detached from practice; detached from career success, status, funding, publication, prizes and awards ...</i><br /><br /><i>Real science had become a thing done for subjective personal satisfaction, </i><b><i>merely</i></b><i> a lifestyle choice -- nobody else was interested.</i><br /><br /><i>Maybe real science was being done, maybe it was published, maybe it was cited, maybe it was funded, maybe people made careers from doing it?</i><br /><br /><i>But in the end, real science did not make any difference: real science had become just a </i><b><i>private hobby.</i></b><br /><br /><i>Those few who were lucky enough to find a niche that supported real science did so by accident; not by necessity; and the niches were shrinking all the time.</i><br /><br /><i>And we who thought we were participating in the group activity of real science were deluded -- pleasantly deluded, perhaps; but deluded.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The Empty Vacuum of Space Mistake","summary":"\"Space\" Was Invented on Earth Before We Knew What Was Out There / Possibly the Most Serious Mistake Ever Made in the Sciences / The Public Still Does Not Understand It Happened","image":"https://lh3.googleusercontent.com/-euOuooXNFxE/VtyztESggiI/AAAAAAAAHH8/7sqNsokXD0M/w7000-h9800/the-empty-vacuum-of-space-mistake-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/QofRTvS6B5b","publishDate":"2016-03-06T22:49:17.807Z","updateDate":"2017-01-03T02:46:57.387Z","text":"<b>The Empty Vacuum of Space Mistake: &quot;Space&quot; Was Invented on Earth Before We Knew What Was Out There / Possibly the Most Serious Mistake Ever Made in the Sciences / The Public Still Does Not Understand It Happened</b><br /><br />(this is a transcript of the graphic; if you can see the graphic well enough to read it, it&#39;s probably the more interesting way to follow along)<br /><br /><b>1963 Popular Science Article: &quot;Space&quot; Was Invented on Earth Before We Knew What Was Out There</b><br /><br />A glaring, largely undiscussed problem is that most of our popular theories in the space sciences today based upon a false assumption that space is simply an empty vacuum.<br /><br />Let&#39;s look at four examples:<br /><br /><b>(1) Albert Einstein</b><br /><br /><i>&quot;4. Cosmological Problems</i><br /><br /><i>Even before his new theory had been completely understood by the great majority of physicists, it was already evident to Einstein that it was unable to give a correct presentation of the universe as a whole.</i><br /><br /><b><i>During the nineteenth century the commonest conception of the universe was that there are groups of material bodies like our Milky Way, and outside this region is &#39;empty&#39; space, which extends infinitely far.</i></b><i> This view had, however, already aroused doubts among some scientists around the end of the century.  For in this case the stars would behave like a cloud of vapor and there was nothing to prevent them from dispersing into the surrounding empty space.&quot;</i> (<i>Einstein: His Life and Times</i>, Philipp Frank)<br /><br /><b>(2) Arthur Eddington</b><br /><br /><b><i>&quot;Since we are limited to energy liberated in the deep interior of the star, extraneous sources of supply are ruled out, and it is scarcely possible to escape the conclusion that the supply of energy for future expenditure is already hidden in the star.</i></b><i> Energy, however, cannot be successfully hidden; it betrays itself by its manifestation as mass.  Energy and mass are equivalent, and we know the masses of the stars.&quot;</i> (<i>The Internal Constitution of the Stars</i>, Arthur Eddington at <a href=\"https://archive.org/stream/TheInternalConstitutionOfTheStars/Eddington-TheInternalConstitutionOfTheStars#page/n299/mode/2up/search/%22since+we+are+limited+to+energy%22\" class=\"ot-anchor\">https://archive.org/stream/TheInternalConstitutionOfTheStars/Eddington-TheInternalConstitutionOfTheStars#page/n299/mode/2up/search/%22since+we+are+limited+to+energy%22</a>)<br /><br />Wal Thornhill provides some additional context at <a href=\"http://www.holoscience.com/wp/science-politics-and-global-warming/\" class=\"ot-anchor\">http://www.holoscience.com/wp/science-politics-and-global-warming/</a><br /><br /><b><i>&quot;When Eddington put together his solar model in the 1920s the Sun was thought to be isolated in the vacuum of space.  There could be no external source of energy causing it to shine.</i></b><i> Therefore, it was assumed, the Sun must provide its own fuel to shine for bilions of years.  Decades earlier, Kristian Birkeland determined that charged particles from the Sun must cause the auroras.  So the Sun has an electrical environment. </i><b><i>But Birkeland&#39;s discovery was not considered. It had no explanation at the time.</i></b><br /><br /><b>(3) Hannes Alfven</b><br /><br /><i>&quot;In 1937, his proposal of a galactic magnetic field met with widespread resistance (if not scorn), </i><b><i>as it directly contradicted the prevailing wisdom that a vacuum filled interstellar space&quot;</i></b> (<a href=\"https://books.google.com/books?id=1YDrMFug17cC&amp;pg=PA8&amp;lpg=PA8&amp;dq=sydney+chapman+kristian+birkeland+vacuum+space&amp;source=bl&amp;ots=-sMxPqsbM_&amp;sig=H_yU7XOqkfStr_0nKK_6TbA5hco&amp;hl=en&amp;sa=X&amp;ved=0CE8Q6AEwBmoVChMIgan354XBxwIVCiqICh1ZqwDm#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=1YDrMFug17cC&amp;pg=PA8&amp;lpg=PA8&amp;dq=sydney+chapman+kristian+birkeland+vacuum+space&amp;source=bl&amp;ots=-sMxPqsbM_&amp;sig=H_yU7XOqkfStr_0nKK_6TbA5hco&amp;hl=en&amp;sa=X&amp;ved=0CE8Q6AEwBmoVChMIgan354XBxwIVCiqICh1ZqwDm#v=onepage&amp;q&amp;f=false</a>)<br /><br />What is interesting about this is that the subject remains troubling to the experts to this day ...<br /><br /><i>&quot;The mention of cosmic-scale magnetic fields is still likely to [be] met with an uncomfortable silence in some astronomical circles -- and after a bit of foot-shuffling and throat-clearing, the discussion will be moved on to safer topics.* But look, they&#39;re out there.  They probably do play a role in galaxy evolution, if not galaxy formation -- and are certainly a feature of the interstellar medium and the intergalactic medium.&quot;</i> (<a href=\"http://www.universetoday.com/85087/astronomy-without-a-telescope-cosmic-magnetic-fields/\" class=\"ot-anchor\">http://www.universetoday.com/85087/astronomy-without-a-telescope-cosmic-magnetic-fields/</a>)<br /><br /><b>(4) The Positivists</b><br /><br />(for an explanation of positivism, see this card here: <a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/H1akZRDWs5Y\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/H1akZRDWs5Y</a>)<br /><br /><i>&quot;The matter-motion term, space-time, came along just in time to set the stage for the similarly indeterministic interpretation that Hubble&#39;s redshifts were evidence for universal expansion.  Be aware that Hubble, himself, never believed that the redshifts of faraway galaxies meant the universe was expanding.  He always thought it was a measure of distance rather than recessional velocity -- more of a &#39;tired light&#39; effect. </i><b><i>For positivists who actually believed in perfectly empty space,</i></b><i> there was no reason to entertain such a heresy. </i><b><i>Why would light lose energy traveling through perfectly empty space?&quot;</i></b> (Glenn Borchardt at <a href=\"http://www.researchgate.net/publication/221706055_Einstein%27s_most_important_philosophical_error\" class=\"ot-anchor\">http://www.researchgate.net/publication/221706055_Einstein&#39;s_most_important_philosophical_error</a>)<br /><br /><b>Then We Sent Rockets to Space Around 1958, Conclusively Demonstrating the Mistake for the First Time</b><br /><br />The following quotes all come from a 1963 Popular Science article dedicated to James Van Allen titled &quot;James Van Allen Tells What Space is Really Like&quot;, here:<br /><br /><a href=\"https://books.google.com/books?id=wiADAAAAMBAJ&amp;pg=PA73&amp;lpg=PA73&amp;dq=van+allen+empty+space&amp;source=bl&amp;ots=-3Y7jZrrqx&amp;sig=PjtIX3XnKqJ7ZRkZU8cnCq2id7U&amp;hl=en&amp;sa=X&amp;ved=0CDwQ6AEwBGoVChMIheuM2ZDBxwIVhJmICh1rrwlT#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=wiADAAAAMBAJ&amp;pg=PA73&amp;lpg=PA73&amp;dq=van+allen+empty+space&amp;source=bl&amp;ots=-3Y7jZrrqx&amp;sig=PjtIX3XnKqJ7ZRkZU8cnCq2id7U&amp;hl=en&amp;sa=X&amp;ved=0CDwQ6AEwBGoVChMIheuM2ZDBxwIVhJmICh1rrwlT#v=onepage&amp;q&amp;f=false</a><br /><br /><i>&quot;I found Dr. Van Allen in Boston, at Massachusetts Institute of Technology, where he was conferring with other space scientists.  That evening, over dinner, I asked him about newly discovered phenomena of &#39;empty&#39; space. </i><b><i>&#39;Most people still think of space as a cold, black vacuum,&#39; I said. &#39;Is it true that scientists shared this misconception until very recently?&#39;</i></b><br /><br /><b><i>&#39;Most scientists did think of space as a barren waste,&#39; he said. &#39;When we started getting real information, it was quite a revelation.&#39;&quot;</i></b><br /><br />Perhaps the ultimate understatement.  Further in the interview ...<br /><br /><i>&quot;&#39;Can the conditions of space be compared to anything we&#39;re more familiar with on earth?&#39;</i><br /><br /><b><i>&#39;Yes, I think so,&#39; said Dr. Van Allen. &#39;You see, in inventing electron tubes and other devices involving vacuums and plasmas, in exploring ionization and various magnetic effects in the laboratory, we&#39;ve accidentally stumbled on many of the conditions that exist naturally in space.&#39;</i></b><br /><br /><i>&#39;Then what would be the best model of space conditions?&#39;</i><br /><br /><b><i>&#39;To me, the closest thing would be the beam of accelerated particles in a cyclotron. However, the material is not as thinly distributed as in space.</i></b><br /><br /><i>[...]</i><br /><br /><i>&#39;Is the material of the solar wind and the radiation belts the same thing as plasma?&#39;</i><br /><br /><i>&#39;Yes, it is plasma, the fourth state of matter -- gas in which the atoms have been broken into charged particles.&#39;</i><br /><br /><i>&#39;Now that we know that space is not empty, is it fair to say that plasma is what space is full of?&#39;</i><br /><br /><b><i>&#39;It&#39;s not full in the sense that it couldn&#39;t hold some more,&#39; said Dr. Van Allen. &#39;But plasma is what largely occupies solar space.</i></b><i> Of course there are other things out there -- dust, gas that&#39;s not ionized (in interstellar space), cosmic rays and other specialized particles, electromagnetic radiation such as radio waves and light waves, electric currents, and other energy manifestations, like gravitational and magnetic fields.&quot;</i><br /><br />Some background on Van Allen from wikipedia at <a href=\"https://en.wikipedia.org/wiki/James_Van_Allen\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/James_Van_Allen</a>:<br /><br /><i>&quot;Van Allen played an important role in planning the 1957-58 International Geophysical Year (IGY) and carried out shipboard expeditions to Greenland and southward to the Ross Sea off the coast of Antarctica in 1957. IGY culminated in the Jan. 31, 1958 launch of Explorer 1 and its scientific payload. Van Allen&#39;s instruments included a Geiger–Müller tube, which provided data and information that regions of intense radiation surround the Earth. The discovery marked the birth of the research field of magnetospheric physics, an enterprise that grew to involve more than 1,000 investigators in more than 20 countries.&quot;</i><br /><br /><b>The Most Important Discovery of the &quot;Space Age&quot; Is Still Working Its Way Through Our Society</b><br /><br />It&#39;s an amazing observation, but the discoveries that Van Allen discusses in his 1963 interview are still, to this day, not widely understood (in 2016!).<br /><br /><b>What is remarkable is that even though our conception of space fundamentally changed from empty vacuum to plasma, our textbook theories were never significantly affected.</b><br /><br />Does this sound right to you?  To emphasize the point, let&#39;s review the various textbooks on astrophysics and plasma physics which note plasma&#39;s pervasiveness.  Each adds a little bit more detail to our understanding of what this plasma state for matter actually means ...<br /><br /><b><i>&quot;Plasmas play a fundamental role in nature.</i></b><i> Probably more than 99 percent of visible matter in the universe exist in the plasma state. Plasmas exist, e.g. as interstellar gas, in stellar atmospheres, inside the sun, in giant planets, and in white dwarfs.&quot;</i><br /><br /><b>Quantum Statistics of Nonideal Plasmas</b><br /><a href=\"https://books.google.com/books?vid=ISBN3540652841&amp;id=wLN69UmwyJsC&amp;pg=PP1&amp;lpg=PP1&amp;dq=plasma+99%25+universe&amp;sig=DObg7XXvjV_CYCYIAGDnJd_PvDE#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?vid=ISBN3540652841&amp;id=wLN69UmwyJsC&amp;pg=PP1&amp;lpg=PP1&amp;dq=plasma+99%25+universe&amp;sig=DObg7XXvjV_CYCYIAGDnJd_PvDE#v=onepage&amp;q&amp;f=false</a><br /><br /><i>&quot;How was it determined that 99% of the Universe is in a plasma state?</i><br /><br /><b><i>Most of the gas in interstellar space is ionized (astronomers can tell by the wavelengths of light the gas absorbs and emits), and all of the gas in stars [is] ionized, that&#39;s where the 99% comes from.</i></b><i> The 99% ignores any dark matter which might be out there.&quot;</i><br /><br /><b>NASA&#39;s Cosmicopia - Ask Us: Cosmic Rays, Energetic Particles, and Plasma</b><br /><a href=\"http://helios.gsfc.nasa.gov/qa_cr.html#plasma\" class=\"ot-anchor\">http://helios.gsfc.nasa.gov/qa_cr.html#plasma</a><br /><br /><b><i>&quot;Today it is recognized that 99.999% of all observable matter in the universe is in the plasma state</i></b><i> and plasmas are found at temperatures and densities far exceeding those that will support matter in the first three states&quot;</i><br /><br /><b>Plasma Physics</b><br /><a href=\"https://books.google.com/books?vid=ISBN079235527X&amp;id=ZSlJRAeL95sC&amp;pg=PA57&amp;lpg=PA57&amp;ots=jnEIJGfx6t#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?vid=ISBN079235527X&amp;id=ZSlJRAeL95sC&amp;pg=PA57&amp;lpg=PA57&amp;ots=jnEIJGfx6t#v=onepage&amp;q&amp;f=false</a><br /><br /><i>&quot;Because a plasma (partly) consists of free charges, it is a conductor.  Moving electric charges are currents.  These currents induce magnetic fields which in turn influence the motion of the very particles forming the field-generating currents.  Thus the particle motion in a plasma is not only controlled by external electric and magnetic fields, but also creates fields which add to the external ones and modify the motion of the particles: a plasma can interact with itself. </i><b><i>Consequently, dynamics in a plasma are more complex than in a neutral gas.</i></b><i> This is most obvious in the large number of different types of plasma waves ...</i><br /><br /><b><i>In apparently simple situations, a plasma can behave counter-intuitively.</i></b><i> Pouring milk into our coffee, we expect the milk to heat up and mix with the coffee.  A sunspot is a sharply bordered volume of cool gas embedded in the hot solar photosphere; but it stays stable for several months prevented by strong magnetic fields from warming or mixing with its environment.  A cold and dense volume of gas or liquid in a hot environment sinks.  A solar filament is cold and dense compared with the ambient corona but it is held in position against gravity by strong magnetic fields. </i><b><i>Such discrepencies between our daily experience and the behavior of ionized gases clearly show that plasmas do not form a significant part of our environment.</i></b><i> Why then do we study such exotic phenomena?  Are there applications for plasmas?</i><br /><br /><i>First, plasmas are not exotic but quite common.  The interplanetary and interstellar medium and the stars are made of ionized gases.  Thus about 99% of matter in the universe is plasma.  Nearest regions dominated by plasmas are the magnetosphere with its radiation belts, the ionosphere, lightning bolts in the atmosphere, and, in a wider sense, the Earth&#39;s core; thus even in the system Earth plasmas are not uncommon.  Plasma physics, therefore, contributes to the understanding of our environment.  In turn, the natural plasma laboratories, i.e. the ionosphere, the magnetosphere, and interplanetary space, help to test the concepts of plasma physics on spatial scales and at densities unattainable in a laboratory.</i><br /><br /><b><i>Even some everyday materials can be described as plasmas because they show similarities to the free-electron plasma described above:</i></b><i> the conduction electrons in metals and electron hole pairs in semiconductors are charges which can move quasi-freely and lead to a behavior of the matter which can be described in the same way as for a plasma.  The free-electron gas in metals is therefore also included as example of a plasma ...&quot;</i><br /><br /><b>Space Physics</b><br /><a href=\"https://books.google.com/books?vid=ISBN3540206175&amp;id=HnyPP4B4n-4C&amp;pg=PA4&amp;lpg=PA4&amp;ots=JYLozZJ9Sq&amp;dq=plasma+99+universe+date:2003-2007&amp;num=50&amp;sig=CsyG2Lat1MiND3usk1bMYyItd9Q#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?vid=ISBN3540206175&amp;id=HnyPP4B4n-4C&amp;pg=PA4&amp;lpg=PA4&amp;ots=JYLozZJ9Sq&amp;dq=plasma+99+universe+date:2003-2007&amp;num=50&amp;sig=CsyG2Lat1MiND3usk1bMYyItd9Q#v=onepage&amp;q&amp;f=false</a><br /><br /><i>&quot;Plasmas can be divided into two broad categories: natural and man-made.  It is an interesting fact that most of the material in the visible universe, as much as 99% according to some estimates, is in the plasma state.  This includes the Sun, most stars, and a significant fraction of the interstellar medium. </i><b><i>Thus, plasmas play a major role in the universe.</i></b><i> Plasma physics is relevant to the formation of planetary radiation belts, the development of sunspots and solar flares, the acceleration of high velocity winds that flow outward from the Sun and other stars, the generation of radio emissions from the Sun and other astrophysical objects, and the acceleration of cosmic rays.&quot;</i><br /><br /><b>Introduction to Plasma Physics</b><br /><a href=\"https://books.google.com/books?vid=ISBN0521364833&amp;id=VcueZlunrbcC&amp;pg=PA2&amp;lpg=PA2&amp;ots=SwBeJMiXx9&amp;dq=plasma+99%25+universe&amp;sig=YdQ9lsMb-JB1BK19STcBe_vumXw#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?vid=ISBN0521364833&amp;id=VcueZlunrbcC&amp;pg=PA2&amp;lpg=PA2&amp;ots=SwBeJMiXx9&amp;dq=plasma+99%25+universe&amp;sig=YdQ9lsMb-JB1BK19STcBe_vumXw#v=onepage&amp;q&amp;f=false</a><br /><br /><i>&quot;If one wants to produce a good plasma in a laboratory, one usually needs to construct a fairly large apparatus. </i><b><i>If one goes outside the earth, however, the plasma state is the most abundant state of matter.</i></b><i> It is thought that more than 99.9% of matter in the universe is in plasma.  Indeed, plasmic matter begins at about 50km above the earth&#39;s surface in the ionosphere.  There are various types of plasmas in the universe, ranging from very high density (n ~ 10^36/m^3) inside a white dwarf to very low density (n ~ 10^6/m^3) in interstellar space.&quot;</i><br /><br /><b>Plasma Physics</b><br /><a href=\"https://books.google.com/books?vid=ISBN354065285X&amp;id=4cHkd77TSHcC&amp;pg=PA11&amp;lpg=PA11&amp;ots=xdaxcYVbbx&amp;dq=plasma+99.9%25+universe&amp;sig=m9CJtNeOS5UD_v8_C6OC-gxJ6Pk#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?vid=ISBN354065285X&amp;id=4cHkd77TSHcC&amp;pg=PA11&amp;lpg=PA11&amp;ots=xdaxcYVbbx&amp;dq=plasma+99.9%25+universe&amp;sig=m9CJtNeOS5UD_v8_C6OC-gxJ6Pk#v=onepage&amp;q&amp;f=false</a><br /><br /><b><i>&quot;Plasmas in nature should not be forgotten.</i></b><i> It is sometimes said that more than 99 percent of the material in the universe is in the form of a plasma.  Just two examples are given Fig. 1.2.  The Earth is surrounded by several layers of ionosphere.  It starts at about 100 km above the Earth&#39;s sruface and extends up to some 500 km.  Another example is the solar corona surrounding the Sun&quot;</i><br /><br /><b>Plasma Spectroscopy</b><br /><a href=\"https://books.google.com/books?id=w1CJHagviVsC&amp;dq=plasma%2099%20universe%20date%3A2003-2007&amp;source=gbs_book_other_versions\" class=\"ot-anchor\">https://books.google.com/books?id=w1CJHagviVsC&amp;dq=plasma%2099%20universe%20date%3A2003-2007&amp;source=gbs_book_other_versions</a><br /><br /><b><i>&quot;Plasma: A state of matter characterized by unbound negative electrons and positive ions that may conduct electrical current.</i></b><i> Plasma is often called the fourth state of matter, along with the other three: solids, liquids, and gases.  It is estimated that more than 99 percent of matter in the universe exists as plasma; examples include stars, nebulae, and interstellar particles.&quot;</i><br /><br /><b>Burning Plasma</b><br /><a href=\"http://www.nap.edu/openbook.php?record_id=10816&amp;page=188\" class=\"ot-anchor\">http://www.nap.edu/openbook.php?record_id=10816&amp;page=188</a><br /><br /><b><i>&quot;While only a few natural plasmas, such as flames or lightning strokes, can be found near the Earth&#39;s surface, plasmas are abundant in the universe.</i></b><i> More than 99% of all known matter is in the plasma state.&quot;</i><br /><br /><b>Basic Space Plasma Physics</b><br /><a href=\"https://books.google.com/books?vid=ISBN186094079X&amp;id=e4yupcOzJxkC&amp;pg=PP1&amp;lpg=PP1&amp;ots=6xtjT18iOQ&amp;dq=plasma+99%25+universe&amp;sig=i96aK5VoNeZsM9B3rN7igy2umyE#v=onepage&amp;q=plasma%2099%25%20universe&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?vid=ISBN186094079X&amp;id=e4yupcOzJxkC&amp;pg=PP1&amp;lpg=PP1&amp;ots=6xtjT18iOQ&amp;dq=plasma+99%25+universe&amp;sig=i96aK5VoNeZsM9B3rN7igy2umyE#v=onepage&amp;q=plasma%2099%25%20universe&amp;f=false</a><br /><br /><i>&quot;It is estimated that as much as 99.9% of the universe is comprised of plasma.&quot;</i><br /><br /><b>Analytical and Numerical Methods for Wave Propagation in Fluid Media</b><br /><a href=\"https://books.google.com/books?vid=ISBN9812381554&amp;id=v99GV71BNccC&amp;pg=PA14&amp;lpg=PA14&amp;ots=nyCdxEoDvA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?vid=ISBN9812381554&amp;id=v99GV71BNccC&amp;pg=PA14&amp;lpg=PA14&amp;ots=nyCdxEoDvA#v=onepage&amp;q&amp;f=false</a><br /><br /><b><i>&quot;Plasmas, often called the fourth state of matter, are the most common form of matter in the universe.</i></b><i> More than 99% of all matter across the universe is found in the plasma state, that is, a state in which at least one atomic electron has enough energy to escape the coulomb attraction of the atomic nucleus resulting in the independent motions of the free electrons and the atomic ions.&quot;</i><br /><br /><b>Spacecraft Power Technologies</b><br /><a href=\"http://www.amazon.com/Spacecraft-power-technologies-Space-Technology/dp/1860941176/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1436332400&amp;sr=1-1&amp;keywords=9781860941177\" class=\"ot-anchor\">http://www.amazon.com/Spacecraft-power-technologies-Space-Technology/dp/1860941176/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1436332400&amp;sr=1-1&amp;keywords=9781860941177</a><br /><br /><i>&quot;Plasma is not a gas, liquid, or solid -- it is the fourth state of matter.  Plasma often behaves like a gas, except that it conducts electricity and is affected by magnetic fields. </i><b><i>On an astronomical scale, plasma is common.</i></b><i> The Sun is composed of plasma, fire is plasma, fluorescent and neon lights contain plasma.</i><br /><br /><i>&#39;99.99 percent of the Universe is made up of plasma,&#39; says Dr. Dennis Gallagher, a plasma physicist at NASA&#39;s Marshall Space Flight Center. </i><b><i>&#39;Very little material in space is made of rock like the Earth.&#39;&quot;</i></b><br /><br /><b>NASA Science News - Plasma, Plasma, Everywhere - A New Model of the Plasmasphere Surrounding Our World</b><br /><a href=\"http://science.nasa.gov/science-news/science-at-nasa/1999/ast07sep99_1/\" class=\"ot-anchor\">http://science.nasa.gov/science-news/science-at-nasa/1999/ast07sep99_1/</a><br /><br /><i>&quot;It has often been said that 99% of the matter in the universe is in the plasma state; that is, in the form of </i><b><i>an electrified gas</i></b><i> with the atoms dissociated into positive ions and negative electrons. </i><b><i>This estimate may not be very accurate, but it is certainly a reasonable one in view of the fact that stellar interiors and atmospheres, gaseous nebulae, and much of the interstellar hydrogen are plasmas.</i></b><i> In our neighborhood, as soon as one leaves the earth&#39;s atmosphere, one encounters the plasma comprising the Van Allen radiation belts and the solar wind. On the other hand, in our everyday lives encounters with plasmas are limited to a few examples: the flash of a lightning bolt, the soft glow of the Aurora Borealis, the conducting gas inside a fluorescent tube or neon sign, and the slight amount of ionization in a rocket exhaust. </i><b><i>It would seem that we live in the 1% of the universe in which plasmas do not occur naturally.&quot;</i></b><br /><br /><b>Introduction to Plasma Physics</b><br /><a href=\"https://books.google.co.uk/books?id=u8nvAAAAMAAJ&amp;q=99%25+plasma+universe&amp;dq=99%25+plasma+universe&amp;hl=en&amp;sa=X&amp;ei=mfirUIu2DLG00QWy9YH4AQ\" class=\"ot-anchor\">https://books.google.co.uk/books?id=u8nvAAAAMAAJ&amp;q=99%25+plasma+universe&amp;dq=99%25+plasma+universe&amp;hl=en&amp;sa=X&amp;ei=mfirUIu2DLG00QWy9YH4AQ</a><br /><br /><b>The Story the Universities Refuse to Teach: What Happened Next Remains a Huge Problem</b><br /><br />Yet, rather than propose new ideas for astronomical observations which seek to incorporate this new conception of space as a plasma, theorists instead modeled the cosmic plasma very differently than our observations of laboratory plasmas.<br /><br />From &quot;The Plasma Universe of Hannes Alfven&quot; by David Talbott in <i>EdgeScience</i>, Number 9, October-December 2011 (you can get the full article here: <a href=\"http://coincider.com/wp-content/uploads/2014/10/edgescience-09.pdf\" class=\"ot-anchor\">http://coincider.com/wp-content/uploads/2014/10/edgescience-09.pdf</a>):<br /><br /><i>&quot;Through much of the 19th and 20th century, most astronomers and cosmologists had assumed the &#39;vacuum&#39; of space would not permit electric currents. </i><b><i>Later, when it was discovered that all of space is a sea of electrically conductive plasma, the theorists reversed their position, asserting that any charge separation would be immediately neutralized.</i></b><i> Here they found what they were looking for in Alfvén’s frozen-in magnetic fields and in his magnetohydrodynamic equations. </i><b><i>Electric currents could then be viewed as strictly localized and temporary phenomena—needed just long enough to create a magnetic field, to magnetize plasma, a virtually &#39;perfect&#39; conductor.</i></b><br /><br /><i>The underlying idea was that space could have been magnetized in primordial times or in early stages of stellar and galactic evolution, all under the control of higher-order kinetics and gravitational dynamics. All large scale events in space could still be explained in terms of disconnected islands, and it would only be necessary to look inside the &#39;islands&#39; to discover localized electromagnetic events—no larger electric currents or circuitry required. In this view, popularly held today, we live in a &#39;magnetic universe&#39; (the title of several recent books and articles), but not an electric universe. The point was stated bluntly by the eminent solar physicist </i><b><i>Eugene Parker, &#39;... No significant electric field can arise in the frame of reference of the moving plasma.&#39;</i></b><br /><br /><i>But the critical turn in this story, </i><b><i>the part almost never told within the community of astronomers and astrophysicists,</i></b><i> is that Alfvén came to realize he had been mistaken. Ironically -- and to his credit -- </i><b><i>Alfvén used the occasion of his acceptance speech for the Nobel Prize to plead with scientists to ignore his earlier work.</i></b><i> Magnetic fields, he said, are only part of the story. </i><b><i>The electric currents that create magnetic fields must not be overlooked, and attempts to model space plasma in the absence of electric currents will set astronomy and astrophysics on a course toward crisis</i></b><i>, he said.</i><br /><br /><i>In accord with Alvén’s observations, American physicist, professor Alex Dessler, former editor of the journal Geophysical Research Letters, notes that he himself had originally fallen in with an academic crowd that believed electric fields could not exist in the highly conducting plasma of space. &#39;My degree of shock and surprise in finding Alfvén right and his critics wrong can hardly be described.&#39;</i><br /><br /><i>In retrospect, it seems clear that </i><b><i>Alfvén considered his early theoretical assumption of frozen-in magnetic fields to be his greatest mistake</i></b><i>, a mistake perpetuated first and foremost by mathematicians attracted to Alfvén’s magnetohydrodynamic equations. Alfvén came to recognize that real plasma behavior is too &#39;complicated and awkward&#39; for the tastes of mathematicians. It is a subject &#39;not at all suited for mathematically elegant theories.&#39; It requires hands-on attention to plasma dynamics in the laboratory. Sadly, he said, the plasma universe became &#39;the playground of theoreticians who have never seen a plasma in a laboratory. </i><b><i>Many of them still believe in formulae which we know from laboratory experiments to be wrong.&#39;</i></b><br /><br /><i>Again and again Alfvén reiterated the point: the underlying assumptions of cosmologists today &#39;are developed with the most sophisticated mathematical methods and </i><b><i>it is only the plasma itself which does not ‘understand’ how beautiful the theories are and absolutely refuses to obey them.&#39;&quot;</i></b><br /><br />These points are supported with additional excerpts in the graphic.<br /><br />Not all important things in science occurred in the past.  The realization that we live in a plasma universe is an unfolding historical situation which everybody -- today -- can participate in ...<br /><br />... Because one thing, by now, is very clear: <b>The problem is not going to resolve itself.</b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"The Skywave","summary":"It's the Bouncing of AM Radio Signals Off of the Ionosphere, Permitting them to Travel Far Beyond the Horizon / Can Possibly Be Explained Via Plasma Frequency and Double Layers / The Region Where This Occurs is Very Difficult to Closely Observe / Surprisingly, Nobody Seems to Bring Up Skywave When Talking About the Fermi Paradox","image":"https://lh3.googleusercontent.com/-ffn2Dgv4mlY/VtvKgcVqagI/AAAAAAAAHGc/K6H_FPyc0gE/w3600-h5040/the-skywave-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/hshwAxCywDy","publishDate":"2016-03-06T06:16:56.888Z","updateDate":"2017-01-03T02:47:30.124Z","text":"<b>The Skywave: It&#39;s the Bouncing of AM Radio Signals Off of the Ionosphere, Permitting them to Travel Far Beyond the Horizon / Can Possibly Be Explained Via Plasma Frequency and Double Layers / The Region Where This Occurs is Very Difficult to Closely Observe / Surprisingly, Nobody Seems to Bring Up Skywave When Talking About the Fermi Paradox</b><br /><br />What if some of the biggest paradoxes and riddles of science have actually already been answered?  Is it possible that the answer would be denied by peer reviewers, crowded out by the noise of the Internet <b>and automatically rejected by a public who has largely stopped asking questions about their surroundings?</b> What if most modern attention spans are simply too short to learn the set of concepts required to understand the answer?  What if the man who arrived at the answer is still struggling to figure out how to get people to think about the idea for themselves?<br /><br />The answer to the Fermi Paradox -- this mystery about why we do not see alien chatter over cosmic radio waves -- might one day become classified as <b>E. All of the Above</b>.  At least, that is what I&#39;m going to make a case for over the coming controversy cards.<br /><br />But, to get there, you&#39;re going to have to bear with me, as I try to teach you some crucial concepts which have historically proven difficult for people to pick up.<br /><br />Now, let&#39;s start with something which <b>should</b> be an obvious starting place on this topic of the Fermi Paradox, yet does not seem to ever pop up when the experts are consulted ...<br /><br /><b>Why Do AM Radio Signals Bounce Off of the Ionosphere?</b><br /><br />Don&#39;t you think that such a question would bear at least a <b>little</b> bit of relevance to the riddle of the Fermi Paradox?  The 300 page review of possible explanations for the Fermi Paradox by Stephen Webb, <i>If the Universe is Teeming with Aliens ... Where is Everybody?</i>, <b>makes no mention at all of the ionosphere.</b><br /><br />Let&#39;s observe a random assortment of people online attempting to answer the question ...<br /><br /><a href=\"https://ca.answers.yahoo.com/question/index?qid=20070829171112AAZuRmx\" class=\"ot-anchor\">https://ca.answers.yahoo.com/question/index?qid=20070829171112AAZuRmx</a><br /><br /><i>&quot;Why do AM radio signals travel farther than FM ones?&quot;</i><br /><br /><b><i>&quot;Best Answer:</i></b><i> AM signals are at lower frequencies than FM station.s The lower frequencies are reflected by ionized layers high in the atmosphere, So </i><b><i>after reflection</i></b><i>, they come bck to earth at a greater distance than a straight line path which is what FM signals follow.&quot;</i><br /><br /><b><i>&quot;AM Signals get refracted back to earth from the ionosphere, sky wave.</i></b><i> That is why you end up with an AM signal in another state or country. FM will go further, but you have to remember that the earth is round so after 50 miles or so, you can not catch the signal, unless you are on Mars. FM is for the most part line of sight that is why FM towers are higher than AM ones. and FM will go through the ionosphere and will not be refracted. FM signals can be hindered or reflected by buildings and when u get a chance to drive around a big city, you can probably hear the reflections.&quot;</i><br /><br /><b><i>&quot;AM signals bounce off a layer of the atmosphere and back to Earth a few times.</i></b><i> FM signals travel through the atmosphere, they are relayed in line of sight from an antenna.&quot;</i><br /><br /><b><i>&quot;AM waves bounce off of the ionosphere</i></b><i>, back on the ground, until they reach your radio. also it is a much lower frequency, so it travels farther. one other thing, FM stations tend to be closer together, interfere with each other in certain places, and since they can&#39;t be too close, less powerful transmitters.&quot;</i><br /><br />(etc)<br /><br />At another page, a similar question is asked ...<br /><br /><a href=\"https://www.quora.com/Why-do-F-M-radio-waves-have-less-range-than-A-M-radio-waves-Why-are-they-less-able-to-travel-around-obstacles-such-as-hills-and-large-buildings\" class=\"ot-anchor\">https://www.quora.com/Why-do-F-M-radio-waves-have-less-range-than-A-M-radio-waves-Why-are-they-less-able-to-travel-around-obstacles-such-as-hills-and-large-buildings</a><br /><br /><i>&quot;Why do F.M. radio waves have less range than A.M. radio waves? Why are they less able to travel around obstacles such as hills and large buildings?&quot;</i><br /><br />The answers at both pages are revealing in that nobody is actually explaining the most important reason why the AM signals travel further -- the physical mechanism for the reflection of a wave off of a plasma.<br /><br />Probably a lot of people are at this point thinking about wikipedia.  Here&#39;s their entry on <i>skywave</i> at <a href=\"https://en.wikipedia.org/wiki/Skywave\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Skywave</a> ...<br /><br /><i>&quot;In radio communication, skywave or skip refers to the propagation of radio waves reflected or refracted back toward Earth from the ionosphere, an electrically charged layer of the upper atmosphere. Since it is not limited by the curvature of the Earth, skywave propagation can be used to communicate beyond the horizon, at intercontinental distances. It is mostly used in the longwave frequency bands.</i><br /><br /><i>As a result of skywave propagation, a signal from a distant AM broadcasting station, a shortwave station, or -- during sporadic E propagation conditions (principally during the summer months in both hemispheres) -- a low frequency television station can sometimes be received as clearly as local stations. Most long-distance shortwave (high frequency) radio communication -- between 3 and 30 MHz -- is a result of skywave propagation. Since the early 1920s amateur radio operators (or &quot;hams&quot;), limited to lower transmitter power than broadcast stations, have taken advantage of skywave for long distance (or &quot;DX&quot;) communication.&quot;</i><br /><br />Again, the answers seem far more <b>descriptive</b> than <b>explanatory</b>.  In the interest of building a case, I&#39;m going to list out what we can safely conclude from the observations I&#39;ll be pointing to.  In this instance ... <br /><br /><b>(1)</b> Some people understand that AM radio signals bounce off of the ionosphere (although an informal poll of the people around you will quickly reveal that some do not).<br /><br /><b>(2)</b> Of those who do know, very few -- if any -- of them appear to actually understand <b>why</b> this occurs.<br /><br />Let&#39;s get back to this later, and for now switch to another very unusual feature of the Earth&#39;s surroundings ...<br /><br /><b>Why Are Incoming High-Speed Electrons Sharply Cut Off in the Van Allen Radiation Belts?</b><br /><br />The riddle of the barrier was recently emphasized by a press release at <a href=\"http://www.latimes.com/science/sciencenow/la-sci-sn-star-trek-invisible-shield-electron-van-allen-radiation-belts-20141126-story.html\" class=\"ot-anchor\">http://www.latimes.com/science/sciencenow/la-sci-sn-star-trek-invisible-shield-electron-van-allen-radiation-belts-20141126-story.html</a>, which includes a mix of observations, expectations and speculations ...<br /><br /><i>&quot;Star Trek-like invisible shield protects Earth from &#39;killer electrons&#39;</i><br /><i>By Amina Khan</i><br /><br /><i>Scientists detected an invisible shield roughly 7,200 miles above the Earth’s surface that is protecting us from harmful, super-fast electrons flying close to the speed of light.</i><br /><br /><i>It may sound like Star Trek tech, but this mysterious protective barrier isn&#39;t science fiction. The findings, described in the journal Nature, could help scientists better understand the complex dynamics of the Van Allen radiation belts.</i><br /><br /><i>The Van Allen radiation belts, discovered in 1958, are </i><b><i>two doughnut-shaped rings of energetic particles</i></b><i> circling the Earth up to about 25,000 miles above the surface, and are held in place by the planet’s magnetic fields. </i><b><i>Scientists have found that there appears to be an inner zone full of high-energy protons and an outer zone full of high-energy electrons.</i></b><i> These belts are thought to be fed by cosmic rays and the solar wind, and they can swell and shrink over time in response to changes in space weather.</i><br /><br /><i>The high-energy &#39;killer electrons&#39; in the belt can wreak havoc with the sensitive electronics of orbiting satellites and even potentially harm the health of astronauts in space. So how is it that these high-speed particles, traveling faster than 100,000 miles per second, don’t regularly cause problems at the Earth’s surface?</i><br /><br /><i>To find out why the particles don’t seem to reach Earth, scientists studied the belt using NASA’s twin Van Allen Probes. As it turns out, </i><b><i>there seems to be a sharp cutoff of high-energy electrons around the 7,200-mile mark – almost as if the particles are hitting a glass wall</i></b><i>, lead author Daniel Baker, director of the University of Colorado Boulder’s Laboratory for Atmospheric and Space Physics, said in a statement.</i><br /><br /><b><i>&#39;The presence of such a clear, persistent and seemingly impenetrable barrier to inward transport of ultrarelativistic electrons at this very specific location presents a substantial puzzle,&#39;</i></b><i> the study authors wrote.</i><br /><br /><b><i>The sharp cutoff took the scientists by surprise -- they had expected to see a more natural, smooth transition.</i></b><i> Previous theories suggested that the magnetic fields might be holding the electrons in place, or that human-generated radio signals from the ground were somehow blocking the onslaught. But now, given what the scientists were seeing, those theories didn’t make sense.</i><br /><br /><i>The researchers think it could have to do with electrically charged cold gas in a zone called the plasmasphere, which starts around 600 miles above the Earth and stretches thousands of miles into the outer, electron-dominated zone in the Van Allen belt. The low-level, hissing white noise generated by low frequency electromagnetic waves coming from the gas could be scattering the electrons at the gas&#39; border, the scientists said.</i><br /><br /><i>The findings provide fresh insight into the complex workings of plasma physics -- and could give engineers a better idea about where to safely park their orbiting satellites.&quot;</i><br /><br />Ignoring the speculations, we learn some new things ...<br /><br /><b>(3)</b> The Van Allen radiation belts exhibit an almost perfect barrier to high-speed electrons, perhaps similar to what happens to radio waves with the ionosphere.<br /><br /><b>(4)</b> This barrier is surprisingly sharp relative to what scientists were expecting to see (Why?!)<br /><br /><b>(5)</b> The Van Allen radiation belts are layered and these layers are dominated by opposing electrical charges (How strange!)<br /><br />This brings us to the next question you should be asking ...<br /><br /><b>Why Are Both the Van Allen Radiation Belts and the Ionosphere Layered?</b><br /><br />When seeking to understand some complex science, it helps to have a go-to explainer who is able to review the history, and who tends to excel at reducing the complexity of science to something comprehensible; for me, that is Isaac Asimov.  Let&#39;s listen to Asimov review some of the history in 1960 with his <i>Asimov&#39;s Guide to Science</i> (republished in &#39;65 and &#39;72) ...<br /><br /><i>&quot;The discovery of ions in the atmosphere did not emerge until after Guglielmo Marconi started his experiments with wireless.  When, on December 12, 1901, he sent signals from Cornwall to Newfoundland, across 2,100 miles of the Atlantic Ocean, scientists were startled.  Radio waves travel only in a straight line.  How had they managed to go around the curvature of the earth and get to Newfoundland?</i><br /><br /><i>A British physicist, Oliver Heaviside, and an American electrical engineer, Arthur Edwin Kennelly, soon suggested that the radio signals might have been reflected back from the sky by a layer of charged particles high in the atmosphere.  The &#39;Kennelly-Heaviside layer,&#39; as it has been called ever since, was finally located in the 1920&#39;s.  The British physicist Edward Victor Apleton discovered it by paying attention to a curious fading phenomenon in radio transmission.  He decided that the fading was the result of interference between two versions of the same signal, one coming directly from the transmitter to his receiver, the other by a roundabout route via reflection from the upper atmosphere.  The delayed wave was out of phase with the first, so the two waves partly canceled each other; hence the fading.</i><br /><br /><i>It was a simple matter then to find the height of the reflecting layer.  All he had to do was to send signals at such a wavelength that the direct signal completely canceled the reflected one -- that is, the two signals arrived at directly opposite phases.  From the wavelength of the signal used and the known velocity of radio waves, he could calculate the difference in the distances the two trains of waves had traveled.  In this way, he determined that the Kennelly-Heaviside layer was some sixty-five miles up.</i><br /><br /><i>The fading of radio signals generally occurred at night.  Appleton found that shortly before dawn radio waves were not reflected back by the Kennelly-Heaviside layer but were reflected from still higher layers (now sometimes called the &#39;Appleton layers&#39;) which begin at a height of 140 miles.</i><br /><br /><i>For all these discoveries Appleton received the Nobel Prize in physics in 1947.  He had defined the important region of the atmosphere called the &#39;ionosphere,&#39; a word introduced in 1930 by the Scottish physicist Robert Alexander Watson-Watt.  It includes the later-named mesosphere and thermosphere, and is now divided into a number of layers.  From the stratopause up to sixty-five miles or so is the &#39;D region.&#39;  Above that is the Kennelly-Heaviside layer, called the &#39;D layer.&#39;  Above the D layer, to a height of 140 miles, is the &#39;E region&#39; -- </i><b><i>an intermediate area relatively poor in ions.</i></b><i> This is followed by the Appleton layers: the &#39;F1 layer&#39; at 140 miles and &#39;F2 layer&#39; at 200 miles.  The F1 layer is the richest in ions, the F2 layer being significantly strong only in the daytime.  Above these layers is the &#39;F region.&#39;</i><br /><br /><b><i>These layers reflect and absorb only the long radio waves used in ordinary radio broadcasts.</i></b><i> The shorter waves, such as those used in television, pass through, </i><b><i>for the most part.</i></b><i> That is why television broadcasting is limited in range -- a limitation which can be remedied by satellite relay stations in the sky ... </i><b><i>The radio waves from space (e.g., from radio stars) also pass through the ionosphere, fortunately; if they did not, there would be no radio astronomy.</i></b><br /><br /><i>The ionosphere is strongest at the end of the day, after the day-long effect of the sun&#39;s radiation, and weakens by dawn because many ions and electrons have recombined.  Storms on the sun, intensifying the streams of particles and high-energy radiation sent to the earth, cause the ionized layers to strengthen and thicken. </i><b><i>The regions above the ionosphere also flare up into auroral displays. During these electric storms long-distance transmission of radio waves on the earth is disrupted and sometimes blacked out altogether.&quot;</i></b> (p180-181, <i>Asimov&#39;s Guide to Science</i>, Isaac Asimov)<br /><br />There are a few important observations -- and one interesting question -- to note here:<br /><br /><b>(6)</b> The ionosphere has numerous layers (Why?!)<br /><br /><b>(7)</b> The ionosphere is acting as a filter for radio waves based upon frequency.  And the radio reflectivity is in some manner a function of the charge density of these layers, which varies from one layer to the next (Again, why?!)<br /><br /><b>(8)</b> When there is sufficient charge density -- like during the aurora caused by solar storms -- long-distance transmission of radio waves can become blacked out at the Earth&#39;s surface.<br /><br /><b>(9) There&#39;s lots of room for theorists to be wrong about why AM radio waves bounce off of the ionosphere, because the region is extremely inaccessible</b> ...<br /><br /><a href=\"http://www.space.com/31510-missing-atmospheric-electrons-explained.html?cmpid=514639_20160105_56951076&amp;adbid=z12ptlxiqrazszwmp22evpgaspejtbhsx&amp;adbpl=gp&amp;adbpr=109556515093730290049\" class=\"ot-anchor\">http://www.space.com/31510-missing-atmospheric-electrons-explained.html?cmpid=514639_20160105_56951076&amp;adbid=z12ptlxiqrazszwmp22evpgaspejtbhsx&amp;adbpl=gp&amp;adbpr=109556515093730290049</a><br /><br /><i>&quot;that region [53 miles (85 km) above Earth] of the atmosphere is so inaccessible, Cohen said.</i><br /><br /><b><i>&#39;It&#39;s too high to reach with balloons, and it&#39;s too low to hit with satellites,&#39;</i></b><i> Cohen told Live Science ...&quot;</i><br /><br /><b>(10)</b> Atmospheric scientists have been surprised at how sharply the concentration of electrons suddenly plunges at the boundary between the electrical ionosphere and the neutral atmosphere beneath it.<br /><br />To refer to the anomaly, scientists use the term, <i>D-region ledge</i>.<br /><br />From the same article ...<br /><br /><b><i>&quot;&#39;It&#39;s the most dramatic gradient anywhere in the ionosphere,&#39; Williams said, referring to the part of Earth&#39;s upper atmosphere where the D-region ledge is found. &#39;It really is very conspicuous, so it&#39;s begging for an explanation.&#39;&quot;</i></b><br /><br />The article then proposes what they see as a possible explanation for the skywave phenomenon, where AM and other electromagnetic waves bounce off of the ionosphere.  We will label this explanation <b>Exhibit A</b>:<br /><br /><i>&quot;The D-region ledge plays a critical role in modern communication. The planet itself conducts electricity, as does the layer of the ionosphere above the ledge, </i><b><i>but electromagnetic waves don&#39;t travel through nonconductive materials, like the electron-depleted region below the ledge. Below the ledge, the electron-depleted air acts as an insulator, forming a layer between the Earth and its atmosphere that enables radio waves and very low frequency electromagnetic waves to circle the globe.&quot;</i></b><br /><br />Now, let&#39;s add yet another clue to our list of peculiar observations about Earth&#39;s immediate surroundings ...<br /><br /><b>(11) Something within the Van Allen radiation belts is accelerating electrons and ions to energies of millions of eV.</b> (What is it?)<br /><br />One group of researchers believe they have the answer, at <a href=\"http://physics.aps.org/articles/v6/131\" class=\"ot-anchor\">http://physics.aps.org/articles/v6/131</a> ...<br /><br /><i>&quot;Viewpoint: A Particle Accelerator in the Radiation Belts</i><br /><i>Gaetano Zimbardo, Dipartimento di Fisica, Università della Calabria, I-87030 Arcavacata di Rende, Italy</i><br /><i>December 2, 2013• Physics 6, 131</i><br /><br /><i>Satellites in the radiation belts reveal plasma structures that can jumpstart the acceleration of electrons to very high energies.</i><br /><br /><i>One of the most intriguing problems of astrophysics is the existence in a variety of environments of anomalously high-energy particles, for example, extragalactic cosmic rays up to 10^20 electron volts (eV). Closer to home, </i><b><i>the Earth’s Van Allen radiation belts, discovered at the dawn of the space age, contain some electrons and ions with energies of millions of eV. In spite of a wealth of observations and many proposed models, clarifying the various acceleration mechanisms represents a long-standing challenge.</i></b><i> In Physical Review Letters, Forrest Mozer at the University of California, Berkeley, and colleagues [1] report </i><b><i>the first in situ observations in the radiation belts of streams of thousands of &#39;double layers&#39;-moving sheets of separated positive and negative charge that have long been thought to accelerate electrons. Data from NASA’s Van Allen probes show an astonishing potential drop (voltage difference) of a million volts passing the satellites, in streams that appear in association with strong enhancements of 2.52.5-MeV electron fluxes.</i></b><i> The energy that electrons can gain in the double-layer electric field could be the missing step in the radiation-belt acceleration scenario that has been envisaged by researchers in the last few years (see Fig. 1).</i><br /><br /><i>The magnetosphere is a large region of space where the Earth’s magnetic field controls the motion of charged particles, namely electrons, protons, and other ion species, which form space plasmas. Energetic particle populations are observed in various parts of the magnetosphere [2, 3]. Recently, using the Van Allen probes launched in August 2012, </i><b><i>researchers showed that electrons with energies over 2.52.5 MeV had been accelerated within the radiation belts [4], rather than being transported from elsewhere.</i></b><i> But what is the local acceleration mechanism? The discovery by the STEREO spacecraft [5] of very strong &#39;whistler&#39; waves in the Earth’s radiation belts suggested that electrons could &#39;surf&#39; these waves to accelerate to MeV energies. Whistlers are particular electromagnetic modes propagating in a magnetized plasma that can resonantly interact with fast particles. However, to be effective in the radiation belts, this process requires a seed population of electrons that already have energies of about a hundred keV [6, 7], much larger than the electron thermal energy.</i><br /><br /><b><i>A double layer is a perturbation that occurs in a &#39;collisionless&#39; plasma, traveling along the magnetic field and consisting of two layers of charge. As in a parallel-plate capacitor, the electric field between the layers gives rise to a corresponding potential drop. In space plasmas, the two charged layers grow out of plasma instabilities, and they do not recombine because they are moving, so, for instance, an electron density enhancement leads the ion density enhancement.</i></b><i> The Van Allen probes are equipped with measuring spheres tethered on 50-meter-long cables that allow determination of the three-dimensional electric field at 512 samples per second. For occasional short periods, data can be taken much faster, at 16,384 samples per second. </i><b><i>Such a high time resolution is needed to characterize double layers, since their transit past the spacecraft typically lasts less than a millisecond.</i></b><br /><br /><b><i>The remarkable result reported by Mozer et al. is that streams of 7000 double layers were observed in one minute</i></b><i>: it was like ascending a stairway of 7000 steps very quickly. Each double layer is detected as an electric field spike of a typical duration of 0.45 milliseconds and a potential step of about 30 volts. The train of double layers propagates along the magnetic field lines at 3100 kilometers per second, which is the speed of an unusual mode called electron acoustic waves that can exist in a plasma containing both hot and cold electron populations [8] ...</i><br /><br /><i>These results have several far-reaching implications. First, they complete the long-standing goal of understanding extreme acceleration of electrons in the Earth’s radiation belts. Second, the observations show that the Earth’s magnetosphere can be used as a laboratory for studying energetic particle acceleration while &#39;changing&#39; the experimental conditions (with or without double layers and with or without whistlers), just as we would do in our laboratory. Third, </i><b><i>a detailed understanding of the acceleration mechanisms in the Earth’s magnetosphere can be exported to other astrophysical systems</i></b><i>, like the solar corona, with observed particle energies up to 11 GeV, and to the other planetary magnetospheres, as well </i><b><i>as far more distant objects like supernova remnants [9], with cosmic-ray energies as high as 10^15 eV and more. Indeed, beams of particles moving along the magnetic field, which are the likely origin of double layers, are also envisaged for those objects.</i></b><i> This prompts new questions: Why are electron acoustic waves and the associated double layers formed? Why do double layers occur in streams of thousands? What conditions allow such large electric potential differences to exist? What drivers of the magnetospheric dynamics can also be found in other astrophysical systems?&quot;</i><br /><br /><b>Some More Examples of Attempts to Explain Skywave</b><br /><br /><b>Exhibit B:</b><br /><br /><a href=\"http://www.arrl.org/files/file/Technology/pdf/119962.pdf\" class=\"ot-anchor\">http://www.arrl.org/files/file/Technology/pdf/119962.pdf</a><br /><br /><i>&quot;The D Layer The first layer a signal reaches is the D layer. This layer acts as an attenuator, especially at low frequencies. In fact, the attenuation varies as the inverse square of the frequency. That is, doubling the frequency reduces the level of attenuation by a factor of four. This is why low-frequency signals are prevented from reaching the higher layers, except at night when the D layer disappears. Signals are attenuated as they pass through the D layer because they cause free electrons to vibrate. When this occurs, the electrons collide with other molecules, consuming a small amount of energy and dissipating a proportionately small amount of the radio signal. We can see that the level of attenuation depends upon the number of collisions that take place. In turn, this depends on a number of other factors. One of the most obvious is the number of gas molecules that are present. More gas molecules mean more collisions and increased attenuation. Ionization levels are also important, as is the frequency of the radio signal. As frequency increases, wavelengths become shorter and collisions between free electrons and gas molecules decrease. As a result, low-frequency signals are attenuated far more than those at higher frequencies. Even so, high-frequency signals still suffer some reduction in strength.</i><br /><br /><i>The E and F Layers</i><br /><br /><b><i>As with the D layer, when signals enter the E and F layers they cause free electrons to vibrate. Here the air density is much lower and there are fewer collisions. As a result, much less energy is lost and these layers affect radio signals in a different way. Rather than colliding with gas molecules and losing energy, the electrons tend to re-radiate the signal. Because the signal is traveling in an area where electron density is increasing, the farther it progresses into the layer, the more the signal is refracted away from the area of higher electron density.</i></b><i> At HF, this refraction is often sufficient to bend the signals back toward Earth. In effect, the layer appears to have “reflected” the signal.&quot;</i><br /><br /><b>Exhibit C:</b><br /><br /><a href=\"http://encyclopedia2.thefreedictionary.com/Radio-Wave+Propagation\" class=\"ot-anchor\">http://encyclopedia2.thefreedictionary.com/Radio-Wave+Propagation</a><br /><br /><i>&quot;Propagation in the ionosphere. In the ionosphere, which is a multicomponent plasma located within the earth’s magnetic field, the mechanism of radio-wave propagation is more complicated than in the troposphere. Under the action of radio waves in the ionosphere there may occur both forced oscillations of electrons and ions and various kinds of collective natural oscillations called plasma oscillations. One of these types plays the principal role, depending on the frequency ω of the radio wave. Consequently, the electrical properties of the ionosphere are different for different frequency ranges. When the frequency ω is high, only electrons participate in the propagation of radio waves, and the natural frequency of their oscillations—the Langmuir frequency—is ...</i><br /><br /><i>... Here e is the charge and m the mass of the electron, and N is the electron concentration. </i><b><i>The forced oscillations of free electrons in the ionosphere lag behind the electric field of the high-frequency wave by a phase shift of almost 2π; this behavior differs from that of electrons in the troposphere, which are tightly bound to atoms. This displacement of the electrons in the ionosphere reinforces the field E of the wave</i></b><i> ...&quot;</i><br /><br />What seems a bit peculiar is the invocation in Exhibit C of an electric field.  Astrophysicists have gone to great lengths to claim that E-fields do not form in cosmic plasmas.  It does not appear that whoever wrote this encyclopedia entry got that memo.<br /><br />Now, with three attempts to explain skywave, it should be apparent that they each differ in their focus.  It may be that the B and C explanations are actually making the same claim (?), but Exhibit A asserts that the reflection is due to the depletion of electrons -- the <i>D-Region Ledge</i> -- which acts as an insulator, which in turn do not transmit EM waves.  This does not seem particularly consistent with explanations B and C.<br /><br />Let&#39;s now look at a fourth attempt to explain skywave, <b>Exhibit D</b>, from Don Scott&#39;s <i>The Electric Sky</i>, pages 75-76 ...<br /><br /><i>&quot;Plasma Frequency</i><br /><br /><i>&quot;Irving Langmuir also discovered what is called the plasma frequency.  *Electrons, being less massive than ions, move more rapidly. He found that in plasma, free electrons move back and forth around the slowly moving +ions, a movement called harmonic motion. The frequency of this back and forth oscillation of the electrons is called the plasma frequency.*</i><br /><br /><b><i>If an electromagnetic wave (e.g., a radio signal) tries to pass through a plasma, it may get through -- or it may not.  The determining factor is whether the frequency of the incoming radio signal is higher than the plasma frequency.  If it is, the radio wave goes through the plasma as if it were transparent. If the frequency of the incoming radio signal is lower than the plasma frequency, energy from the signal is used up in accelerating the plasma&#39;s electrons.  This results in the signal energy being absorbed or partially reflected from the plasma.</i></b><br /><br /><b><i>This is why radio waves are reflected back from different plasma layers in Earth&#39;s upper atmosphere. This effect enables intercontinental radio communication at certain (low) frequencies. Very-high-frequency signals penetrate the upper atmospheric layers and are lost into space.</i></b><i> Of course, if our goal is to communicate with a spacecraft, then we want to use sufficiently high-frequency signals to penetrate the ionospheric plasma.</i><br /><br /><b><i>This is also why spacecraft reentering the atmosphere experience the familiar &#39;blackout&#39; period. Radio communication cannot penetrate the plasma sheath that forms around a spacecraft as it reenters.</i></b><br /><br /><i>For sensors in space, our Earth is a source of kilometer-long wavelength radio signals.  The notion that planets such as Earth could emit radio frequency signals was long derided by astronomers.  These transmissions are due to auroras high above the ionosphere.  Since their frequency is much lower than the plasma frequency of the ionosphere, they cannot be detected on Earth&#39;s surface and were first heard only when satellites traveled above the ionosphere.&quot;</i><br /><br />Now, we can bring our list of peculiar features of the ionosphere and Van Allen radiation belts together, with a couple more added at the end ...<br /><br /><b>(1)</b> Some people understand that AM radio signals bounce off of the ionosphere (although an informal poll of the people around you will quickly reveal that some do not).<br /><br /><b>(2)</b> Of those who do know, very few -- if any -- of them appear to actually understand <b>why</b> this occurs.<br /><br /><b>(3)</b> The Van Allen radiation belts exhibit an almost perfect barrier to high-speed electrons, perhaps similar to what happens to radio waves with the ionosphere.<br /><br /><b>(4)</b> This barrier is surprisingly sharp relative to what scientists were expecting to see<br /><br /><b>(5)</b> The Van Allen radiation belts are layered and these layers are dominated by opposing electrical charges<br /><br /><b>(6)</b> The ionosphere has numerous layers<br /><br /><b>(7)</b> The ionosphere is acting as a filter for radio waves based upon frequency.  And the radio reflectivity is in some manner a function of the charge density of these layers, which varies from one layer to the next (Again, why?!)<br /><br /><b>(8)</b> When there is sufficient charge density -- like during the aurora caused by solar storms -- long-distance transmission of radio waves can become blacked out at the Earth&#39;s surface.<br /><br /><b>(9)</b> There&#39;s lots of room for theorists to be wrong about why AM radio waves bounce off of the ionosphere, because the region is extremely inaccessible.<br /><br /><b>(10)</b> Atmospheric scientists have been surprised at how sharply the concentration of electrons suddenly plunges at the boundary between the electrical ionosphere and the neutral atmosphere beneath it.<br /><br /><b>(11)</b> Double layers have actually been observed associated with the Van Allen radiation belts.<br /><br /><b>(12)</b> These double layers have been observed accelerating electrons and ions to energies of millions of eV.<br /><br /><b>(13)</b> Plasmas exhibit an inherent frequency cutoff at which lower incoming radio frequencies are absorbed or reflected, whereas higher frequencies tend to pass through.<br /><br /><b>(14)</b> With sufficient plasma frequency -- as with a spacecraft reentering Earth&#39;s atmosphere -- it is possible to fully block out all radio communications through the plasma (!).<br /><br />And to this list, I want to add one final important point:<br /><br /><b>(15)</b> Plasmas naturally form layers within the laboratory around objects which exhibit a charge that differs from their surroundings.  This is commonly demonstrated within the plasma laboratory.  The complex layering of the ionosphere and Van Allen radiation belts is suggestive of the simple claim that the Earth&#39;s charge differs from its surrounding space.  The layering is then simply a reaction of the plasma to form a sheath around this charge, as if to protect it.<br /><br />So, why does this matter?<br /><br />Because the details of a planet might vary from one to the next, such that for at least some portion of these planets, <b>the inhabitants probably see no cosmic radio waves at all ... ever.</b> It is even possible that some don&#39;t even see any starlight.  I&#39;ll be digging into how this can be a lot more, but isn&#39;t it curious that none of the discussions about the Fermi Paradox bring up the situation of the skywave?<br /><br />Bell jar experimental photographs come from the paper titled &quot;Study of striations in a spherically symmetric Hydrogen discharge&quot; from <a href=\"http://www.everythingselectric.com/product-category/free-eu-pdf/\" class=\"ot-anchor\">http://www.everythingselectric.com/product-category/free-eu-pdf/</a><br /><br />The skywave and ionosphere diagrams come from paper at <a href=\"http://www.arrl.org/files/file/Technology/pdf/119962.pdf\" class=\"ot-anchor\">http://www.arrl.org/files/file/Technology/pdf/119962.pdf</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Plasma Double Layer","summary":"Nobel Laureate Hannes Alfven Argued that they Should Be Classified as a New Type of Celestial Object / They've Been Studied in the Laboratory Since the 1920's / Yet, Most Scientists Today Have Never Heard of Them / They Can Possibly Explain Why AM Radio Waves Bounce off of the Ionosphere, Allowing Us to at Times Hear Them on the Other Side of the Planet / The Drake Equation Makes No Mention of Cosmic Double Layers Even Though They Can Potentially Offer a Classical Physics Explanation for Why We Don't See Alien Chatter","image":"https://lh5.googleusercontent.com/-WVQqq7yZrqQ/VtuPm8LLOoI/AAAAAAAAHFA/Jw7rZxzNTvk/the-plasma-double-layer-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/DyZpaamnex1","publishDate":"2016-03-06T02:05:24.186Z","updateDate":"2017-01-03T02:47:49.463Z","text":"<b>The Plasma Double Layer: Nobel Laureate Hannes Alfven Argued that they Should Be Classified as a New Type of Celestial Object / They&#39;ve Been Studied in the Laboratory Since the 1920&#39;s / Yet, Most Scientists Today Have Never Heard of Them / They Can Possibly Explain Why AM Radio Waves Bounce off of the Ionosphere, Allowing Us to at Times Hear Them on the Other Side of the Planet / The Drake Equation Makes No Mention of Cosmic Double Layers Even Though They Can Potentially Offer a Classical Physics Explanation for Why We Don&#39;t See Alien Chatter</b><br /><br />If the universe was an organism, and the plasma was the lifeblood of this creature, then the double layer would be akin to a cell membrane or wall which acts as the boundary which keeps the various functional organs separated.<br /><br />The very notion of a double layer is at once both extraordinary and absurd: A gas, when supplied with sufficient unbound electrons and/or ions, will in the plasma laboratory <b>demonstrably</b> self-transform into complex, coherent, hub-and-spoke-connected filamentary and cellular structures.  We see it in the laboratory, and there&#39;s good reason to believe that it happens in space at much larger scales, as well.<br /><br />The double layer is a sandwiching of plus and minus charge next to each other in various geometric forms -- like filaments, sheets and cells.  They have historically been difficult to confirm as present in space; in fact, we&#39;ve only very recently developed an ability to observe them with space probes that were built for this explicit purpose.  Part of the problem is that they can be extremely small (on the millimeter scale), and they do not necessarily emit electromagnetic radiation (although they definitely can!).<br /><br />The double layer might actually be one of the most controversial subjects in science, for <b>it is common practice today in the astrophysical discipline to confidently assert that astronomical structures which look almost exactly like double layer-constructed structures definitively observed within the laboratory are without a doubt not related.</b><br /><br />Let&#39;s review all of the reasons why double layers are so controversial in this astrophysical context ...<br /><br /><b>(1)</b> It is an observed feature of laboratory plasmas whereby plus and minus charges exist side-by-side without recombining.  Such a configuration can be expected to create a strong electric field at these boundaries. <b>Yet, the most commonly used astrophysical plasma models rule this sort of behavior out</b> ...<br /><br /><b><i>&quot;Magnetohydrodynamic (MHD) theory has been used in space physics for more than forty years, yet many important questions about space plasmas remain unanswered ... Questions have been raised from the beginning of the space era whether MHD theory can describe correctly space plasmas that are collisionless and rarely in thermal equilibrium ...</i></b><br /><br /><b><i>Serious objections have been raised from the beginning of the space era about the application of MHD theory to collisionless space plasmas</i></b><i> (Chamberlain, 1960; Lemaire and Scherer, 1973; Heikkila, 1973, 1997; Alfvén, 1977; Scudder, 1997; Lui, 2001; Song and Lysak, 2001). </i><b><i>Although it is well-known that MHD theory is applicable only to a restricted class of plasma problems of which collisionless plasmas are not a part (Krall and Trivelpiece, 1973), MHD and ideal MHD theories have been used in space without due regard to these restrictions.</i></b><i> MHD theory is useful in the lower ionosphere and lower solar corona where plasmas are collision dominated. However, plasmas in the solar wind and magnetosphere are collisionless and their distributions are non-Maxwellian and include beams that can be nongyrotropic (Marsch et al., 1982; Lin et al., 1997; Parks et al., 2001; Mezianne et al., 2003). MHD theory will not describe the physics of these plasmas correctly.</i><br /><br /><b><i>Another issue in space physics is treating MHD fluids as ideal</i></b><i> (Parker, 1996). Ideal fluids have infinite conductivity (zero resistance) and the implicit charge mobility prevents them from supporting any electric field. The ideal fluid was originally conceptualized by Alfvén (1953) to study how MHD waves would behave if conductivity were imagined to be infinite. In such an ideal limit, magnetic fields would become frozen in the fluid. However, the frozen-in-field concept requires the strict criterion E · B = 0 which is not always satisfied in space* (Alfvén and Fälthammar, 1963; Alfvén, 1977; Fälthammar, 1989). This criterion is violated in Earth’s ionosphere where E|| has been observed (Evans, 1974; Mozer et al., 1977), in the vicinity of the magnetopause where it has been shown that the ion and electron motions are decoupled and thus the frozen-in-field condition is violated (Mozer et al., 2002), in the plasma sheet during substorms when large variations of the magnetic field are observed to generate new inductive currents (Lui, 1991; Parks, 2003) and in the solar corona where the collisionless solar wind originates (Chamberlain, 1960; Jockers, 1970; Lemaire and Scherer, 1973; Scudder, 1994; Meyer-Vernet, 1999; Maksimovic et al., 2001; Issautier et al., 2001; Pierrard et al., 2001; Zouganelis et al., 2003).&quot;</i> (&quot;Why Space Physics Needs to Go Beyond the MHD Box&quot;, George K Parks, <i>Space Science Reviews</i>, 2004)<br /><br />MHD theory treats plasmas as a fluid with a magnetic field frozen-into it.  Using this approach, electric currents and fields are not necessary to explain the magnetic field.<br /><br />It must be stressed that this is not at all what we see within the plasma laboratory.  Laboratory plasmas are <b>electrodynamic</b> phenomena (meaning the study of moving electric charges, aka electric currents and dynamic magnetic fields).  Laboratory plasmas are known to exhibit some small electrical resistance (contrary to the astrophysical models); their magnetic fields are electrodynamic rather than frozen-in (as the astrophysicists model them); and lab plasmas exhibit electric fields (an impossibility in the astrophysicists&#39; models).<br /><br />Double layers require electric fields, and so are not possible in the MHD approach to space physics which dominates astrophysics today.<br /><br />The small table in the graphic is an excerpt from Hannes Alfven&#39;s Nobel 1970 lecture where he contrasts the MHD- and plasma-based perspectives.<br /><br /><b>(2)</b> Critics have claimed for a number of decades now that <b>astrophysicists refuse to infer double layers for the simple reason that DL&#39;s are not taught as part of the astrophysical curriculum.</b>  From the same paper ...<br /><br /><i>&quot;A study of how a number of the most used textbooks in astrophysics treat important concepts such as double layers, critical velocity, pinch effects, and circuits [all electrodynamic plasma concepts] is made.  It is found that students using these textbooks remain essentially ignorant of even the existence of these concepts, despite the fact that some of them have been well known for half a century (e.g., double layers, Langmuir, 1929; pinch effect, Bennet, 1934).&quot;</i><br /><br />This type of claim, in particular, can be observed to inspire deep emotional reactions online and amongst professional scientists.<br /><br /><b>(3)</b> The predicament for scientists is that the presence of a double layer signifies a current flowing between the Earth and the solar plasma -- <b>which sounds too much for their tastes like modeling the solar system as an electric circuit.</b> This creates the remarkable situation that the simplest explanation for the sharp cutoff for high-speed electrons in the van allen belt -- an electric field -- is also the most controversial inference amongst astrophysicists.  They&#39;d prefer to propose some alternative cause, even if it is not observed within the laboratory.<br /><br /><b>(4) There have been numerous papers published both against and in support of this notion that double layers should be classified as a new type of celestial object.</b><br /><br />Nobel laureate, Hannes Alfven, has been outspoken on this subject ...<br /><br /><i>&quot;As neither double layer nor circuit can be derived from magnetofluid models of a plasma, such models are useless for treating energy transfer by means of double layers. They must be replaced by particle models and circuit theory.</i><br /><br /><i>A simple circuit is suggested which is applied to the energizing of auroral particles, to solar flares, and to intergalactic double radio sources ... Double layers in space should be classified as a new type of celestial object.&quot;</i> (&quot;Double Layers and Circuits in Astrophysics&quot;, Hannes Alfven, <i>IEEE Transactions on Plasma Science</i>, Dec 1986)<br /><br />For example, NASA held a conference in Huntsville, Alabama on March 17-19, 1986, titled Double Layers in Astrophysics.  Papers at that conference which discussed double layers within an astrophysical context are bolded below ...<br /><br />00 - Contents<br />01 - Keynote Address<br />02 - Formation Mechanisms of Laboratory Double Layers<br />03 - Some Dynamical Properties of Very Strong Double Layers in a Triple Plasma Device<br />04 - Pumping Potential Wells<br />05 - A Laboratory Investigation of Potential Double Layers<br />06 - Experimental Observation of Ion-Acoustic Double Layers in Laboratory Plasma<br />07 - A New Hydrodynamic Analysis of Double Layers<br />08 - Ion Phase-Space Vortices and Their Relation to Small Amplitude Double Layers<br />09 - <b>Effect of Double Layers on Magnetosphere-Ionospheric Coupling</b><br />10 - Current Driven Weak Double Layers<br />11 - Electric Fields and Double Layers in Plasmas<br />12 - Electron Acceleration in Stochastic Double Layers<br />13 - <b>Anomalous Transport in Discrete Arcs and Simulation of Double Layers in a Model Auroral Circuit</b><br />14 - <b>Weak Double Layers in the Auroral Ionosphere</b><br />15 - <b>Particle Simulation of Auroral Double Layers</b><br />16 - <b>Conditions for Double Layers in the Earth&#39;s Magnetosphere and Perhaps in Other Astrophysical Objects</b><br />17 - Some Aspects of Double Layer Formation in a Plasma Constrained by a Magnetic Mirror<br />18 - Electric Potential Distributions at the Interface Between Plasma Sheet Clouds<br />19 - <b>Double Layers Above the Aurora</b><br />20 - <b>Beamed Emission from Gamma-Ray Burst Sources</b><br />21 - <b>Double Layers and Plasma-Wave Resistivity in Extragalactic Jets - Cavity Formation and Radio-Wave Emission</b><br />22 - <b>Accretion Onto Neutron Stars with the Presence of a Double Layer</b><br />23 - The Formation of a Double Layer Leading to the Critical Velocity Phenomenon<br /><br />In a 1992 paper titled, &quot;Double Layers Do Accelerate Particles in the Auroral Zone,&quot; we see in the abstract that there exists an active debate ...<br /><br /><i>&quot;Space Plasma Physics Group, Los Alamos National Laboratory, Los Alamos, New Mexico 87545 ...</i><br /><br /><i>In response to a recent report [D.A. Bryant, R. Bingham, and U. de Angelis, Phys. Rev. Lett. 68, 37 (1991)] that makes the claim that electrostatic fields are weak in the auroral zone and that electrostatic fields cannot accelerate particles, it is pointed out that </i><b><i>the evidence for electrostatic fields in the auroral zone is overwhelming and that these electrostatic fields often are accelerating electrons to produce aurora. The literature cited in the article above as evidence against double layers (strong electric fields) is reexamined and is found not to be evidence against double layers.&quot;</i></b><br /><br />Later in the same paper, they clarify ...<br /><br /><b><i>&quot;the direct observational evidence for substantial (multi-kV) electrostatic potential structures in the auroral zone is plentiful [16-27].</i></b><br /><br /><b><i>The Earth&#39;s auroral zone is far from being fully understood, but observations clearly show that electrostatic-potential structures (called double layers or electrostatic shocks) reside in the auroral magnetosphere.&quot;</i></b><br /><br />For those that look, there&#39;s substantial literature on this subject.  The point intended here is to simply give a taste for it.<br /><br /><b>(5)</b> Plasma double layers generate copious radio and microwave noise, which implies that <b>the phenomenon could also be used to explain the Cosmic Microwave Background as simply a localized electromagnetic fog.</b> This ratchets up the controversy to spectacular levels, as it plainly contradicts the widespread belief that the only way to explain microwaves coming at us from all directions (the CMB) is as a remnant of the primordial Big Bang.<br /><br />The only work left to do here, truthfully, is to explain why the spikey noise from double layers is &quot;thermalized&quot; into this smooth bell curve that has sparked so much interest in recent years.  Do not be fooled into thinking that classical science solutions are impossible on that.<br /><br /><b>(6) Many theorists and online critics today insist that there exists a theoretical limit to EM forces dictated by a concept known as Debye screening.</b><br /><br />This subject has already been covered in a different controversy card, here:<br /><br /><b>Debye Screening</b><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/SLm2ki1TTqi\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/SLm2ki1TTqi</a><br /><br />Irving Langmuir discovered the double layer in the 1920s.  He invoked the notion of blood plasma when he coined the &quot;fourth&quot; state of matter, <i>plasma</i>, in 1928 because this special state of matter exhibited many of the collective lifelike properties of living blood plasma.<br /><br />The fact that a fundamental state of matter can exhibit behaviors which resemble living organisms should probably inspire more thought and conversation than we see today.  It&#39;s an extraordinary observation which raises the peculiar question of whether or not we are like bacteria living within a far larger creature of some sort.<br /><br />The double layer concept will prove enormously important as I introduce additional concepts that Wal Thornhill has collectively proposed as a reasonable resolution to the Fermi Paradox.  It surely sounds a bit too difficult to believe at this early stage of the explanation, but we can actually use science and logic to identify the origin of life, and likely location of other civilizations, in the universe.  But, <b>the solution only appears when we are willing to question the dominant scientific framework.</b><br /><br />What I am saying is that the paradox inherent to the Fermi Paradox is a direct implication of some of our most popular scientific theories of the day.  Switch the framework, and the paradox can disappear.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Debye Screening","summary":"A Theoretical Limit on the Reach of Electromagnetic Forces / The Debye Length is Used to Calculate the Maximum Volume at Which Electrostatic Discharges Can Occur in a Particular Medium / The Problem is that the Rule Does Not Apply to Electrodynamic Plasmas Which Exhibit Double Layers, a Common Feature of Laboratory Plasmas / This Theoretical Limit Has Been Repeatedly Violated By Actual Space Observations / Yet, Astrophysicists Continue to Assert Its Validity, And the Public Never Calls Them Out On It","image":"https://lh4.googleusercontent.com/-M1xlSVoIW18/VttZF6s-OCI/AAAAAAAAHD4/pamwQcwDBcI/debye-screening-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/SLm2ki1TTqi","publishDate":"2016-03-05T22:10:36.927Z","updateDate":"2017-01-03T02:48:02.243Z","text":"<b>Debye Screening: A Theoretical Limit on the Reach of Electromagnetic Forces / The Debye Length is Used to Calculate the Maximum Volume at Which Electrostatic Discharges Can Occur in a Particular Medium / The Problem is that the Rule Does Not Apply to Electrodynamic Plasmas Which Exhibit Double Layers, a Common Feature of Laboratory Plasmas / This Theoretical Limit Has Been Repeatedly Violated By Actual Space Observations / Yet, Astrophysicists Continue to Assert Its Validity, And the Public Never Calls Them Out On It</b><br /><br />Many theorists and online critics today insist that there exists a theoretical limit to EM forces dictated by a concept known as Debye screening.  The argument commonly goes like this ...<br /><br /><a href=\"http://www.internationalskeptics.com/forums/showpost.php?s=82d765b8736001a16ce79acee470d4bd&amp;p=10331157&amp;postcount=1698\" class=\"ot-anchor\">http://www.internationalskeptics.com/forums/showpost.php?s=82d765b8736001a16ce79acee470d4bd&amp;p=10331157&amp;postcount=1698</a><br /><br /><i>&quot;The good old &#39;EM is 10^39 times stringer [sic] than gravity&#39; myth.</i><br /><i>This is ignorance of both gravity and EM! Gravity is weaker but there is only one &#39;charge&#39; -- it cannot be shielded. </i><b><i>EM has two charges -- it can and is shielded. In plasma there are basically no EM forces on scales larger than a few Debye lengths. This is 10 meters in the solar wind.&quot;</i></b><br /><br />The argument superficially makes sense: We know that electrons will tend to be attracted to ions, so the narrative that the light electrons would tend to surround the slower, heavier ions in space, seems believable.  And if that was to consistently occur, then perhaps the two types of charge would essentially negate one another at some distance out, given some charge density.  And we would never observe any excess charge build up sufficient to create electric fields within interstellar gases (aka plasmas).<br /><br />Wikipedia informs us at <a href=\"https://en.wikipedia.org/wiki/Debye_length\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Debye_length</a><br /><br /><i>&quot;In plasmas and electrolytes the Debye length (also called Debye radius), named after the Dutch physicist and physical chemist Peter Debye, is the measure of a charge carrier&#39;s net electrostatic effect in solution, and </i><b><i>how far those electrostatic effects persist. A Debye sphere is a volume whose radius is the Debye length, with each Debye length, charges are increasingly electrically screened.</i></b><i> Every Debye‐length, the electric potential will decrease by 1/e.&quot;</i><br /><br />The chart on that page clearly lists these limits to electromagnetism&#39;s reach, per medium:<br /><br />Solar core - 10^-11 m<br />Tokamak - 10^-4 m<br />Gas discharge - 10^-4 m<br />Ionosphere - 10^-3 m<br />Magnetosphere - 10^2 m<br />Solar wind - 10 m<br />Interstellar medium - 10 m<br />Intergalactic medium - 10^5 m<br /><br />Siggy breaks the claim down for us in terms that anybody can understand at <a href=\"http://www.thunderbolts.info/forum/phpBB3/viewtopic.php?p=39227&amp;sid=d0d6af87c1442042ff85e9f8d0a0d569\" class=\"ot-anchor\">http://www.thunderbolts.info/forum/phpBB3/viewtopic.php?p=39227&amp;sid=d0d6af87c1442042ff85e9f8d0a0d569</a>:<br /><br /><i>&quot;Debye screening is used as an explanation by mainstream to point to why discharges won&#39;t occur in galactic scenarios. I.e. </i><b><i>the charged particles within a plasma wouldn&#39;t be affected by an external electric field outside the Debye length/radius. This length also signifies the volumes where discharges can occur; a few meters in the solar wind and up to a hundred kilometers in intergalactic medium (still not very long distances).</i></b><i>&quot;</i><br /><br /><b>The problem with this concept is that observations have repeatedly shown us that it fails to actually constrain EM forces.</b> Even if it has proven an accurate <i>general</i> guideline, the idea has repeatedly failed us as a rule or law.  It&#39;s important to go through a few of the most obvious failures.<br /><br /><b>(1)</b> NASA-funded research has already produced evidence for double layers as observable aspects of the aurora: <i>&quot;Direct observations of magnetic-field-aligned (parallel) electric fields in the downward current region of the aurora provide decisive evidence of naturally occurring double layers.&quot;</i> (&quot;Double Layers in the Downward Current Region of the Aurora&quot;, <i>Nonlinear Processes in Geophysics</i>, 2003)<br /><br />In these regions, wikipedia reports that the theoretical Debye length should restrict electric fields to <b>1 millimeter</b>.  When one yet further considers that red sprites, a form of upper-atmospheric lightning which can connect to the lower ionosphere, can reach <b>50km in length</b>, it would seem that the Debye length has failed to constrain the discharge by a factor of 50 million-to-1.<br /><br /><b>(2)</b> In another very obvious violation of the theoretical limit, mentioned at <a href=\"http://www.jpl.nasa.gov/news/news.php?feature=4340\" class=\"ot-anchor\">http://www.jpl.nasa.gov/news/news.php?feature=4340</a>, which assumes that the observed electric current is a form of static electricity ...<br /><br /><i>&quot;Static electricity is known to play an important role on Earth&#39;s airless, dusty moon, but evidence of static charge building up on other objects in the solar system has been elusive until now. A new analysis of data from NASA&#39;s Cassini mission has revealed that, </i><b><i>during a 2005 flyby of Saturn&#39;s moon Hyperion, the spacecraft was briefly bathed in a beam of electrons coming from the moon&#39;s electrostatically charged surface</i></b><i> ...</i><br /><br /><i>Measurements made by several of Cassini&#39;s instruments during a close encounter with Hyperion on </i><b><i>September 26, 2005</i></b><i>, indicate that something unexpected took place in the charged particle environment around the spacecraft. Among those instruments, the Cassini Plasma Spectrometer (CAPS) detected that </i><b><i>the spacecraft was magnetically connected to the surface of Hyperion for a brief period, allowing electrons to escape from the moon toward the robotic probe</i></b><i> ...</i><br /><br /><i>The finding is surprising, as the small but odd-looking moon was thought to be a simple inert object, which would not undergo any strong interactions with the Saturnian magnetosphere. Nevertheless, </i><b><i>the team&#39;s analysis indicates that Cassini remotely detected a strongly negative voltage on Hyperion. &#39;It was rather like Cassini receiving a 200-volt electric shock from Hyperion, even though they were over 2,000 kilometers [1,200 miles] apart at the time,&#39;</i></b><i> said Nordheim ...&quot;</i><br /><br />The solar wind is theoretically limited to EM discharges of only 10 meters.  In most scientific disciplines, a result of 2,000 km when you were expecting a maximum of just 10 would be viewed as a catastrophic failure of the idea.<br /><br />Further, the language used to describe this 1,200 mile electrical discharge should be viewed with some suspicion ... &quot;electrostatically charged&quot;, &quot;magnetically connected&quot;, &quot;electric shock&quot; ... It seems to liken the effect to the slight jolt of static electricity a person might receive from a doorknob.  Yet, at 1,200 miles, what sense is there discussing the magnetic field and electrostatic (<b>the science of stationary charges</b>) aspect of the object <b>without any mention at all of the electric currents which are, by definition, involved at such enormous distances.</b> At these scales, it makes far more sense to use the language of electrodynamic plasmas (which is, of course, <b>the science of moving electric charges and dynamic magnetic fields in a gas</b>).<br /><br />What is remarkable about the Hyperion announcement -- titled &quot;Detection of a strongly negative surface potential at Saturn’s moon Hyperion&quot; -- was that it was not submitted to Geophysical Research Letters until July 4, 2014 -- <b>a full 9 years after the observation was made</b> -- which raises questions about scientific protocol when observations of electricity in space are observed to plainly defy the box that astrophysicists have historically attempted to put cosmic electricity into.<br /><br /><b>(3)</b> In a third obvious instance at <a href=\"http://science.nasa.gov/science-news/science-at-nasa/2008/30oct_ftes/\" class=\"ot-anchor\">http://science.nasa.gov/science-news/science-at-nasa/2008/30oct_ftes/</a> titled &quot;Magnetic Portals Connect Earth to the Sun&quot;, we see yet again that the 10 meter limit to EM discharges within the Sun&#39;s solar wind environment is completely misleading our efforts to accurately model these regions ...<br /><br /><i>&quot;Oct. 30, 2008: </i><b><i>During the time it takes you to read this article, something will happen high overhead that until recently many scientists didn&#39;t believe in. A magnetic portal will open, linking Earth to the sun 93 million miles away. Tons of high-energy particles may flow through the opening before it closes again, around the time you reach the end of the page.</i></b><br /><br /><i>&#39;It&#39;s called a flux transfer event or &#39;FTE,&#39;&#39; says space physicist David Sibeck of the Goddard Space Flight Center. </i><b><i>&#39;Ten years ago I was pretty sure they didn&#39;t exist, but now the evidence is incontrovertible.&#39;</i></b><br /><br /><i>Indeed, today Sibeck is telling an international assembly of space physicists at the 2008 Plasma Workshop in Huntsville, Alabama, that </i><b><i>FTEs are not just common, but possibly twice as common as anyone had ever imagined.</i></b><br /><br /><i>Researchers have long known that the Earth and sun must be connected. Earth&#39;s magnetosphere (the magnetic bubble that surrounds our planet) is filled with particles from the sun that arrive via the solar wind and penetrate the planet&#39;s magnetic defenses. They enter by following magnetic field lines that can be traced from terra firma all the way back to the sun&#39;s atmosphere.</i><br /><br /><b><i>&#39;We used to think the connection was permanent and that solar wind could trickle into the near-Earth environment anytime the wind was active,&#39; says Sibeck. &#39;We were wrong. The connections are not steady at all. They are often brief, bursty and very dynamic.&#39;</i></b><br /><br /><i>Several speakers at the Workshop have outlined how FTEs form: On the dayside of Earth (the side closest to the sun), Earth&#39;s magnetic field presses against the sun&#39;s magnetic field. </i><b><i>Approximately every eight minutes, the two fields briefly merge or &#39;reconnect,&#39; forming a portal through which particles can flow. The portal takes the form of a magnetic cylinder about as wide as Earth.</i></b><i> The European Space Agency&#39;s fleet of four Cluster spacecraft and NASA&#39;s five THEMIS probes have flown through and surrounded these cylinders, measuring their dimensions and sensing the particles that shoot through. </i><b><i>&#39;They&#39;re real,&#39;</i></b><i> says Sibeck ...&quot;</i><br /><br />The Earth is 92 million miles from the Sun.  Thus, the theoretical Debye limit in this case was off by a factor of 9-million-to-1.<br /><br />These failures raise what should be a logical question: If the Debye length is spectacularly failing to constrain electrical discharges at these scales, <b>then what about much larger galactic-scale discharges?</b><br /><br />Why should we trust this theoretical limit on EM forces at those cosmological scales when it is so easy to find examples of failures at the smaller scales we&#39;re more familiar with?<br /><br />The public has, to date, failed to directly confront the astrophysical community on these matters.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"Crowding Out","summary":"A Form of Media Manipulation / It Suppresses the Spread of an Idea By Inducing Others to Stop Reading or Listening / It's an Attack Upon the Communication Channel Itself / The Public's Need to Pick Wisdom Out from the Crowd Should Inspire the Public to Design Niche Communication Channels Which Are Resistant to this Public Relations Technique","image":"https://lh3.googleusercontent.com/-4OvcHYdfi5g/VtcXNojOo4I/AAAAAAAAHC8/KhyszSB1m2s/w1800-h2520/crowding-out-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/F9jWwzDfbdu","publishDate":"2016-03-02T16:40:19.819Z","updateDate":"2017-01-03T02:48:19.842Z","text":"<b>Crowding Out: A Form of Media Manipulation / It Suppresses the Spread of an Idea By Inducing Others to Stop Reading or Listening / It&#39;s an Attack Upon the Communication Channel Itself / The Public&#39;s Need to Pick Wisdom Out from the Crowd Should Inspire the Public to Design Niche Communication Channels Which Are Resistant to this Public Relations Technique</b><br /><br /><a href=\"https://en.wikipedia.org/wiki/Media_manipulation\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Media_manipulation</a><br /><br /><i>&quot;Media manipulation is a series of related techniques in which partisans create an image or argument that favours their particular interests. Such tactics may include the use of logical fallacies and propaganda techniques, </i><b><i>and often involve the suppression of information or points of view by crowding them out, by inducing other people or groups of people to stop listening to certain arguments, or by simply diverting attention elsewhere.&quot;</i></b><br /><br />Crowding out is foremost an attack upon the medium -- the information channel -- than the messenger or message, analogous to the way that WWII-era flak attacks the flying space.  The arguments put forth as flak might not even be compelling or well-reasoned.<br /><br />There&#39;s a subtle difference between crowding out and censorship, in that it occurs for conversations on two-way, open-communication channels like Internet forums and website comments.<br /><br />The reason why it works is that it disinterests the reader in further reading the conversation, making the task of picking out wisdom from the crowd much more tedious.  These are organized groups of people acting in their own interests and against the public&#39;s need to know.<br /><br />The technique is very effective, and it should logically inspire us to contemplate the construction of a new medium for conversation dedicated specifically to scientific discourse.  The problem could, for example, be fixed with a more sophisticated rating scheme that implements values -- such as creative problem-solving and critical thinking standards.  If such a system was implemented and worked as designed, people might subsequently wonder why academia did not create it themselves.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Speed of Gravity","summary":"Special Relativity Dictates that Nothing Can Propagate in Forward Time at a Speed Greater than that of Light in a Vacuum / Yet, Computer Simulations Leave No Doubt that a Delay in the Gravitational Force Cannot Reproduce a Stable Solar System / 'What is the Speed of Gravity?' Has Traditionally Been One of the Most Commonly Asked Science Questions Online / The Question is Treated as Answered for Today's Students of Science / And Journals Have Had a Policy to Reject Papers Which Ask the Question Since Dingle","image":"https://lh3.googleusercontent.com/-l54kpN8Yf_Q/VtRxTXm3pjI/AAAAAAAAHBk/eVkSTDIxcBs/w1800-h2520/the-speed-of-gravity-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/NaN8qrGovon","publishDate":"2016-02-29T16:27:03.474Z","updateDate":"2017-01-03T02:48:33.266Z","text":"<b>The Speed of Gravity: Special Relativity Dictates that Nothing Can Propagate in Forward Time at a Speed Greater than that of Light in a Vacuum / Yet, Computer Simulations Leave No Doubt that a Delay in the Gravitational Force Cannot Reproduce a Stable Solar System / &#39;What is the Speed of Gravity?&#39; Has Traditionally Been One of the Most Commonly Asked Science Questions Online / The Question is Treated as Answered for Today&#39;s Students of Science / And Journals Have Had a Policy to Reject Papers Which Ask the Question Since Dingle</b><br /><br />From <a href=\"http://www.metaresearch.org/cosmology/speed_of_gravity.asp\" class=\"ot-anchor\">http://www.metaresearch.org/cosmology/speed_of_gravity.asp</a><br /><br /><b><i>&quot;The most amazing thing I was taught as a graduate student of celestial mechanics at Yale in the 1960s was that all gravitational interactions between bodies in all dynamical systems had to be taken as instantaneous.</i></b><i> This seemed unacceptable on two counts. In the first place, it seemed to be a form of &#39;action at a distance&#39;. Perhaps no one has so elegantly expressed the objection to such a concept better than Sir Isaac Newton: &#39;That one body may act upon another at a distance through a vacuum, without the mediation of any thing else, by and through which their action and force may be conveyed from one to the other, is to me so great an absurdity, that I believe no man who has in philosophical matters a competent faculty of thinking, can ever fall into it.&#39; (See Hoffman, 1983.) But mediation requires propagation, and finite bodies should be incapable of propagation at infinite speeds since that would require infinite energy. So instantaneous gravity seemed to have an element of magic to it.</i><br /><br /><b><i>The second objection was that we had all been taught that Einstein’s special relativity (SR), an experimentally well-established theory, proved that nothing could propagate in forward time at a speed greater than that of light in a vacuum. Indeed, as astronomers we were taught to calculate orbits using instantaneous forces; then extract the position of some body along its orbit at a time of interest, and calculate where that position would appear as seen from Earth by allowing for the finite propagation speed of light from there to here. It seemed incongruous to allow for the finite speed of light from the body to the Earth, but to take the effect of Earth’s gravity on that same body as propagating from here to there instantaneously. Yet that was the required procedure to get the correct answers.</i></b><br /><br /><b><i>These objections were certainly not new when I raised them. They have been raised and answered thousands of times in dozens of different ways over the years since general relativity (GR) was set forth in 1916. Even today [writing in 1998] in discussions of gravity in USENET newsgroups on the Internet, the most frequently asked question and debated topic is &#39;What is the speed of gravity?&#39; It is only heard less often in the classroom because many teachers and most textbooks head off the question by hastily assuring students that gravitational waves propagate at the speed of light, leaving the firm impression, whether intended or not, that the question of gravity’s propagation speed has already been answered.</i></b><br /><br /><b><i>Yet, anyone with a computer and orbit computation or numerical integration software can verify the consequences of introducing a delay into gravitational interactions. The effect on computed orbits is usually disastrous because conservation of angular momentum is destroyed.</i></b><i> Expressed less technically by Sir Arthur Eddington, this means: &#39;If the Sun attracts Jupiter towards its present position S, and Jupiter attracts the Sun towards its present position J, the two forces are in the same line and balance. But if the Sun attracts Jupiter toward its previous position S’, and Jupiter attracts the Sun towards its previous position J’, when the force of attraction started out to cross the gulf, then the two forces give a couple. This couple will tend to increase the angular momentum of the system, and, acting cumulatively, will soon cause an appreciable change of period, disagreeing with observations if the speed is at all comparable with that of light.&#39; (Eddington, 1920, p. 94) See Figure 1.</i><br /><br /><i>Indeed, it is widely accepted, even if less widely known, that the speed of gravity in Newton’s Universal Law is unconditionally infinite. (E.g., Misner et al., 1973, p. 177) </i><b><i>This is usually not mentioned in proximity to the statement that GR reduces to Newtonian gravity in the low-velocity, weak-field limit because of the obvious question it begs about how that can be true if the propagation speed in one model is the speed of light, and in the other model it is infinite.&quot;</i></b><br /><br /><i>[...]</i><br /><br />From <a href=\"http://physics.stackexchange.com/questions/186186/resolving-general-relativity-and-newtonian-mechanics-to-a-computer\" class=\"ot-anchor\">http://physics.stackexchange.com/questions/186186/resolving-general-relativity-and-newtonian-mechanics-to-a-computer</a><br /><br /><i>&quot;I know this is considered an old subject long ridiculed by many as the folly of layman. But I work in the field of computer simulation, specifically in producing fully functional 3D interactive systems that are based in physics. I been trying to update a universe simulator, which yes, deals with orbiting planets. But goes much farther and predicts the temperatures of planets. Uses particle physics to generate esoteric things such as comet tails. Black body radiation, Albedo, Light intensity. All without data fitting. (without fudging constants to resolve observed discrepancies). In this system you should be able to take a virtual telescope and see what you would expect in the real world, except it has unlimited range.</i><br /><br /><b><i>My problem is I have never been able to incorporate GR into the system, for a number of reasons. But the most glaring is that it is quite clear that the moment you introduce anything less than zero latency (speed of gravity). The entire system falls apart, planets fly off, everything dissapates. Not even a infinitesimally small amount of latency is tolerated. Not to mention using the fairly big speed of light value for it.</i></b><i> There&#39;s even issues if I ignore this gravity problem and use it purely for light. I know most will give the usual response with it only works on the macroscale. With numerous examples of why it works in the real world. And how silly of me to even bring up this question. But regardless, of the arguments. I am stuck. I even asked a NASA engineer how they resolve this. And his answer was they don&#39;t. They use Newtonian physics. So I don&#39;t want to open up a old can of worms and end up sidelining my original issue ...</i><br /><br />One response suggests that the answer to the riddle is to <i>&quot;ignore the bulk of the complexity of Einstein&#39;s field equations&quot;</i> ...<br /><br /><i>&quot;Newton himself didn&#39;t quite like the instantaneous action at a distance as implied by his law of gravitation. The only saving grace is that it worked. Laplace explicitly tried to add a finite transmission speed (non-zero latency) to Newtonian gravitation. </i><b><i>He found that the only way to make this work was to make the transmission speed many millions of times that of the speed of light (essentially indistinguishable from instantaneous). Anything less and the solar system blew itself apart.</i></b><br /><br /><i>The solution is &#39;Don&#39;t do that then.&#39; General relativity doesn&#39;t do that.</i><br /><br /><i>General relativity is a complex, non-linear system with non-linear feedbacks (gravity begets gravity). If your main concern is modeling the behavior of close binary pair of pulsars, those non-linearities are essential to capturing the essential dynamics; it even won a Nobel prize. The deviations from Newtonian mechanics are tiny if your main concern is modeling behaviors within our solar system, and the non-linear nature of relativistic gravity is tinier yet. Those non-linearities aren&#39;t that important even for modeling the relativistic precession of Mercury. This suggests a &#39;simple&#39; solution (&#39;simple&#39; being a relative term), </i><b><i>which is to ignore the bulk of the complexity of Einstein&#39;s field equations.</i></b><i> Linearize the deviations from Newtonian gravity and throw out the rest. This is the core of a post-Newtonian expansion. It works quite nicely in our solar system.</i><br /><br />Another response at that same page.  I&#39;m realizing that probably most people won&#39;t understand this ...<br /><br /><b><i>&quot;You have taught yourself through your own experimenting and curiousity a famous lesson. You are indeed doing exactly as Laplace did and your findings are the same as Laplace&#39;s.</i></b><br /><br /><i>To add to David Hammen&#39;s Answer: </i><b><i>David is correct that Laplace&#39;s attempts at introducing latency lead to an unstable solar system.</i></b><i> But there is a way to mostly succeed with Laplace&#39;s method and that is to make the latency Lorentz invariant. That is, you begin with the electrostatic law (or its equivalent Gauss law for gravity), postulate that the Gauss form this holds for dynamic systems and find the simplest Lorentz-invariant tensor that contains your gravity field. The result if you begin with electrostatics is Maxwell&#39;s equations and the Lorentz force law; the mathematically wholly analogous result if you begin with Newton&#39;s gravitation is Gravitoelectromagnetism. Gravitoelectromagnetism is consistent with all the results of Gravity Probe B and other results observed in the solar system (more on this here, here and here). In particular, </i><b><i>although the orbits are still theoretically unstable, they are stable over timeframes many times the age of the universe for all observations made in the solar system. Gravitoelectromagnetism is thus an extremely accurate approximation of General Relativity for our solar system.</i></b><br /><br />Aberration and the Speed of Gravity at <a href=\"http://arxiv.org/pdf/gr-qc/9909087.pdf\" class=\"ot-anchor\">http://arxiv.org/pdf/gr-qc/9909087.pdf</a><br /><br /><b><i>&quot;It is certainly true, although perhaps not widely enough appreciated, that observations are incompatible with Newtonian gravity with a light-speed propagation delay added in ...&quot;</i></b><br /><br /><i>&quot;the gravitational interaction propagates at the speed of light, but velocity-dependent terms in the interaction nearly cancel the effect of aberration ...&quot;</i><br /><br /><i>&quot;It is worth noting that the cancellation between aberration and velocity-dependent terms in general relativity is not quite exact. If gravity could be described exactly as an instantaneous, central interaction, the mechanical energy and angular momentum of a system such as a binary pulsar would be exactly conserved, and orbits could not decay. In general relativity, the gravitational radiation reaction appears as a slight mismatch between the effects of aberration and the extra noncentral terms in the equations of motion&quot;</i><br /><br />This is maybe the most comprehensible answer ...<br /><br />From <a href=\"https://www.reddit.com/r/askscience/comments/2rjj3k/if_gravity_propagates_at_the_speed_of_light_we/\" class=\"ot-anchor\">https://www.reddit.com/r/askscience/comments/2rjj3k/if_gravity_propagates_at_the_speed_of_light_we/</a><br /><br /><i>&quot;&gt; If gravity propagates at the speed of light...</i><br /><br /><i>No, this is wrong. Gravitational &quot;changes&quot; travel at the speed of light. </i><b><i>A static gravitational field does not travel at any speed, since it is static by definition. It just sits there.</i></b><br /><br /><i>&gt; we orbit where the sun used to be, not where it is.</i><br /><br /><i>This is also wrong. </i><b><i>The gravitational field created by our Sun is effectively static, so that the Earth orbits where the Sun is currently at and not where it used to be. It&#39;s not that the gravity travels instantaneously, but that since the field is static, the current field configuration is the same as the past field configuration. Therefore, mathematically, a static field acts as if it is instantaneous. As the Earth moves through this field, it samples different points in the field, but the field itself is not changing.&quot;</i></b><br /><br />Aberration and the Speed of Gravity at <a href=\"http://arxiv.org/pdf/gr-qc/9909087.pdf\" class=\"ot-anchor\">http://arxiv.org/pdf/gr-qc/9909087.pdf</a><br /><br /><i>&quot;... The second term in this expression is essentially a linear extrapolation from the retarded direction [in] toward the &#39;instantaneous&#39; direction ... Now, however, there are additional corrections of higher order in v. It is not hard to show that the effect of these corrections is to further &#39;extrapolate&#39; from the retarded position toward the &#39;instantaneous&#39; position ... In other words, the gravitational acceleration is directed toward the retarded position of the source quadratically extrapolated toward its &#39;instantaneous&#39; position, up to small nonlinear terms and corrections of higher order in velocities.&quot;</i><br /><br />On the gravitational field of a moving body: redesigning general relativity (Eric Baird, 26 October 2015) at <a href=\"http://www.relativitybook.com/papers/On_the_gravitational_field_of_a_moving_body,_Eric_Baird_%282015%29.pdf\" class=\"ot-anchor\">http://www.relativitybook.com/papers/On_the_gravitational_field_of_a_moving_body,_Eric_Baird_(2015).pdf</a><br /><br /><i>&quot;The subject does seem to have been addressed in peer-reviewed papers comparatively recently (Carlip, 2000 15), with the conclusion that in general [there] should be no measurable first-order gravitational aberration effect in gravitational physics, because of cancellation with additional GEMVEL-like effects (&#39;aberration in general relativity is almost exactly canceled by velocity-dependent interactions.&#39;). Carlip argues that the absence of the effect is not arbitrary but is a result of solid calculations, best explained by making an analogy to a similar effect that appears under electromagnetism – we then find ourselves discussing velocity-dependent gravitoelectromagnetic effects whose magnitude is not insignificant, whose presence has detectable consequences, which play an integral part in gravitational physics, and whose presence appears to be essential and unavoidable ...</i><br /><br /><i>&quot;It is tempting to assume that if these velocity-dependent effects really are necessary to a consistent gravitational theory, then they must of course already be fully-integrated components of GR1916, and therefore cannot be in conflict with other parts of the 1916 theory. However, we will see in section 6 that these effects, though apparently unavoidable (Carlip: &#39;This cancellation is dictated by conservation laws and the quadrupole nature of gravitational radiation&#39;, &#39;any Lorentz-invariant model of gravitation necessarily requires additional velocity-dependent interactions, which can provide &#39;a more or less perfect compensation&#39; for the effects of aberration&#39;), do not appear to have been taken into account in the 1916 theory&#39;s design, and might be fundamentally incompatible with SR-based physics.&quot;</i><br /><br />Let&#39;s revisit Van Flandern to provide some additional context ...<br /><br /><a href=\"http://www.ldolphin.org/vanFlandern/\" class=\"ot-anchor\">http://www.ldolphin.org/vanFlandern/</a><br /><br /><i>Rethinking Relativity</i><br /><i>by Tom Bethell</i><br /><br /><i>No one has paid attention yet, but a well-respected physics journal just published an article whose conclusion, if generally accepted, will undermine the foundations of modern physics -- Einstein&#39;s theory of relativity in particular. Published in Physics Letters A (December 21, 1998), the article claims that the speed with which the force of gravity propagates must be at least twenty billion times faster than the speed of light. This would contradict the special theory of relativity of 1905, which asserts that nothing can go faster than light. This claim about the special status of the speed of light has become part of the world view of educated laymen in the twentieth century.</i><br /><br /><b><i>Special relativity, as opposed to the general theory (1916), is considered by experts to be above criticism, because it has been confirmed &#39;over and over again.&#39; But several dissident physicists believe that there is a simpler way of looking at the facts, a way that avoids the mind-bending complications of relativity.</i></b><i> Their arguments can be understood by laymen. I wrote about one of these dissidents, Petr Beckmann, over five years ago (TAS, August 1993, and Correspondence, TAS, October 1993). The present article introduces new people and arguments. The subject is important because if special relativity is supplanted, much of twentieth-century physics, including quantum theory, will have to be reconsidered in that light.</i><br /><br /><i>The article in Physics Letters A was written by Tom Van Flandern, a research associate in the physics department at the University of Maryland. He also publishes Meta Research Bulletin, which supports &#39;promising but unpopular alternative ideas in astronomy.&#39; </i><b><i>In the 1990&#39;s, he worked as a special consultant to the Global Positioning System (GPS), a set of satellites whose atomic clocks allow ground observers to determine their position to within about a foot. Van Flandern reports that an intriguing controversy arose before GPS was even launched. Special relativity gave Einsteinians reason to doubt whether it would work at all.</i></b><i> In fact, it works fine. (But more on that later.)</i><br /><br /><b><i>The publication of his article is a breakthrough of sorts. For years, most editors of mainstream physics journals have automatically rejected articles arguing against special relativity. This policy was informally adopted in the wake of the Herbert Dingle controversy.</i></b><i> A professor of science at the University of London, Dingle had written a book popularizing special relativity, but by the 1960&#39;s he had become convinced that it couldn&#39;t be true. So he wrote another book, Science at the Crossroads (1972), contradicting the first. Scientific journals, especially Nature, were bombarded with his (and others&#39;) letters.</i><br /><br /><i>An editor of Physics Letters A promised Van Flandern that reviewers would not be allowed to reject his article simply because it conflicted with received wisdom. Van Flandern begins with the &#39;most amazing thing&#39; he learned as a graduate student of celestial mechanics at Yale: that all gravitational interactions must be taken as instantaneous. At the same time, students were also taught that Einstein&#39;s special relativity proved that nothing could propagate faster than light in a vacuum. </i><b><i>The disagreement &#39;sat there like an irritant,&#39; Van Flandern told me. He determined that one day he would find its resolution.</i></b><i> Today, he thinks that a new interpretation of relativity may be needed.</i><br /><br /><i>The argument that gravity must travel faster than light goes like this. If its speed limit is that of light, there must be an appreciable delay in its action. By the time the Sun&#39;s &quot;pull&quot; reaches us, the Earth will have &quot;moved on&quot; for another 8.3 minutes (the time of light travel). But by then the Sun&#39;s pull on the Earth will not be in the same straight line as the Earth&#39;s pull on the Sun. </i><b><i>The effect of these misaligned forces &#39;would be to double the Earth&#39;s distance from the Sun in 1200 years.&#39; Obviously, this is not happening. The stability of planetary orbits tells us that gravity must propagate much faster than light.</i></b><i> Accepting this reasoning, Isaac Newton assumed that the force of gravity must be instantaneous.</i><br /><br /><i>Astronomical data support this conclusion. We know, for example, that the Earth accelerates toward a point 20 arc-seconds in front of the visible Sun --that is, toward the true, instantaneous direction of the Sun. </i><b><i>Its light comes to us from one direction, its &#39;pull&#39; from a slightly different direction. This implies different propagation speeds for light and gravity.</i></b><br /><br /><b><i>It might seem strange that something so fundamental to our understanding of physics can still be a matter of debate. But that in itself should encourage us to wonder how much we really know about the physical world.</i></b><i> In certain Internet discussion groups, &#39;the most frequently asked question and debated topic is &#39;What is the speed of gravity?&#39;&#39; Van Flandern writes. It is heard less often in the classroom, but only &#39;because many teachers and most textbooks head off the question.&#39; They understand the argument that it must go very fast indeed, but they also have been trained not to let anything exceed Einstein&#39;s speed limit.</i><br /><br /><i>So maybe there is something wrong with special relativity after all.</i><br /><br /><b><i>In The ABC of Relativity (1925), Bertrand Russell said that just as the Copernican system once seemed impossible and now seems obvious, so, one day, Einstein&#39;s relativity theory &#39;will seem easy.&#39; But it remains as &#39;difficult&#39; as ever, not because the math is easy or difficult (special relativity requires only high-school math, general relativity really is difficult), but because elementary logic must be abandoned. &#39;Easy Einstein&#39; books remain baffling to almost all. The sun-centered solar system, on the other hand, has all along been easy to grasp. Nonetheless, special relativity (which deals with motion in a straight line) is thought to be beyond reproach. General relativity (which deals with gravity, and accelerated motion in general) is not regarded with the same awe. Stanford&#39;s Francis Everitt, the director of an experimental test of general relativity due for space-launch next year, has summarized the standing of the two theories in this way: &#39;I would not be at all surprised if Einstein&#39;s general theory of relativity were to break down,&#39; he wrote. &#39;Einstein himself recognized some serious shortcomings in it, and we know on general grounds that it is very difficult to reconcile with other parts of modern physics. With regard to special relativity, on the other hand, I would be much more surprised. The experimental foundations do seem to be much more compelling.&#39; This is the consensus view.</i></b><br /><br /><i>Dissent from special relativity is small and scattered. But it is there, and it is growing. Van Flandern&#39;s article is only the latest manifestation. In 1987, Petr Beckmann, who taught at the University of Colorado, published Einstein Plus Two, pointing out that the observations that led to relativity can be more simply reinterpreted in a way that preserves universal time. The journal he founded, Galilean Electrodynamics, was taken over by Howard Hayden of the University of Connecticut (Physics), and is now edited by Cynthia Kolb Whitney of the Electro-Optics Technology Center at Tufts. Hayden held colloquia on Beckmann&#39;s ideas at several New England universities, but could find no physicist who even tried to put up an argument.</i><br /><br /><i>A brief note on Einstein&#39;s most famous contribution to physics -- the formula that everyone knows. </i><b><i>When they hear that heresy is in the air, some people come to the defense of relativity with this question: &#39;Atom bombs work, don&#39;t they?&#39; They reason as follows: The equation E = mc2 was discovered as a byproduct of Einstein&#39;s (special) theory of relativity. (True.) Relativity, they conclude, is indispensable to our understanding of the way the world works. But that does not follow. Alternative derivations of the famous equation dispense with relativity. One such was provided by Einstein himself in 1946. And it is simpler than the relativistic rigmarole. But few Einstein books or biographies mention the alternative. They admire complexity, and cling to it.</i></b><br /><br /><b><i>Consider Clifford M. Will of Washington University, a leading proponent of relativity today. &#39;It is difficult to imagine life without special relativity,&#39; he says in Was Einstein Right? &#39;Just think of all the phenomena or features of our world in which special relativity plays a role. Atomic energy, both the explosive and the controlled kind. The famous equation E=mc2 tells how mass can be converted into extraordinary amounts of energy.&#39; Note the misleading predicate, &#39;plays a role.&#39; He knows that the stronger claim, &#39;is indispensable,&#39; would be pounced on as inaccurate.</i></b><i> Is there an alternative way of looking at all the facts that supposedly would be orphaned without relativity? Is there a simpler way? A criterion of simplicity has frequently been used as a court of appeal in deciding between theories. If it is made complex enough, the Ptolemaic system can predict planetary positions correctly. But the Sun-centered system is much simpler, and ultimately we prefer it for that reason.</i><br /><br /><b><i>Tom Van Flandern says the problem is that the Einstein experts who have grown accustomed to &#39;Minkowski diagrams and real relativistic thinking&#39; find the alternative of universal time and &#39;Galilean space&#39; actually more puzzling than their own mathematical ingenuities. Once relativists have been thoroughly trained, he says, it&#39;s as difficult for them to rethink the subject in classical terms as it is for laymen to grasp time dilation and space contraction. For laymen, however, and for those physicists who have not specialized in relativity, which is to say the vast majority of physicists, there&#39;s no doubt that the Galilean way is far simpler than the Einsteinian.</i></b><br /><br /><i>Special relativity was first proposed as a way of sidestepping the great difficulty that arose in physics as a result of the Michelson-Morley experiment (1887). Clerk Maxwell had shown that light and radio waves share the same electromagnetic spectrum, differing only in wave length. Sea waves require water, sound waves air, so, it was argued, electromagnetic waves must have their own medium to travel in. It was called the ether. </i><b><i>&#39;There can be no doubt that the interplanetary and interstellar spaces are not empty,&#39; Maxwell wrote, &#39;but are occupied by a material substance or body, which is certainly the largest, and probably the most uniform body of which we have any knowledge.&#39; As today&#39;s dissidents see things, it was Maxwell&#39;s assumption of uniformity that was misleading.</i></b><br /><br /><i>The experiment of Michelson and Morley tried to detect this ether. Since the Earth in its orbital motion must plow through it, an &#39;ether wind&#39; should be detectable, just as a breeze can be felt outside the window of a moving car. Despite repeated attempts, however, no ethereal breeze could be felt. A pattern of interference fringes was supposed to shift when Michelson&#39;s instrument was rotated. But there was no fringe shift.</i><br /><br /><i>Einstein explained this result in radical fashion. There is no need of an ether, he said. And there was no fringe shift because the speed of an approaching light wave is unaffected by the observer&#39;s motion. But if the speed of light always remains the same, time itself would have to slow down, and space contract to just the amount needed to ensure that the one divided by the other -- space divided by time -- always gave the same value: the unvarying speed of light. The formula that achieved this result was quite simple, and mathematically everything worked out nicely and agreed with observation.</i><br /><br /><i>The skeptical, meanwhile, were placated with this formula: &#39;I know it seems odd that time slows down and space contracts when things move, but don&#39;t worry, a measurable effect only occurs at high velocities -- much higher than anything we find in everyday life. So for all practical purposes we can go on thinking in the same old way.&#39; (Meanwhile, space and time have been subordinated to velocity. Get used to it.)</i><br /><br /><i>Now we come to some modern experimental findings. Today we have very accurate clocks, accurate to a billionth of a second a day. The tiny differentials predicted by Einstein are now measurable. And the interesting thing is this: Experiments have shown that atomic clocks really do slow down when they move, and atomic particles really do live longer. Does this mean that time itself slows down? Or is there a simpler explanation?</i><br /><br /><b><i>The dissident physicists I have mentioned disagree about various things, but they are beginning to unite behind this proposition: There really is an ether, in which electromagnetic waves travel, but it is not the all-encompassing, uniform ether proposed by Maxwell. Instead, it corresponds to the gravitational field that all celestial bodies carry about with them. Close to the surface (of sun, planet, or star) the field, or ether, is relatively more dense. As you move out into space it becomes more attenuated.</i></b><i> Beckmann&#39;s Einstein Plus Two introduces this hypothesis, I believe for the first time, and he told me it was first suggested to him in the 1950&#39;s by one of his graduate students, Jiri Pokorny, at the Institute of Radio Engineering and Electronics in Prague. Pokorny later joined the department of physics at Prague&#39;s Charles University, and today is retired. I believe that all the facts that seem to require special or general relativity can be more simply explained by assuming an ether that corresponds to the local gravitational field. Michelson found no &#39;ether wind,&#39; or fringe shift, because of course the Earth&#39;s gravitational field moves forward with the Earth. As for the bending of starlight near the Sun, the confirmation of general relativity that made Einstein world-famous, it is easily explained given a non-uniform light medium. </i><b><i>It is a well known law of physics that wave fronts do change direction when they enter a denser medium. According to Howard Hayden, refracted starlight can be derived this way &#39;with a few lines of high school algebra.&#39; And derived exactly. The tensor calculus and Riemannian geometry of general relativity gives only an approximation. Likewise the &#39;Shapiro Time-Delay,&#39; observed when radar beams pass close to the Sun and bounce back from Mercury. Some may prefer to try to understand all this in terms of the &#39;curvature of space-time,&#39; to use the Einstein formulation (unintelligible to laymen, I believe). But they should know that a far simpler alternative exists.</i></b><br /><br /><i>The advance of the perihelion of Mercury&#39;s orbit, another famous confirmation of general relativity, is worth a closer look. (The perihelion is the point in the orbit closest to a sun.) Graduate theses may one day be written about this peculiar episode in the history of science. In his book, Subtle Is the Lord, Abraham Pais reports that when Einstein saw that his calculations agreed with Mercury&#39;s orbit, &#39;he had the feeling that something actually snapped in him .... This experience was, I believe, by far the strongest emotional experience in Einstein&#39;s scientific life, perhaps in all his life. Nature had spoken to him.&#39; </i><b><i>Fact: The equation that accounted for Mercury&#39;s orbit had been published 17 years earlier, before relativity was invented. The author, Paul Gerber, used the assumption that gravity is not instantaneous, but propagates with the speed of light. After Einstein published his general-relativity derivation, arriving at the same equation, Gerber&#39;s article was reprinted in Annalen der Physik (the journal that had published Einstein&#39;s relativity papers). The editors felt that Einstein should have acknowledged Gerber&#39;s priority. Although Einstein said he had been in the dark, it was pointed out that Gerber&#39;s formula had been published in Mach&#39;s Science of Mechanics, a book that Einstein was known to have studied.</i></b><i> So how did they both arrive at the same formula?</i><br /><br /><i>Tom Van Flandern was convinced that Gerber&#39;s assumption (gravity propagates with the speed of light) was wrong. So he studied the question. </i><b><i>He points out that the formula in question is well known in celestial mechanics. Consequently, it could be used as a &#39;target&#39; for calculations that were intended to arrive at it. He saw that Gerber&#39;s method &#39;made no sense, in terms of the principles of celestial mechanics.&#39; Einstein had also said (in a 1920 newspaper article) that Gerber&#39;s derivation was &#39;wrong through and through.&#39;</i></b><br /><br /><i>So how did Einstein get the same formula? Van Flandern went through his calculations, and found to his amazement that they had &#39;three separate contributions to the perihelion; two of which add, and one of which cancels part of the other two; and you wind up with just the right multiplier.&#39; So he asked a colleague at the University of Maryland, who as a young man had overlapped with Einstein at Princeton&#39;s Institute for Advanced Study, how in his opinion Einstein had arrived at the correct multiplier. This man said it was his impression that, &#39;knowing the answer,&#39; Einstein had &#39;jiggered the arguments until they came out with the right value.&#39;</i><br /><br /><i>If the general relativity method is correct, it ought to apply everywhere, not just in the solar system. But Van Flandern points to a conflict outside it: binary stars with highly unequal masses. </i><b><i>Their orbits behave in ways that the Einstein formula did not predict. &#39;Physicists know about it and shrug their shoulders,&#39; Van Flandern says. They say there must be &#39;something peculiar about these stars, such as an oblateness, or tidal effects.&#39;</i></b><i> Another possibility is that Einstein saw to it that he got the result needed to &#39;explain&#39; Mercury&#39;s orbit, but that it doesn&#39;t apply elsewhere.</i><br /><br /><b><i>The simplest way to understand all this &#39;without going crazy,&#39; Van Flandern says, is to discard Einsteinian relativity and to assume that &#39;there is a light-carrying medium.&#39; When a clock moves through this medium &#39;it takes longer for each electron in the atomic clock to complete its orbit.&#39; Therefore it makes fewer &#39;ticks&#39; in a given time than a stationary clock. Moving clocks slow down, in short, because they are &#39;ploughing through this medium and working more slowly.&#39; It&#39;s not time that slows down. It&#39;s the clocks. All the experiments that supposedly &#39;confirm&#39; special relativity do so because all have been conducted in laboratories on the Earth&#39;s surface, where every single moving particle, or moving atomic clock, is in fact &#39;ploughing through&#39; the Earth&#39;s gravitational field, and therefore slowing down.</i></b><br /><br /><i>Both theories, Einsteinian and local field, would yield the same results.</i><br /><br />(Note that I have removed some of this article so that the subject of Relativity critique can be broken up into a few parts)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"Maxwell's Spinning Cell Model","summary":"Widely Regarded as One of the Greatest Insights in Human History / The Full Story of How James Maxwell Arrived at His Equations is Fantastically Documented by Basil Mahon, But Few Know of It / We Can Learn a Lot About Scientific Modeling from this Story / What Does It Mean that We Might Accept the Validity of the Model, But then Completely Ignore the Process By Which the Model Was Formulated?","image":"https://lh3.googleusercontent.com/-1nqEfam-fYE/VtOoHx8Qi5I/AAAAAAAAHAc/TR7Hn_nPIK4/w3600-h5040/maxwells-spinning-cell-model-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/7rKJvcyzRjP","publishDate":"2016-02-29T02:11:58.489Z","updateDate":"2017-01-03T02:49:12.684Z","text":"<b>Maxwell&#39;s Spinning Cell Model: Widely Regarded as One of the Greatest Insights in Human History / The Full Story of How James Maxwell Arrived at His Equations is Fantastically Documented by Basil Mahon, But Few Know of It / We Can Learn a Lot About Scientific Modeling from this Story / What Does It Mean that We Might Accept the Validity of the Model, But then Completely Ignore the Process By Which the Model Was Formulated?</b><br /><br /><i>&quot;In his Cambridge paper On Faraday’s Lines of Force he had found a way of representing the lines of force mathematically as continuous fields, and had made a start towards forming a set of equations governing the way electrical and magnetic fields interact with one another. This was unfinished business and he now felt ready to make a serious attempt to settle it. He had made progress so far by using the analogy of a swirling body of incompressible fluid – the pressure in the fluid corresponding to electric or magnetic potential and the direction and speed of flow represented the direction and strength of either an electric or a magnetic field. By extending the imagery, so that, for example, sources and sinks of fluid represented electrically charged surfaces, he had been able to derive all the important formulae for static electricity and magnetism. He had also managed to bring steady electric currents and their effects into the scheme by using the fluid analogy in a different role to represent the flow of electricity.</i><br /><br /><i>The analogy had served well but could take him no further because it worked only when electrical and magnetic fields were static and electric currents steady. </i><b><i>As soon as anything changed, the fields acted in a way that was nothing like the smooth flow of a fluid; in fact their behavior was completely different from that of any known physical process. So to go further he had to find a new approach.</i></b><br /><br /><i>Two courses seemed to be open. One was to desert Faraday and fields, and assume that all effects result from action at a distance between magnetic poles and electrical charges or currents. This was the approach taken by Simeon-Denis Poisson and Andre-Marie Ampere, who had derived the original formulae for static fields and steady currents which James had re-derived by the field approach. It was also the basis of an attempt at a complete theory by Wilhelm Weber, which was mathematically elegant and offered an explanation for most of the known effects. But Weber had made a critical assumption – that the force between two electrical charges depends not only on their distance apart but also on their relative velocity and acceleration along the straight line joining them. James respected Weber’s work but his intuition bridled at this assumption and, more generally, at the whole action at a distance concept.</i><br /><br /><b><i>He therefore chose the second route, which was to go beyond geometrical analogy and make an imaginary mechanical model of the combined electromagnetic field – a mechanism that would behave like the real field. If he could devise a suitable model, the equations governing its operation would also apply to the real field.</i></b><br /><br /><i>As we have seen, all the known experimental results in electricity and magnetism could be attributed to four types of effect; to gain the day, James’ model would have to account for all of them:</i><br /><br /><i>1.\tForces between electrical charges: unlike charges attract; like charges repel, both with a force inversely proportional to the square of the distance between the charges.</i><br /><br /><i>2.\tForces between magnetic poles: unlike poles attract; like poles repel, both with a force inversely proportional to the square of the distance between them; poles always occur in north/south pairs.</i><br /><br /><i>3.\tA current in a wire creates a circular magnetic field around the wire, its direction depending on that of the current.</i><br /><br /><i>4.\tA changing total magnetic field, or flux, through a loop of wire induces a current in the wire, its direction depending on whether the flux is increasing or decreasing.</i><br /><br /><i>And it would need to do so precisely, so that all the established formulae involving electric charge and current, magnetic pole strength, distances and so on could be derived from the model, together with any new formulae.</i><br /><br /><i>James began with effect 2, magnetic forces. </i><b><i>For his model he needed to envisage a medium filling all space which would account for magnetic attraction and repulsion.</i></b><i> To do this, it would need to develop tension along magnetic lines of force and exert pressure at right angles to them – the stronger the field, the greater the tension and the pressure. </i><b><i>And to serve its purpose as a model the imaginary medium would have to be built from components which bore some resemblance to everyday objects.</i></b><br /><br /><i>It seemed an impossible task, but James’ idea was amazingly simple. </i><b><i>Suppose all space were filled by tiny close-packed spherical cells of very low but finite density, and that these cells could rotate. When a cell rotated, centrifugal force would make it tend to expand around the middle and contract along the spin axis, just as the earth’s rotation causes it to expand at the equator and flatten at the poles. Each spinning cell would try to expand around the middle but its neighbors would press back, resisting the expansion. If all the cells in a neighborhood spun in the same direction, each would push outwards against the others; they would collectively exert a pressure at right angles to their axes of spin.</i></b><br /><br /><i>Along the axes of spin the opposite would happen. The cells would be trying to contract in this direction and there would be a tension. So if the spin axes were aligned along lines in space, these lines would behave like Faraday’s lines of force, exerting an attraction along their length and a repulsion sideways. The faster the cells spun, the greater would be the attractive force along the lines and the repulsive force at right angles to them – in other words, the stronger the magnetic field.</i><br /><br /><i>So the field would act along the spin axes of the cells. But which way? Magnetic force is conventionally defined as acting from north to south pole. James built an extra convention into his scheme: the sense of the field would depend on which way the cells were spinning – it would be in the direction a right-handed screw would move if it rotated the same way; if the cells reversed their spin, the field would reverse too.</i><br /><br /><b><i>But if the cells occupied all space, why were they not apparent? And how could they exist in the same space as ordinary matter? James was not put off by such awkward questions. It was, after all, only a model. The cells’ mass density could be so low as to offer no perceptible obstruction to ordinary matter. As long as they had some mass and rotated fast enough they would generate the necessary forces.</i></b><br /><br /><i>The scheme did not yet explain how different materials could have different magnetic characteristics. For example, iron and nickel had a high magnetic susceptibility – they could be readily magnetized – whereas other substances, like wood, seemed to be even less receptive to magnetism than empty space. James solved this problem with his customary sureness of touch. Where cells occupied the same space as an ordinary substance their behavior would be modified according to the magnetic susceptibility of the substance. The modification was equivalent, in mechanical terms, to a change in the mass density of the cells. In iron, for example, they would become much more dense than in air or empty space, thereby increasing the centrifugal forces, and hence the magnetic flux density, for a given rate of spin.</i><br /><br /><i>Here was the basis of a model. The spin axes of the cells gave the direction of the magnetic field at any point in space: their density and rate of spin determined its strength, and the model provided exactly the right equations for effect 2, magnetic forces in static situations.</i><br /><br /><i>So far so good. But there were two problems. First, what set the cells in motion? And second, the cells along one line would be spinning in the same direction as those in neighboring lines, so that where two surfaces made contact they would be moving in opposite directions, rubbing awkwardly against one another. Amazingly, James solved both problems with a single stroke.</i><br /><br /><i>To avoid the cells rubbing against one another, he tried putting even smaller spherical particles between the cells. They would act like ball bearings, or like the ‘idle wheels’ engineers put between two gear wheels which need to rotate in the same direction. The idea seemed crazy but James persevered and suddenly things began to fall into place. Suppose the little idle wheels were particles of electricity. In the presence of an electromotive force they would tend to move along the channels between the cells, constituting an electric current, and it would be this movement that set the cells spinning.</i><br /><br /><i>But everyone knew that currents could flow only in substances which were conductors, like metals. In insulators like glass or mica, or in empty space, there could be no currents. So James proposed a second way in which the behavior of the cells would be modified according to the type of substance which shared their space. In an insulator the cells, or perhaps local groups of cells, would hold on to their little particles so that they could rotate but not move bodily. But in a good conductor like a copper wire the particles could move bodily with very little restriction and a current would flow. In general, the lower the electrical resistance of the substance, the more freely the particles could move.</i><br /><br /><i>An essential feature of James’ little particles was that they had rolling contact with the cells – there was no sliding. Where the magnetic field was uniform the particles would just rotate, along with the cells. But if the particles in a conductor moved bodily without rotating, they would cause the cells on either side of the current to spin in opposite directions, exactly the condition to create a circular magnetic field around a current-carrying wire – effect 3. If the particles rotated and moved, the circular magnetic field due to their movement would be superimposed on the linear one due to their rotation.</i><br /><br /><b><i>So, by this extraordinary assemblage of tiny spinning cells interspersed with even smaller ‘idle wheel’ particles, James had succeeded in explaining two of the four main properties of electricity and magnetism.</i></b><i> A highly satisfactory start, but there was much more to do. The next task was to explain effect 4: a changing magnetic flux through a loop of wire induces a current in the wire. James chose to explain an equivalent effect – that when a current is switched on in one circuit, it induces a pulse of current in a nearby but separate circuit by creating a magnetic field that links the two. </i><b><i>He drew a diagram to illustrate his argument, giving the spherical cells a hexagonal cross-section ‘purely for artistic reasons’. We can see it, slightly adapted for our purpose, in Figures 2a-d.</i></b><br /><br /><i>The diagram shows a cross-section of a small region of space. The idle wheel particles along AB are in a wire which is part of a circuit with a battery and a switch, initially open. Those along PQ are in another wire which is part of a separate circuit having no battery or switch. The idle wheels along AB and PQ are free to move because they are in conductors, but others in the neighborhood are in a non-conducting material and can only rotate in their fixed positions. AB and PQ are, of course, impossibly thin wires and impossibly close together, but this is to just keep the diagram compact; the argument James produced would apply equally well to normal-sized and normally spaced wires containing many rows of idle wheels and cells. The argument runs like this.</i><br /><br /><i>Suppose the magnetic field is zero at first, and the switch open, so that all the cells and idle wheels are stationary (Figure 2a). When the battery is brought into the circuit by closing the switch, the idle wheels along AB move bodily from left to right without rotating. This causes the rows of cells on either side of AB to rotate in opposite directions, thus creating a circular magnetic field around the wire. The idle wheels in PQ are now pinched between rotating cells on the AB side and stationary ones on the other, so they start to rotate (clockwise) and also to move from right to left, the opposite direction from those in AB (Figure 2b).</i><br /><br /><i>But the circuit containing the wire along PQ has some resistance (all circuits do), so the idle wheels there, after their initial surge, will slow down, causing the cells above PQ to begin rotating anticlockwise. Soon, the sideways movement of the idle wheels will stop, although they will continue to rotate. By this time the cells above PQ will be rotating at the same rate as those in the row below PQ (Figure 2c).</i><br /><br /><i>When the switch is opened again, disconnecting the battery, the idle wheels along AB stop moving and the rows of cells on either side of AB stop rotating. The idle wheels in PQ are now pinched between stationary cells on the AB side and rotating ones on the other, so they start to move from left to right, the same direction as the original AB current (Figure 2d).</i><br /><br /><i>Once again the resistance of the circuit containing PQ causes the idle wheels there to slow down. This time, when their sideways movement stops they will not be rotating: we are back to the state represented in Figure 2a.</i><br /><br /><i>Thus, switching on a steady current in AB induces a pulse of current in PQ in the opposite direction and switching the current off induces another pulse in PQ, this time in the same direction as the original current. More generally, any change of current in the AB circuit induces a current in the separate PQ circuit through the changing magnetic field that links them. Equivalently, any change in the amount of magnetic flux passing through a loop of wire induces a current in the loop – effect 4 is explained. If the battery in the AB circuit were replaced by an a.c. generator, the alternating AB current would induce an alternating current in PQ. This is exactly the way transformers work in our electrical power supply systems.</i><br /><br /><i>And here, at last, was a mechanical analogy for Faraday’s electrotonic state. It was the effect at any point in the field of the angular momentum of the spinning cells. Like a flywheel, the cells would act as a store of energy, reacting with a counterforce to resist any change in their rotation; this took the form of an electromotive force which would drive a current if a conductor was present.</i><br /><br /><b><i>James had now explained three of the four effects.</i></b><i> He had not yet found a way of using the model to account for effect 1, forces between electric charges, commonly called electrostatic forces. But he wrote up the results with full mathematical rigor in a paper called On Physical Lines of Force, which was published in the Philosophical Magazine in monthly installments: Part1 appeared in March 1861 and Part 2 was spread over April and May. </i><b><i>Not wanting to be misunderstood, he was at pains to point out that his bizarre arrangement of whirling cells and idle wheels was merely a model:</i></b><br /><br /><b><i>&#39;The conception of a particle having its motion connected with that of a vortex by perfect rolling contact may appear somewhat awkward. I do not bring it forward as a mode of connexion existing in nature, or even as that which I would willingly assent to as an electrical hypothesis. It is, however, a mode of connexion which is mechanically conceivable, and easily investigated, and it serves to bring out the actual mechanical connexions between the known electromagnetic phenomena; so that I venture to say that anyone who understands the provisional and temporary nature of this hypothesis, will find himself rather helped than hindered by it in his search after the true interpretation of the phenomena.&#39;</i></b><br /><br /><i>He had failed to achieve a full theory and it was with a feeling of disappointment that he and Katherine left London to spend the summer vacation at Glenlair. But it was good to get back to the easy rhythm of country life, with its concerns over crops, plantations and farm animals, and to the fresh Galloway air. He had not planned to do any serious work on electricity and magnetism during the summer and took no reference books. But his thoughts ran on and an idea began to crystallize.</i><br /><br /><i>It seemed a small idea at first. The material making up his little cells had to transmit the twisting forces internally so that each cell would rotate as a body. To do this without dissipating energy the material needed to have a degree of springiness, or elasticity. The idea grew. Could this elasticity be the source of the forces between electric charges which he had so far been unable to explain?</i><br /><br /><i>In conductors currents could flow because the electrical particles, the idle wheels, were free to move bodily in response to an electromotive force. Continuous currents could not flow in insulators because the particles were bound to their neighboring cells. But elastic cells would distort, allowing the particles to move a short distance. The distortion in the cells would cause a restoring force, like a spring. The particles would move until the spring-back force balanced the electromotive force.</i><br /><br /><i>So, for example, if a battery were connected by metal wires across two metal plates separated by an insulating material, there would be a small displacement of the electrical particles in the insulator away from one plate and towards the other. This small movement was, in effect, a brief electric current. The movement of electricity would be the same all round the circuit so that in the wires, where particles were not bound to cells, the same brief current would flow. This would result in a surplus of particles on the surface of one plate and a shortage on the surface of the other, so one plate would become positively charged and the other negatively. The distorted cells in the insulating material between the charged metal plates would act like a wound-up spring, exerting a mechanical force of attraction between them. So the elasticity of the cells explained the force between the charged plates.</i><br /><br /><i>Even when the battery was disconnected the spring would stay wound up, storing energy. If the plates were then connected by a metal wire, the spring’s energy would be released: a brief current would flow in the wire, the charges on the plates would fall back to zero and the cells and idle wheels would return to their rest positions.</i><br /><br /><i>Following his earlier thinking, James proposed that the elasticity, or spring-stiffness, of the cells would be modified if they were sharing their space with ordinary matter, and that the degree of modification would depend on the type of matter. The higher the electrical susceptibility of the substance, the softer the spring and the greater the electrical displacement for a given electromotive force. For example, filling the space between the metal plates with mica rather than air would soften the spring and increase the amount of charge on the plates for a given voltage across them.</i><br /><br /><b><i>He wrote up the mathematics and everything fitted together. James had shown how the electrical and magnetic forces which we experience could have their seat not in physical objects like magnets and wires but in energy stored in the space between and around the bodies. Electrostatic energy was potential energy, like that of a spring: magnetic energy was rotational, like that in a flywheel, and both could exist in empty space. And these two forms of energy were immutably linked: a change in one was always accompanied by a change in the other. The model demonstrated how they acted together to produce all known electromagnetic phenomena.</i></b><br /><br /><i>A triumph. But there was more. The model predicted two extraordinary and entirely new physical phenomena which took physics into undreamt-of territory.</i><br /><br /><i>One was that there could be electric currents anywhere, even in perfect insulators or in empty space. According to the model, as we have seen, there would be a little twitch of current whenever an electromotive force was first applied to an insulator, because the electrical idle wheels would move slightly before being halted by the spring-back force of their parent cells. </i><b><i>In the model all space is filled with cells, so these twitches of current would occur even in empty space.</i></b><br /><br /><i>This new type of current would arise whenever the electric field changed. Its value, at any point, would depend on the rate of change of the electric field at that point. In fact, it was simply the rate of displacement of electricity due to the small movement of the particles. James asserted that it was in every way equivalent to an ordinary current. He gave it the name ‘displacement current’.</i><br /><br /><i>The equations governing electrical and magnetic effects had hitherto just dealt with the ordinary conduction current. In James’ new theory, the displacement current had to be added in. When this was done the system of equations was transformed from a motley collection into a beautifully coherent set. This was not immediately evident, however, even to James; he had seen something even more interesting.</i><br /><br /><b><i>All materials that have elasticity transmit waves. James’ all-pervading collection of cells was elastic, so it must be capable of carrying waves. In an insulating material, or in empty space, a twitch in one row of idle wheels would be transmitted via their parent cells to the surrounding rows of idle wheels, then to the rows surrounding them, and so on. Because the cells have inertia they would not transmit the motion instantly but only after a short delay – the twitch would spread out as a ripple. So any change in the electric field would send a wave through all space.</i></b><br /><br /><i>What is more, any twitch in a row of idle wheels would make the neighboring cells turn a bit and so generate a twitch in the magnetic field along the cells’ axes of spin. All changes in the electric field would therefore be accompanied by corresponding changes in the magnetic field, and vice versa. The waves would transmit changes in both fields; they were electromagnetic waves.</i><br /><br /><b><i>What kind of waves were they? Waves in the sea or along a rope are called ‘transverse’ because the individual particles of a sea or rope move at right angles to the direction of the wave. Waves like sound are called ‘longitudinal’ or ‘compression’ waves because the particles move back and forth along the same line as the wave. James’ electromagnetic waves are clearly transverse because the changing electric and magnetic fields were both at right angles to the direction of the wave.</i></b><br /><br /><i>James felt he was on the verge of a great discovery. Light waves were known to be transverse. Could light consist of waves of the kind his model predicted? The speed of light was known with reasonable accuracy from experiments and astronomical observations. It was also well known that the speed of waves in any elastic medium is given by the square root of the ratio of the medium’s elasticity to its density. In the model, the elasticity of the cells controlled the electrostatic (spring-back) forces and their density the magnetic (centrifugal) forces. James’ calculations showed that the spring stiffness of his cells in empty space was not completely determined: it could vary over a factor of three. But if he set it at the lowest value in this range – equivalent to assuming that the cell material was a hypothetically perfect solid – a remarkable result would follow. The wave speed in empty space, or in air, would then be exactly equal to the ratio of the electromagnetic and electrostatic units of electric charge . It seemed impossible that such a simple and natural result could be wrong, so James confidently set the elasticity of his cells to fit it.</i><br /><br /><i>He now had a very simple formula for the speed of his waves. To check their speed against that of light he first needed to look up the result of an experiment by Wilhelm Weber and his colleague Rudolf Kohlrausch. They had measured the ratio of the electrodynamic and electrostatic units of charge; electrodynamic units are closely related to electromagnetic ones, so James would easily be able to convert their result to give the value for the ratio he needed. He also needed to look up the exact values for the experimentally measured speed of light. But he had brought no reference books; this would have to wait until he got back to London in October. The summer passed in a glow of anticipation.</i><br /><br /><i>He had left for Glenlair disappointed at having failed to produce a complete theory of electromagnetism. He returned to London not only with a complete theory but with two entirely new predictions, displacement current and waves. Moreover, the waves might turn out to include light. He eagerly looked up Weber and Kohlrausch’s experimental result and from it worked the speed of his predicted waves. In empty space or air they would travel at 310,740 kilometers per second. Armand-Hippolyte-Louis Fizeau had measured the speed of light in air as 314,850 kilometers per second.</i><br /><br /><b><i>The correspondence seemed too close for coincidence, even allowing for a possible error of a few percent in each of the experimental results. Light must consist of electromagnetic waves. Some of the greatest leaps in science have come when two sets of apparently different phenomena are explained by a single new theory. This was one such leap: at a stroke, he had united the old science of optics with the much newer one of electromagnetism.</i></b><br /><br /><i>James had not expected to extend his paper On Physical Lines of Force beyond Parts 1 and 2 but now he set about writing Part 3, which covered electrostatics, displacement current and waves, and Part 4, which used his model to explain why polarized light waves change their plane of vibration when they pass through a strong magnetic field – an effect discovered experimentally by Faraday. Even for one with the calmest of temperaments it must have been an exhilarating time. The two further parts of the paper were published early in 1862. In Part 3, James announced:</i><br /><br /><b><i>&#39;We can scarcely avoid the inference that light consists in the transverse undulations of the same medium which is the cause of electrical and magnetic phenomena.&#39;</i></b><br /><br /><b><i>The idea of a medium, an ‘aether’ pervading all space, was far from new. Physicists of the day believed an aether of some kind was necessary to transmit light waves,</i></b><i> so one might have expected ready acceptance of James’ application of the principle to electricity and magnetism. But there were misgivings; the reaction of his friend Cecil Monro was typical:</i><br /><br /><i>&#39;The coincidence between the observed velocity of light and your calculated velocity of a transverse vibration in your medium seems a brilliant result. But I must say I think a few such results are needed before you can get peple to think that every time an electric current is produced a little file of particles is squeezed along between two rows of wheels.&#39;</i><br /><br /><i>The difficulty lay deep in the scientific thinking of the time. People believed that all physical phenomena resulted from mechanical action and that all would be clear to us if, and only if, we could discover the true mechanisms. </i><b><i>With a century and a half of hindsight we can see the spinning cell model as a crucial bridge between old and new ideas – built from old materials but paving the way for a completely new type of theory, one which admits that we may never understand the detailed workings of nature. One cannot blame James’ contemporaries for seeing things differently. To many of them the model was simply an ingenious but flawed attempt to portray the true mechanism, for which the search would continue.</i></b><br /><br /><i>James himself was not entirely content with the model, but for different reasons. </i><b><i>He wanted to free the theory if possible from all speculative assumptions about the actual mechanism by which electromagnetism works. He was to achieve this wish 2 years later by taking an entirely new approach. Scientific historians now look upon his spinning cells paper as one of the most remarkable ever written but hold the one that followed to be greater still.&quot;</i></b> (p95-110, <i>The Man Who Changed Everything: The Life of James Clerk Maxwell</i>, Basil Mahon, Chapter 7: “Spinning Cells, London 1860 – 1862”)<br /><br />For a more thorough treatment, consider also reading ...<br /><br />(1) Chapter 5: “Blue and Yellow Make Pink, Cambridge 1854 – 1856,” pages 59 – 65, which covers the fluid model which preceded the spinning cells.<br /><br />(2) And then, read Chapter 8: “The Beautiful Equations, London 1862 – 1865”, pages 114 – 127, which covers his creation of the mathematical form.<br /><br />My purpose with this particular excerpt is simply to show the process by which Maxwell formulated his equations.  Mahon&#39;s level of detail provides us with an exceptional view of how the process unfolded.<br /><br />But, also, I think this story provides some valuable insight into the notion of aether.  How can Maxwell&#39;s Equations constitute an accurate description of electromagnetism if there is no medium present?  What does it mean that we might accept the validity of the model, but then <b>completely</b> ignore the process by which the model was formulated?  These questions seem to me to go unasked largely because the equations are today commonly taught without any explanation of how Maxwell arrived at them.<br /><br /><b><i>&quot;Some accounts of the theory’s origin make no mention of the spinning cell model, or dismiss it as a makeshift contrivance which became irrelevant as soon as the dynamical theory appeared. In doing so they wrongly present Maxwell as a coldly cerebral mathematical genius. One can hardly dispute the epithet ‘genius’, but his thoughts were firmly rooted in the everyday physical world that all of us experience. The keystone of his beautiful theory, the displacement current, had its origin in the idea that the spinning cells in his construction-kit model could be springy.</i></b><br /><br /><i>James published A Dynamical Theory of the Electromagnetic Field in seven parts and introduced it at a presentation to the Royal Society in December 1864. Most of his contemporaries were bemused. It was almost as if Einstein had popped out of a time machine to tell them about general relativity; they simply did not know what to make of it. </i><b><i>Some thought that abandoning the mechanical model was a backward step;</i></b><i> among these was William Thomson, who, for all his brilliance, never came close to understanding Maxwell’s theory.</i><br /><br /><i>One can understand these reactions. Not only was the theory ahead of its time but James was no evangelist and hedged his presentation with philosophical caution. He thought that his theory was probably right but could not be sure. No-one could until Heinrich Hertz produced and detected electromagnetic waves over 20 years later. The ‘great guns’ had been paraded but it would be a long while before they sounded.</i><br /><br /><b><i>It is almost impossible to overstate the importance of James’ achievement. The fact that its significance was but dimly recognized at the time makes it all the more remarkable. The theory encapsulated some of the most fundamental characteristics of the universe. Not only did it explain all known electromagnetic phenomena, it explained light and pointed to the existence of kinds of radiation not then dreamt of. Professor R. V. Jones was doing no more than representing the common opinion of later scientists when he described the theory as one of the greatest leaps ever achieved in human thought.&quot;</i></b> (p126, <i>The Man Who Changed Everything: The Life of James Clerk Maxwell</i>, Basil Mahon)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"Dingle's Paradox","summary":"Herbert Dingle was a 20th Century Physicist, Philosopher and Historian of Science / Highly Regarded as One of the Foremost Philosophers of Science in Britain / Elected President of the Royal Astronomical Society / Best Known for His Criticism of the Special Theory of Relativity / People Cannot Even Agree on Whether or Not He Agreed with His Critics","image":"https://lh6.googleusercontent.com/-G1ahA2DqXmw/VtI1FpCUJhI/AAAAAAAAG-o/wK_-5thVs8E/dingles-paradox-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/fK3ZPDqZbrF","publishDate":"2016-02-27T23:46:10.640Z","updateDate":"2017-01-03T02:49:25.881Z","text":"<b>Dingle&#39;s Paradox: Herbert Dingle was a 20th Century Physicist, Philosopher and Historian of Science / Highly Regarded as One of the Foremost Philosophers of Science in Britain / Elected President of the Royal Astronomical Society / Best Known for His Criticism of the Special Theory of Relativity / People Cannot Even Agree on Whether or Not He Agreed with His Critics</b><br /><br /><a href=\"http://blog.hasslberger.com/docs/Science%20at%20the%20Crossroads%20revised%202015.pdf\" class=\"ot-anchor\">http://blog.hasslberger.com/docs/Science%20at%20the%20Crossroads%20revised%202015.pdf</a><br /><br /><i>&quot;Now it is this latter kind of reasoning that — according to the argument outlined in the Introduction, to which I can get no answer and which seems to me plainly unanswerable — invalidates the special theory of relativity. The problem here is to find the relation between the rates of two exactly similar standard clocks, A and B, of which one is moving uniformly with respect to the other, on the assumption that the motion is indeed truly relative, i.e. that there is no justification for ascribing it to one rather than to the other. Now this is a problem that can be solved mathematically, and we find that there are two solutions, known technically as the &#39;Galilean transformation&#39; and the &#39;Lorentz transformation&#39;. According to the first the clocks work at the same rate, and according to the second they work at different rates. The special theory of relativity regards the second as true and the first as false; the usual expression is that &#39;a moving clock runs slow&#39;. But, as we have said, it is a condition of the problem that either clock can be regarded as the &#39;moving&#39; one, so this second solution (subject, of course, to the truth of the postulate that the motion is truly relative) requires equally that A works faster than B and that B works faster than A, and just as we know enough about measuring rods to know that they cannot read √(-3) - 1, so we know enough about clocks to know that one cannot work steadily both faster and slower than another. Hence, </i><b><i>without in the least rejecting the Lorentz transformation as a mathematical solution of the problem, we can say at once that it is not a possible physical solution. Nevertheless, in modern physics it is universally assumed to be so, on the sole ground of its mathematical validity.&quot;</i></b><br /><br /><i>&quot;How such an obvious error could have occurred and escaped immediate recognition is explained in Part Two, but </i><b><i>it may be said at once that the apparently simplest way of exposing it — by setting two clocks in relative motion and observing their rates — is impracticable because the difference which the theory requires is too small to be detected except at velocities far too high to be yet attainable.</i></b><i> Experiments have been made in which elementary electrically charged particles (conceptual bodies, such as electrons, protons, etc.) have been used instead of clocks, and observations of what have been regarded as their &#39;rates&#39; have been made, and these have shown that such &#39;rates&#39; differ for particles which, according to electromagnetic theory, have vastly different velocities. These observations have been held to constitute an experimental proof that the Lorentz transformation is a physically valid solution of our problem. But there are two reasons why this argument fails. In the first place, even if it be fully granted, it shows only that one &#39;clock&#39; works more slowly than the other — which would be quite possible if the motion of each was absolute, as Lorentz showed before Einstein&#39;s special relativity theory appeared. If the motion is relative, however, and the Lorentz transformation is a valid solution, then also the second &#39;clock&#39; must work more slowly than the first — and this, it need hardly be said, has been left unproved. The second reason for the failure of the argument is that the interpretation of the particles as &#39;clocks&#39; and of the observed phenomena as their &#39;rates&#39;, and the assumption that they move with velocities, ascribed to them (it is, of course, quite impossible to observe them; their existence and properties have all to be inferred on theoretical grounds) depend on the truth of a theory that itself depends on the truth of the Lorentz transformation (this is explained in Part Two), so </i><b><i>the argument is circular: the observation proves the physical truth of the Lorentz transformation only if we first accept a theory which itself requires that transformation to be physically true.&quot;</i></b> (p18-19, <i>Science at the Crossroads</i>, Herbert Dingle)<br /><br /><i>&quot;The question therefore arises: how does one determine, consistently with the theory, which clock works the more slowly? Unless this question is answerable, the theory unavoidably requires that A works more slowly than B and B more slowly than A -- which it requires no super-intelligence to see is impossible. Now, clearly, a theory that requires an impossibility cannot be true, and scientific integrity requires, therefore, either that the question just posed shall be answered, or else that the theory shall be acknowledged to be false. But, as I have said, more than 13 years of continuous effort have failed to produce either response ...</i><br /><br /><i>It cannot be too strongly emphasised that this question is exactly what it appears to be, with every word and phrase bearing its ordinary, generally understood, meaning; </i><b><i>it is not a profoundly complicated question, artificially simplified to bring it within the scope of the non-scientific reader&#39;s intelligence. It is presented here in its full scientific reality, and the ordinary reader is as fully competent to understand whether a proffered answer is in fact an answer or an evasion as is the most learned physicist or mathematician</i></b><i> — though, of course, he may not be able to judge whether the suggested answer is true or not. For instance, the statement: &#39;the slower-running clock is that judged by a chosen body of experts to be the more beautiful&#39; would be an answer, though it is not likely to be acceptable to anyone. On the other hand, the statement: &#39;I cast my vote for the special theory of relativity and the abandonment of Dingle&#39;s concept of clocks because the latter is equivalent to Newton&#39;s concept of absolute time, and relativistic physics appears to me to represent nature more closely than Newtonian physics does&#39; (sec p. 77 for the fuller statement from which this is taken), </i><b><i>which is the conclusion reached by one generally considered to be among the most authoritative mathematical experts on relativity, can be seen by anyone to be no answer at all, but a clear evasion of the question. Who can gather from this how to tell which clock works the more slowly?</i></b><i> The question is by-passed, and the reader is led into a slough of metaphysical concepts which have nothing whatever to do with it. Nevertheless, the statement serves to confirm the experimenters&#39; conviction that the matter is beyond their understanding but has been competently dealt with by an expert authority, so they need give it no further attention.</i><br /><br /><i>This is typical of all responses to the criticism that have yet appeared: I choose it here because of the outstanding reputation of its author in this field and the fact that it can be expressed more briefly than most — far more briefly, for instance, than the equally evasive and far denser obscurity (given here in the Appendix) that &#39;convinced&#39; the then President of the Royal Society that what he had been &#39;teaching&#39; for many years but confessed he did not understand, was indeed true ...</i><br /><br /><i>It is no doubt generally believed that means exist for preventing the occurrence of such a situation as this, and theoretically, of course, they do. The Royal Society is a body whose function includes the safeguarding of scientific integrity in all matters, and especially those vital to public welfare in this country (the situation is of general significance, of course, but for reasons of space I deal in this book almost wholly with Britain), and accordingly, after great difficulty in overcoming the interposed obstacles, the criticism was submitted to it for consideration. It was rejected on the basis of a report from an anonymous &#39;specialist&#39; that the fallacy invalidating it was too elementary even to be instructive. The &#39;fallacy&#39;, however, was not revealed, nor was the simple but crucial question answered, but the customary paragraphs of mystical comment were supplied, and these satisfied the Society that the criticism was baseless. A letter to the leading scientific journal, Nature, asking, in the public interest and in accordance with the principles of the Society, that the fallacy should be published, was refused publication, on the ground that actions of the Royal Society were not open to question in Nature. An attempt was made to obtain a ruling of the Press Council (one of whose functions is &#39;to keep under review developments likely to restrict the supply of information of public interest and importance&#39;) on this refusal of Nature ... but the officers of the Council would not allow the inquiry to reach it.&quot;</i> (p7-8, <i>Science at the Crossroads</i>, Herbert Dingle)<br /><br />From Wikipedia at <a href=\"https://en.wikipedia.org/wiki/Herbert_Dingle\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Herbert_Dingle</a>, <b>note the bolded claim which quite curiously does not appear with any actual reference!</b> ...<br /><br /><i>&quot;Dingle participated in two highly public and polemical disputes ... The second dispute began in the late 1950s, following Dingle&#39;s retirement and centered on the theory of special relativity.[8] [9] Initially Dingle argued that, contrary to the usual understanding of the famous twin paradox, special relativity did not predict unequal aging of twins, one of whom makes a high-speed voyage and returns to Earth, </i><b><i>but he then came to realize and acknowledge that his understanding had been mistaken.</i></b><i> He then began to argue that special relativity was empirically wrong in its predictions, although experimental evidence showed he was mistaken about this.[10] Ultimately Dingle re-focused his criticism to claim that special relativity was logically inconsistent: &#39;The theory [special relativity] unavoidably requires that A works more slowly than B and B more slowly than A --which it requires no super-intelligence to see is impossible.&#39; [11] Hence he asserted that the well-known reciprocity of the Lorentz transformation is self-evidently impossible.[12] As Whitrow explained in his review of &#39;Science at the Crossroads&#39;, this is not correct.[1][13]</i><br /><br /><i>Dingle carried on a highly public and contentious campaign to get this conclusion accepted by the scientific community, mostly through letters to the editors of various scientific periodicals, including Nature. Dozens of scientists responded with answers to Dingle&#39;s claims, explaining why the reciprocity of the Lorentz transformation does not entail any logical inconsistency,[14] but Dingle rejected all the explanations.[15] This culminated in his 1972 book, Science at the Crossroads in which Dingle stated that &#39;a proof that Einstein&#39;s special theory of relativity is false has been advanced; and ignored, evaded, suppressed and, indeed, treated in every possible way except that of answering it, by the whole scientific world.&#39; He also warned: &#39;Since this theory is basic to practically all physical experiments, the consequences if it is false, modern atomic experiments being what they are, may be immeasurably calamitous.&#39;[16] The consensus in the physics community is that Dingle&#39;s objections to the logical consistency of special relativity were unfounded.[1][17][18][19]&quot;</i><br /><br />Now, read the following completely opposite (and fantastically written) recounting of the controversy by Harry Hamlin Ricker, and decide for yourself: Is the Wikipedia entry actually offering an encyclopedic accounting of the controversy?  What would be the point of leaving out so much detail?<br /><br /><a href=\"http://www.naturalphilosophy.org/site/harryricker/2015/05/25/herbert-dingle-maverick-scientist-and-anti-einstein-philosopher-of-science/\" class=\"ot-anchor\">http://www.naturalphilosophy.org/site/harryricker/2015/05/25/herbert-dingle-maverick-scientist-and-anti-einstein-philosopher-of-science/</a> (I&#39;ve done some minor editing for typos)<br /><br /><i>&quot;In 1955 Dingle precipitated the most famous of his controversies when he objected to a statement made in a book written by George Thompson The Foreseeable Future regarding the famous twins paradox. The ensuing controversy was one of the most famous disputes in 20th century physics. The controversy caused Dingle to investigate the mathematical and physical foundations of the special theory of relativity and this caused him to doubt its validity. Eventually he discovered mathematical demonstrations which he interpreted as proof of flaws in or inconsistencies in the theory. The debate and discussion of these eventually led him to disown relativity as a valid scientific theory and produced his long campaign to establish his refutation of relativity as scientifically valid, which remained his main goal for the rest of his life.</i><br /><br /><b><i>What Dingle discovered in this controversy is different from what the establishment critics of Dingle say about it. They claim that he was wrong, but that claim can not, and has not, been proved,</i></b><i> despite the fact that they claim it is not true. Dingle asserted that there must be a flaw in the theory. </i><b><i>This claim was mitigated by an additional assertion on his part that the mathematics was correct. This has been the source of considerable confusion and controversy. Clearly Dingle did not mean that all the mathematics was correct, because he used the established mathematics to present a logical contradiction. What he meant was that assuming that the mathematics was correct, we deduce a logical contradiction — Dingle politely called this an inconsistency – which must be the result of a flaw within the theory. According to Dingle this flaw was within the logical structure and not within the mathematical structure.</i></b><br /><br /><i>To illustrate this, consider Dingle’s 1962 letter to Nature which claimed an inconsistency. In this letter, </i><b><i>Dingle showed that by the method used by Einstein, it was just as valid to conclude that moving clocks run fast as to conclude that they run slow.</i></b><i> Although Dingle called it an inconsistency, it was really an logical or mathematical contradiction, similar to the other inconsistencies which were labeled as paradoxes in the theory of relativity. Succinctly put, Dingle had discovered another paradox. But it was essentially the same as the clock paradox, which had been discovered much earlier, but it was a more precise statement of it.</i><br /><br /><i>To understand this clock paradox problem, and how the Dingle paradox were related, consider the following. The clock paradox arises from [Einstein’s] 1905 paper where he states that if a synchronized clock is moved from some location to another one in the same reference frame, its motion causes it to [lag] behind the clocks ... in the same frame that do not move. This conclusion, however, implies a contradiction of the principle of relativity, which asserts an equivalence of reference frames. Hence, it is not logically possible to say that one clock was the one that moved and not the opposite one in the other frame. Hence, it was not possible [to] say which clock, if any, actually lagged in time after being moved. </i><b><i>Dingle’s paradox asserted that given any clock being moved, [it] was not possible to say whether the motion caused it to lag behind or to accelerate ahead of the rest clock to which it is compared.</i></b><br /><br /><i>Dingle said that this dilemma had a simple solution. Neither clock was changed by the motion, hence there would be no difference in their readings after the motion, if there was no difference before being moved. This simple answer should have been recognized as the correct solution, but instead was claimed to be wrong. Primarily because of the recourse to experimental evidence which seemed to imply that the moving clock actually ran slow and not fast so Dingle seemed to be clearly wrong. </i><b><i>But this answer didn’t resolve Dingle’s paradox, it just preempted it as a perceived problem within the theory</i></b><i> ...</i><br /><br /><b><i>What Dingle exposed, and the reward for this was eternal damnation in the eyes of the science establishment, was that relativity was unable to prove whether its claim that moving clocks run slow is true within the mathematical and logical structure of the theory, without recourse to experiment. This point has not been carefully studied, because most scientists incorrectly assume that Dingle has been proved wrong. But there is no proof of this, just hearsay ... Dingle showed that the relativistic claim is ambiguous. It is just as valid to claim that moving clocks run fast as to claim that they run slow. No proof can be produced that resolves this difficulty. Relativity is ambiguous ... Dingle demanded an answer, but none was produced, [so] that claim is essentially unproven.&quot;</i></b><br /><br />There are today technical rebuttals to Dingle which the reader should decide for themselves whether or not they properly rebut his claims:<br /><br /><a href=\"http://physics.stackexchange.com/questions/155015/a-sees-bs-clock-running-slow-and-b-sees-as-clock-running-slow\" class=\"ot-anchor\">http://physics.stackexchange.com/questions/155015/a-sees-bs-clock-running-slow-and-b-sees-as-clock-running-slow</a><br /><br />What I recommend, as a general rule, is that if you do not understand something, keep an open mind.  Resist the temptation to come to a conclusion; keep it a controversy.  Keep looking for an explanation which makes sense, but do not commit to an idea which you do not personally understand.  If there is the potential that we might learn from competing ideas, then it&#39;s in our own interest to keep an open mind.<br /><br />That would be the constructivist approach to Relativity, and it should be apparent that when Dingle proposed his inconsistency, there was no Google there to help the experts coalesce to an immediate answer.  And part of this story which makes it worth knowing is the appearance that Dingle&#39;s questions initially elicited evasions and low quality responses amongst the experts.  It suggests that they did not themselves understand the idea very well.  That obviously cannot rule the idea out, but it sharply contrasts with the insistence by people, broadly, that Relativity is a fact.  It&#39;s a sign that there may be psychology and sociology at play, and it invites us to gauge the public&#39;s conceptual understanding of Relativity using the Force Concept Inventory (see <a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/HV5W3xBr1AQ\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/HV5W3xBr1AQ</a> for info on that).<br /><br />From <a href=\"http://www.ldolphin.org/vanFlandern/\" class=\"ot-anchor\">http://www.ldolphin.org/vanFlandern/</a><br /><br /><i>&quot;Dingle&#39;s Question:</i><br /><br /><i>University of London Professor Herbert Dingle showed why special relativity will always conflict with logic, no matter when we first learn it. According to the theory, if two observers are equipped with clocks, and one moves in relation to the other, the moving clock runs slower than the non-moving clock. But the relativity principle itself (an integral part of the theory) makes the claim that if one thing is moving in a straight line in relation to another, either one is entitled to be regarded as moving. It follows that if there are two clocks, A and B, and one of them is moved, clock A runs slower than B, and clock B runs slower than A. Which is absurd.</i><br /><br /><i>Dingle&#39;s Question was this: Which clock runs slow? Physicists could not agree on an answer. As the debate raged on, a Canadian physicist wrote to Nature in July 1973: </i><b><i>&#39;Maybe the time has come for all of those who want to answer to get together and to come up with one official answer. Otherwise the plain man, when he hears of this matter, may exercise his right to remark that when the experts disagree they cannot all be right, but they can all be wrong.&#39;</i></b><br /><br /><b><i>The problem has not gone away. Alan Lightman of MIT offers an unsatisfactory solution in his Great Ideas in Physics (1992). &#39;[T]he fact that each observer sees the other clock ticking more slowly than his own clock does not lead to a contradiction. A contradiction could arise only if the two clocks could be put back together side by side at two different times.&#39; But clocks in constant relative motion in a straight line &#39;can be brought together only once, at the moment they pass.&#39; So the theory is protected from its own internal logic by the impossibility of putting it to a test. Can such a theory be said to be scientific? --TB&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"The Village Venus Effect","summary":"If a Boy Has Lived His Whole Life in a Remote Village, the Village Venus Must be the Most Beautiful Girl in the World / At Least Until He Goes Into Town for the First Time / Edward de Bono Uses the Analogy to Refer to the Strict Adherence to a Belief in Science w/o Thorough Investigation Into Rival Explanations","image":"https://lh3.googleusercontent.com/-_btaCY7PvQ4/VtHj3Z7DpnI/AAAAAAAAG-I/niqL63vwfjY/w1800-h2520/village-venus-effect-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/cToYviP8nEX","publishDate":"2016-02-27T18:17:01.695Z","updateDate":"2017-01-03T02:49:42.175Z","text":"<b>The Village Venus Effect: If a Boy Has Lived His Whole Life in a Remote Village, the Village Venus Must be the Most Beautiful Girl in the World / At Least Until He Goes Into Town for the First Time / Edward de Bono Uses the Analogy to Refer to the Strict Adherence to a Belief in Science w/o Thorough Investigation Into Rival Explanations</b><br /><br />I could be mistaken, but I am under the impression this quote was lifted from one of Charles Ginenthal&#39;s books ...<br /><br /><i>&quot;Evan W. Mackie makes the thinking of scientists clear on the issue of preconception in science and academia.</i><br /><br /><i>&#39;Dr. Edward de Bono&#39;s book on practical thinking ... makes interesting reading for archaeologists, and indeed for those concerned with the problems of interpretation in the historical sciences, with the aid of an ingenious experiment, he analyzes the way the human mind works and identified &#39;five ways to be wrong,&#39; &#39;four ways to be right,&#39; and &#39;five ways to understand.&#39; Among the ways to be right -- </i><b><i>which means ways in which one can convince oneself one is right</i></b><i> -- is what he calls the &#39;village Venus,&#39; or &#39;unique rightness&#39; method, </i><b><i>a mental process which he believes to be particularly common among scientists and academics. If one has lived one&#39;s whole life in a remote village, cut off from contact with other people, the village Venus must be the most beautiful girl in the world because one cannot imagine anyone more beautiful. In the same way a scientist or scholar who cannot imagine, or who has not heard of any explanation which will fit a given body of evidence, as well as the one he has thought of (or, one might add, has been taught), is capable of being fully convinced of its unique rightness. Consciously he tells himself, and believes, that it is right because it fits all the facts; but actually its rightness derives solely from the lack of rival explanations.&#39;&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"The Language of Mathematics","summary":"No Different from Ordinary Language, Not Everything that Can Be Said With Mathematics is Necessarily True / There is Still the Burden to Connect Equations to the Physical World","image":"https://lh3.googleusercontent.com/-j8vMppFACNs/VtBzMW_Q9_I/AAAAAAAAG9M/kCR0Z7g3ezE/w1800-h2520/the-language-of-mathematics-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/4Lbz5TsJrT7","publishDate":"2016-02-26T16:05:33.354Z","updateDate":"2017-01-03T02:50:04.063Z","text":"<b>The Language of Mathematics: No Different from Ordinary Language, Not Everything that Can Be Said With Mathematics is Necessarily True / There is Still the Burden to Connect Equations to the Physical World</b><br /><br />From Herbert Dingle&#39;s <i>Science at the Crossroads</i>, whose Wikipedia entry is plainly written with the obvious intention of making sure that you reject everything he has ever said ...<br /><br /><a href=\"http://blog.hasslberger.com/docs/Science%20at%20the%20Crossroads%20revised%202015.pdf\" class=\"ot-anchor\">http://blog.hasslberger.com/docs/Science%20at%20the%20Crossroads%20revised%202015.pdf</a><br /><br /><i>&quot;It was particularly Galileo who realised that mathematics provided the most effective terms in which to express physical observations, and it was he who contributed most to the introduction of those terms into science. The book of nature, he wrote, &#39;is written in the mathematical language&#39;. But there are two things that should be said about this oft-quoted aphorism. The first is that &#39;nature&#39;, or &#39;the universe&#39;, as Galileo conceived it was a much more restricted concept than that which we hold and that with which modern science is concerned. It comprised only what we study in mechanics; all other phenomena — sights, sounds, smells, etc. — belonged in his view not to the external world but to the observing subject, and it was not at all his idea that mathematics played the all-comprehensive role in science that it is nowadays often assumed to do. </i><b><i>Secondly, a language is a medium for expressing ideas, and it is just as capable of expressing false ideas as true ones. The fact, therefore, that something can be expressed with rigorous mathematical exactitude tells you nothing at all about its truth, i.e. about its relation to nature, or to what we can experience.</i></b><br /><br /><i>The most dangerous intellectual error of modern science ... lies in the fact that this has been overlooked. Mathematics is an immensely more powerful tool than the Aristotelian syllogism, and </i><b><i>its use as a language in which to express the facts of experience has been so successful that the idea has crept unperceived into the minds of physicists that whatever it says must be true. This is openly expressed in the statement already quoted, that everything that is not mathematically forbidden is necessarily observable. Accordingly the habit has developed of assuming that a physical theory is necessarily sound if its mathematics is impeccable: the question whether there is anything in nature corresponding to that impeccable mathematics is not regarded as a question; it is taken for granted.</i></b><br /><br /><b><i>The fact is, however, that mathematical truths are far more general than physical truths: that is to say, the symbols that compose a mathematical expression may, with equal mathematical correctness, correspond both to that which is observable and that which is purely imaginary or even unimaginable. If, therefore, we start with a mathematical expression, and infer that there must be something in nature corresponding to it, we do in principle just what the pre-scientific philosophers did when they assumed that nature must obey their axioms,</i></b><i> but its immensely greater power for both good and evil makes the consequences of its misapplication immensely more serious.</i><br /><br /><i>There are so many instances, even in the most elementary uses of mathematics, in which its indications are obviously false, that it may seem strange that this fact is almost automatically overlooked in the more advanced uses of the tool. But there is a universal tendency, not only in science but in everyday life as well, to pay exaggerated attention to predictions that are realised and to ignore those that are not ...</i><br /><br /><i>Suppose we want to find the number of men required for a certain job under certain conditions. Every schoolboy knows such problems, and he knows that he must begin by saying: &#39;Let x = the number of men required.&#39; But that substitution introduces a whole range of possibilities that the nature of the original problem excludes. The mathematical symbol, x, can be positive, negative, integral, fractional, irrational, imaginary, complex, zero, infinite, and whatever else the fertile brain of the mathematician may devise. The number of men, however, must be simply positive and integral. Consequently, when you say, &#39;Let x = the number of men required,&#39; you are making a quite invalid substitution, and the result of the calculation, though entirely possible for the symbol, might be quite impossible for the men.</i><br /><br /><i>Every elementary algebra book contains such problems that lead to quadratic equations, and these have two solutions, which might be 8 and -3, say. We accept 8 as the answer and ignore - 3 because we know from experience that there are no such things as negative men, and the only alternative interpretation — that we could get the work done by subtracting three men from our gang — is obviously absurd. But what right have we to reject -3? Clearly, none at all if we accept the substitution: &#39;Let x = the number of men required.&#39; If we have proved that 8 is the answer, then with the same inevitability we have proved that -3 is the answer; and if we have not proved that -3 is the answer, then we have not proved that 8 is the answer. The two solutions stand or fall together as soon as we allow mathematical symbols to represent facts of experience. Yet the inexorable fact is that one answer is true and the other false.</i><br /><br /><i>Now in this example </i><b><i>it is experience alone that distinguishes the true from the false solution.</i></b><i> We cannot prove by pure reason that there cannot be creatures who, with regard to the qualities here considered, can be interpreted as negative men; we know from experience alone that they are as unreal as centaurs. If the problem had been one concerning charges of electricity, of which there are two kinds which we call positive and negative, it might have led to the same equation, and then both solutions would in all probability have been true. There is nothing intrinsically impossible in the existence of negative men, any more than in the existence of black swans: experience alone enables us to reject the solution -3 as false.</i><br /><br /><i>But it is possible to obtain perfectly valid mathematical solutions of a problem which we can see without experience to be physically false because the physical interpretation requires what can be seen without experience to be impossible. Here is an example. Suppose we have a cubical vessel whose volume is 8 cubic feet, and we wish to find the length of one of its edges. Now physically what we are asking is the reading of a standard measuring rod when it is placed along the edge. But suppose there is no such rod handy. That does not matter, for we can solve the problem by mathematics. We let x be the required length, and all we have to do is to solve the equation, x3 = 8. But this equation has three solutions, viz. 2, √( - 3) - 1, - √( -3) - 1 — all having the same mathematical validity. But we know that the only one of these solutions that can possibly correspond to the reading of a measuring rod is 2, because of the necessary properties of measuring rods, which we should understand even if we had never made or seen one. We might one day discover negative men, but we cannot conceivably discover a standard measuring rod that can read √(-3) - 1 because, owing to the accepted standards of measurement, such an object would not be a measuring rod. So we just ignore two of the mathematical solutions, and quite overlook the significance of that fact — namely, that </i><b><i>in the language of mathematics we can tell lies as well as truths, and within the scope of mathematics itself there is no possible way of telling one from the other. We can distinguish them only by experience or by reasoning outside the mathematics, applied to the possible relation between the mathematical solution and its supposed physical correlate.&quot;</i></b> (p15-18, <i>Science at the Crossroads</i>, Herbert Dingle)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"Electron Theory as Worldview","summary":"Attempts to Explain Gravitation in Terms of Electromagnetic Interactions Date Back to the 1830s / Henri Poincaré, a Polymath, Theoretical Physicist, Engineer and Philosopher of Science, Was an Important Contributor / No Satisfactory Solution Was Found Prior to the Rise in Popularity of Relativity / Yet, the Idea Never Fully Died Out","image":"https://lh3.googleusercontent.com/-LNApzKL5Bpo/Vs8sLNn-hII/AAAAAAAAG7Q/oqwHr78G2lw/electron-theory-as-worldview-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/TdfCc7ERzDJ","publishDate":"2016-02-25T16:37:38.779Z","updateDate":"2017-01-03T02:50:27.112Z","text":"<b>Electron Theory as Worldview: Attempts to Explain Gravitation in Terms of Electromagnetic Interactions Date Back to the 1830s / Henri Poincaré, a Polymath, Theoretical Physicist, Engineer and Philosopher of Science, Was an Important Contributor / No Satisfactory Solution Was Found Prior to the Rise in Popularity of Relativity / Yet, the Idea Never Fully Died Out</b><br /><br />The picture shows Wal Thornhill&#39;s hypothesis for gravity as a side-effect of electromagnetism.  The idea couches, at least in part, from analogy with the Van der Waals force, which is a localized force that results from the collective arrangement of water molecules in certain instances.<br /><br />More info on Wal&#39;s hypothesis at these links:<br /><br /><a href=\"http://www.holoscience.com/wp/electric-gravity-in-an-electric-universe/\" class=\"ot-anchor\">http://www.holoscience.com/wp/electric-gravity-in-an-electric-universe/</a><br /><br /><a href=\"http://www.holoscience.com/wp/newtons-electric-clockwork-solar-system/\" class=\"ot-anchor\">http://www.holoscience.com/wp/newtons-electric-clockwork-solar-system/</a><br /><br />Few people today appear to realize that there was a strong push in the early 1900s -- which dated all the way back to the 1830s -- to create a new scientific framework which positioned gravity as a consequence of electromagnetic interactions.  Wal&#39;s more modern attempt to do the same follows in this longstanding tradition, but the public is today left with the impression by science journalists and others that there are no ideas worth pursuing which can compare to Relativity.<br /><br />What is curious about the situation is that most of the public, if put to the test, cannot actually explain Relativity.  Their conviction rests almost entirely upon the pronouncements of authorities and claims of Relativity&#39;s validations through experimentation -- <b>which tend to vindicate Relativity&#39;s underlying mathematics, but not Einstein&#39;s claim that space becomes bent by the presence of matter.</b>  Alternatively, theorists sometimes point to observations of gravitational lensing as vindication of Relativity, but simple observation of lensing cannot prove that gravity bends space itself.<br /><br />From <i>Quantum Generations: A History of Physics in the Twentieth Century</i>, Helge Kragh<br /><br /><a href=\"https://books.google.com/books?id=ELrFDIldlawC&amp;pg=PA108&amp;lpg=PA108&amp;dq=%22By+1904+the+electromagnetic+view+of+the+world+had+taken+off%22&amp;source=bl&amp;ots=9HtrACwtS1&amp;sig=xv_mFvvHyPOjW5Q4YyxL7B1KMK0&amp;hl=en&amp;sa=X&amp;ei=Sg0UVbWyNNb9oQSlq4HwBg&amp;ved=0CB4Q6AEwAA#v=onepage&amp;q&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=ELrFDIldlawC&amp;pg=PA108&amp;lpg=PA108&amp;dq=%22By+1904+the+electromagnetic+view+of+the+world+had+taken+off%22&amp;source=bl&amp;ots=9HtrACwtS1&amp;sig=xv_mFvvHyPOjW5Q4YyxL7B1KMK0&amp;hl=en&amp;sa=X&amp;ei=Sg0UVbWyNNb9oQSlq4HwBg&amp;ved=0CB4Q6AEwAA#v=onepage&amp;q&amp;f=false</a><br /><br /><b><i>&quot;ELECTRON THEORY AS A WORLDVIEW</i></b><br /><br /><b><i>By 1904 the electromagnetic view of the world had taken off and emerged as a highly attractive substitute for the mechanical view that was widely seen as outdated, materialistic, and primitive. As an indication of the strength of the new theory, it was not only discussed in specialized journals, but also began to appear in physics textbooks.</i></b><i> For example, Bucherer introduced hls electron theory in a 1904 textbook. Of more importance was the textbook in electrodynamics that Abraham published the same year and which, in its several editions, became widely used both in Germany and abroad during more than twenty years. The work was a revision of a textbook on Maxwell&#39;s theory written by August Foppl in 1894 (used by, among others, the young Einstein), but whereas Foppl had given a mechanical derivation of Maxwell&#39;s equations, Abraham used his revised version to reverse Foppl&#39;s priority between mechanics and electromagnetism. In a companion volume of 1905, Abraham came out without much reserve as a missionary for the electromagnetic worldview.</i><br /><br /><i>In commemoration of the centenary of the United States&#39; purchase of the Louisiana Territory, a Congress of Arts and Sciences was held in St. Louis in September 1904. </i><b><i>Among the physics delegates were several international leaders of physics, including Rutherford, Poincare, and Boltzmann. The general message of many of the addresses was that physics was at a turning point and that electron theory was on its way to establishing a new paradigm in physics.</i></b><i> In his sweeping survey of problems in mathematical physics, Poincare spoke of the &#39;general ruin of the principles&#39; that characterized the period. </i><b><i>Poincare was himself an important contributor to electron theory and he was now willing to conclude that &#39;the mass of the electrons, or, at least of the negative electrons, is of exclusively electro-dynamic origin ... [T]here is no mass other than electro-dynamic inertia&#39;</i></b><i> (Sopka and Moyer 1986, 292). The address of another French physicist, thirty-two-year-old Paul Langevin, was more detailed, but no less grand, no less eloquent, and no less in favor of the electromagnetic world picture. Langevin argued for his own (and Bucberer&#39;s) model of the electron, but the detailed structure of the electron was not what really mattered. </i><b><i>The important thing was the coming of a new era of physics.</i></b><i> As Langevin explained in his closing words:</i><br /><br /><i>&#39;The rapid perspective which I have just sketched is full of promises, and I believe that rarely in the history of physics has one had the opportunity of looking either so far into the past or so far into the future. The relative importance of parts of this immense and scarcely explored domain appears different to-day from what it did in the preceding century: from the new point of view the various plans arrange themselves in a new order. </i><b><i>The electrical idea, the last discovered, appears to-day to dominate the whole, as the place of choice where the explorer feels he can found a city before advancing into new territories .... The actual tendency, of making the electromagnetic ideas to occupy the preponderating place, is justified, as I have sought to show, by the solidity of the double base on which rests the idea of the electron [the Maxwell equations and the empirical electron]</i></b><i> ... Although still very recent, the conceptions of which I have sought to give a collected idea are about to penetrate to the very heart of the entire physics, and to act as a fertile germ in order to crystallize around it, in a new order, facts very far removed from one another ... </i><b><i>This idea has taken an immense development in the last few years, which causes it to break the framework of the old physics to pieces, and to overturn the established order of ideas and laws in order to branch out again in an organization which one foresees to be simple, harmonious, and fruitful&#39; (ibid., 230)</i></b><br /><br /><b><i>Evaluations similar to Langevin&#39;s, and often using the same code words and imagery, can be found abundantly in the literature around 1905. They rarely included references to quantum theory or the new theory of relativity. The methodology behind the electromagnetic research program was markedly reductionistic. Its aim was to establish a unitary theory of all matter and forces that exist in the world. The basis of the theory was Maxwellian electrodynamics, possibly in some modified or generalized version. It was an enormously ambitious program. When it was completed, nothing would be left unexplained at least in principle. In this sense, it was clearly an example of a &#39;theory of everything.&#39; Elementary particles, atomic and quantum phenomena, and even gravitation were held to be manifestations of that fundamental substratum of the world, the electromagnetic field. Attempts to explain gravitation in terms of electromagnetic interactions went back to the 1830s, when such a theory was proposed by the Italian physicist Ottaviano Mossotti.</i></b><i> Later in the century the idea was developed in mathematical details by the Germans Wilhelm Weber and Friedrich Zollner, who based their theories on the notion of electrical particles in instantaneous interaction. Electrogravitational theories continued to be discussed after field electrodynamics became the dominant framework of electricity and magnetism. For example, in 1900 Lorentz derived, on the basis of his electron theory, a gravitational law that he considered a possible generalization of Newton&#39;s. Attempts to unify the two basic forces of the universe, usually by reducing gravitation to electromagnetism, was part of the electromagnetic program, but in spite of much work, no satisfactory solution was found.&quot;</i><br /><br />(Many thanks to my knowledgeable friend, Don Berk, for pointing this historical reference out.)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23ongoing\">#ongoing</a>  ﻿","category":"ongoing"},{"name":"The Extrinsic Motivator","summary":"Controlling Peoples' Behavior with If-Then Rewards Can Do Long-term Damage to their Intrinsic Drive to Continue the Activity / Can Also Narrow Peoples' Focus, Undermining their Creative Problem-Solving Capabilities / One of the Most Robust Findings in the Social Sciences is Also One of the Most Ignored","image":"https://lh3.googleusercontent.com/-UjYP4zIAnc0/Vsv1gyC07lI/AAAAAAAAG54/Y9JsePNEkIM/w1800-h2520/the-extrinsic-motivator-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/QT56smaxXh4","publishDate":"2016-02-23T06:03:22.784Z","updateDate":"2017-01-03T02:50:55.190Z","text":"<b>The Extrinsic Motivator: Controlling Peoples&#39; Behavior with If-Then Rewards Can Do Long-term Damage to their Intrinsic Drive to Continue the Activity / Can Also Narrow Peoples&#39; Focus, Undermining their Creative Problem-Solving Capabilities / One of the Most Robust Findings in the Social Sciences is Also One of the Most Ignored</b><br /><br />From Daniel Pink&#39;s book, <i>Drive: The Surprising Truth About What Motivates Us</i><br /><br /><i>&quot;Creativity</i><br /><br /><i>For a quick test of problem-solving prowess, few exercises are more useful than the candle problem. Devised by psychologist Karl Duncker in the 1930s, the candle problem is used in a wide variety of experiments in behavioral science. Follow along and see how you do.</i><br /><br /><i>You sit at a table next to a wooden wall and the experimenter gives you the materials shown below: a candle, some tacks, and a book of matches.</i><br /><br /><i>Your job is to attach the candle to the wall so that the wax doesn&#39;t drip on the table. Think for a moment about how you&#39;d solve the problem. Many people begin by trying to tack the candle to the wall. But that doesn&#39;t work. Some light a match, melt the side of the candle, and try to adhere it to the wall. That doesn&#39;t work either. But after five or ten minutes, most people stumble onto the solution, which you can see below.</i><br /><br /><b><i>The key is to overcome what&#39;s called functional fixedness. You look at the box and see only one function as a container for the tacks. But by thinking afresh, you eventually see that the box can have another function as a platform for the candle. To reprise language from the previous chapter, the solution isn&#39;t algorithmic (following a set path) but heuristic (breaking from the path to discover a novel strategy).</i></b><br /><br /><i>What happens when you give people a conceptual challenge like this and offer them rewards for speedy solutions? Sam Glucksberg, a psychologist now at Princeton University, tested this a few decades ago by timing how quickly two groups of participants solved the candle problem. He told the first group that he was timing their work merely to establish norms for how long it typically took someone to complete this sort of puzzle. To the second group he offered incentives. If a participant&#39;s time was among the fastest 25 percent of all the people being tested, that participant would receive $5. If the participant&#39;s time was the fastest of all, the reward would be $20. Adjusted for inflation, those are decent sums of money for a few minutes of effort a nice motivator.</i><br /><br /><b><i>How much faster did the incentivized group come up with a solution? On average, it took them nearly three and a half minutes longer.</i></b><i> Yes, three and a half minutes longer. (Whenever I&#39;ve relayed these results to a group of businesspeople, the reaction is almost always a loud, pained, involuntary gasp.) In direct contravention to the core tenets of Motivation 2.0, an incentive designed to clarify thinking and sharpen creativity ended up clouding thinking and dulling creativity. </i><b><i>Why? Rewards, by their very nature, narrow our focus. That&#39;s helpful when there&#39;s a clear path to a solution. They help us stare ahead and race faster. But if-then motivators are terrible for challenges like the candle problem. As this experiment shows, the rewards narrowed people&#39;s focus and blinkered the wide view that might have allowed them to see new uses for old objects.&quot;</i></b> (p42-44, <i>Drive: The Surprising Truth About What Motivates Us</i>, Daniel Pink)<br /><br /><i>&quot;One of Lepper and Greene&#39;s early studies (which they carried out with a third colleague, Robert Nisbett) has become a classic in the field and </i><b><i>among the most cited articles in the motivation literature. The three researchers watched a classroom of preschoolers for several days and identified the children who chose to spend their &#39;free play&#39; time drawing. Then they fashioned an experiment to test the effect of rewarding an activity these children clearly enjoyed.</i></b><br /><br /><i>The researchers divided the children into three groups. The first was the &#39;expected-award&#39; group. They showed each of these children a &#39;Good Player&#39; certificate -- adorned with a blue ribbon and featuring the child&#39;s name -- and asked if the child wanted to draw in order to receive the award. The second group was the &#39;unexpected-award&#39; group. Researchers asked these children simply if they wanted to draw. If they decided to, when the session ended, the researchers handed each child one of the &#39;Good Player&#39; certificates. The third group was the &#39;no-award&#39; group. Researchers asked these children if they wanted to draw, but neither promised them a certificate at the beginning nor gave them one at the end.</i><br /><br /><i>Two weeks later, back in the classroom, teachers set out paper and markers during the preschool&#39;s free play period while the researchers secretly observed the students. Children previously in the &#39;unexpected-award&#39; and &#39;no-award&#39; groups drew just as much, and with the same relish, as they had before the experiment. </i><b><i>But children in the first group -- the ones who&#39;d expected and then received an award -- showed much less interest and spent much less time drawing. The Sawyer Effect had taken hold. Even two weeks later, those alluring prizes -- so common in classrooms and cubicles -- had turned play into work.</i></b><br /><br /><i>To be clear, it wasn&#39;t necessarily the rewards themselves that dampened the children&#39;s interest. Remember: When children didn&#39;t expect a reward, receiving one had little impact on their intrinsic motivation. </i><b><i>Only contingent rewards -- if you do this, then you&#39;ll get that -- had the negative effect. Why? &#39;If-then&#39; rewards require people to forfeit some of their autonomy. Like the gentlemen driving carriages for money instead of fun, they&#39;re no longer fully controlling their lives. And that can spring a hole in the bottom of their motivational bucket, draining an activity of its enjoyment.</i></b><br /><br /><i>Lepper and Greene replicated these results in several subsequent experiments with children. As time went on, other researchers found similar results with adults. </i><b><i>Over and over again, they discovered that extrinsic rewards -- in particular, contingent, expected, &#39;if-then&#39; rewards -- snuffed out the third drive.</i></b><br /><br /><i>These insights proved so controversial -- after all, they called into question a standard practice of most companies and schools -- that in 1999 Deci and two colleagues reanalyzed nearly three decades of studies on the subject to confirm the findings. &#39;Careful consideration of reward effects reported in 128 experiments lead to the conclusion that tangible rewards tend to have a substantially negative effect on intrinsic motivation,&#39; they determined. </i><b><i>&#39;When institutions -- families, schools, businesses, and athletic teams, for example -- focus on the short-term and opt for controlling people&#39;s behavior,&#39; they do considerable long-term damage.</i></b><br /><br /><i>Try to encourage a kid to learn math by paying her for each work-book page she completes -- and she&#39;ll almost certainly become more diligent in the short term and lose interest in math in the long term. Take an industrial designer who loves his work and try to get him to do better by making his pay contingent on a hit product -- and he&#39;ll almost certainly work like a maniac in the short term, but become less interested in his job in the long term. As one leading behavioral science textbook puts it, &#39;People use rewards expecting to gain the benefit of increasing another person&#39;s motivation and behavior, but in so doing, they often incur the unintentional and hidden cost of undermining that person&#39;s intrinsic motivation toward the activity.&#39;</i><br /><br /><b><i>This is one of the most robust findings in social science and also one of the most ignored.</i></b><i> Despite the work of a few skilled and passionate popularizers -- in particular, Alfie Kohn, whose prescient 1993 book, Punished by Rewards, lays out a devastating indictment of extrinsic incentives -- we persist in trying to motivate people this way.&quot;</i> (p37-39, <i>Drive: The Surprising Truth About What Motivates Us</i>, Daniel Pink)<br /><br /><i>&quot;Behavioral scientists often divide what we do on the job or learn in school into two categoreis: &#39;algorithmic&#39; and &#39;heuristic.&#39; An algorithmic task is one in which you follow a set of established instructions down a single pathway to one conclusion.  That is, there&#39;s an algorithm for solving it.  A heuristic task is the opposite.  Precisely because no algorithm exists for it, you have to experiment wit possibilities and devise a novel solution.  Working as a grocery checkout clerk is mostly algorithmic.  You do pretty much the same thing over and over in a certain way.  Creating an ad campaign is mostly heuristic.  You have to come up with something new.&quot;</i> (p29, <i>Drive: The Surprising Truth About What Motivates Us</i>, Daniel Pink)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"Ausubel's Six Principles of Knowledge Construction","summary":"Published First Paper in 1960, Before Kuhn's Structure of Scientific Revolutions in 1962 / His Ideas Were Discordant with the Prevailing Behaviorist Ideas, Which Obstructed His Publication / Distinguished Meaningful from Rote Learning, a Problem Ignored by Educators to this Day / Ausubel's Six Basic Principles Explain How Knowledge Is Constructed","image":"https://lh3.googleusercontent.com/-BWsRijjmKBw/Vsqov5pY8TI/AAAAAAAAG4g/NivWIa3xl4E/w7125-h9975/david-ausubel-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/UdVWixSTKhy","publishDate":"2016-02-22T06:20:57.263Z","updateDate":"2017-01-03T02:51:32.113Z","text":"<b>Ausubel&#39;s Six Principles of Knowledge Construction: Published First Paper in 1960, Before Kuhn&#39;s Structure of Scientific Revolutions in 1962 / His Ideas Were Discordant with the Prevailing Behaviorist Ideas, Which Obstructed His Publication / Distinguished Meaningful from Rote Learning, a Problem Ignored by Educators to this Day / Ausubel&#39;s Six Basic Principles Explain How Knowledge Is Constructed</b><br /><br /><i>&quot;Our research group first became familiar with Ausubel&#39;s work in 1964 when we began careful study of his Psychology of Meaningful Verbal Learning.  The theory put forward explained many of the difficulties we found in interpreting data we were gathering on student problem solving.  Working initially with an information processing model of learning (Novak, 1958), we assumed that problem solving was a function of two independent traits: knowledge stored in the mind, and information processing capability. </i><b><i>What we found suggested in Ausubel&#39;s theory was that these two processes are confounded in the process of new learning, where integration of new and old knowledge is a function of both the quantity and the quality of cognitive structure organization. This interpretation closely followed the pattern of our research results.&quot;</i></b> (p57, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><b>Progressive Differentiation</b><br /><br /><i>&quot;As meaningful learning proceeds, development and elaboration of subsuming concepts necessarily occurs.  The refinement of concept meanings in cognitive structure giving more precision and specificity to these concepts is called progressive differentiation of cognitive structure ...</i><br /><br /><i>In Ausubel&#39;s view, concept development proceeds best when the most general, most inclusive concepts are introduced first and then these concepts are progressively differentiated in terms of detail and specificity.&quot;</i> (p70, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><i>&quot;Determination of what in a body of knowledge are the most general, most inclusive concepts and what are subordinate concepts is not easy ... good curriculum design requires an analysis first of the concepts in a field of knowledge and second consideration of some relationships among these concepts that can serve to illustrate which concepts are most general and superordinate and which are most specific and subordinate.  One reason school instruction and instruction in corporate training programs has been so ineffective is that curriculum planners rarely sort out the concepts they hope to teach and even more rarely do they try to search for possible hierarchical relationships among these concepts.&quot;</i> (p70, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><b>Subsumption</b><br /><br /><i>&quot;In the course of meaningful learning, new information is linked with concept in cognitive structure.  Usually this linkage occurs when more specific, less inclusive concepts are linked to more general existing concepts in cognitive structure ... A subsuming concept is not a kind of mental fly-paper to which information is stuck; the role of a subsuming concept in meaningful learning is an interactive one, facilitating movement of relevant information through the perceptual barriers and providing a base for linkage between newly perceived information and previously acquired knowledge.  Furthermore, in the course of this linkage, the subsuming concept becomes slightly modified, and the stored information is also altered somewhat.&quot;</i> (p65-66, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><b>Obliterative Subsumption</b><br /><br /><i>&quot;To the extent that conceptual knowledge is subsumed under a broader, more inclusive representation of it, lack of use can result in the more detailed refinement being merged back into the subsumer from which it sprang, becoming indistinguishable from it.&quot;</i> (from <a href=\"http://www.indiana.edu/~idtheory/methods/m6c.html\" class=\"ot-anchor\">http://www.indiana.edu/~idtheory/methods/m6c.html</a>)<br /><br /><i>&quot;Rote learning has one important advantage over meaningful learning; we have already noted that sometimes it is useful to recall knowledge learned in precisely the same form as the original message.  This process is all too frequently required in school testing. </i><b><i>Meaningful learning has three important advantages over rote learning. First, knowledge acquired meaningfully is retained longer -- much, much longer in many instances. Second, subsumed information results in increased differentiation of subsumers, thus adding to the capacity for easier subsequent learning of related materials. Third, information that is forgotten after obliterative subsumption has occurred has left a residual effect on the subsuming concept, thus facilitating new related learning even after forgetting has occurred.</i></b><br /><br /><i>The last point in favor of meaningful forgetting is very important. </i><b><i>Laboratory studies have shown that information learned by rote inhibits subsequent learning of additional similar information. Moreover, even information learned by rote that is forgotten inhibits learning of similar new information.</i></b><i> The reverse effect operates with meaningful forgetting.  While it is true that restudy or relearning of the same information is facilitated by prior retention in both rote and meaningful learning, the &#39;savings&#39; (as psychologists refer to this facilitation) in rote learning is only for relearning of precisely the same material, whereas meaningful learning will result in savings for relearning and facilitation (rather than inhibition) of learning new, similar (relevant to the same subsumer[s]) information.&quot;</i> <br /><br /><b><i>Many students experience the feeling of being snowed under. Usually this feeling becomes most intense six to eight weeks into a course. Some studies (see Howard Hagerman, 1966) indicate that most information learned by rote in schools is lost within six to eight weeks. As a result students recognize that they have forgotten much of the information presented earlier and that their earlier but now lost learning is interfering with new learning. They must force themselves into review and meaningful restudy of earlier materials, cram for hours to overlearn earlier material, or give up hopes of passing the course.</i></b> (p84-85, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br /><b>Superordinate Learning</b><br /><br /><i>&quot;Occasionally in the history of any discipline, or in the life-span of an individual, new concepts are constructed that pull together and integrate large domains of knowledge which were not recognized as intimately related previously.  This was the case when Newton conceived of his law of gravitation that explained not only why objects fall toward the earth, as cannon balls do, but also why planets follow their orbits around the sun.  Newton&#39;s concept of universal gravitation brought together domains of knowledge that most people saw as totally unrelated.&quot;</i> (p75-76, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><b>Integrative Reconciliation</b><br /><br /><i>&quot;Subsumption and progressive differentiation lead to more than quantitative addition of knowledge to a conceptual framework.  There are also qualitative changes in that each of the concepts in the relevant structure are modified in meaning to some extent.  It is evident that as we subsume concepts into a mapped hierarchy, the meanings of all the concepts are modified at least slightly because there are meaningful connections vertically and horizontally across the structure ...</i><br /><br /><i>Another form of cognitive differentiation arises when new interrelationships are seen between concepts in cognitive structure, relationships we can represent as &#39;crosslinks&#39; on a concept map.  These crosslinkages represent what Ausubel and others (1978) described as integrative reconciliation ... There is integration because new concepts and/or relationships are substantively incorporated into the cognitive structure and there is reconciliation when meanings of similarities and/or differences are incorporated into cognitive structure.&quot;</i> (p71-72, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><b>Advance Organizers</b><br /><br /><i>&quot;Ausubel is perhaps best known for his idea of an advance organizer.  In order to help learners bridge the gap between knowledge they already possess and new knowledge to be learned, Ausubel suggested that a small segment of instruction should be offered prior to the larger instructional unit that is more general and more abstract than the material in the larger unit.  This prior instruction can serve as an advance organizer, helping the learner relate new knowledge to knowledge the learner already has.</i> (p79, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><i>&quot;In the four-and-a-half decades during which we have used Ausubel&#39;s assimilation theory to guide our research and instructional innovations, we have not obtained any results that refute or cast doubt upon the major ideas described above.&quot;</i> (p82, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><b>Creativity</b><br /><br /><i>&quot;There are numerous definitions and descriptions of creativity. </i><b><i>My view is that creativity is simply successful integrative reconciliation and/or superordinate learning -- and the emotional desire to do this.</i></b><i> As such, it ranges on a scale from relatively modest &#39;creative insights,&#39; when relatively common integrative reconciliations are formed by a person, to those extraordinary integrative reconciliations and/or superordinate concept constructions that lead to Nobel or Pulitzer prizes.  Everyone of us has some creative capacity (i.e., we make our own unique integrative reconciliations), but only a small fraction of the population appears to have the capacity and emotional drive to make the creative leaps that advance science, music, literature, or other fields of human endeavor.  Koshland (2007), the former Editor of Science, divides creative discovery into three categories: charge, challenge and chance. </i><b><i>Charge discoveries</i></b><i> solve problems that are quite obvious, and Koshland quotes Nobel Laureate Szent-Gyorgyi as saying these discoveries occur when someone &#39;sees what everyone else has seen, and thinks what no one else has thought.&#39;  Einstein&#39;s theory of relativity would be an example here. </i><b><i>Challenge discoveries</i></b><i> occur from the accumulation of facts or concepts, and the discoverer perceives a new concept or theory that pulls these together into one coherent whole.  Darwin&#39;s (1873) theory of evolution would be an example of this. </i><b><i>Chance discoveries</i></b><i> occur when people have what Louis Pasteur called &#39;the prepared mind.&#39;  The discovery of penicillin and x-rays are two examples.  In one way or another, I see all three of these forms of creativity as evidence of some high level of meaningful learning ...</i><br /><br /><i>According to my view, the creative performance of everyone can be enhanced by improving the capacity and desire of people to learn meaningfully.  It is retarded or inhibited by inordinate emphasis and reward for learning by rote. </i><b><i>Since the latter has been so common in most school learning, it is not surprising that the biographies of geniuses often refer pejoratively to their experiences in schools.</i></b><i> Schooling can be changed to encourage, reward and enhance meaningful learning and creativity.&quot;</i> (p82-83, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><b>Intelligence</b><br /><br /><i>&quot;The people who design and administer these tests [e.g., IQ and SAT] are not quacks or charlatans; they really believe in the validity and reliability of their tests.  The reliability of the tests is relatively good; that is, these tests tend to produce similar scores for the same individual on repeated testing. </i><b><i>It is the validity issue that is the problem. When one correlates IQ, SAT, or similar test scores with measures of real-world job performance, the correlations are usually near zero! What IQ tests measure may be useful for creative thinking, but it is not the same aptitude as creativity</i></b><i> (Getzelz and Jackson, 1962; Guilford and Christensen, 1973) ...</i><br /><br /><i>More recently, there is a growing concern with the use of tests for selecting graduate students.  Georgi (1996) reported that at Harvard University, Graduate Record Exam (GRE) scores in physics showed no correlation with graduate course grades.  Moreover, Georgi observed that some of his most brilliant women students did very poorly on the physics GRE test.  In general, women scored 100 points lower (out of a possible 990) than men, thus leading to a serious gender bias against women in selection for graduate studyin physics. </i><b><i>At Cornell University our dean of the graduate school did an informal study of correlation of GRE scores with professors&#39; rating of PhD students. He found a correlation of 0.02, about as close to zero as you can get.&quot;</i></b> (p86, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><b>But, How Does the Process Get Started?</b><br /><br /><i>&quot;The question is frequently asked whenever I attempt to describe Ausubel&#39;s theory briefly.  If meaningful learning is desirable and if it requires relevant concepts (subsumers) in cognitive structure, how does the process start?  The answer is that, in young children, concept formation is the principal process by which concepts are acquired.  This is a type of discovery learning involving hypothesis generation and testing as well as generalization from specific instances.  For example, through repeated encounters with dogs, or chairs, or hot objects, so labeled by older children and adults, the young child gradually discovers the criterial attributes that characterize these concepts, and their language labels. </i><b><i>After a child has acquired one or two thousand concepts (usually recognized as the child&#39;s functional vocabulary) through concept formation, additional differentiation of these concepts and development of new concepts proceeds principally through concept assimilation.</i></b><i> Those who have been around two-year-old children know how repetitious their questions can be as they try to sharpen their understanding of simple concepts and the appropriate language labels. </i><b><i>By school age, most children have an adequate framework of concepts to allow meaningful reception learning to proceed and although concept formation may occasionally occur, most new concepts are acquired through concept assimilation, progressive differentiation and integrative reconciliation.</i></b><i>&quot;</i> (p77-78, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br /><i>&quot;Ausubel (1968, pp. 524-525) distinguishes between primary concepts and secondary concepts.  Primary concepts are formed by young children by directly observing objects or events and recognizing regularity in these in the hypothesis testing phase of concept formation, and subsequent incorporation of concepts into cognitive structure.  Dog, mom, growing, and eating are examples of primary concepts formed by young children.  As the child builds cognitive structure he or she can acquire secondary concepts by the process of concept assimilation. </i><b><i>Here concepts and propositions in the child&#39;s cognitive structure are used to acquire new concept meanings, including concepts that have no visible exemplars such as molecule, love and history. New concepts are acquired by using spoken or written words and propositions that already have meaning for the learner.  By school age almost all concept learning is concept assimilation.&quot;</i></b> (p49, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><i>&quot;When we use a dictionary to acquire the meaning of an unfamiliar concept label (word), we are engaged in at least the first stage of conceptual assimilation.  Unfortunately, sometimes the synonyms or definition given are not familiar and we cannot begin to grasp the meaning of the new concept (word) from the dictionary.  Even if the synonyms or definitions are familiar, however, we have only the first beginning of developing a full, functional meaning for the concept.  Some concepts, such evolution, bureacracy, or capitalism, may undergo growth and changes in meaning over our lifetime. </i><b><i>Concept assimilation for most significant concepts is a process of meaning building that is never finished.&quot;</i></b> (p49, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><i>&quot;Young children are very competent at learning new concepts.  I recall when two of my children were under three, they both referred to a thing adults call umbrellas as </i><b><i>underbrellas</i></b><i>.&quot;</i> (p44, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><i>&quot;Brown and others have recognized the constraints that the context of learning places on concept development, describing what they call </i><b><i>situated cognition</i></b><i> (Brown, Collins &amp; Duguid, 1989; Greeno, 1998).  They argue that &#39;knowledge is situated, being in part a product of activity, context and culture in which it is developed and used&#39; (p.32).  My children&#39;s development of the concept of </i><b><i>underbrella</i></b><i> was a clear case of situated cognition.  The word made sense in terms of the context in which they used it.  In widely differing cultures or contexts, the same word may have substantially different meanings or connotations, sometimes leading to embarrassment of either speaker or listener.&quot;</i> (p50, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"Ignorance","summary":"The True Engine of Science / Its Mishandling Can Be More Costly and Harder to Perceive than the Mishandling of Data / The Public Tends to Focus Upon the Conclusions and Facts Pronounced By Scientists / Yet, Comfortability With Ignorance is an Important Part of What Distinguishes a Student from a Scientist","image":"https://lh3.googleusercontent.com/-Z48y2uBGIZU/VsjkXwrA30I/AAAAAAAAG28/gX68BIy1-PE/w1800-h2520/ignorance-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/L3tT3EbmyKP","publishDate":"2016-02-20T22:11:28.361Z","updateDate":"2017-01-03T02:52:07.138Z","text":"<b>Ignorance: The True Engine of Science / Its Mishandling Can Be More Costly and Harder to Perceive than the Mishandling of Data / The Public Tends to Focus Upon the Conclusions and Facts Pronounced By Scientists / Yet, Comfortability With Ignorance is an Important Part of What Distinguishes a Student from a Scientist</b><br /><br />The now-famous story of the man searching for his car keys under the street lamp teaches us that how we engage our own ignorance will have a significant say in what sort of life we end up living ...<br /><br /><i>&quot;A story many of us tell our graduate students is about a scientist searching the ground under a street lamp late at night.  A man walks up to him and asks him what he has lost.  &#39;My car keys,&#39; says the scientist, and the friendly chap helps him look.  After a while, and no success, the fellow asks whether he is sure this is where he dropped them.  &#39;No, I think probably over there,&#39; he says pointing to a dark portion of the street.  &#39;Then why are you looking here?&#39;  &#39;Well,&#39; says the canny scientist, &#39;the light is so much better here.&#39;  This story is often told in a way to make the seeker seem ridiculous (in fact, in some tellings he is not a scientist but a drunk, or maybe a drunken scientist), but I think it&#39;s just the opposite.  One very decent strategy in science is to look where you have a good chance of finding something, anything.  The lesson here is to recognize the value of the observable and to leave the unmeasurable stuff for later.  Anyway, if you&#39;re drunk it&#39;s better not to find your car keys.&quot;</i> (p64-65, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><b>Stuart Firestein&#39;s Thesis</b><br /><br /><i>&quot;Knowledge is a big subject, says Stuart Firestein, but ignorance is a bigger one.  And it is ignorance -- not knowledge -- that is the true engine of science.</i><br /><br /><i>Most of us have a false impression of science as a surefire, deliberate, step-by-step method for finding things out and getting things done.  In fact, says Firestein, more often than not, science is like looking for a black cat in a dark room, and there may not be a cat in the room.  The process is more hit-or-miss than you might imagine, with much stumbling and groping after phantoms.  But it is exactly this &#39;not knowing,&#39; this puzzling over thorny questions or inexplicable data, that gets researchers into the lab early and keeps them there late, the thing that propels them, the very driving force of science.&quot;</i> (inside jacket of book, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><b>Who Is Stuart Firestein?</b><br /><br />Professor Stuart Firestein teaches Neuroscience and is Chairman of the Department of Biology at Columbia University, where his laboratory investigates the mysteries of the mammalian olfactory system, the sense of smell.  He has published more than 100 papers in scientific and scholarly journals.  Dedicated to promoting the accessibility of science to a public audience, he serves as an advisor for the Alfred P Sloan Foundation&#39;s program for the Public Understanding of Science.  He was awarded the 2011 Lenfest Distinguished Columbia Faculty Award for excellence in scholarship and teaching and was recently elected as a Fellow of the AAAS.<br /><br /><i>&quot;Here then I have decided to use my personal story, my own case history.  It has some unusual elements in it, but they serve to highlight many of the decisive factors, both intended and serendipitous, common to the creation stories of scientific careers.  And there is certainly more than enough ignorance woven through it to provide a lesson or two.</i><br /><br /><i>So here it is: Stuart Firestein, Professor of Neuroscience at the Department of Biological Sciences, Columbia University, New York.</i><br /><br /><i>I came to science late, after a career in, of all things, the theater, where I spent more than 15 years working professionally as a stage manager and director and had the opportunity to run my own repertory company.  Although now there are university programs for training in the theater, the traditional path then was to apprentice yourself to professionals and learn by hanging around with accomplished artists.  You started as a stage worker, setting up scenery and lighting equipment, attending production planning meetings, working the lights, or moving scenery at performances in the evening.  You then graduated, if you desired and were any good, to assistant stage manager, an especially interesting position because you attended all the rehearsals, but the position had no set of assigned responsibilities.  You simple did whatever came up, from getting coffee to copying scripts to organizing rehearsal props, to whatever.  The nice thing about this position was that you spent a lot of time at the rehearsals, involved in the production but not burdened with responsibilities, and so you had the opportunity to see how things work -- and just as important how they don&#39;t work.  You could watch actors work out scenes and develop character business while directors tried to find the right staging and develop an ensemble feel.  You were part of the production but also had a critical distance that allowed you to learn while doing.  It&#39;s a system, I realize, not so different from the way we train graudate students in our laboratories, and it is one that I recommend highly. </i><b><i>It is not so easy to create the condition where you can have perspective and involvement simultaneously, where you can be invested but not fully responsible, immersed yet without the pressures of liability. But having such an environment seems fundamental to a mentoring process that allows us to explore questions. This is one lesson from this case history.&quot;</i></b> (p152-153, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><b>Ignorance as a Core Concept in &#39;Citizen Science&#39;</b><br /><br /><b><i>&quot;Today, however, we find ourselves in a situation where science is as inaccessible to the public as if it were written in classical Latin. The citizenry is largely cut off from the primary activity of science and at best gets secondhand translations from an interposed media. Remarkable new findings are trumpeted in the press, but how they came about, what they may mean beyond a cure or new recreational technology, is rarely part of the story. The result is that the public rightly sees science as a huge fact book, an insurmountable mount of information recorded in a virtually secret language.&quot;</i></b> (p171, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><b><i>&quot;Clearly what we need is a crash course in citizen science -- a way to humanize science so that it can be both appreciated and judged by an informed citizenry. Aggregating facts is useless if you don&#39;t have a context to interpret them, and this is even true for most scientists when faced with information outside their particular field of expertise.</i></b><br /><br /><b><i>I believe this can be changed by introducing into the public discourse explanations of science that emphasize the unknown.  Puzzles engage us, questions are more accessible than answers, and perhaps most important, emphasizing ignorance makes everyone feel more equal, the way the infinity of space pares everyone down to size.</i></b><i> Journalists can aid in this cause, but scientists themselves must take the lead.  They have to learn to talk in public about what they don&#39;t know without feeling this is an admission of stupidity.  In science, dumb and ignorant are not the same.  We all know this; it&#39;s how we talk to each other and to our graduate students.  Can we also let the public in on the secret?&quot;</i> (p173, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><b>Agnotology: The Study of Ignorance</b><br /><br /><i>&quot;This crucial element in science was being left out for the students.  The undone part of science that gets us into the lab early and keeps us there late, the thing that &#39;turns your crank,&#39; the very driving force of science, the exhilaration of the unknown, all this is missing from our classrooms.  In short, we are failing to teach the ignorance, the most critical part of the whole operation.&quot;</i> (p4, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;This curious revelation grew into an idea for an entire course devoted to, and titled, Ignorance.  A science course.  That course, in its current incarnation, began in the spring of 2006. </i><b><i>At the heart of the course are sessions, I hesitate to call them classes, in which a guest scientist talks to a group of students for a couple of hours about what he or she doesn&#39;t know.</i></b><i> They come and tell us about what they would like to know, what will happen if they do find this or that thing out, what might happen if they don&#39;t.  About what could be known, what might be impossible to know, what they didn&#39;t know 10 or 20 years ago and know now, or still don&#39;t know.  Why they want to know this or not that, this more than that. </i><b><i>In sum, they talk about the current state of their ignorance.</i></b> <br /><br /><i>Recruiting my fellow scientists to do this is always a little tricky -- &#39;Hello, Albert, I&#39;m running a course on ignorance and I think you&#39;d be perfect.&#39; But in fact almost every scientist realizes immediately that he or she would indeed be perfect, that this is truly what they do best, and once they get over not having any slides prepared for a talk on ignorance, it turns into a surprising and satisfying adventure.&quot;</i> (p5, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;One kind of ignorance is willful stupidity; worse than simple stupidity, it is a callow indifference to facts or logic.  It shows itself as a stubborn devotion to uninformed opinions, ignoring (same root) contrary ideas, opinions, or data.  The ignorant are unaware, unenlightened, uninformed, and surprisingly often occupy elected offices.  We can all agree that none of this is good.</i><br /><br /><i>But there is another, less pejorative sense of ignorance that describes a particular condition of knowledge: the absence of fact, understanding, insight, or clarity about something.  It is not an individual lack of information but a communal gap in knowledge.  It is a case where data don&#39;t exist, </i><b><i>or more commonly, where the existing data don&#39;t make sense, don&#39;t add up to a coherent explanation, cannot be used to make a prediction or statement about some thing or event. This is knowledgeable ignorance, perceptive ignorance, insightful ignorance. It leads us to frame better questions, the first step to getting better answers. It is the most important resource we scientists have, and using it correctly is the most important thing a scientist does.</i></b><i> James Clerk Maxwell, perhaps the greatest physicist between Newton and Einstein, advises that &#39;Thoroughly conscious ignorance is the prelude to every real advance in science.&#39;&quot;</i> (p6-7, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;On her way into life-threatening surgery, Gertrude Stein was asked by her lifelong companion, Alice B Toklas, &#39;What is the answer?&#39; Stein replied, &#39;What is the question?&#39; There are a few different versions of this story, but they all come to the same thing: </i><b><i>Questions are more relevant than answers.  Questions are bigger than answers. One good question can give rise to several layers of answers, can inspire decades-long searches for solutions, can generate whole new fields of inquiry, and can prompt changes in entrenched thinking. Answers, on the other hand, often end the process.</i></b><i>&quot;</i> (p11, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><b><i>&quot;Working scientists don&#39;t get bogged down in the factual swamp because they don&#39;t care all that much for facts.</i></b><i> It&#39;s not that they discount or ignore them, but rather that they don&#39;t see them as an end in themselves.  They don&#39;t stop at the facts; they begin there, right beyond the facts, where the facts run out.  Facts are selected, by a process that is a kind of controlled neglect, for the questions they create, for the ignorance they point to. </i><b><i>What if we cultivated ignorance instead of fearing it, what if we controlled neglect instead of feeling guilty about it, what if we understood the power of not knowing in a world dominated by information? As the first philosopher, Socrates, said, &#39;I know one thing, that I know nothing.&#39;&quot;</i></b> (p12, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;Curiously, as our collective knowledge grows, our ignorance does not seem to shrink.  Rather, we know an ever smaller amount of the total, and our individual ignorance, as a ratio of the knowledge base, grows.&quot;</i> (p13, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;[T]here is no denying that there are a lot of facts that you have to know to be a professional scientist.  But clearly you can&#39;t know all of them, and knowing lots of them does not automatically make you a scientist, just a geek.&quot;</i> (p15, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;Too much emphasis on the answers and too little attention to the questions have produced a warped view of science.  And this is a pity, because it is the questions that make science such a fun game.&quot;</i> (p16, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;Erwin Schrodinger, one of the great philosopher-scientists, says, &#39;In an honest search for knowledge you quite often have to abide by ignorance for an indefinite period.&#39; ... Being a scientist requires having faith in uncertainty, finding pleasure in mystery, and learning to cultivate doubt. </i><b><i>There is no surer way to screw up an experiment than to be certain of its outcome.&quot;</i></b> (p 17, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;No matter how objective the measurement, someone still had to decide to make that measurement, providing ample opportunity for bias to enter the scheme right there ... Scientific facts are &#39;disinterested,&#39; which certainly doesn&#39;t sound like much fun and may be why they have become so uninteresting.&quot;</i> (p20-21, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;It may seem surprising to the nonscientist, but all scientists know that it is facts that are unreliable.  No datum is safe from the next generation of scientists with the next generation of tools.&quot;</i> (p21, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;Science, then, is not like the onion in the often used analogy of stripping away layer after layer to get at some core, central, fundamental truth.  Rather, it&#39;s like the magic well: no matter how many buckets of water you remove, there&#39;s always another one to be had.  Or even better, it&#39;s like the widening ripples on the surface of a pond, the ever larger circumference in touch with more and more of what&#39;s outside the circle, the unknown.  This growing forefront is where science occurs. </i><b><i>Curious then that in so many settings -- the classroom, the television special, the newspaper accounts -- it&#39;s the inside of the circle that seems so enticing, rather than what&#39;s out there on the ripple.  It is a mistake to bob around in the circle of facts instead of riding the wave to the great expanse lying outside the circle. But that&#39;s still where most people who are not scientists find themselves.&quot;</i></b> (p28-29, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;We might even go a step further and recognize that there are unknowable unknowns -- things that we cannot know due to some inherent and implacable limitation.  History, as a subject, could be said to be fundamentally unknowable; the data are lost and they are not recoverable.</i><br /><br /><i>So it&#39;s not so much that there are limits to our knowledge, more critically there may be limits to our ignorance.  Can we investigate these limits?  Can ignorance itself become a subject for investigation?  Can we construct an epistemology of ignorance like we have one for knowledge?  Robert Proctor, a historian of science at Stanford University, and perhaps best known as an implacable foe of the tobacco industry&#39;s misinformation campaigns, has coined the word agnotology as the study of ignorance.  We can investigate ignorance with the same rigor as philosophers and historians have been investigating knowledge.&quot;</i> (p30, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;Comedian philosopher George Carlin wryly observed that &#39;One can never know for sure what a deserted area looks like.&#39;&quot;</i> (p34, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><b><i>&quot;Whatever it may look like from outside the science establishment, the incorrect management of ignorance has far more serious consequences than screwing up with the data.  There are correction procedures for mishandled data -- they must be replicable, must answer to the scrutiny of peers -- but mishandled ignorance can be costly, harder to perceive, and so harder to correct.&quot;</i></b> (p44, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><b><i>&quot;Unbridled enthusiasm for scientific progress is good public relations, but it is often bad science.  Things never go the way we think they will; there are always unexpected findings and unexpected consequences that may redirect or even stymie a field for years.&quot;</i></b> (p48, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;Although not the explicit goal, the best science can really be seen as refinining ignorance.  Scientists, especially young ones, can get too enamored with results.  Society helps them along in this mad chase.  Big discoveries are covered in the press, show up on the University&#39;s home page, garner awards, help get grants, and make the case for promotions and tenure.  But it&#39;s wrong. </i><b><i>Great scientists, the pioneers that we admire, are not concerned with results but with the next questions. The eminent physicist Enrico Fermi told his students that an experiment that successfully proves a hypothesis is a measurement; one that doesn&#39;t is a discovery. A discovery, an uncovering -- of new ignorance.&quot;</i></b> (p57, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;A question is interesting if it leads somewhere and is connected to other questions.&quot;</i> (p61, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><b><i>&quot;I have a colleague who always suggests that his students look not to yesterday&#39;s issue of Nature of Science for experimental ideas but rather to work that is at least 10 or more years old.  This is work that is ready to be revisited, ready for revision.  Questions still lurk in these data, questions that have now ripened and matured, that could not be answered then with the available techniques.  More than likely they could not even have been asked because they didn&#39;t fit any current thinking.  But now they come alive, suddenly possible, potential, promising.  Here is another fertile, if unintuitive, place to look for ignorance -- among what&#39;s known.&quot;</i></b> (p69, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><b><i>&quot;The hypothesis is a statement of what one doesn&#39;t know and a strategy for how one is going to find it out.  I hate hypotheses.  Maybe that&#39;s just a prejudice, but I see them as imprisoning, biasing, and discriminatory.  Especially in the public sphere of science, they have a way of taking on a life of their own.  Scientists get behind one hypothesis or another as if they were sports teams or nationalities -- or religions.  They have conferences where different laboratories or theorists present evidence supporting their hypothesis and derogating the other guy&#39;s.&quot;</i></b> (p77, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;At the personal level, for the individual scientist, I think the hypothesis can be just as useless.  No, worse than useless, it is a real danger.  First, there is the obvious worry about bias.  Imagine you are a scientist running a laboratory, and you have a hypothesis and naturally you become dedicated to it -- it is, after all, your very clever idea about how things will turn out.  Like any bet, you prefer it to be a winner.  Do you now unconsciously favor the data that prove the hypthesis and overlook the data that don&#39;t?  Do you, ever su subtly, select one data point over another -- ther eis always an excuse to leave an outlying data point out of the analys (e.g., &#39;Well, that was a bad day, nothing seemed to work,&#39; &#39;The instruments probably had to be recalibrated,&#39; &#39;Those observations were made by a new student in the lab&#39;).  In this way, slowly but surely, the supporting data mount while the opposing data fade away.  So much for objectivity.</i><br /><br /><i>Worse even than this, you may often miss data that would lead to a better answer, or a better question, because it doesn&#39;t fit your idea. </i><b><i>Alan Hodgkin, a famous neurophysiologist responsible for describing how the voltage in neurons changes rapidly when they are stimulated (for which he won a Nobel Prize), would go around the laboratory each day visiting with each student or postdoctoral researcher working on one project or another. If you showed him data from yesterday&#39;s experiments that were the expected result, he would nod approval and move on. The only way to get his attention was to have an anomalous result that stuck out. Then he would sit down, light his pipe, and go to work with you on what this could mean. But there are not many like Alan Hodgkin.&quot;</i></b> (p78-79, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;Anyone who thinks we aren&#39;t all on a fishing expedition is just kidding himself.  The trick is to have some idea about where to fish.&quot;</i> (p80, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;It is often said that much in science is serendipitous; crucial discoveries are as much happenstance as the result of a directed search.  This makes for nice stories, but it&#39;s rarely that simple.  As Louis Pasteur, himself a beneficiary of some good fortune, noted, &#39;Chance favors the prepared mind.&#39;  Lawyers don&#39;t make scientific discoveries by accident; only scientists do.  That&#39;s because their curiosity is driving them to screw around with things to see what will happen.  And often, it is true, the thing they find is not what they were looking for, but something unexpected and more interesting.  Nonetheless, they have to be looking. </i><b><i>The serendipity stories don&#39;t teach us that it&#39;s mostly dumb luck, but rather that we are often not smart enough to predict how things should be, and that it&#39;s better to be curious and try to remain open minded and see what happens. Most important, never dismiss anomalous data; it&#39;s often the best stuff.&quot;</i></b> (p80-81, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;Chaim Weizmann, the first president of Israel and the namesake of the Weizmann Institute of Science in Tel Aviv ... and Einsteni were on an Atlantic crossing together, and they determined that for 2 hours each morning they would sit on the ship&#39;s deck and Einstein would explain relativity to Weizmann.  At the end of the crossing, Weizmann claimed that he was &#39;now convinced that Einstein understood relativity&#39;&quot;</i> (p83, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;Here are some examples of what have turned out to be good questions in my class:</i><br /><br /><i>Do you think things are unknowable in your field?  What?</i><br /><br /><i>What are the current technological limits in your work?  Can you see solutions?</i><br /><br /><i>Where are you currently stuck?</i><br /><br /><i>How do you talk about what you don&#39;t know?</i><br /><br /><i>What was the main thrust of your last grant proposal?</i><br /><br /><i>What will be the main thrust of your next grant proposal?</i><br /><br /><i>Is there something you would like to work on knowing but can&#39;t?</i><br /><br /><i>Because of technical limitations?  Money, manpower?</i><br /><br /><i>What was the state of ignorance in your field 10, 15, or 25 years ago, and how has that changed?</i><br /><br /><i>Are there data from other labs that don&#39;t agree with yours?</i><br /><br /><i>How often do you guess?</i><br /><br /><i>Are you often surprised?  When?</i><br /><br /><i>Do things come undone?</i><br /><br /><i>What questions are you generating?</i><br /><br /><i>What ignorance are you generating?&quot;</i> (p86-87, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;Why does there have to be a unified theory of everything before physicists will be content?  There is no comparable drive in chemistry or biology.  Are those fields fundamentally different or perhaps not as mature as physics?  Physics is probably the most remarkable success story in science in its ability explain everything -- or almost everything.  And perhaps there&#39;s the rub: </i><b><i>there seem to be just a few bits missing, and that&#39;s sometimes worse than having whole swaths of ignorance.&quot;</i></b> (p108, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;&#39;You know that if you are too risky in your research you&#39;ll get nothing done.  Or you can play it safe and reap rewards for doing essentially the same thing over and over again,&#39; but that&#39;s not getting much done and &#39;you have to force yourself not to do that.&#39;  You have to find the &#39;stuff that pushes the edges for you,&#39; and to do that you have to be honest and say, &#39;What can I personally tackle?&#39;  &#39;Also you have to know the times you live in.  Is there enough information for me to make progress here?  When do you yourself say you&#39;re not going to be able to solve this?&#39;  &#39;So you have to introspect and that&#39;s the good part.  But you have to guess too, and you could guess wrong.  There are no guarantees.&#39;  I hope you can see the struggle here and recognize that this struggle goes on in parallel with the actual work that he does, and it is a constant struggle, back and forth, always trying to locate that sweet spot of ignorance.&quot;</i> (p136-137, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><b><i>&quot;Getting comfortable with ignorance is how a student becomes a scientist.&quot;</i></b> (p167, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /><i>&quot;We often use the word ignorance to denote a primitive or foolish set of beliefs.  In fact, I would say that &#39;explanation&#39; is often primitive or foolish, and the recognition of ignorance is the beginning of scientific discourse.  When we admit that something is unknown and inexplicable, then we admit also that it is worthy of investigation.&quot;</i> (p167, <i>Ignorance: How it Drives Science</i>, Stuart Firestein)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"The Weakest Force - Gravity","summary":"The Gravitational Force Between Our Sun and Its Nearest Neighbor is Remarkably Small / Can the Addition of Dark Matter Turn the Weakest Force Into the Universe's Dominant Force?","image":"https://lh3.googleusercontent.com/-NS0JGcYE3Ec/VsdBx-X6TRI/AAAAAAAAG1c/zTQhAoewj58/w1800-h2520/the-weakest-force-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/AxPyeVEv7Hc","publishDate":"2016-02-19T16:26:42.585Z","updateDate":"2017-01-03T02:52:32.900Z","text":"<b>The Weakest Force - Gravity: The Gravitational Force Between Our Sun and Its Nearest Neighbor is Remarkably Small / Can the Addition of Dark Matter Turn the Weakest Force Into the Universe&#39;s Dominant Force?</b><br /><br />Calculation is from <a href=\"https://www.thunderbolts.info/wp/2011/09/02/essential-guide-to-eu-chapter-1/\" class=\"ot-anchor\">https://www.thunderbolts.info/wp/2011/09/02/essential-guide-to-eu-chapter-1/</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The Burnham Model for Stellar Distances","summary":"These Distances are Incomprehensibly Large, so Robert Burnham Created a Model to Translate them to a Metric We Can Understand / It's Probably the Case that Most People Who Agree that Gravity is Dominant at the Larger Scales of the Universe Have Never Deeply Engaged Burnham's Model","image":"https://lh3.googleusercontent.com/-C7NFyIkt9-o/VsXuZHQgpJI/AAAAAAAAG0U/JJX_0hbXol8/w1800-h2520/the-burnham-model-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/jBTeGFK52AK","publishDate":"2016-02-18T16:17:36.091Z","updateDate":"2017-01-03T02:54:08.966Z","text":"<b>The Burnham Model for Stellar Distances: These Distances are Incomprehensibly Large, so Robert Burnham Created a Model to Translate them to a Metric We Can Understand / It&#39;s Probably the Case that Most People Who Agree that Gravity is Dominant at the Larger Scales of the Universe Have Never Deeply Engaged Burnham&#39;s Model</b><br /><br /><a href=\"https://www.thunderbolts.info/wp/2011/09/02/essential-guide-to-eu-chapter-1/\" class=\"ot-anchor\">https://www.thunderbolts.info/wp/2011/09/02/essential-guide-to-eu-chapter-1/</a><br /><br /><i>&quot;Robert Burnham developed a model to show us in ordinary terms how much space there is out there between the stars. To understand its scale we need to know a couple of real distances.</i><br /><br /><i>... [T]he distance from the Earth to the Sun is around 92,960,000 miles (149,605,000 km). Usually rounded off to 93 million miles (150 million km), this distance is called the Astronomical Unit (AU).</i><br /><br /><i>A light-year (ly) is equal to 63,294 AU. </i><b><i>Coincidentally, this is about the same number as the number of inches in a statute mile, 63,360. Therefore, there is around the same number of inches in 1 AU (63,360 x 92,960,000) as the number of miles in 1 light-year (63,294 x 92,960,000).</i></b><i> Those are really big numbers. Let’s stick to inches.</i><br /><br /><b><i>Burnham set the scale in his model so that 1 inch (1&quot;) equals 1 AU or 93 million miles. Then 1 mile in our model would equal 1 ly.</i></b><i> This scale would be expressed as 1:6,000,000,000,000. That’s one unit represents six million million units, which is a scale of one to 6 trillion or 1:6×10¹².</i><br /><br /><i>Let’s start describing a Burnhamesque miniature scale model of our solar system using this scale. We know the distance from Earth to the Sun (1 AU) will be one inch. How big will the Sun be? The Sun’s diameter is about 870,000 miles, so </i><b><i>in our scale model the Sun will be a little under 1/100th of an inch across. That’s a very tiny speck. The Earth will be one inch away from the Sun but so small (0.00009&quot;, or 9 one hundred thousandths of an inch) that we would not be able to see it without a microscope.</i></b><br /><br /><b><i>Pluto’s orbital radius is 39.5 times larger than Earth’s, so Pluto will be 39.5 inches, or almost exactly 1 meter, from the Sun.</i></b><br /><br /><b><i>The heliosphere, the region around the Sun which the solar wind permeates, is about 7 feet in our model.</i></b><br /><br /><b><i>So where is the nearest star in our model? Our nearest neighbor is Alpha Centauri, which is over 4 light-years away. That’s more than 4 miles in our model.</i></b><br /><br /><i>Yes, 4 miles. Our model Sun is one tiny speck, and it’s 4 miles to the next nearest speck. That’s a lot of space in between. So how big is our galaxy in this tiny model? The model galaxy would stretch 100,000 miles across. The thin disk and spiral arms would be a thousand miles thick. Its central bulge of stars would be well over 6000 miles from top to bottom. Our galaxy is but one of hundreds of billions of galaxies visible in the observable Universe with our present instruments. The nighttime sky appears to be crowded with stars, but stars are separated typically by over 10 million times their diameters.&quot;</i><br /><br />The small yellow dot within the smaller image, incidentally, shows how far Earth&#39;s radio waves have so far penetrated into the Milky Way.  Diagram is from <a href=\"http://www.dailymail.co.uk/sciencetech/article-2107061/Earth-calling-Tiny-yellow-dot-shows-distance-radio-broadcasts-aliens-travelled.html\" class=\"ot-anchor\">http://www.dailymail.co.uk/sciencetech/article-2107061/Earth-calling-Tiny-yellow-dot-shows-distance-radio-broadcasts-aliens-travelled.html</a>.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"The Believable Narrative","summary":"We Don't Have to Conjure Them / They Just Happen to Us / In a World of Information Overload, We Rely Upon Narratives for Efficiency in Decision-Making / But, When the Subject is Complex, They Tend to Trivialize Reality / And in a World of Specialization, the Experts Are Subject to Each Others' Narratives","image":"https://lh3.googleusercontent.com/-K85FhmQb7kk/VsVUS9GbVcI/AAAAAAAAGzE/6ryDp5SwtpY/w1800-h2520/the-narrative-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/4FS7w9bXSmg","publishDate":"2016-02-18T05:20:27.755Z","updateDate":"2017-01-03T02:54:29.490Z","text":"<b>The Believable Narrative: We Don&#39;t Have to Conjure Them / They Just Happen to Us / In a World of Information Overload, We Rely Upon Narratives for Efficiency in Decision-Making / But, When the Subject is Complex, They Tend to Trivialize Reality / And in a World of Specialization, the Experts Are Subject to Each Others&#39; Narratives</b><br /><br />Three polar bears tell one consistent story through their pictures -- that man is &quot;killing&quot; the planet.  But, a single picture cannot tell the entire story of what happened to an animal over the course of its lifetime, and so the audience simply fills in the blanks with what turns out to be a fairly consistent narrative.<br /><br />Climate change skeptic, Anthony Watts, has done a commendable job covering the polar bear narrative -- like this commentary, from <a href=\"http://wattsupwiththat.com/2015/02/19/twenty-good-reasons-not-to-worry-about-polar-bears-and-climate-change/\" class=\"ot-anchor\">http://wattsupwiththat.com/2015/02/19/twenty-good-reasons-not-to-worry-about-polar-bears-and-climate-change/</a><br /><br /><i>&quot;Polar bears are still a conservation success story — with a global estimate almost certainly greater than 25,000, </i><b><i>we can say for sure that there are more polar bears now than there were 40 years ago</i></b><i> (Fig. 1). Sadly, although completing a global survey was one of the primary objectives of the PBSG at its inception 47 years ago, it has still not provided one. The current PBSG estimate is about 20,000-25,000 bears, although with several subpopulations still uncounted (Fig. 1), the actual figure is almost certainly a good deal higher (e.g. see point #3). </i><b><i>Even with this lack of precision, the global estimate is too high to qualify the polar bear as &#39;threatened&#39; with extinction based on current population levels – all of the concerns expressed regarding polar bears are about the future.&quot;</i></b><br /><br />But, the world of science is a large, complex beast.  And to make the point that the situation of narratives is ubiquitous, let me show how easy the tables are turned.  Here&#39;s Anthony Watts making the case on his own site for the existence of gravitational waves ...<br /><br /><a href=\"http://wattsupwiththat.com/2016/02/17/the-detection-of-gravitational-waves-a-triumph-of-science-enabled-by-fossil-fuels/#comment-2147542\" class=\"ot-anchor\">http://wattsupwiththat.com/2016/02/17/the-detection-of-gravitational-waves-a-triumph-of-science-enabled-by-fossil-fuels/#comment-2147542</a><br /><br /><i>&quot;Think about radio astronomy, we didn’t think stars emitted more than light less than 100 years ago, The initial detection of radio waves from an astronomical object was made in the 1933 when Karl Jansky of Bell labs observed radiation coming from the Milky Way interfering with long distance telephone communications and published his paper on it. Once we figured out how to listen to what Jansky observed, we discovered a cacaphony of radio signals and radiation signatures in the Universe.</i><br /><br /><i>Beyond needing a much much better receiver than Jansky, this new science in it’s infancy is no different.</i><br /><br /><i>Given the billions upon billions of stars in the Universe, such GW events are likely to be more common than we realize. I predict we’ll see more gravity wave events from LIGO and the soon to be running advanced VIRGO observatory.&quot;</i><br /><br />The only problem is that Watts has completely left out the <b>actual</b> history of the discovery of radio waves.  From Gerrit Verschuur&#39;s <i>The Invisible Universe</i> ...<br /><br /><i>&quot;2.1. Caught between Two Disciplines</i><br /><br /><i>In 1933 John Kraus, then at the University of Michigan, attempted to detect the sun by using a searchlight reflector to focus the radio waves. He failed because the receiver was not sensitive enough. This was the first use of a reflector-type radio telescope. At the Serendipity meeting, Kraus stated that meaningful accidental discovery occurs only as the result of &#39;being in the right place with the right equipment doing the right experiment at the right time.&#39; </i><b><i>Another noted astronomer, R. Hanbury Brown, added that the person should &#39;not know too much,&#39; otherwise the discovery might not be made!</i></b><br /><br /><i>This summarizes a very interesting phenomenon. </i><b><i>Many research scientists, especially the theoretically inclined, &#39;know&#39; so much that their chance of making a lucky or creative discovery may be severely curtailed. If we know too much, our vision is sometimes narrowed to the point where new opportunities are not seen.</i></b><i> Jansky knew a little astronomy, but not enough for it to get in his way and cause him to reject the possibility that radio waves originating in the cosmos might be real.</i><br /><br /><i>Grote Reber, a professional engineer and radio ham in his spare time, was one of the few people who recognized the interesting implications of Jansky’s discovery. Reber was certainly not hampered by any astronomical prejudices about whether or not the cosmic radio waves could exist. Instead, he was interested in verifying their existence and followed up on Jansky’s work. To this end, Reber built the world’s first steerable radio dish antenna (Figure 2.1) in his backyard and mapped the Milky Way radiation during the period 1935–1941 ... He pointed out that the new field of radio astronomy was originally caught between two disciplines. </i><b><i>Radio engineers didn’t care where the radio waves came from, and the astronomers</i></b><br /><br /><b><i>&#39;... could not dream up any rational way by which the radio waves could be generated, and since they didn’t know of a process, the whole affair was (considered by them) at best a mistake and at worst a hoax.&#39;&quot;</i></b><br /><br />Contrasting these two histories -- the manufactured version against the historical one -- it should be apparent that <b>they convey completely opposite lessons.</b><br /><br />Anthony Watts specializes in climate change.  And although I (and many others, in fact) have much respect for his views on that subject, it is clear in this case that there remain gaps in his understanding of the history of science -- and that he has filled those gaps with the narrative that makes his point. <b>Truth is that people tend not to believe nothing when they know little about something; most people will tend to fill in those gaps with an imagined narrative that is essentially consistent with their worldview.</b><br /><br />Those who follow Anthony Watts will mostly agree that he is a well-informed climate change skeptic. <b>But, like many other skeptics in science, specialization in the sciences has fragmented his learning and he is, as a result, a skeptic of some other skeptics.</b> Watts actually has a notice on his site warning potential contributors:<br /><br /><i>&quot;Per the WUWT policy page, certain topics are not welcome here and stories submitted concerning them will be deleted.  This includes topics on religion, discussions of barycentrism, astrology, aliens, bigfoot, chemtrails, 911 Truthers, UFO&#39;s, </i><b><i>Electric Universe,</i></b><i> mysticism, and Principia/Slaying the Sky Dragon&quot;</i><br /><br />What&#39;s worth pointing out, since most might miss it, is that the more accurate radio astronomy history given above is written by an actual radio astronomer who has published papers which were intended to support some of these same arguments about plasmas made by the Electric Universe proponents.<br /><br />Anthony&#39;s just doing what most of us do practically every day for all sorts of things: There is too much information in the world to meaingfully engage all of it, so we specialize, and we are prone to taking shortcuts to the heavy lifting of deep, rational thought outside of these specialties.  In most mundane day-to-day contexts, these shortcuts serve us very well.  But, when it comes to the most complex subjects, they tend to oversimplify the true complexity of the situation.<br /><br />The subject of narratives, with an explanation of how the mind forms them, was formerly brought up for Martín López Corredoira here:<br /><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/gVUCTWLWHin\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/gVUCTWLWHin</a><br /><br />It is a topic which proves difficult to ignore when discussing scientific controversies, for we are all apparently caught in the web of one anothers&#39; narratives.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"The Observer's Expectation","summary":"There is No Scientific Training Which Can Guard Against the Desire to Obtain a Particular Result / The History of Science Records a Number of Significant Examples Where the Entire Scientific Community Falls Prey to a Common Delusion","image":"https://lh3.googleusercontent.com/-SOz4z-aGQ64/VsSngS7MYwI/AAAAAAAAGx8/FhqbXDBiQTM/w3600-h5040/the-observers-expectation-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/YEptHKXJFQo","publishDate":"2016-02-17T17:02:36.229Z","updateDate":"2017-01-03T02:54:48.793Z","text":"<b>The Observer&#39;s Expectation: There is No Scientific Training Which Can Guard Against the Desire to Obtain a Particular Result / The History of Science Records a Number of Significant Examples Where the Entire Scientific Community Falls Prey to a Common Delusion</b><br /><br /><b>Unexpected: Parallax is Very Small Because Stars Are Farther Away than Expected</b><br /><br />From William Broad and Nicholas Wade, &quot;Self-deception and Gullibility,&quot; <i>Betrayers of the Truth</i> (New York, 1982), pp. 107-108<br /><br /><i>&quot;In 1669, the distinguished English physicist, Robert Hooke, made a wonderful discovery. He obtained the long-sought proof of Copernicus&#39; heliocentric theory of the solar system by demonstrating stellar parallax -- a perceived difference in [the] position of a star due to the Earth&#39;s motion around the Sun. One of the first to use a telescope for this purpose, Hooke observed the star Gamma Draconis and soon reported to the Royal Society that he had found what he was looking for: The star had a parallax of almost [30] seconds of arc. Here, at last, was impeccable experimental proof of the Copernican theory.</i><br /><br /><i>This heartening triumph of empirical science was only momentarily dashed when the Frenchman, Jean Picard, announced he had observed the star Alpha Lyrae by the same method but had failed to find any parallax at all. A few years later, England&#39;s first Astronomer Royal, the brilliant observer, John Flamsteed, reported that the Pole Star had a parallax of at least [40] seconds.</i><br /><br /><i>Hooke and Flamsteed, outstanding scientists of their day, are leading lights in the history of science. But they fell victim to an effect that, to this day, has continued to trap ... scientists in its treacherous coils. It is the phenomenon of experimenter expectancy, or, </i><b><i>seeing what you want to see. There is, indeed, a stellar parallax, but, because of the vast distances of all stars from Earth, the parallax is extremely small -- about [1] second of arc. It cannot be detected by the relatively crude telescopes used by Hooke and Flamsteed.</i></b><br /><br /><b><i>Self-deception is a problem of pervasive importance in science. The most rigorous training in objective observations is often a feeble defense against the desire to obtain a particular result. Time and again, an experimenter&#39;s expectation of what he will see has shaped the data he recorded to the detriment of the truth. This unconscious shaping of results can come about in numerous, subtle ways. Nor is it a phenomenon that affects only individuals. Sometimes a whole community of researchers falls prey to a common delusion ....</i></b><br /><br /><b><i>Expectancy leads to self-deception, and self-deception leads to the propensity to be deceived by others.&quot;</i></b><br /><br /><b>Also Unexpected: Parallax is Only Accurate to 1% the Diameter of the Milky Way</b><br /><br /><a href=\"http://www.thunderbolts.info/eg_draft/eg_chapter_1.htm\" class=\"ot-anchor\">http://www.thunderbolts.info/eg_draft/eg_chapter_1.htm</a><br /><br /><i>&quot;Triangulation, or trigonometric parallax, is a direct way of using the measured angular difference from two positions to measure the distance to some object. By observing a star&#39;s position relative to the background stars from opposite sides of our orbit about the Sun, we have a wide baseline that will allow us to get an angular difference from observations 6 months apart and be able to measure the distance to something as far away as a star.</i><br /><br /><i>The Earth averages about 93 million miles from the Sun, so that is its nearly-circular orbit&#39;s radius. This distance is often called an astronomical unit (AU) in astronomy. So the distance from one side of the Earth&#39;s orbit to the opposite side is 2 AU, or about 186 million miles. When we measure the angle to the nearest star (Alpha Centauri) from one side of the orbit, wait six months, and measure it again, we find that the angular difference is rather small, requiring enormous precision of measurement ...</i><br /><br /><i>The European Space Agency (ESA) launched its automated Hipparcos satellite telescope to take measurements of over 118,000 stars during its lifetime from 1989–1993. Mission: improve the precision of catalogued locations of many stars and update the Tycho and Tycho 2 catalogs. Out of the newly measured parallaxes, 20,870 stars met the criterion of having 10% or less stellar parallax error.</i><br /><br /><i>Even with the more accurate Hipparcos satellite data, distance measurements to stars out to around 200-220 light-years have up to 10% error, and they are increasingly less accurate out to about 500 light-years. Beyond that, trigonometric parallax measurements should not be considered reliable. Pogge, in the link above to his Lecture 5, claims Hipparcos data give &quot;good distances out to 1000 light-years&quot;, yet an estimated distance of only 500 light years with ±20%–30% error is already off by too much to be of much use. </i><b><i>1000 light-years is an almost incomprehensible distance, yet it is only about 1% of the way across our Milky Way galaxy.</i></b><br /><br />(assuming, of course, that we&#39;ve correctly identified the diameter of the Milky Way)<br /><br /><b>There Have Been Many Instances Where Expectations Led the Entire Scientific Community Astray</b><br /><br />Some of the more well-known examples ...<br /><br />1.  The maser, the laser&#39;s precursor, was originally thought by the most respected quantum theorists of the day to be impossible.  See the story here: <a href=\"https://plus.google.com/108466508041843226480/posts/PpwuZ24rTgQ\" class=\"ot-anchor\">https://plus.google.com/108466508041843226480/posts/PpwuZ24rTgQ</a><br /><br />2.  When radio waves were first observed from space by radio <b>engineers</b>, they were assumed by <b>astronomers</b> to be either a hoax or a mistake.  Learn the history here: <a href=\"https://plus.google.com/108466508041843226480/posts/AFh4Q5Ceggb\" class=\"ot-anchor\">https://plus.google.com/108466508041843226480/posts/AFh4Q5Ceggb</a><br /><br />3.  When Robert Goddard, the inventor of the rocket, suggested that science fiction books were right -- that we could indeed go to the Moon with rockets -- he was ridiculed for not knowing that a rocket would have nothing to push against in space (his critics obviously not understanding Newton&#39;s laws of motion).  This ridicule continued virtually all the way up to the point that that the Nazis were raining ICBMs down upon London in 1944.  See that incredible story here: <a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Jgikude95Wa\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Jgikude95Wa</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23historical\">#historical</a>  ﻿","category":"historical"},{"name":"Crank Dot Net","summary":"An Informal Taxonomy of Against-the-Mainstream Ideas / Ideas Are Rated on a Crank Scale / Everyone Will Find Ideas on the Site They Consider to Be Ridiculous / But, Mixed in With Those Are Critiques of Modern Scientific Theory / The Commentary is Just as Much About a Society Where Nobody Has Taken the Time, to Date, to Figure Out Which Ones Are Authentic Scientific Controversies","image":"https://lh4.googleusercontent.com/-mpOMTPBmsTA/VsLFP62dU8I/AAAAAAAAGvw/Rxrxb86kbLI/crank-dot-net-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/VSZhb529Qmy","publishDate":"2016-02-16T06:46:22.339Z","updateDate":"2017-01-03T02:55:39.446Z","text":"<b>Crank Dot Net: An Informal Taxonomy of Against-the-Mainstream Ideas / Ideas Are Rated on a Crank Scale / Everyone Will Find Ideas on the Site They Consider to Be Ridiculous / But, Mixed in With Those Are Critiques of Modern Scientific Theory / The Commentary is Just as Much About a Society Where Nobody Has Taken the Time, to Date, to Figure Out Which Ones Are Authentic Scientific Controversies</b><br /><br /><a href=\"http://www.ebtx.com/oats/cranknet.htm\" class=\"ot-anchor\">http://www.ebtx.com/oats/cranknet.htm</a><br /><br /><i>&quot;Crank Dot Net</i><br /><i>My Site Review</i><br /><br /><i>Having received a review by Crank Dot Net, I thought it only fair to review their site. When I found that my site was listed there, I sent them an email thanking them for the listing but suggesting that I be placed under a different category ... something other than &#39;new age&#39; which I believe to be a misplacement.</i><br /><br /><i>This site (a Service to Humanity List, protecting all from the blight of wrongheaded ideas) must be fairly new ...</i><br /><br /><i>This is a kids page</i><br /><br /><i>It shows in the content. Well ... there is no content actually. It&#39;s just OP content (other people&#39;s).</i><br /><br /><i>These are college age kids with no critical experience in the areas they seek to capitalize on. You can tell they haven&#39;t been on the Web for more than about a year or two because they put up a &#39;list&#39; ... and as every savvy surfer knows ... lists are now passe&#39;. They are universally considered to be of little value ... because ... this function has been completely taken over by search engines.</i><br /><br /><i>To be useful a site must now project original content.</i><br /><br /><i>What I would like to see is a crank site that offers at least a few sentences of critical appraisal. If someone says to me &#39;You are a crank&#39;, it carries no meaning for me. I get insulted on my D-board and by email fairly often. I need some meat on this bone ;o).</i><br /><br /><i>However, when you write even a few sentences, you reveal yourself to the reader. You must &#39;take a chance&#39; and drop your pants&#39;. Comments like they make here Crankish, Cranky, Crankier, Crankiest and Illucid simply aren&#39;t enough to draw surfers. They are looking for something original in a site. If they want a list of cranky sites all they have to do is &#39;Yahoo!&#39;- Alternative Physics or the like.</i><br /><br /><i>My advice to the Webmaster</i><br /><br /><i>Is ... Put more of yourself into the site. Try to be a &#39;rabble-rouser&#39;. This will get you a lot of attention ... and ... that&#39;s what this site is about, isn&#39;t it? ...</i><br /><br /><i>Actually, I would love it if some well-known scientist stated publicly that my site was the &#39;worst on earth&#39;. I would get so much traffic that I would have to invoke my 3Gig limit to prevent extra charges at Hiway. </i><b><i>But that will never happen because of Velikovski. Today, all scientists know that any attention paid to &#39;crank&#39; ideas only causes them to proliferate through the culture. Hence, their only option is to ignore. Which they do with absolute regularity ...</i></b><br /><br /><i>F.Y.I.</i><br /><b><i>The reasons for &#39;crankery&#39;</i></b><br /><br /><b><i>The advancement of science requires - absolutely - what I call &#39;forced&#39; induction (as opposed to &#39;free&#39; induction - what animals do). This requires that you go off on your own to think independently. The extent to which you &#39;go off alone&#39; determines whether you will become an acceptable scientist or what you call a &#39;crank&#39;. Unfortunately, the originality of your ideas is tied inextricably to the measure of your &#39;aloneness&#39;.</i></b><br /><br /><i>Other people act as guides and supports (a frame of reference) upon which you can rely to &#39;set you straight&#39; when you stray into the realm of the &#39;illucid&#39;. As I have said, </i><b><i>&#39;Self delusion is the bane of induction&#39;. I know this to be true from extensive personal experience. It is a real struggle to keep one&#39;s thoughts on track without the assistance of other readily available opinion.</i></b><br /><br /><i>Thus, if a scientist at Cern has a really bad idea, he may mention it to a colleague who says,</i><br /><br /><i>&#39;Did you slip on a bar of soap in the shower? Don&#39;t you remember the &#39;X&#39; factor we were just talking about last week?&#39;</i><br /><br /><i>And then the first guy says, &#39;Oh yeah, I forgot about that. Forget it.&#39;</i><br /><br /><i>Now he&#39;s back on track in less than ten seconds.</i><br /><br /><b><i>Someone alone however, may struggle for weeks in the same situation, unable to see a simple thing that another disinterested person would notice immediately. He may pursue the wrongheaded matter to some new bizarre conclusion and believe that he has found the Holy Grail. And the more effort he has put into it, the less he will be willing to give it up.</i></b><br /><br /><b><i>Therefore, if you go off alone you tend to become a &#39;crank&#39;... but if you remain with the herd you tend to discover nothing new, i.e. and become a &#39;pundit&#39;.</i></b><br /><br /><i>There is a Gaussian distribution here.</i><br /><br /><i>There are perhaps five or six thousand individuals who actually try to do &#39;forced induction&#39; at the highest level. Half of them fall on the left &#39;crank&#39; side of the distribution and half fall on the right &#39;pundit&#39; side. </i><b><i>Each half needs the other.</i></b><br /><br /><b><i>You could make the case that the &#39;extremes&#39; ought to be cut off. But I would say, &#39;Who is to decide the cutoff point?&#39;. I certainly wouldn&#39;t want to make such a momentous decision.</i></b><i> Hence, I don&#39;t criticize other people&#39;s stuff in general since I understand how difficult it is to produce anything at all.</i><br /><br /><i>The same applies in the larger sense to wide groups of individuals. </i><b><i>If the &#39;ship of science&#39; (or one of its smaller boats) decides to drop anchor and wait for the truth to come to it ... they will stagnate and you will find that many more &#39;cranks&#39; pop up to point out the paucity of perpendicular progress ... at the same time offering new and evermore bizarre solutions to present problems.</i></b><br /><br /><b><i>This is actually the present situation. The physics establishment has decided that they can proceed by experiment alone (data gathering) and that the data will tell them what to &#39;induce&#39; next.</i></b><br /><br /><i>In fact, it will.</i><br /><br /><b><i>But this is the method of the animal population ... free induction. It is highly accurate but it takes forever to get where you want to go. Hence, humans have opted for &#39;forced induction&#39; (they try everything and see what works ... fast progress with lots of mistakes). So their relative stagnation has engendered a new &#39;raft&#39; of adventuresome &#39;cranks&#39;.</i></b><br /><br /><i>It really doesn&#39;t matter though</i><br /><br /><i>As long as a free exchange of ideas is possible (in the political sense), then I don&#39;t see any need to protect anyone from either new ideas or stultifying academia. The truth will win out easily and eventually in an open forum.&quot;</i><br /><br /><b>Erik Max Francis</b><br /><br />A simple bio at <a href=\"http://www.alcyone.com/max/info/personal.html\" class=\"ot-anchor\">http://www.alcyone.com/max/info/personal.html</a> tells us a little bit about Crank.net&#39;s creator.<br /><br /><i>&quot;I have a very strong interest in science, and also in how science relates to science fiction (the kind of science fiction that pays attention to science, that is). My interest in science and technology was first piqued as a kid by Carl Sagan&#39;s Cosmos, and has taken off ever since, and has been helped along through the years by such other programs as Connections, and The Day the Universe Changed, The Mechanical Universe, Ethics in America.&quot;</i><br /><br /><b><i>&quot;I think cranks -- people with pseudoscientific theories or irrational ideas that are totally incapable of understanding why they are wrong -- are amusing; I&#39;ve been arguing with them on and off for several years in the various science newsgroups. I set up Crank Dot Net as a storinghouse for information about them.&quot;</i></b><br /><br />Erik makes no mention of the possibility that some of these ideas catalogued on his site may eventually be proven to be right.<br /><br />But, what is clear is that Erik is a very busy person.  Between <i>&quot;a lot of recreational programming,&quot;</i> his 25,000 posts to Usenet newsgroups, his seven other websites that he runs, <i>&quot;a lot of television,&quot;</i> and <i>&quot;a lot of computer games,&quot;</i> it&#39;s obvious that even if Erik <b>wanted</b> to meaningfully evaluate these ideas, he&#39;d have to give up his hobbies.  Studying scientific controversies can be enormously time-consuming; <b>without somebody there, on call, guiding your investigation, it could easily take a decade to follow a single controversy to the end of the debate.</b><br /><br />The site&#39;s disclaimer page reminds us that it&#39;s all just a joke ...<br /><br /><i>&quot;Crank Dot Net is intended for entertainment purposes only. Don&#39;t get mad, it&#39;s all in good fun. Whenever possible, quotes from the featured sites themselves are used as a description, so that sites are not misrepresented.&quot;</i><br /><br /><i>&quot;Not all Web sites featured on Crank Dot Net are indeed the work of cranks. Some are fringe science material, some are humor and parody. It is ultimately up to the astute reader to decide for themselves. In particular, sites marked as parody, fringe, or bizarre are not cranky.&quot;</i><br /><br />I am certainly not the first to note that software developers might not be the most culturally sensitive people.  I myself live in San Francisco&#39;s Mission District, which is today world-famous as ground zero for tech workers displacing the local Latino population.<br /><br /><b>Power Laws: Why Erik Just Sees Cranks</b><br /><br />Although we are taught in school about statistics, I don&#39;t quite recall ever being taught that certain statistical phenomenon follow what are known as power laws.<br /><br />This list includes book and music sales, word usage, internet hits, salaries, avalanches, wildfires, wikipedia authorship, and as it would turn out, <b>useful (aka innovative) ideas.</b><br /><br />As EBTX plainly notes, <b>innovation is a function of interdependent human behavior; thus, useful ideas are rare for multiplicative reasons.</b> Wikipedia explains ...<br /><br /><a href=\"https://en.wikipedia.org/wiki/Power_law\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Power_law</a><br /><br /><i>&quot;In statistics, a power law is a functional relationship between two quantities, where a relative change in one quantity results in a proportional relative change in the other quantity, independent of the initial size of those quantities: </i><b><i>one quantity varies as a power of another. For instance, considering the area of a square in terms of the length of its side, if the length is doubled, the area is multiplied by a factor of four.&quot;</i></b><br /><br />Unlike your typical bell curve, there is no average or standard deviation which can help to guide decisions about the data.  With power laws, the best we can do is to categorize any spot within the curve as either 80 or 20.  The 80 is the 80% of the curve&#39;s area which has some meat to it, and the 20 is what is commonly referred to as the &quot;long tail&quot;.<br /><br />As Innovation Expert, Rob Spencer of Pfizer, points out, <i>&quot;Long tails have profound implications for how you find information, how you value people and ideas, and how you manage innovation.&quot;</i> (See <a href=\"http://scaledinnovation.com/innovation/conferences/2008-11-longtail.pdf\" class=\"ot-anchor\">http://scaledinnovation.com/innovation/conferences/2008-11-longtail.pdf</a>)<br /><br />Given that books ranked over 30,000 account for about half of their sales, Amazon.com&#39;s original business model was largely based upon the commercialization of the long tail.  The Internet made this business possible by lowering the cost of mining the tail.  By offering obscure books and music (tens of thousands that sell a few each), they possessed a unique competitive advantage compared with other more traditional audio and book outlets which sold hundreds of each item.<br /><br /><b>When it comes to scientific controversies and challenges to textbook theories, there is not yet an equivalent to Amazon.com.</b><br /><br />Corporations and academic research institutions have yet to meaningfully engage the large corpus of <a href=\"http://crank.net\" class=\"ot-anchor\">crank.net</a> and other against-the-mainstream ideas which by chance escaped listing.  It remains a completely disorganized, wide open field that just sits there waiting for somebody to bring some order to it.  The best we have today is <a href=\"http://crank.net\" class=\"ot-anchor\">crank.net</a>.  And that perhaps says less about Erik and his site, and more about the public&#39;s worldview in science.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23person\">#person</a>  ﻿","category":"person"},{"name":"The Force Concept Inventory Test","summary":"A Multiple-Choice Test Which Places Misconceptions as the Wrong Answers / Lacks Jargon, So It Can Be Given Both Before & After Course, to Measure Change / Invented in 1985 to Measure Students' Conceptual Comprehension / 1st in Physics, But Now Others / In One Instance, 40% of Harvard Class Did Better on Conventional Problems than Conceptual Ones!","image":"https://lh3.googleusercontent.com/--D_4j6958Uo/VsJsR7gFiDI/AAAAAAAAGu0/wBZD6K6yGQ8/w1800-h2520/the-force-concept-inventory-test-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/HV5W3xBr1AQ","publishDate":"2016-02-16T00:28:46.780Z","updateDate":"2017-01-03T02:55:51.964Z","text":"<b>The Force Concept Inventory Test: A Multiple-Choice Test Which Places Misconceptions as the Wrong Answers / Lacks Jargon, So It Can Be Given Both Before &amp; After Course, to Measure Change / Invented in 1985 to Measure Students&#39; Conceptual Comprehension / 1st in Physics, But Now Others / In One Instance, 40% of Harvard Class Did Better on Conventional Problems than Conceptual Ones!</b><br /><br />The man in the icon is Eric Mazur, a professor of physics at Harvard, and the plot shown represents his students&#39; scores on a test which contained two types of problems: typical problem-solving introductory physics problems alongside conceptual questions regarding the same class topics.<br /><br />The first time that the professor gives the conceptual questions, he asks them open-ended.  Once the most common wrong answers are identified -- if he does not already know them -- then he provides those misconceptions as the wrong answers for the next round of students.<br /><br />What is remarkable is that the two types of questions do not appear to significantly correlate with one another.  And not only that, but there appears to be a fair amount of students who are able to get every problem set correct, but then fail to demonstrate any conceptual comprehension at all!<br /><br />Professor Mazur explains:<br /><br /><i>&quot;So let&#39;s plot, for each student, the performance on one type of problem as a function of the other type of problems.  Well, I&#39;m afraid that there&#39;s not much of a correlation.  The data point is all over the place, but if you&#39;re willing to relax your definition of a line somewhat, you find that about 50% of the students do equally well here, on both types of problems ... The student who does well on the conceptual problem tends to do well on the conventional problem, so if you understand the basics, you&#39;re gonna do well on the conventional problem. </i><b><i>However, there&#39;s 40% that does well on the conventional problem, but has no clue on the basics. What are these 40%? When I saw this diagram, everything fell into place. How would you characterize these 40%? How do they get there? Who are these students? What are they doing? Memorizing ... Plug and chug, right?&quot;</i></b><br /><br />This runs counter to what one would expect if the student actually understood the material on the test.  The conceptual questions should be easy for them.<br /><br />Now that you see the problem, let&#39;s back up and review the fascinating history of this discovery ...<br /><br /><a href=\"http://ww2.kqed.org/mindshift/2011/09/13/what-drives-college-students-to-learn/\" class=\"ot-anchor\">http://ww2.kqed.org/mindshift/2011/09/13/what-drives-college-students-to-learn/</a><br /><br /><b><i>&quot;What Drives College Students to Learn?</i></b><br /><i>By Tina Barseghian</i><br /><i>SEPTEMBER 13, 2011</i><br /><br /><i>By Emily Hanford, American RadioWorks</i><br /><br /><i>Back in the late 1970s a colleague came to David Hestenes with a problem.</i><br /><br /><i>The two of them were physics professors at Arizona State University. Hestenes was teaching mostly graduate students, but his colleague was teaching introductory physics, and the students in his classes were not doing well. Semester after semester, the class average on his exams never got above about 40 percent.</i><br /><br /><b><i>&#39;And I noted that the reason for that was that his examination questions were mostly qualitative, requiring understanding of the concepts,&#39;</i></b><i> says Hestenes.</i><br /><br /><b><i>Most professors didn’t test for this kind of understanding; students just had to solve problems to pass the exams.</i></b><br /><br /><i>This observation prompted a series of conversations between Hestenes and his colleague about the difference between being able to solve problems and really understanding the concepts behind those problems. </i><b><i>They had a sneaking suspicion students were just learning the problem-solving part and never really getting the concepts.</i></b><br /><br /><b><i>Testing Understanding</i></b><br /><br /><b><i>Hestenes and one of his graduate students, Ibrahim Halloun, decided to test this theory by coming up with a way to probe students’ conceptual understanding of physics.</i></b><br /><br /><b><i>They developed a multiple-choice test, now known as the Force Concept Inventory, or FCI.</i></b><i> Here is the first and probably easiest question on that test:</i><br /><br /><i>Two metal balls are the same size but one weighs twice as much as the other. The balls are dropped from the roof of a two-story building at the same instant. The time it takes the balls to reach the ground will be</i><br /><br /><i>1. about half as long for the heavier ball as for the lighter one.</i><br /><i>2. about half as long for the lighter ball as for the heavier one.</i><br /><i>3. about the same time for both balls.</i><br /><i>4. considerably less for the heavier ball but not necessarily half as long.</i><br /><i>5. considerably less for the lighter ball but not necessarily half as long.</i><br /><br /><i>This is a fundamental concept in physics but even some people who have taken physics courses get this question wrong. The correct answer is not what intuition suggests (we’ll get to the right answer in a minute).</i><br /><br /><b><i>Hestenes and Halloun gave their test to about 1,000 students in introductory physics courses taught by seven different instructors at two different schools.</i></b><br /><br /><i>Each class was taught in a traditional lecture mode, though the instructors had different styles. One did a lot of elaborate demonstrations. One emphasized problem-solving. Another was a theoretical physicist who devoted a lot of time to talking about the conceptual structure of physics.</i><br /><br /><i>Students in these seven different classes took the test </i><b><i>at the beginning of the semester.</i></b><i> Perhaps not surprisingly, they didn’t do very well (though many of them had already taken high school physics; Hestenes and Halloun expected them to do better).</i><br /><br /><b><i>The students took the test again at the end of the semester. And they still didn’t do very well. Their scores went up by only about 14 percent, meaning that, after an entire semester, they understood only about 14 percent more about the fundamental concepts of physics than they had at the beginning.</i></b><br /><br /><i>The results were published in a series of articles in the American Journal of Physics in </i><b><i>1985.</i></b><br /><br /><i>Hestenes says the editor of the American Journal of Physics was fascinated by the findings, </i><b><i>but there was not a lot of immediate interest from his fellow physics professors.</i></b><br /><br /><b><i>Even the ones who’d participated in the experiment &#39;didn’t pay any attention&#39; to the results, says Hestenes. &#39;One reason may be that they really didn’t know what to do.&#39;</i></b><br /><br /><i>Lecturing was the way just about everyone taught introductory physics. To think there was something wrong with the lecture meant physics instructors would &#39;have to really change the way they do things,&#39; says Hestenes.</i><br /><br /><b><i>A lot of them ignored his study and kept teaching the way they always had. They insisted their lectures were working just fine.</i></b><br /><br /><b><i>Taking It to Heart</i></b><br /><br /><i>But Eric Mazur was unusual, says Hestenes. &#39;He was the first one who took it to heart.&#39;</i><br /><br /><b><i>Mazur is a physics professor at Harvard University.</i></b><i> He came across Hestenes’s articles in 1990, five years after they’d been published.</i><br /><br /><i>To understand why the articles had such a big impact on Mazur you have to know some things about his history.</i><br /><br /><i>Mazur grew up dreaming of becoming an astronomer.</i><br /><br /><i>&#39;When I was five years old I fell in love with the universe,&#39; he says. &#39;I tried to get my hands on to every accessible book on astronomy. I was so excited by the world of science.&#39;</i><br /><br /><b><i>But when Mazur got to university, he hated the astronomy classes.</i></b><br /><br /><b><i>&#39;It was all sitting in the lecture, and then scribbling down notes and cramming those notes and parroting them back on the exam,&#39; he says. &#39;Focusing on the details, focusing on memorizing and regurgitation, the whole beauty of astronomy was lost.&#39;</i></b><br /><br /><i>So he switched to physics. It wasn’t as heartbreaking for him to sit in a physics lecture and memorize things.</i><br /><br /><i>Mazur eventually got a Ph.D. in physics and a job at Harvard University. </i><b><i>Like most Ph.D.s, Mazur never got any training in how to teach.</i></b><br /><br /><b><i>&#39;I just mimicked what my instructors had done to me. I think that’s what we all do. So, I lectured.&#39;</i></b><br /><br /><i>Turns out he loved lecturing. It’s a lot more fun being on stage delivering a lecture than it is sitting in the audience watching. And that’s exactly what a lecture is, says Mazur: a performance. He decided to make it fun.</i><br /><br /><i>&#39;Thanks to the setup we have here at Harvard, it was very flashy, like a Hollywood show,&#39; he says. &#39;Attention-grabbing demos, me shooting through the lecture hall in a rocket car.&#39;</i><br /><br /><i>Mazur’s students apparently loved it. His classes were full and he got great evaluations from the students at the end of every semester.</i><br /><br /><i>&#39;For a long while, I thought I was doing a really, really good job,&#39; he says.</i><br /><br /><b><i>Not My Students</i></b><br /><br /><i>Then Mazur read the articles by Hestenes and Halloun. </i><b><i>Mazur’s first instinct was to dismiss the results. The test covered such basic material; he was sure his students were learning this stuff.</i></b><br /><br /><i>But what if they weren’t? How boring it would be to learn physics and never really understand the fundamental concepts that make physics so fascinating. Mazur thought back to his own experience with astronomy; if his students were just memorizing information and solving problems, he had to know, and he had to do something about it.</i><br /><br /><i>So he gave them the FCI, and he was shocked.</i><br /><br /><b><i>&#39;They didn’t do much better,&#39; he says. &#39;In fact, when they looked at the test that I gave to them some students asked me, &#39;How should I answer these questions? According to what you taught me, or according to the way I usually think about these things?&#39; That’s when it started to dawn on me that something was really amiss.&#39;</i></b><br /><br /><b><i>What Mazur and other physicists have come to understand is that one reason it’s hard for students to learn physics is that they come into class with a very strong set of intuitive beliefs about how the physical world works.</i></b><br /><br /><b><i>&#39;We can function quite well using these intuitive beliefs,&#39; says Mazur. &#39;We can push a chair on the floor, we can throw a ball, even though we’ve never studied parabolic trajectories and even though we’ve never really understood forces and friction.&#39;</i></b><br /><br /><i>But it turns out that many of these intuitive notions do not square with what physicists have discovered about how things actually work.</i><br /><br /><i>Most people’s intuition tells them if you drop two balls of different weights from the second story of a building, the heavier ball will reach the ground first.</i><br /><br /><i>But it doesn’t. The answer to the question above is #3: The balls reach the ground at the same time.</i><br /><br /><b><i>According to the results of the Force Concept Inventory test ... most people who take conventional lecture-based courses don’t end up with a good understanding of the fundamental concepts of physics.</i></b><br /><br /><i>This is a very difficult concept for most students to understand because they already have a concept in their mind that’s in conflict with this new concept.</i><br /><br /><i>&#39;Once you understand physics you can connect those two concepts and you can see everything as part of a coherent set of laws and framework of laws,&#39; says Mazur.</i><br /><br /><i>But according to the results of the Force Concept Inventory test, which has now been given to tens of thousands of students around the world, most people who take conventional lecture-based courses don’t end up with a good understanding of the fundamental concepts of physics.&quot;</i><br /><br />A person reading this article might not fully appreciate from that sentence the seriousness of the claim being made.  Let&#39;s take a quick look at the abstract of Hestenes&#39; paper titled &quot;Force Concept Inventory&quot; at <a href=\"http://modeling.asu.edu/R&amp;E/FCI.PDF\" class=\"ot-anchor\">http://modeling.asu.edu/R&amp;E/FCI.PDF</a> ...<br /><br /><i>&quot;Published in: The Physics Teacher, Vol. 30, March 1992, 141-158</i><br /><b><i>Force Concept Inventory</i></b><br /><i>David Hestenes, Malcolm Wells, and Gregg Swackhamer</i><br /><br /><i>Every student begins physics with a well-established system of commonsense beliefs about how the physical world works derived from years of personal experience. </i><b><i>Over the last decade, physics education research has established that these beliefs play a dominant role in introductory physics. Instruction that does not take them into account is almost totally ineffective, at least for the majority of students.</i></b><br /><br /><i>Specifically, it has been established that (1) commonsense beliefs about motion and force are incompatible with Newtonian concepts in most respects, (2) conventional physics instruction produces little change in these beliefs, and (3) this result is independent of the instructor and the mode of instruction. </i><b><i>The implications could not be more serious. Since the students have evidently not learned the most basic Newtonian concepts, they must have failed to comprehend most of the material in the course. They have been forced to cope with the subject by rote memorization of isolated fragments and by carrying out meaningless tasks. No wonder so many are repelled! The few who are successful have become so by their own devices, the course and the teacher having supplied only the opportunity and perhaps inspiration.&quot;</i></b><br /><br />It should be clear that this finding bears meaning beyond just the microcosm of science education.  The researchers have demonstrated that the universities have so completely failed to evaluate the effectiveness of their own instruction techniques that they have truthfully left the public rather vulnerable to any claim that the scientific community would have them believe.<br /><br />Getting back to the article ...<br /><br /><b><i>&quot;&#39;I think what many students in their introductory physics courses do is they retain their intuitive notions,&#39; says Mazur. They memorize what the professor tells them and &#39;parrot it back&#39; on the exam but they never really connect what they are learning to what they already think about how the physical world works.</i></b><br /><br /><b><i>The way he really learned physics was to teach himself.</i></b><br /><br /><i>&#39;I am what I am not because of my education but probably in spite of it,&#39; he says.</i><br /><br /><b><i>&#39;They Learn It on Their Own.&#39;</i></b><br /><br /><i>David Hestenes says this is true for a lot of students.</i><br /><br /><b><i>&#39;If you look at what’s happening in the introductory classes, even at the best schools, the classes only seem to be really working for about 10 percent of the students,&#39; he says. &#39;And I think all the evidence indicates that these 10 percent are the 10 percent of students that would learn it even without the instructor. They essentially learn it on their own.&#39;</i></b><br /><br /><i>A whole field of education research has emerged from what physicists have learned about the problems with the traditional lecture. There are now Physics Education Research groups at dozens of universities and a long list of peer-reviewed studies that confirm what they have found. </i><b><i>It’s not just physics where lectures fail; the traditional lecture is not an effective way to teach any subject.</i></b><br /><br /><b><i>&#39;Students have to be active in developing their knowledge,&#39; says Hestenes. &#39;They can’t passively assimilate it.&#39;</i></b><br /><br /><i>The idea that people learn better when they’re actively engaged is one of the central findings from an explosion of cognitive research conducted over the last several decades.</i><br /><br /><b><i>&#39;Students come to the classroom with preconceptions about how the world works,&#39; according to a major report by the National Research Council. &#39;If their initial understanding is not engaged, they may fail to grasp the new concepts and information that are taught, or they may learn for the purposes of a test but revert to their preconceptions outside the classroom.&#39;</i></b><br /><br /><b><i>Changing the Way Professors Teach</i></b><br /><br /><i>Researchers and instructors have developed a number of &#39;interactive-engagement&#39; techniques in recent years that have proven more effective than lectures for teaching large classes. Mazur uses an approach that he calls &#39;peer instruction.&#39; It’s hard to know how many instructors are using these approaches, but experts say most large, introductory classes – especially in the sciences – are still taught using the conventional lecture method.</i><br /><br /><i>Mazur says even now, years after the first articles about the FCI and the establishment of dozens of Physics Education Research programs, </i><b><i>a lot of professors still have a hard time getting their heads around the idea that there is anything wrong with lecturing.</i></b><br /><br /><i>&#39;Most of the people who are teaching are products of this approach to teaching,&#39; he says. &#39;They were successful in this approach.&#39;</i><br /><br /><i>He says they are the ones, like him, who taught themselves, because they were interested and motivated to learn. But Mazur says professors need to do better, because the economy is changing; more people need to succeed in college now.</i><br /><br /><i>Mazur believes the challenge for educators in the 21st century is to find new ways to reach more students, and to help them learn better than they ever have before.&quot;</i><br /><br />That article has by now become quite famous amongst science education reformers, but the subject is rarely brought up outside of the specific context of science education.  Few people have thought to draw any connections to things happening within other domains, and this is a mistake.<br /><br />For example, the intuitive beliefs which the physics education researchers mention are surely the same phenomenon as Nobel laureate Daniel Kahneman&#39;s System 1 which he discusses at <a href=\"https://www.youtube.com/watch?v=MgQutgSwY88\" class=\"ot-anchor\">https://www.youtube.com/watch?v=MgQutgSwY88</a> (See <a href=\"https://plus.google.com/108466508041843226480/posts/Q12YRp5rwFP\" class=\"ot-anchor\">https://plus.google.com/108466508041843226480/posts/Q12YRp5rwFP</a> for transcript and diagrams).<br /><br />Of course, education critics like John Taylor Gatto (<a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/EfntTNmBpyB\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/EfntTNmBpyB</a>) and Jeff Schmidt (<a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/JUHGSuFSXPQ\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/JUHGSuFSXPQ</a>) would probably both argue that this failure of the university system to teach is not actually an accident.<br /><br />Whether or not they are right, a reasonable guess is that the universities fear the FCI.  It&#39;s not difficult to imagine a scenario where some universities start reporting their FCI-delta&#39;s (diff between start and end of first-semester physics course), and suddenly they would be forced to once again compete on the quality of their instruction.  And, the list of concept inventories is growing.  Wikipedia mentions statistics, chemistry, astronomy, basic biology, natural selection, genetics, engineering, and geoscience -- which is honestly impressive, because these tests must completely lack any technical jargon that would preclude a student from being able to understand the questions before they&#39;ve taken the course.<br /><br />Mazur&#39;s story is really quite important, and therefore the correct follow-up to these FCI claims.<br /><br />From Confessions of a Converted Lecturer, a talk by Eric Mazur at <a href=\"https://www.youtube.com/watch?v=WwslBPj8GgI\" class=\"ot-anchor\">https://www.youtube.com/watch?v=WwslBPj8GgI</a><br /><br />05:23 - 06:10<br /><i>&quot;Now, all along, there were signs that something was wrong.  For example, some students would write at the bottom of their end-of-semester evaluation &#39;Physics is boring&#39; or &#39;Physics Sucks&#39;.  I could never make any sense of it, and therefore, I preferred to concentrate on the positive side and ignore these negative ones.  You know, my dentist … recently told me … &#39;Aaaahhhh, you&#39;re a physicist.  I got an A for physics in college, but I really didn&#39;t understand anything.&#39;  It always bothered me when I hear these things, and I never know how to react.  I never understood what exactly the cause of it is.&quot;</i><br /><br />[Explains FCI&#39;s]<br /><br />13:22 - 13:29<br /><b><i>&quot;In other words, it doesn&#39;t make any difference what we do in front of the students, they learn next to nothing.&quot;</i></b> [laughter]<br /><br />15:36 - 16:17<br /><i>&quot;Professor Mazur, how should I answer these questions?  According to what you taught me, or according to the way I usually think about these things.  I had no idea how to answer that question.  And by the time the test had been completed, I had been dragged out of my ivory tower ... that really changed my whole perception of teaching, and changed my teaching career ... </i><b><i>In spite of high evaluations and in spite of good performance, my students were not really learning very much.&quot;</i></b><br /><br />17:21 - 18:04<br /><b><i>&quot;It&#39;s very important to have data in education. I often go to faculty meetings where my colleagues and I talk about teaching, and there are quite a few Nobel laureates around me. It is always surprising to me how whenever the discussion shifts from physics to education, people - even the most reputable scientist - completely abandon the scientific method.  All of a sudden, the discussion is about anecdotes ... My students learn better when blah blah blah ... Or, my students like it when I do this, as if liking equates better learning.&quot;</i></b><br /><br />22:38 - 22:51<br /><i>&quot;Somebody once told me that the lecture method is a process whereby the lecture notes of the instructor get transferred to the notebooks of the students without passing through the brains of either!&quot;</i><br /><br />24:40 - 25:37<br /><i>&quot;Imagine I had been teaching Shakespeare rather than physics, would have asked the students to come to class, and then said: Today, we are going to cover a Midsummer Night&#39;s Dream, and then opened the collective works of Shakespeare and then started reading a Midsummer Night&#39;s Dream to them?  No, of course not.  I would have said to them: Next Wednesday, we&#39;re going to discuss a Midsummer Night&#39;s Dream.  If you haven&#39;t read a Midsummer Night&#39;s Dream, read it before coming to class.  And as a student, you would have known that if you had not read the play, or at least Cliff Notes of the play, you might as well stay in bed because you would not be able to contribute to the discussion.  But, that&#39;s not what we do in the sciences.  In the sciences, we still mostly focus on what I call information transfer, because </i><b><i>this method of teaching really focuses upon just a one-way transfer of information.&quot;</i></b><br /><br />25:49 - 26:06<br /><i>&quot;500 years ago, Gutenberg invented the printing press -- the biggest invention of IT that you can imagine.  And here we are in the 21st century, and we&#39;re really not using this invention to its full potential.&quot;</i><br /><br />26:44 - 27:18<br /><i>&quot;So, I would argue that </i><b><i>there&#39;s much more that needs to happen than just delivering information.</i></b><i> And the fact that delivering information was not enough became clear when I gave this FCI.  What is it that needs to happen? ... </i><b><i>What needs to happen is that the student needs to make sense of the information, build mental models -- not just remembering facts, but try to understand it and build mental models that you can use in other contexts. I would call that assimilating the information.</i></b><i>&quot;</i><br /><br />27:18 - 28:05<br /><i>&quot;Now, where did that happen for us, who became faculty members or teachers?  Did that happen in the classroom? ... I think, in retrospect, I always thought it was my teachers who taught me everything.  I think that a lot of it happened outside of the classroom when I tried to figure out things and do things -- not in the classroom.  In the classroom was mostly taking down information and perhaps writing down questions which I had to perhaps address later on. </i><b><i>So, in the traditional approach to teaching, we put all of the emphasis on the easy part -- the transfer of information. We leave the hard part -- because we would all quickly agree that the second part is the hard part -- we leave that to the student on his or her own.&quot;</i></b><br /><br />28:34 - 29:54<br /><i>&quot;Anybody scoring below 23 has no clue of Newtonian mechanics ... Is still, partially, an Aristotelian thinker ... </i><b><i>Now, notice that the incoming students at Harvard, most of whom had a 5 on an AP physics exam, are still Aristotelian thinkers ... Here is the post-class distribution. Okay, yeah, there is some gain. But, notice that still more than 50% of the class scores 23 or lower ... They have not understood the material taught in week two, upon which everything else hinges.&quot;</i></b><br /><br />31:34 - 31:55<br /><i>&quot;And notice that if you take all of these data together, there&#39;s a trend.  I can draw a line through it, and that shows that regardless of the setting, and the selectivity of the school, and the instructor, the gain that is accomplished in a traditionally taught class is only a quarter of what it could -- and should -- be.&quot;</i><br /><br />32:11 - 32:23<br /><i>&quot;When I saw how poorly my students had done, my first reaction was: Well, maybe you&#39;re not such a good teacher after all ... But, that could obviously not be true, right, so I didn&#39;t think about that too long.&quot;</i><br /><br />38:55 - 39:13<br /><i>&quot;Well, when I graded it, it was clear that we had a problem.  So, here are the histograms.  On the left, you see the histogram for the conventional problem, and the average was about 7 out of 10.  And on the right, you see the histogram for the conceptual problem, with an average of about 5 out of 10.&quot;</i><br /><br />41:22 - 43:07<br /><i>&quot;So let&#39;s plot, for each student, the performance on one type of problem as a function of the other type of problems.  Well, I&#39;m afraid that there&#39;s not much of a correlation.  The data point is all over the place, but if you&#39;re willing to relax your definition of a line somewhat, you find that about 50% of the students do equally well here, on both types of problems ... The student who does well on the conceptual problem tends to do well on the conventional problem, so if you understand the basics, you&#39;re gonna do well on the conventional problem. </i><b><i>However, there&#39;s 40% that does well on the conventional problem, but has no clue on the basics. What are these 40%? When I saw this diagram, everything fell into place. How would you characterize these 40%? How do they get there? Who are these students? What are they doing? Memorizing ... Plug and chug, right?&quot;</i></b><br /><br />44:12 - 44:38<br /><i>&quot;I told you in the beginning that some students told me that physics was boring, and i could never imagine it.  But, imagine that it has been reduced to applying recipes -- recipes you do not understand -- as you can see from the data, blindly, and I&#39;d say, &#39;yeah ... physics then becomes very boring because there is no mental exercise other than following a recipe that is involved.&#39;&quot;</i><br /><br />44:40 - 45:19<br /><i>&quot;Here&#39;s the recipe.  There&#39;s the problem.  And you try, and you try, and it doesn&#39;t work.  Why? </i><b><i>Because, as we all know, not all problems can be solved by recipe.</i></b><i> Now, imagine that you are a student who has been led to believe that there is such a thing as a recipe-based approach to solving problems.  But, then you discover on your first homework set that the recipe only works 75% of the time.  This is madness, right?  I mean, why would a professor teach you something that only works 75% of the time?  And you don&#39;t understand why it works sometimes, and sometimes it doesn&#39;t.  I can imagine getting frustrated.  So, frustration, boredom is very likely to happen in that large group in my class.&quot;</i><br /><br />49:33 - 51:19<br /><b><i>&quot;So, I didn&#39;t really know what to do anymore because still they could not articulate a coherent question ... But, I knew that 40% of the students had given the right answer on this question on the fci, so ... I said, &#39;Why don&#39;t you turn to your neighbor and start discussing it with your neighbor?&#39; And the whole classroom erupted in chaos. I had never seen that.</i></b><i> Usually, I would lecture.  Then, I would stop, and I would say, &#39;Does anybody have a question?  Anybody?&#39;  And you wait, students look down, because ... no eye contact ... And then if you wait long enough, it&#39;s always the same person on the front row who reluctantly raises her hand.  It&#39;s like pulling teeth.  They forget about me standing at the front of the classroom. </i><b><i>And what happened is that the students who had the right answer pretty quickly convinced those who did not have the right answer.</i></b><i> And you know there&#39;s a certain irony to that.  What I found out was that if you have two students sitting next to each other, let&#39;s say John and Mary.  Mary has the right answer for the right reason.  John does not have the right answer because he doesn&#39;t understand it yet.  Mary is more likely to convince John than the other way around, simply force of logic.  But, what I realized is that Mary is more likely to convince John than Professor Mazur at the front of the class there.  Why?  Because Professor Mazur learned it such a long time ago that he has no clue about the difficulties going on in John&#39;s mind.  Right?  The better you know something, the more difficult it becomes to teach because you&#39;re no longer aware of the conceptual difficulties of the beginning learner, whereas Mary has only recently learned it, and still has a pretty good idea of what John is struggling with.&quot;</i><br /><br />51:51 - 51:56<br /><i>&quot;So, it&#39;s ironic: The better you know something, the more difficult it becomes to teach.&quot;</i><br /><br />59:42 - 1:00:16<br /><i>&quot;In 1991, I decided you don&#39;t derive benefit from seeing a physicist solve problems ... Suppose you want to learn to play the piano ... you have to play the piano. When you want to train for a marathon, you don&#39;t sit on the couch eating popcorn watching dvd tapes of marathon runners. You have to do the running! If you wanna learn problem-solving, you&#39;ve gotta do the problem-solving. You don&#39;t learn by watching somebody else do it.&quot;</i><br /><br />I would say that the same is true, actually, of &quot;thinking like a scientist&quot;: You cannot simply &quot;think what scientists think&quot;, and expect to use that information to evaluate scientific controversies or the big questions in science.  You have to actually engage scientific controversies, and do so in a meaningful manner.<br /><br />1:01:51 - 1:02:38<br /><i>&quot;This means that better understanding leads to better problem solving.  Makes sense in hindsight, right?  But -- and this is the most important message I want to leave you with -- the converse of this statement is not true. </i><b><i>Good problem solving does not necessarily mean understanding. I had been fooling myself for many years to believe that I was an effective professor, based upon the performance of my students.  But, after all, it was just a house of cards. They were just plugging and chugging ... And other studies have shown that retention is very, very low when you learn problem-solving by rote.&quot;</i></b><br /><br />1:04:33 - 1:04:40<br /><b><i>&quot;Change always comes from outside, not from inside -- which is probably why I&#39;m here talking about it.&quot;</i></b><br /><br />1:09:57 - 1:10:03<br /><b><i>&quot;Professor Mazur is not teaching us anything.  We have to learn it all ourselves.&quot;</i></b><br /><br /><b>But, Why Don&#39;t Students Simply Enjoy Meaningful Learning More than Rote Memorization?</b><br /><br /><i>&quot;We have found  in our work with both secondary students and university students that the majority would prefer to get by with memorizing information rather than working to build conceptual understanding.  Years of school experience with evaluation practices that require little more than rote recall may be at least party to blame.  Perkins  (1992) also identifies other factors that discourage what he calls complex cognition:</i><br /><br /><i>&#39;Complex cognition has more intrinsic interest and promises more payoff outside of school and later in life.  But consider the cost to learners: complex cognition demands much more effort.  It creates greater risk of failure.  It introduces the discomforts of disorientation, as learners struggle to get their heads around difficult ideas.  Peer status for complex cognition is certainly mixed; who wants to be known as a &#39;brain?&#39;  And very commonly, so far as grades and teacher approval go, complex cognition buys students no more than the simpler path of getting the facts straight and the algorithms right.  No wonder, then, that students perfectly reasonably do not automatically gravitate toward complex cognition. (pp.59-60)&#39;&quot;</i> (p77, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, Joseph D. Novak)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Positivists vs the Constructivists","summary":"Two Radically Different Approaches to Science Education / The Top-Down Version Has Traditionally Dominated / Its Bottom-Up Opposite is How Ideas in the Sciences Change","image":"https://lh3.googleusercontent.com/-56bg_lObUgc/VsF5tIvKl-I/AAAAAAAAGuE/aWYRUCNA10w/w3600-h5040/the-positivists-vs-the-constructivists-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/H1akZRDWs5Y","publishDate":"2016-02-15T07:11:31.811Z","updateDate":"2017-01-03T02:57:03.535Z","text":"<b>The Positivists vs the Constructivists: Two Radically Different Approaches to Science Education / The Top-Down Version Has Traditionally Dominated / Its Bottom-Up Opposite is How Ideas in the Sciences Change</b><br /><br /><i>&quot;Schools have been impressively successful in spreading the myth that science has a special method of arriving at truth, that scientific truth is free from value judgments, transcends all cultures, and holds for all time.  Any discipline that canot use the method of science, the myth holds, cannot establish &#39;objective&#39; knowledge; in short, the discipline cannot establish immutable truths.  Given the historical fact that scientific &#39;truth&#39; has changed from the times of Copernicus, Galileo, Kepler, Newton and Dalton, it seems incredible that the myth of immutable and culture-free science is so persistent.&quot;</i> (p38, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br /><b>The Origin of Constructivism: Objective Science, Without Worldview</b><br /><br /><i>&quot;From 1620 on, the Novum Organum was probably the most influential writing on scientific method; that is, until 1892, when the first edition of Karl Pearson&#39;s Grammar of Science was published.  Pearson was even more adamant than Bacon on the fundamental importance of observation.  Here are a few passages from his Grammar (1900):</i><br /><br /><i>&#39;The unity of all science consists alone in its method, not in its material.  The man who classifies facts of any kind whatever, who sees their mutual relation and describes their sequences, is applying the scientific method and is a man of science ...&#39;</i><br /><br /><i>&#39;... The civil law is valid only for a special community at a special time; the scientific law is valid for all normal human beings, as is unchanging ...&#39;</i><br /><br /><i>&#39;... The right of science to deal with the beyond of sense-impressions is not the subject of contest, for science confessedly claims no such right ...&#39;</i><br /><br /><i>&#39;... Science, as I have so often reiterated, takes the universe of perceptions as it finds it, and endeavors briefly to describe it.  It asserts no perceptual reality for its own shorthand ...&#39;</i><br /><br /><b><i>We see in the writings of Bacon and Pearson that the pursuit of science is primarily the dispassionate observation of nature. Moreover, it is assumed that all persons see the same events when they observe a phenomenon; personal experience, frames of reference, past conceptual development, emotional responses to a phenomenon -- none of these should influence what the &#39;scientific&#39; observer sees. This is the myth that has been sold as the scientific method for generations.&quot;</i></b> (p40-41, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br /><b>Science as Careful Observation Takes a Hit</b><br /><br /><i>&quot;Some very important developments occurred after the publication of Pearson&#39;s book.  Albert Einstein published his theory of relativity (1905); Gregor Mendel&#39;s original (1859) papers were rediscovered early in the twentieth century and the science of genetics was born; in the same period the cell theory was elaboratred, connecting the structures and functions of cells with those of organisms, and the science of psychology was developed; in 1930 the quantum theory was advanced. </i><b><i>All of these developments put considerable strain on the Baconian view that science consists mainly of careful observation. It began to appear that much remained to be said about hypothesis and theory building, and about the testing of theories.&quot;</i></b> (p41, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br /><b>Karl Popper&#39;s Oversight</b><br /><br /><i>&quot;Continuing in the Baconian tradition, Karl Popper published The Logic of Scientific Discovery (1934, English translation 1959), in which he analyzed the methods by which science advances through the &#39;falsification&#39; of untenable hypotheses.  Popper recognized that the stress on inductive reasoning from observed facts which characterized the philosophy of the positivists or logical positivists, as they were called, led to a trap, since one must finally deal with the question of what is a fact.  To Popper, the positivsts&#39; view, common in the 1920s and 1930s, must lead to an infinite regress, in which the quest for the irreducible facts must at some point lead to a priori assumptions. </i><b><i>The problem of what constitutes observable facts becomes inextricably confounded with the psychological problem of what it is we perceive.&quot;</i></b> (p40-41, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br />From wikipedia at <a href=\"https://en.wikipedia.org/wiki/A_priori_and_a_posteriori\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/A_priori_and_a_posteriori</a>: <i>&quot;Consider the proposition, &#39;If George V reigned at least four days, then he reigned more than three days.&#39; This is something that one knows a priori, because it expresses a statement that one can derive by reason alone.&quot;</i><br /><br /><i>&quot;Popper&#39;s emphasis on the refutation of theories through the falsification of hypotheses derived from theories that took cognizance of the changing nature of scientific truth. </i><b><i>His contention that a theory can be regarded as true until it is falsified, however, continued to place emphasis on science as a search for &#39;truth,&#39; rather than as a means of developing functional conceptual models, in the knowledge that in time they will be modified or discarded.</i></b><i> Popper&#39;s work did give recognition to the evolutionary character of scientific knowledge even though his attention was focused on the methodology of science and not on the changing theories or scientific conceptual frameworks.  Thus his work represented a transitional stage from Baconian and Pearsonian views of science to more contemporary ones.&quot;</i> (p42, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br /><b>Worldviews in Science Become Recognized</b><br /><br /><i>&quot;By 1950 another view of science was being popularized by persons whose attention was focused on the history of scientific discoveries rather than on analysis of scientific method.  Perhaps the most distinguished in this group was James Conant, chemist, historian of science, and later president of Harvard University.  Conant published a popular account of his view that methods of science could be understood best by the study of case histories selected to illustrate ways in which scientists have worked in the past.  His book On Understanding Science (1947) influenced my thinking, and the course he instituted at Harvard influenced the thinking of many students.  One person whose association with Conant led to a change in careers was Thomas Kuhn.  As a member of Conant&#39;s case history experimental course, Kuhn was impressed by the views of science suggested by the case histories, views that were radically different from those he had come to know as a student of physics.  As a result, Kuhn shifted to the study of the history and philosophy of science.</i><br /><br /><b><i>Kuhn&#39;s analysis led him to see that science was characterized more by the paradigms employed by scientists than by their methods of inquiry.</i></b><i> In his Structure of Scientific Revolutions (1962), Kuhn describes paradigms in a variety of ways (for which he has been criticized); </i><b><i>his primary notion, however, is that a paradigm is a conceptual scheme or a pair of perceptual goggles through which scientists in a given discipline view problems in that field. In Kuhn&#39;s view, a researchable problem and the methods to be used to solve that problem are primarily determined by the relevant paradigm used by scientists</i></b><i> ...</i><br /><br /><i>The history of science shows, of course, that paradigms used by scientists have changed over time. </i><b><i>Kuhn&#39;s central thesis is that this change is comparatively abrupt. He describes two kinds of scientific activity, one of which is characteristic of most of the work scientists do when they conduct experiments and make observations, and which he calls &#39;puzzle solving.&#39; The other kind of scientific work is much less common and most scientists never contribute to it, since only rare creative scientific genius can invent a new paradigm -- Kuhn&#39;s requirement for revolutionary science.</i></b><i> In contrast to puzzle solving, which constitutes much of normal science, in which research is guided by an accepted paradigm, revolutionary science arises when conflicting results are obtained or a paradigm fails to apply to an important group of problems; in this case a new paradigm must be invented and a scientific revolution ensues.&quot;</i> (p42-44, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br /><i>&quot;As Thomas Kuhn (1962, 1970) has pointed out, the history of science shows the changes that have occurred in the conceptual spectacles through which scientists have viewed reality.  An Aristotelian sees rocks as falling to the earth because that is where rocks seek to go, whereas a contemporary physicist views the earth and rock moving together at a rate determined by their masses and velocities and by the distance between them.  Both views are interpretations or &#39;conceptual goggles,&#39; but, unlike the Aristotelian view, the modern view explains why a rock (or satellite) can stay in orbit around the earth.  Kuhn has described the role of </i><b><i>conceptual goggles</i></b><i> -- or, as he calls them, paradigms -- in guiding the work of scientists.  Kuhn shows how over the past millenium new paradigms have been developed by creative scientists when old paradigms failed to explain apparent inconsistencies in what scientists were observing and what they expected to see.  Viewed with Aristotelian goggles, the sun is pulled around the earth in an invisible chariot, but through Copernican goggles the earth revolves around the sun, and through Einsteinian goggles the sun and earth are in motion relative to one another and to all other masses in the universe.  That old goggles (paradigms) are not always joyfully discarded is evident in the travails of Galileo, threatened with death, and from the continuing resistance to evolution as a concept helpful in understanding the origins of human beings.</i><br /><br /><i>Yehuda Elkana (1972) agrees in general with Kuhn&#39;s stress on the importance of paradigms in the work of scientists, </i><b><i>but he sees changes in paradigms as more gradual, more evolutionary than does Kuhn. Instead of conceiving of a &#39;scientific revolution&#39; in which an old paradigm is discarded and a new paradigm invented, Elkana contends that a kind of social evolution occurs, in which increasing numbers of scientists begin to see that more and more phenomena in a given field of inquiry are better explained by a new paradigm.</i></b><br /><br /><i>The differences between the thinking of Kuhn and Elkana are minor compared with the differences between their views and those of philosophers who place central emphasis on the research methodology used by scientists.  For example, Karl Popper, in his widely cited Logic of Scientific Discovery (1934, 1959), describes the role of instrumentation, research protocols, and logical processes of scientific inference as one proceeds from observations to conclusions. </i><b><i>Popper minimizes the conceptual framework of the scientist and stresses instead the scientist&#39;s method of analysis, as if they were independent of the goggles he is wearing. The difference between the views of Popper and those of Kuhn and Elkana might be of only esoteric interest were it not for the fact that much federally supported curriculum development in this country has been carried out as if Popper were right and Kuhn were wrong.&quot;</i></b> (p22-23, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br /><b>Toulmin&#39;s World of Competing Paradigm Populations</b><br /><br /><i>&quot;A problem with Kuhn&#39;s thesis is that sometimes competing paradigms coexist and both continue to function for years, as the wave and particle theories of light did.  When one paradigm eventually becomes dominant, it may prevail as a result of the gradual accumulation of information rather than through a sudden gestalt shift in the minds of practicing scientists.&quot;</i> (p45, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br />Toulmin: <i>&quot;A man demonstrates his rationality, not by a commitment to fixed ideas, stereotyped procedures, or immutable concepts, but by the manner in which , and the occasions on which, he changes those ideas, procedures and concepts.&quot;</i> (p47, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br />Kierkegaard: <i>&quot;Concepts, like individuals, have their histories, and are just as incapable of withstanding the ravages of time as are individuals.&quot;</i><br /><br /><i>&quot;In the evolution of concepts as in biological evolution, Toulmin argues, changes occur in </i><b><i>populations.</i></b><i> Novel concepts come and go, but they influence thinking in a given discipline only when conditions are favorable; only when the new concepts have &#39;advantages&#39; can they displace others.&quot;</i> (p52, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br /><b>Science as a Framework of Evolving Concepts and Contingent Methods for Gaining New Knowledge</b><br /><br /><i>&quot;Roszak and others who lead the counterculture movement against scientific objectivity and its product technology correctly identify many limitations of rational or objective methods of knowing.  Roszak recognizes that knowledge gained through so-called objective methods is of limited validity, for as our models of the world and our instrumentation change, old &#39;objective truths&#39; are often discarded.  He argues that the experience of the individual provides another kind of validity; it may not possess the &#39;objectivity&#39; of scientific or technological knowledge, yet it can be valuable in guiding actions.  Roszak (1969) states:</i><br /><br /><b><i>&#39;This would mean that our appraisal of any course of personal or social action would not be determined simply by the degree to which the proposal before us squares with objectively demonstrable knowledge, but by the degree to which it enlarges our capacity to experience: to know ourselves and others more deeply, to feel more fully the awesomeness of our environment.</i></b><i> This, in turn, means that we must be prepared to trust that the expanded personality becomes more beautiful, more creative, more humane than the search for objective correctness can make it ...&#39;</i><br /><br /><i>Too many people have been led to believe that science can solve all of our problems.  The notion that it can was derived in part from a misunderstanding of the nature and limitations of science bothy by spokesmen for science and technology and by scientists themselves. </i><b><i>When science is recognized as a framework of evolving concepts and contingent methods for gaining new knowledge, we see the very human character of science, for it is creative individuals operating from the totality of their experiences who enlarge and modify the conceptual framework of science.&quot;</i></b> (p19-20, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br /><b>The Effect of Past Legacy Philosophies on Education Research</b><br /><br /><i>&quot;Rote learning may be useful on occasions, such as when we memorize a poem, the score for a piece of music, or multiplication tables.  But the real value of rote learning comes when we also move to understand the meaning of what we have memorized and it is meaning that confers power to our learning. </i><b><i>The person who simply plays the notes he or she has memorized is, at best, a technician, whereas the artist understands and interprets the meaning of the music intended by the composer.</i></b><i> The good teacher helps to move the learner beyond rote learning by negotiating meanings with the learner.</i><br /><br /><i>For almost a century, most of the &#39;scientific&#39; research on learning was done with animals in laboratory settings.  The idea was that as &#39;basic knowledge&#39; about learning processes were elucidated through studies with animals, this knowledge could later be applied to improving education of human beings.  One of the prominent psychologists who had his early training in this &#39;scientific&#39; behavioral psychology later observed, </i><b><i>&#39;What was important was the promissory note that, once we understood simple conditioning [in animals], we would understand complex behavior [of humans]. The promissory note turned out to be a rubber check. At least, by 1966, nobody has been able to cash it in.&#39; (Mandler, 1967, p. 6). Nevertheless, the dogmas generated by behavioral psychologists remain very much in vogue and continue to guide practice in schools and corporations.</i></b><i> For example, Glasser (1994) observes this problem in corporations:</i><br /><br /><i>&#39;To review briefly, boss-managers, like almost all human beings, believe in and manage according to the traditional theory of human behvior; stimulus-response (S-R) psychology.  They follow it mostly because it supports their common-sense belief that people can be made, through reward or punishment, to do what the manager wants them to do whether they like it or not.  And, to some extent, they follow it because no one has ever offered them another theory.  They have nothing to turn to if they suspect, as I am sure many do, that what they believe may be wrong.  Therefore, it is not that they believe in S-R theory so absolutely that they cannot change.  It is more that, for almost all people, stimulus-response theory is all there is (p.48)&#39;&quot;</i> (p19, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><i>&quot;It should be remembered that the late 1930s to the early 1960s when Ausubel was formulating his ideas was also the heyday of behavioral psychology.  Not only in the field of psychology in general, but also in educational psychology, behaviorism was the overwhelmingly dominant paradigm, and, along with it, positivistic epistemology was also strongly in control ... </i><b><i>The key idea in positivistic views is that there is &#39;one true answer&#39; to questions, and these answers will be self-evident if we simply observe and record events carefully. Current ideas see that the nature of questions we ask, the kinds of records we make and especially the ways we interpret these records are dependent upon a whole set of contextual and conceptual factors. The views that Ausubel put forward in the early 1960s were strongly in discord with the prevailing behaviorist ideas and Ausubel experienced considerable difficulty in finding publication outlets in respectable journals of psychology or educational psychology. Recall also that Kuhn&#39;s book, The Structure of Scientific Revolutions, was published in 1962, and the movement toward newer epistemologies was only in its infancy at the time Ausubel&#39;s work emerged.</i></b><br /><br /><i>The prevailing behaviorist dominance ... created a hostile climate for many of Ausubel&#39;s ideas ...</i><br /><br /><i>Not surprisingly, Ausubel&#39;s ideas on learning made slow progress in the 1960s, even though there was a relatively immediate recognition of the importance of his work in some circles and the beginning of a substantial worldwide acceptance of his ideas outside of North America.  Remember also behavioral psychology did not succeed in dominating the thinking in most European and Eastern countries.&quot;</i> (p56-57, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><i>&quot;The insistence on restricting observation to external behavior is a psychological variation of the old philosophical doctrine of peripheralism, which emphasizes external sensorimotor processes rather than internal cognitive processes as determinants of behavior.  This doctrine arose in part from psychologists&#39; disenchantment with introspection, since one person&#39;s introspections too often do not agree with antoher&#39;s.  Peripheralism was also fostered by the empiricists&#39; view that only things publicly visible can count as observations.  Given the theological and mystical origins of much dogma regarding human behavior, it is understandable that peripheralism flourished in intellectual circles, and indeed it served some useful purpose in advancing human understanding at one time.  The problem is that cultures change, conceptions change, and peripheralism is an anachronism today.  Unhappily, behaviorist doctrines continue to flourish in education, largely without awareness of their inherent peripheralist philosophy, and thus they crowd educational niches that might be utilized more fruitfully by other concepts ...</i><br /><br /><i>&quot;Some people appear to be unalterably attached to a search for absolutes.  They don&#39;t want good or better answers, they want right answers.  People in search of absolutes frequently feel frustrated by human recalcitrance.  When such people work their way into high political offices, they can cause enormous damage, for their certainty of their goals leads them to believe that the ends justify any means they may use to reach those goals.  But society is not formed of absolute truths, and those who think they have found some eventually come into conflict with the rest of humanity, a conflict they have always lost.&quot;</i> (p62-63, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br /><b>The Effect of the Positivistic Worldview on Learning</b><br /><br /><i>&quot;We found a strong trend toward meaningful learning for those students who held constructivist ideas and an inclination to prefer rote learning for those students who held positivistic ideas (Edmondson and Novak, 1993).  Songer and Linn (1991) reported similar findings.  One of the reasons I believe it is difficult to move some students toward meaningful learning strategies is the deterrent effect of their positivistic thinking.&quot;</i> (p107-108, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><b>The Distinction Between Public Knowledge and Private Learning</b><br /><br />From chapter 3 of <i>Cognitive Structure and Conceptual Change</i>, titled &quot;Describing the Cognitive Structures of Learners Following Instruction in Chemistry&quot; (page 30) ...<br /><br /><i>&quot;The nature of knowledge has interested philosophers for over 400 years and </i><b><i>it is certainly not easy to separate the epistemological from psychological aspects of it. Nonetheless, we have found it useful to distinguish between public knowledge and private understandings,</i></b><i> relating them (approximately) with knowledge and the result of learning, respectively. Science exists as public knowledge, in text books and in scientific papers. When individuals read (or are told) this public knowledge, they interpret and internalize it in their own way. We have all experienced this. We read a paper, and understand it. Then we read it again a while later and find we understood it much more. Our &#39;understandings&#39; between these two readings is different. So too are the &#39;understanding&#39; of other people who read the paper. In reading a paper (or in learning any public knowledge via any learning mode) we may miss some bits of the information or we may put the bits together in different ways from that presented or implied. Further, when we learn any bit of information, we relate it to our own previous knowledge and experience, so that the understanding of each of those bits of knowledge, the meanings that we give to them, are idiosyncratic to some extent. </i><b><i>Thus, our private understanding is not fixed; it is not the same as that of other people; it is not the same as the public knowledge.</i></b><br /><br /><i>We should now turn this discussion on its head. Public knowledge is derived from the private understandings of individuals. Public knowledge exists because there is a substantial overlap between the private understandings of different individuals. Sutton (1981) explains it this way:</i><br /><br /><i>&#39;Though a metal does not mean exactly the same thing to you as to me, there are many common features, upon which we can form an agreed definition -- a metal is conducting, formable, tending to be electropositive, etc. However, such public knowledge represents only a part (the denotative part) of the meaning that particular persons carry, and is to some extent an abstraction, not existing in any individual&#39; (p. 3)</i><br /><br /><i>Public knowledge is defined and definable.</i><br /><br /><i>[…]</i><br /><br /><i>These things are knowledge, but they are not learning (see, for example, Popper, 1974, The Philosophy of Karl Popper, pp 147-148). </i><b><i>Learning is giving personal meaning to public knowledge. It is developing one&#39;s own understanding of that public knowledge.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"Rote Memorization v.s. Meaningful Learning","summary":"School Testing and University Programs Commonly Encourage Rote Memorization, Yet Research Suggests that Meaningful Learning is the Better Approach / Distinction Explains Why Students Can Ace a Course, Then Completely Forget It / Also Why Some Become Better at Learning Large, Complex Subjects and Creative Problem-Solving","image":"https://lh4.googleusercontent.com/--rexLslf2NM/Vr-6dKHxTYI/AAAAAAAAGrs/J_VctjCPGDs/meaningful-vs-rote-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/TLvchcLi2Zn","publishDate":"2016-02-13T23:23:51.759Z","updateDate":"2017-01-03T02:57:34.102Z","text":"<b>Rote Memorization v.s. Meaningful Learning: School Testing and University Programs Commonly Encourage Rote Memorization, Yet Research Suggests that Meaningful Learning is the Better Approach / Distinction Explains Why Students Can Ace a Course, Then Completely Forget It / Also Why Some Become Better at Learning Large, Complex Subjects and Creative Problem-Solving</b><br /><br />The collage of students pictured have just graduated from Harvard.  And on their graduation day, they are asked -- on camera -- to explain the cause of seasons on Earth.  The misconception that the seasons were the result of a highly elliptical orbit around the Sun dominated the responses, raising very serious questions about the effectiveness of our approach to science education.  The opening narration on this clip (at <a href=\"https://www.youtube.com/watch?v=p0wk4qG2mIg\" class=\"ot-anchor\">https://www.youtube.com/watch?v=p0wk4qG2mIg</a>) demonstrates the seriousness of the problem in under two minutes flat ...<br /><br /><i>&quot;Despite a lifetime of the very best education, students in our classrooms are failing to learn science.  Many of these students will graduate from college with the same scientific misconceptions they had on entering grade school.  To test how a lifetime of education affects our understanding of science, we asked these recent graduates some simple questions in astronomy.  Consider, for example, that the causes for the seasons is a topic taught in every standard curriculum.&quot;</i><br /><br />The 9-minute version is here with more detail:<br /><a href=\"https://www.youtube.com/watch?v=TrXaQu_qGeo\" class=\"ot-anchor\">https://www.youtube.com/watch?v=TrXaQu_qGeo</a><br /><br />(it&#39;s fascinating)<br /><br />If you have the time, the full documentary can be viewed at <a href=\"https://www.learner.org/resources/series28.html\" class=\"ot-anchor\">https://www.learner.org/resources/series28.html</a>.<br /><br />This now-famous video has helped to inspire a re-think on current educational practices -- perhaps most notably today in the physics education research (PER) discipline.<br /><br />That research leaves little doubt today as to the source of the problem, but to understand what has been discovered and how to fix it, we first have to introduce the key concepts and findings of the constructivist field of research in science education.  Of those concepts, the distinction between meaningful learning and rote memorization is essential.<br /><br /><b>&quot;The most important single factor influencing learning is what the learner already knows&quot;</b><br /><br /><i>&quot;In the Preface of his Educational Psychology: A Cognitive View (1968), Ausubel states: &#39;The most important single factor influencing learning is what the learner already knows.  Ascertain this and teach him accordingly&#39; (p. vi)</i><br /><br /><i>Simple as this statement appears, some profound issues are to be found in it.  To ascertain what the learner already knows means to identify those elements in the learner&#39;s existing knowledge store that are relevant to what we hope to teach, or, in Ausubel&#39;s terms, to identify the relevant subsuming concepts that are available in the learner&#39;s cognitive structure.</i><br /><br /><i>Even the term cognitive structure has special meaning fo Ausubel.  He views the storage of information in the brain as highly organized, with linkages formed between various older and newer elements leading to a conceptual hierarchy in which minor elements of knowledge are linked with (subsumed under) larger, more general, more inclusive concepts.  Thus cognitive structure represents a framework of hierarchically organized concepts, which are the individual&#39;s representations of sensory experience. (See Ausubel, 1968, p. 506)</i><br /><br /><b><i>Since every individual has had a unique history of sensory experiences, we should expect that any specific element in an individual&#39;s cognitive structure is idiosyncratic; that is, the concepts of one individual are to some degree different from the concepts of another.  These differences are usually not great enough to prevent communication;</i></b><i> your concept of learning is close enough to mine that this &#39;concept label&#39; (learning) means to you approximately what I want to convey when I use the term ...</i><br /><br /><i>We see, then, that to &#39;ascertain what a learner already knows&#39; in an area of study requires that we determine the relevant concepts the learner possesses and the extent to which they are differentiated.  This is no simple task.</i><br /><br /><i>What does Ausubel mean by &#39;teach him accordingly?&#39; </i><b><i>Individuals can learn information that has little or no association with existing elements in cognitive structure; for example, they can learn to associate such nonsense syllables as LEU and JEX. In rote learning one acquires new information without specific association with existing elements (concepts) in cognitive structure. Meaningful learning occurs when new information is linked with existing concepts (subsuming concepts or subsumers, in Ausubel&#39;s terms). According to Ausubel&#39;s view, new information acquired in meaningful learning is stored in a somewhat altered form (as a product of assimilation with the subsuming concept[s]) and modifies (differentiates further) the subsumers to which it is linked. Since subsuming concepts can be substantially more differentiated in one individual than in another, the same new material can be learned very meaningfully by one person and almost by rote by another.</i></b><i> News reports that a high-protein sorghum plant has been found may have little meaning to the average citizen, but to a plant specialist this news connects with a whole panoply of concepts, from selective breeding and hybridization to production of commercial seed and resulting increases in production of plant protein.</i><br /><br /><b><i>Sometimes rote learning is desirable.</i></b><i> If I give a person my phone number, I don&#39;t want him to remember that it is a Murray Hill number in the upper five thousands.  Anything other than the precise number will not suffice to call my home. </i><b><i>Most of the things we consider important to transmit to students, however, do not require subsequent recall in a form identical to that in which they are presented, even though much of school teaching and testing appears to contradict this view.</i></b><i> If we teach students that photosynthesis is the process by which plants convert light energy into food, it should be quite acceptable if they define photosynthesis as &#39;a food-making process in plants that utilizes light energy.&#39;  If the concept of photosynthesis is to be learned meaningfully, however, the student must have some available concept of plant, food, light, energy, and making or converting.  If we do not want a definition of photosynthesis to be learned by rote, we must ascertain to what degree the assocated subsuming concepts are present and developed or differentiated.&quot;</i> (p24-27, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br /><i>&quot;When relevant concepts do not exist in the cognitive structure of an individual, new information must be learned by rote.  That is, each knowledge bit or unit must be arbitrarily stored in the cognitive structure. </i><b><i>In rote learning, new information is not associated with existing concepts in cognitive structure, and therefore little or no interaction occurs between newly acquired information and information already stored.</i></b><br /><br /><i>Rote learning is always necessary when an individual acquires new information in a knowledge area completely unrelated to what he already knows.  Also, some types of information are inherently meaningless; telephone numbers, nonsense syllables, and other information cannot be linked in substantive ways to existing cognitive structure elements and hence must be arbitrarily stored in cognitive strcuture.  Knowledge elements learned by rote are arbitrarily distributed in cognitive structure; they are not linked to specifically relevant concepts in cognitive structure.&quot;</i> (p77, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br /><b>It&#39;s a Continuum, not Simply a Dichotomy</b><br /><br /><b><i>&quot;It is very important to recognize that rote -&gt; meaningful is a continuum and not a dichotomy.  This is a fundamental idea, since we doubt if any school learning occurs in an absolutely rote fashion.&quot;</i></b> (p80, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br /><b>Learners Must Decide to Learn Meaningfully, Or It Won&#39;t Happen</b><br /><br /><i>&quot;To Ausubel, meaningful learning is a process in which new information is related to an existing relevant aspect of an individual&#39;s knowledge structure. </i><b><i>However, the learner must choose to do this. The learner must actively seek a way to integrate the new information with existing relevant information in her or his cognitive structure.&quot;</i></b> (p59-60, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><b>Meaningful Learning is Both a Blessing and a Curse, and this Can Explain Why Many Students Avoid It</b><br /><br /><i>&quot;Highly meaningful learning that includes novel problem solving and creativity is only possible in domains of knowledge where the learner has considerable, well-organized prior knowledge.  Thoughtful practice or rehearsal also contribute. </i><b><i>The dependence of meaningful learning on the adequacy of our prior relevant knowledge is both a blessing and a curse. The more we learn and organize knowledge in a given domain, the easier it is to acquire and use new knowledge in that domain. The curse is that when we try to learn new knowledge in a domain where we know little, and/or what we know is poorly organized, meaningful learning is difficult, usually time-consuming and tiring. Too often we may escape the challenge by resorting to rote learning, even though we know that what we learn will soon be forgotten and it will not be of value in future learning. Such fraudulent learning may allow us to pass school exams, but contributes little or nothing to future learning or acting (Edmondson and Novak, 1993).&quot;</i></b> (p29-30, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><b>The History of the Distinction</b><br /><br /><i>&quot;Ausubel has not been the first to recognize the importance of meaning as a factor in learning.  D.O. Lyon (1914), M.G. Jones, and H.B. English (1926), and others are cited by Ausubel as early research workers who demonstrated that meaningful learning does not proceed in the same way as rote learning.  Peripheralist philosophy, however, has been the dominant psychological dogma during the twentieth century.  In an address to the Psychological Division of the American Association for the Advancement of Science, H.B. Reed (1938) argued that more attention should be paid to meaning as a factor in learning.  He acknowledged that there was much &#39;mystery&#39; in such terms as insight and understanding, but pleaded for more psychological research on learning of meaningful material. </i><b><i>History has shown that Reed&#39;s solicitation went largely unheeded, for most research continued to be of S-R and associationist nature, and Skinner&#39;s peripheralist philosophy, expounded in his Conditions of learning (1938), has dominated the field of psychology in North America.&quot;</i></b> (p76, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br /><b>Why We Can Ace a Course, and Just Six Months Later, Fail to Remember Any Of It</b><br /><br /><b><i>&quot;Most careful research on retention has been done in laboratories where subjects are given nonsense syllables or word pairs to memorize and are then tested for later rote recall of information. Some studies have used poetry, story passages, and ordinary school materials for analysis of retention.  These studies show that substantial forgetting occurs in a matter of hours for nonsense syllables; for poetry and story passages, much is lost in a matter of days; and for science, history, or other classroom information, retention drops to a fraction of original learning in a matter of weeks. Some information, however, is retained for months or years.</i></b><br /><br /><b><i>In Ausubel&#39;s theory variation in rates of forgetting depends primarily on the degree of meaningfulness associated with the learning process.</i></b><i> Information learned by rote (nonsense syllables and meaningless word pairs) cannot be anchored to major elements in cognitive structure and hence form a minimum linkage with it.  Unless materials learned by rote are restudied repeatedly to achieve overlearning (continued study after error-free recall has been achieved), they cannot be recalled several hours or several days after learning.  Information that is learned meaningfully (associated with subsumers in cognitive structure) can usually be recalled for weeks or months after acquisition.  The process of subsumption results in some modification of the stored information, however.  As a result recalled information may appear in a form slightly different from that originally learned ...</i><br /><br /><i>Rote learning has one important advantage over meaningful learning; we have already noted that sometimes it is useful to recall knowledge learned in precisely the same form as the original message.  This process is all too frequently required in school testing. </i><b><i>Meaningful learning has three important advantages over rote learning. First, knowledge acquired meaningfully is retained longer -- much, much longer in many instances. Second, subsumed information results in increased differentiation of subsumers, thus adding to the capacity for easier subsequent learning of related materials. Third, information that is forgotten after obliterative subsumption has occurred has left a residual effect on the subsuming concept, thus facilitating new related learning even after forgetting has occurred.</i></b><br /><br /><i>The last point in favor of meaningful forgetting is very important. </i><b><i>Laboratory studies have shown that information learned by rote inhibits subsequent learning of additional similar information. Moreover, even information learned by rote that is forgotten inhibits learning of similar new information.</i></b><i> The reverse effect operates with meaningful forgetting.  While it is true that restudy or relearning of the same information is facilitated by prior retention in both rote and meaningful learning, the &#39;savings&#39; (as psychologists refer to this facilitation) in rote learning is only for relearning of precisely the same material, whereas meaningful learning will result in savings for relearning and facilitation (rather than inhibition) of learning new, similar (relevant to the same subsumer[s]) information.&quot;</i> <br /><br /><b><i>Many students experience the feeling of being snowed under. Usually this feeling becomes most intense six to eight weeks into a course. Some studies (see Howard Hagerman, 1966) indicate that most information learned by rote in schools is lost within six to eight weeks. As a result students recognize that they have forgotten much of the information presented earlier and that their earlier but now lost learning is interfering with new learning. They must force themselves into review and meaningful restudy of earlier materials, cram for hours to overlearn earlier material, or give up hopes of passing the course.</i></b> (p84-85, <i>A Theory of Education</i>, Joseph D. Novak)<br /><br /><i>&quot;Because rote learning takes relatively little effort on the part of a learner initially, it is relatively efficient; that is, a learner can repeat verbatim some of the key concept definitions and propositions presented in the instruction.  However, because the latter are stored arbitrarily and non-substantively in cognitive structure, they soon cannot be recalled and confer interference with new, related learning and recall of related information.  During interviews on almost any topic, persons who have been learning by rote may recall bits of information but relate these in very inappropriate ways.  For example, in a video developed at Harvard University, 21 out of 23 graduates and faculty members knew that the earth&#39;s orbit was not a perfect circle (actually, they thought it was much more elliptical than is the case), but they erroneously believed that the seasons on the earth (at least in the Boston area) were caused by the earth&#39;s proximity to the sun (Private Universe Project, 1989 ...). </i><b><i>Even in our best universities students arrive with misconceptions, and because many engage primarily in rote learning, these misconceptions do not get remediated. One must never under estimate the tenacity to which learners will hold on to old, faulty ideas in both school and corporate settings.&quot;</i></b> (p69-70, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br /><b>Transferability of Learned Information is the Key to Creative Thinking</b><br /><br /><b><i>&quot;information learned meaningfully can be applied in a wide variety of new problems or contexts; the transferability of knowledge is high.  It is this power of transferability that is necessary for creative thinking.&quot;</i></b> (p68, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br />The diagram from the graphic originally comes from <a href=\"http://images.slideplayer.com/15/4574769/slides/slide_3.jpg\" class=\"ot-anchor\">http://images.slideplayer.com/15/4574769/slides/slide_3.jpg</a>.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Hedgehog vs The Fox","summary":"Two Distinctive Approaches to Complex Problem-Solving / The One that is More Predictive & Adaptive Tends to Lose the Debate","image":"https://lh5.googleusercontent.com/-A42XQXjDoVU/Vr6a8dTDJOI/AAAAAAAAGqg/OAbyvbL1qu8/the-hedgehog-vs-the-fox-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/6ABQGftvYh2","publishDate":"2016-02-13T03:10:47.154Z","updateDate":"2017-01-03T02:57:51.689Z","text":"<b>The Hedgehog vs The Fox: Two Distinctive Approaches to Complex Problem-Solving / The One that is More Predictive &amp; Adaptive Tends to Lose the Debate</b><br /><br />I want to offer this dichotomy as a tool for thinking about scientific controversies.  It seems to me to accurately convey the essence of these challenges to conventional theory, and the fact that it traces to a root psycho-sociological pattern that is much larger than the endeavor of science lends the analysis some extra credence.<br /><br />This entire summary comes from <a href=\"http://www.chforum.org/library/choice12.shtml\" class=\"ot-anchor\">http://www.chforum.org/library/choice12.shtml</a><br /><br /><b><i>&quot;The Fox and the Hedgehog: cognitive styles in decision-taking</i></b><br /><br /><b><i>Summary:</i></b><i> two distinctive &#39;thinking styles&#39; characterise expert views of future-oriented and complex problems. A new book shows that one of these is markedly superior to the other in predicting events and adapting to new information. Unhappily, the superior approach is less able to capture the high ground of debate, suggesting that active measures may be needed to make space for it.</i><br /><br /><i>Philip Tetlock has published &#39;Expert Political Judgement&#39; (PUP 2006), in which he shows the idiosyncrasy and fallibility of &#39;expert&#39; judgements about future events.</i><br /><br /><i>The book is complex and subtle, but it is fair to say that it offers two forms of analysis. First, it describes the characteristic ways in which such judgement tends to be framed - described as &#39;cognitive styles&#39; - and the personality types which are most prone to make use of these. Second, it assesses the effectiveness of these modes of judgement and ways of thinking. The research points to stark differences in the effectiveness of the two styles.</i><br /><br /><b><i>Isaiah Berlin once drew on European folk tales for the metaphor of the fox and the hedgehog. Hedgehogs, he said, have just one, powerful response to a threat: they roll themselves into a ball, presenting spikes to predators (and to cars.) They &#39;know just one big thing&#39;. Foxes, by contrast, have no single response to challenges, for they &#39;know many little things&#39;. They react to challenge by drawing on a pattern of general, pragmatic understanding, often making mistakes but seldom committing themselves to a potentially catastrophic grand strategy. Berlin was concerned with the roles of the expert and the generalist,</i></b><i> and Tetlock has used this metaphor to name the two ends of the dimension which he has extracted from his research.</i><br /><br /><b><i>People who rely upon the Hedgehog cognitive style need closure -- a sense of finality, of &#39;that&#39;s settled, then&#39; - in order to feel happy. That is, they need an unambiguous model to support their decision-taking, and the data against which to calibrate this. They like their model to be actually simple and conceptually parsimonious, decisive - that is, delivering a binary verdict, not a balance of probabilities - and repeatable. Hedgehog experts have a tendency to reach for formulaic solutions, for precedent and for the approbation of their peers, and to resent and resist challenge to their model. They prefer to capture a sub-set of the problem in a tractable form than to reach for a less precise, but perhaps more comprehensive, overview of the issues that are involved.</i></b><br /><br /><b><i>Experts who think in the &#39;Fox&#39; cognitive style are suspicious of a commitment to any one way of seeing the issue, and prefer a loose insight that is nonetheless calibrated from many different perspectives. They use quantification of uncertain events more as calibration, as a metaphor, than as a prediction. They are tolerant of dissonance within a model - for example, that an &#39;enemy&#39; regime might have redeeming qualities - and relatively ready to recalibrate their view when unexpected events cast doubt on what they had previously believed to be true.</i></b><br /><br /><b><i>In contrast to this, Hedgehogs work hard to exclude dissonance from their models. They prefer to treat events which contradict their expectations as exceptions,</i></b><i> and to re-interpret events in such a way as to allocate exceptions to external events. For example, positive aspects of an enemy regime may be assigned to propaganda, either on the part of the regime or through its sympathisers. Tetlock makes the point that this is neither an exclusive characteristic of the political Left or the Right, but a feature of the Hedgehog ideologues within both.</i><br /><br /><b><i>Hedgehogs tend to flourish and excel in environments in which uncertainty and ambiguity have been excluded, either by actual or artificial means.</i></b><i> The mantra of &#39;targets and accountability&#39; was made by and for Hedgehogs. </i><b><i>Foxes, by contrast, use a style which works best where neither the interpretation of the operating environment nor the correct nature of or balance amongst targets is clear.</i></b><br /><br /><i>Tetlock collected data from nearly 300 experts in fields such as international affairs, economics, commerce, law and public policy over a five year period. These were asked to make assessments on a wide range of subjects, from the presence of absence of weapons of mass destruction in Iraq to the prospects for the Internet bubble. The quality of these projections were assessed after the event. The experts had been classified on the &quot;Fox to Hedgehog&quot; scale, making it possible to assess which of the two cognitive styles gave the best approach to making judgements about uncertain, complex events. The quality was assessed both as an ex ante forecast and as an adaptive commentator, absorbing and processing information as it became available.</i><br /><br /><b><i>It is, perhaps, a particular irony that the worst judges turned out to be Hedgehog experts holding forth on their own particular area of expertise.</i></b><i> This group were particularly prone to overstate the likelihood of extreme changes: wars, financial crises and the like. They tended to calibrate their position against the peer group, creating &#39;friends&#39; and &#39;foe&#39; and over-stating their difference from their supposed enemies. They exhibited false memories, claiming to have said or believed things which they had not, in fact, espoused earlier. </i><b><i>In general, Hedgehogs were less likely to re-evaluate their views - and in particular, extreme views - when these were challenged by new information.</i></b><br /><br /><b><i>Foxes were more commonly correct in their forecasts, and much more open to re-calibration of their views in the light of new information. Their media coverage was poor, however, because their statements were nuanced and took some effort to understand.</i></b><i> They were less inclined to exclude extreme events - perhaps less focused on them - than were Hedgehogs, who typically received stronger media coverage and who were ready to make such exclusions. (This was the one area in which Hedgehogs performed better than foxes.)</i><br /><br /><b><i>One conclusion to be drawn from this analysis is, therefore, that we live in a world that needs a Foxy outlook, but that we tend to be faced with messages that derive from Hedgehogs, from Hedgehog-friendly criteria for commercial and public policy, and that these messages have the virtue of simplicity and closure. The problem for the Fox perspective is that it does not come with a neat, closed model, with defined goals and easy metrics. It is ambiguous, complex, open-ended and often self-referential, requiring a gradual iteration between tools and goals in order to find a satisfactory way forward. This puts it at a disadvantage, notably when faced with high-profile issues, urgency and governance which is so embedded in the Hedgehog style that it cannot (or will not) make the time needed to entertain new thoughts.</i></b><br /><br /><b><i>However - and it is an immense, overwhelming &#39;however&#39; in the right circumstances - the Fox style tends to be the right one to use when the world has become too complex for the Hedgehogs. Rolling in a ball will not do when cars threaten. The Fox approach is demonstrated to give more right answers. It responds to new information. It creates the conditions in which future Hedgehogs can thrive. This symbiosis between the two styles, once so common in commerce and in the policy process, has all but disappeared from great swaths of public life and from whole industries. It may be that the knowledge economy demands the renaissance of the fox.&quot;</i></b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23thinking\">#thinking</a>  ﻿","category":"thinking"},{"name":"The Structure of Science","summary":"If Asked, Most People Today Would Not Be Able to Explain What a Worldview Is / Yet, It Is at the Root of Every Belief They Have And Every Claim they Make","image":"https://lh6.googleusercontent.com/-lBa8LPDteis/VrrBTk70_FI/AAAAAAAAGoY/ua-YQkunvwk/the-structure-of-science-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Qw5RnxEJsQZ","publishDate":"2016-02-10T05:11:53.341Z","updateDate":"2017-01-03T02:58:15.094Z","text":"<b>The Structure of Science: If Asked, Most People Today Would Not Be Able to Explain What a Worldview Is / Yet, It Is at the Root of Every Belief They Have And Every Claim they Make</b><br /><br />This should be drilled into every school child&#39;s head, so why did I not learn it until I was 30 years old?<br /><br /><b><i>&quot;Concepts</i></b><i> are perceived regularities in events or objects, or records of events or objects, designated by a label&quot;</i> (Joseph Novak)<br /><br />There is no such thing as an isolated concept, because we define concepts according to their relationships with other concepts.  These connections are called <b>propositions.</b><br /><br />If we take the concept to be the atomic unit, then propositions are the molecules. <i>&quot;The meaning we acquire for a given concept is formed from the composite of propositions we know that contain that concept.&quot;</i> (Joseph Novak). This is also, incidentally, exactly how definitions in dictionaries are constructed -- by tracking the meanings of word usages within recent published works across the entire population.<br /><br />When a proposition applies to more than a single instance -- when it is generalizable -- the proposition is called a <b>principle.</b><br /><br />Not all conceptual creations specify regularities in events.  For instance, nobody has actually observed a family with 2.7 children -- and yet, the average number of children is nevertheless a useful idea which exists between a concept and a principle. <b>Constructs</b> are placeholders which can connect sets of concepts, and their meaning can change over time.<br /><br /><i>&quot;A </i><b><i>theory</i></b><i> (or </i><b><i>model</i></b><i>) is a set of interrelated principles that enables explanations or predictions of interactions among objects and events.&quot;</i> (Joseph Novak)<br /><br />A <b>hypothesis</b> is a theory under construction.<br /><br /><i>&quot;The hypothesis is a statement of what one doesn&#39;t know and a strategy for how one is going to find it out&quot;</i> (Stuart Firestein)<br /><br />It is a scientist&#39;s <i>&quot;idea about how something works based on past data, perhaps some casual observations, and a lot of thinking typically ending in an insightful and potential new explanation for how something works.  The best of these, in fact the legitimate ones, suggest experiments that could prove them to be true or false -- the false part of that equation being the most important.  There are many experimental results that could be consistent with a hypothesis yet not prove it true.  But it only has to be shown to be false once for it to be abandoned.&quot;</i> (Stuart Feinstein)<br /><br />An <b>inference</b> is an explanation of an observational or experimental result, based upon a theory or hypothesis.<br /><br />Both theories and hypotheses can contain <b>assumptions</b> -- which are propositions that are not guided by either experiment or observation.<br /><br />The purpose of scientific research is to supply answers to key questions (Gowan).  But, the first step to getting better answers is to frame better questions (Stuart Feinstein).<br /><br /><i>&quot;Thoroughly conscious ignorance is the prelude to every real advance in science.&quot;</i> (James Maxwell)<br /><br />The purpose of <b>focus questions</b> is to sharpen and magnify a view by limiting the range of the view.  But, since limiting the view can obstruct our perception of the whole, the selection of focus questions is incredibly important (Bob Gowan).<br /><br /><i>&quot;A question is interesting if it leads somewhere and is connected to other questions.&quot;</i> (Stuart Firestein)<br /><br />Focus questions must necessarily emerge from prior knowledge, interests and a complex host of other human factors.  Scientists are also guided in their choice of focus questions, as well as how to interpret their findings, by a set of views about the nature of knowledge and the structure of the universe.  Although this set of views -- commonly referred to as a <b>philosophy</b> -- is rarely stated in the text of scientific papers, it nevertheless guides the focus questions -- and, hence, the entire intellectual enterprise (Bob Gowan).<br /><br />But, not all philosophies lead to focus questions.  A <b>worldview</b> or <b>paradigm</b> is more of a philosophical statement of belief -- the <i>&quot;set of experiences, beliefs and values that affect the way an individual perceives reality.&quot;</i> (Wikipedia: paradigm)  Paradigms drive the elaboration of hypotheses, using experimentation and observation as a guide.  If a hypothesis fails a test, the paradigm can supply alternative possibilities which keep the hypothesis alive.<br /><br /><b>That&#39;s the basic idea, but fitting it to a society of people demands that we introduce human nature.  And at the heart of science, there exists a riddle that we must rely upon our worldviews to formulate hypotheses.  And it is this riddle which ultimately leads to paradigm changes ...</b><br /><br /><i>&quot;When mathematicians (and geometry students) &#39;derive a proof,&#39; they are developing a sequence of logical steps that leads to a final statement that is consistent with the first statement in the derivation.  As an example, if we accept the basic definitions [concepts], axioms and postulates [principles] of Euclid&#39;s geometry, we can &#39;prove&#39; that &#39;lines parallel with another line are parallel to each other.&#39;  But this is not a proof of the existence of any real-world physical mechanism -- it is an exercise in the logical manipulation of a set of basic mathematical axioms [principles].  Such manipulations are completely internal to mathematics and remain disassociated from the real world unless and until such an association is demonstrated by observation and experiment.</i><br /><br /><i>[In the </i><b><i>deductive method</i></b><i>] one starts with a presumed law of nature -- an obviously correct (accepted) generalization about the way things work -- and deduces (works out, derives) its logical consequences.</i><br /><br /><i>A hypothesis arrived at via this deductive method is promoted to the status of being a theory when and if a large enough body of experts accepts it.  This is an application of the Socratic method, also sometimes called the &#39;dialectic method.&#39;  Socrates (469-399 B.C.) believed that truth was discovered through intense conversations with other informed people.  In this method, a vote of the experts determines when and if a theory is correct.  Once such a theory has been accepted, it is not easily rejected in light of conflicting evidence.  It is, however, often modified -- made more complicated.  When over time a theory becomes officially accepted, the essence of the matter has been settled and fixed.  Modifications to the fine points of the theory can then be proposed and debated, but the backbone structure of the theory is set.  That framework has already been firmly established.</i><br /><br /><b><i>An inherent flaw lurking in this method is: What if your &#39;obviously correct,&#39; basic, starting-point presumption is wrong?&quot;</i></b> (Don Scott)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"The Science Celebrity","summary":"Generally Fleeting Fame and Little Importance to the Larger History of Science / Do Not Usually Teach Science as a System of Thought / Oftentimes Leave Out the Actual Reasons for Belief, Leaving the Audience Without Any Ability to Question the Information they are Provided / Helped Establish Image of Science as the Pronouncement of Authorities","image":"https://lh3.googleusercontent.com/-sC1kVqyjiJw/Vrey9df9UKI/AAAAAAAAGnU/uLTR5Ao1cHA/w1800-h2520/the-science-celebrity-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/W8WQwHNVBhu","publishDate":"2016-02-07T21:41:27.086Z","updateDate":"2017-01-03T02:58:32.263Z","text":"<b>The Science Celebrity: Generally Fleeting Fame and Little Importance to the Larger History of Science / Do Not Usually Teach Science as a System of Thought / Oftentimes Leave Out the Actual Reasons for Belief, Leaving the Audience Without Any Ability to Question the Information they are Provided / Helped Establish Image of Science as the Pronouncement of Authorities</b><br /><br /><b>Each situation is of course unique,</b> but science historian John C. Burnham explains some of the features we can expect to generally see associated with the science celebrity in <i>How Superstition Won and Science Lost: Popularizing Science and Health in the United States:</i><br /><br /><i>&quot;One variety of the pseudoevent was of particular importance in the field of science and health: the media celebrity. </i><b><i>Boorstin contrasts the celebrity, whose fame is fleeting and, more important, who has no importance in history, with the enduring impact of a truly great person; indeed, just by converting someone into a celebrity, the media denied any greater significance to the person.</i></b><i> Various figures from science and medicine suffered prominence in the media (including some whose reputations were nevertheless substantial and lasting), from Albert Einstein, Bernarr Macfadden, and Madame Curie to Joyce Brothers, Linus Pauling, Carl Sagan and various astronauts.  As the achievements of each figure diminished in relevance to the media agenda, the personality and quotability often continued, at times aided by the omnipresent public relations personnel. </i><b><i>The entire process of exploiting celebrities served to emphasize personalities at the expense of ideas.</i></b><i> The editors of the New York Magazine of Mysteries (1901-1914), for instance, had no fear that glorifying real scientists as personalities would detract from stories about occult events and miracles that, with patent medicine and other ads, filled the rest of the pages of the publication.  By the 1960s a prominant feature of Science Digest was &#39;Personality of the Month,&#39; not unlike the corresponding approach of Psychology Today.  Playing up prominent figures was of course a traditional tactic in popularizing science, but with the celebrity mode, journalists were creating &#39;image&#39; rather than attempting as in earlier years to anthropomorphize an idea, as in the Health Heroes series of the early twentieth century or the glorification of martyrs of science in the nineteenth-century Popular Science Monthly. </i><b><i>Moreover, a superficially positive image easily transformed into a negative one, and even the extreme of media image -- for example, Bob Newhart as a psychologist -- did not, in the end, further communicating ideas from psychology.</i></b><br /><br /><b><i>One important by-product of the personality or celebrity to whom journalists could attribute opinions was that the media established an image of science as the pronouncement of authorities.</i></b><i> And, of course, since all facts were equally acceptable, reporters tried to interview authorities who would contradict each other -- and, as the notorious polywater debate showed at the end of the 1960s, this pattern ... held true not just in the field of psychology. </i><b><i>The media winner usually turned out to be the most photogenic or articulate or both, or the most cooperative with the press, rather than the best exponent of research,</i></b><i> much less the religion of science (many scientists were outraged by the prominence the media of the 1970s gave to some of the less acceptable of astronomer Carl Sagan&#39;s views for example). </i><b><i>Thus by the late twentieth century, authority in popular science led far away from the scientific way of thinking and even, for authority, was based more on media standards than on peer review.</i></b><i> Journalists handled the authorities&#39; opinions not only as inconsequential facts but as productions of scientists, with the result that the media workers treated even the best opinion in the same way that they treated a new cure.&quot;</i> (p238-239)<br /><br /><b>Example Celebrity: Neil degrasse Tyson</b><br /><br />The celebrity in the image, of course, is Neil degrasse Tyson, probably most famous at this point for having hosted the recent <i>Cosmos</i> remake which was originally hosted by the better known science celebrity, Carl Sagan (shown in the icon).<br /><br />Tyson has in recent years been accused of fabrication ...<br /><br /><i>&quot;Neil Tyson, a prominent popularizer of science (he even has his own television show) was recently found to have repeatedly fabricated multiple quotes over several years. The fabrications were not a one-off thing. </i><b><i>They were deliberate and calculated, crafted with one goal in mind: to elevate Tyson, and by extension his audience,</i></b><i> at the expense of know-nothing, knuckle-dragging nutjobs who hate science. Tyson targeted journalists, members of Congress, even former President George W. Bush. And what was their crime? They were guilty of rejecting science, according to Tyson.</i><br /><br /><i>There’s only one problem. None of the straw man quotes that Tyson uses to tear them down are real. The quote about the numerically illiterate newspaper headline? Fabricated. The quote about a member of Congress who said he had changed his views 360 degrees? It doesn’t exist. That time a U.S. president said &#39;Our God is the God who named the stars&#39; as a way of dividing Judeo-Christian beliefs from Islamic beliefs? It never happened ...&quot;</i><br /><br />The more interesting bit is probably the critique attached to these fabrication claims:<br /><br /><b><i>&quot;These lovers of science don’t actually love science, because science requires you to go where the evidence takes you, even if it goes against your original hypothesis. What many of Tyson’s cultists really like is the notion that one can become more intelligent via osmosis</i></b><i> — that you can become as smart and as credentialed as Tyson by merely clapping like a seal at whatever he says, as long as what he says fits the political worldview of your average progressive liberal.</i><br /><br /><i>Tyson may be a great scientist, but what he’s selling at a price of $70 per ticket isn’t science. He’s selling the self satisfaction that comes from moral preening. </i><b><i>Neil Tyson is adored by people who want the sweet feeling of smug, intellectual superiority without all the baggage of actually being intellectually superior in any way.</i></b><i> They love math and science up to the point at which one of them needs to figure out a restaurant tip, and then out comes the iPhone calculator. The more self-aware ones will just round up to the nearest dollar and then pretend it’s because they’re generous. But overall, we’re dealing with people who love science so much that they picked college majors just to avoid the subject they allegedly love so dearly ...</i><br /><br /><i>... if a skeptic of climate alarmism, for example — were accused of wholesale fabrication of evidence, he would have already been run out of town. But not Tyson. Why the disparity? That’s easy: </i><b><i>because Tyson’s sins were committed out of a pure desire to further the common good. He believes the &#39;right&#39; things, which means his rather serious iniquities can be forgiven. A little fabrication can be swept under the rug so long as it’s in service of a higher agenda.</i></b><br /><br /><b><i>That is not the kind of attitude that is supposed to form the actual foundation of science, which consists of following the facts and the evidence wherever it takes you, no matter how unpleasant. Science is supposed to be the search for truth, not the search for stuff that just happens to support your political agenda.</i></b><i> Science certainly isn’t the creation of bogus evidence out of thin air — the intelligent design of quotes ex nihilo, if you will — in order to support a political agenda.</i><br /><br /><i>Fabrication isn’t science. Ignoring inconvenient evidence isn’t science. And faithfully nodding your head whenever somebody says something you go agree with doesn’t make you a scientist. It makes you a parrot, and a religiously zealous one at that.&quot;</i><br /><br />(from <a href=\"http://thefederalist.com/2014/09/18/why-is-wikipedia-deleting-all-references-to-neil-tysons-fabrication/\" class=\"ot-anchor\">http://thefederalist.com/2014/09/18/why-is-wikipedia-deleting-all-references-to-neil-tysons-fabrication/</a>)<br /><br />Tyson&#39;s recent <i>Cosmos</i> remake was arguably quite successful insofar as it accomplished its goal of further popularizing science.  Yet, the show&#39;s viewers are of course not being given enough detail to meaningfully engage the claims being made.<br /><br />To make the point, in one scene in Episode 2 (at 27 mins, 25 secs), Tyson argues that <i>&quot;Some claim that evolution is just a theory, as if it were merely an opinion.  The theory of evolution, like the theory of gravity, is a scientific fact.&quot;</i><br /><br /><b>Then, just 13 minutes later in the same episode</b> (at 40 mins, 10 secs) ...<br /><br /><i>&quot;Science works on the frontier between knowledge and ignorance.  We&#39;re not afraid to admit what we don&#39;t know.  There&#39;s no shame in that. </i><b><i>The only shame is to pretend that we have all of the answers.&quot;</i></b><br /><br />Tyson&#39;s audience largely fail to notice or care about such contradictions.  After all, it is typically not philosophy which science celebrities offer.<br /><br /><b>Example of Science By Celebrity: The Seas of Titan</b><br /><br />In this same episode of <i>Cosmos</i>, Tyson is shown traveling among and through the seas of Titan on a spaceship.  He boldly narrates:<br /><br /><i>&quot;Titan is the only other world in the solar system where it ever rains.  It has rivers, coastlines.  Titan has hundreds of lakes, one of them larger than Lake Superior in North America.  Vapor rising from the lakes condenses and falls again as rain.  The rain feeds rivers which carve valleys into the landscape, just like on Earth -- but with one big difference: On Titan, the seas and the rain are made not of water, but of methane and ethane.  On Earth, those molecules form natural gas.  On frigid Titan, they&#39;re liquid.&quot;</i><br /><br />(see clip at <a href=\"https://www.youtube.com/watch?v=coDhadT636A\" class=\"ot-anchor\">https://www.youtube.com/watch?v=coDhadT636A</a>)<br /><br />This could very well become a historical scene, but not in the way that Tyson and his audience currently imagine -- for it remains to be seen that these seas actually exist.<br /><br />What Tyson fails to inform his audience is that the seas are inferred based upon a lengthy chain of assumptions. The fact is that none of the scenario that Tyson presents has actually been directly observed.  It is inferred by the radar signatures and the presence of atmospheric methane on Titan, which scientists agree should be rapidly destroyed by solar photolysis.  Since the planet must be old, they reason, then the methane they are seeing must come from a large reservoir.  You&#39;d not know it from Tyson&#39;s computer-generated voyage through Titan, but we actually know very little about this place so far.<br /><br />From <a href=\"http://saturn.jpl.nasa.gov/photos/imagedetails/index.cfm?imageId=2214\" class=\"ot-anchor\">http://saturn.jpl.nasa.gov/photos/imagedetails/index.cfm?imageId=2214</a> ...<br /><br /><i>&quot;The Cassini spacecraft, using its radar system, has discovered very strong evidence for hydrocarbon lakes on Titan. Dark patches, which resemble terrestrial lakes, seem to be sprinkled all over the high latitudes surrounding Titan&#39;s north pole.&quot;</i><br /><br />Yet, in another press release (at <a href=\"http://phys.org/news/2013-07-mystery-titan.html\" class=\"ot-anchor\">http://phys.org/news/2013-07-mystery-titan.html</a>) we learn of a problem:<br /><br /><i>&quot;The idea that Titan is a wet world with its own alien waters is widely accepted by planetary scientists. </i><b><i>Nothing else can account for the observations:</i></b><i> NASA&#39;s Cassini spacecraft has flown by Titan more than 90 times since 2004, pinging the Moon with radar and mapping its lakes and seas. ESA&#39;s Huygens probe parachuted to the surface of Titan in 2005, descending through humid clouds and actually landing in moist soil.</i><br /><br /><i>Yet something has been bothering Alex Hayes, a planetary scientist on the Cassini radar team at Cornell University.</i><br /><br /><b><i>If Titan is really so wet, he wonders, &quot;Where are all the waves?&quot;</i></b><br /><br /><i>Here on Earth, bodies of water are rarely still. Breezes blowing across the surface cause waves to ripple and break; raindrops striking sea surfaces also provide some roughness. </i><b><i>Yet on Titan, the lakes are eerily smooth, with no discernable wave action down to the millimeter scale, according to radar data from Cassini.</i></b><br /><br />The missing context here is that <b>glass is really quite common both in interstellar space and on the Moon.</b> That fact adds some additional complexity to the interpretation which would seem to undermine the confident narrative.<br /><br />In fact, we can extend the counter-argument, if we wish.  There does exist an alternative, to-date largely ignored, argument against this official narrative which is described by the Thunderbolts Group at <a href=\"https://www.thunderbolts.info/tpod/2006/arch06/060802methanelakes.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2006/arch06/060802methanelakes.htm</a> ...<br /><br /><i>&quot;The Cassini investigators have also noted that sinuous channels or &#39;riverbeds&#39; appear to feed into the presumed methane lakes, giving the appearance of consistency to the interpretation.</i><br /><br /><i>But other considerations can hardly be ignored. </i><b><i>Every observed &#39;river&#39; on Titan is dry. All of the channels have exposed floors. There is no evidence of any liquid flowing into the flat dark areas.</i></b><i> In fact, the situation is not unlike the darkened &#39;mares&#39; of our own moon into which &#39;flow&#39; empty channels called sinuous rilles. </i><b><i>Acknowledging the absence of flowing methane on Titan at this time, NASA investigators have suggested that a &#39;cycle&#39; of methane downpours and surface droughts must occur on the moon. &#39;The lakes are presumably filled by rainfall, perhaps by seasonal storms, and then evaporate slowly to replenish the atmosphere and complete Titan&#39;s methane cycle.&#39;</i></b><br /><br /><i>But as noted by Mel Acheson, NASA’s &#39;big problem is explaining the loss of atmospheric methane to space. Early speculations were of methane volcanoes to bring up vast underground supplies&#39; — a process claimed to have been underway for eons to maintain the necessary atmospheric levels.</i><br /><br /><i>Acheson points out that a methane rain cycle just takes existing methane around a phase-shift circle (vapor to liquid to vapor). That&#39;s pretty much the entire meaning of &#39;cycle&#39;. Cycling doesn&#39;t produce &#39;new&#39; methane to replenish any loss. A loss is a loss: there is less methane to cycle. </i><b><i>They need to speculate that the lakes are fed by underground springs, or some such mechanism. But no such volcanoes have been observed, leading NASA theorists to suppose that this phase of &#39;activity&#39; is &#39;temporarily&#39; quiescent.</i></b><br /><br /><i>As Acheson comments, &#39;the attraction of &#39;underground&#39; anything is that it can&#39;t be tested — </i><b><i>so the search for an explanation is shifted from empirical ground to a ground of theoretical &#39;consensus.&#39;&#39;</i></b><br /><br /><i>Electrical theorist Wallace Thornhill offers an interpretation easily contrasted with that of NASA scientists: &#39;The radar images show flat-floored depressions with circular scalloped edges, typical of arc machining of the surface. I would compare them directly to the scalloped scarring on Jupiter’s moon Io and the flat, melted floor depressions that result. </i><b><i>Such floors would be expected to give a dark radar return.&#39;</i></b><br /><br /><i>The fact that the &#39;lakes&#39; have only been discovered in the polar region and are associated with empty sinuous &#39;rilles&#39; and fulgurite-like dunes also suggests an electrical origin through powerful auroral currents sometime in the past, according to Thornhill.</i><br /><br />Wal provides additional critique here: <a href=\"http://www.holoscience.com/wp/titan-puzzles-scientists/\" class=\"ot-anchor\">http://www.holoscience.com/wp/titan-puzzles-scientists/</a><br /><br />And further updates can found by simply Googling &quot;<a href=\"http://thunderbolts.info\" class=\"ot-anchor\">thunderbolts.info</a> titan&quot;.<br /><br />The quote above includes a couple of terms which are worth further elaborating on.  Fulgurites are glassy features which can be observed on Earth to be associated with lightning strikes.  Glassy melt provides a possible explanation for the very bright glint recently observed through Titan&#39;s fog, which many have seized on as proof of fluids (see here: <a href=\"http://www.nasa.gov/mission_pages/cassini/whycassini/cassini20091217.html\" class=\"ot-anchor\">http://www.nasa.gov/mission_pages/cassini/whycassini/cassini20091217.html</a>).<br /><br />Rilles are channels that are observed on a number of planets, and which have questionable origin in light of their features:<br /><br />(1) They are, for instance, commonly observed to follow the terrain -- not just down, <b>but also commonly up</b> (<i>&quot;The most extraordinary example is the Baltis Vallis on Venus, which rises and falls dozens of times, with some two kilometers separating its high and low points along its 6,800 kilometer length.&quot;</i>, more info on rilles at <a href=\"https://www.thunderbolts.info/tpod/2006/arch06/060315rille.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2006/arch06/060315rille.htm</a>);<br /><br />(2) Rilles oftentimes lack the features of riverbeds, such as outflows of debris that fluids would be expected to pick up along the length of the flow (for instance, on Mars: <i>&quot;Valles Marineris is four times deeper than the Grand Canyon, and stretches for almost 3,000 miles across the face of Mars. Its presence has continued to baffle scientists. When the Mariner probes returned the first pictures of this continental-scale trench, many proposed catastrophic flooding as its cause. But scrutiny of later images revealed no outwash or debris field left by erosion, and no sign of ponding. Nor did the short &#39;tributaries&#39;, ending in cleanly cut alcoves, fit any reasonable profile of a drainage system.&quot;</i>, more info at <a href=\"https://www.thunderbolts.info/tpod/2005/arch05/050408marineris.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2005/arch05/050408marineris.htm</a>);<br /><br />(3) And in some remarkable instances, rilles have been observed to plainly transition to chains of craters, blurring the distinction between craters and rilles as possibly two aspects of the same space-based phenomenon -- like in this notorious image of the Martian Tractus Catena region:<br /><br /><a href=\"http://www.esa.int/spaceinimages/Images/2012/04/Pit-chains_at_high_resolution\" class=\"ot-anchor\">http://www.esa.int/spaceinimages/Images/2012/04/Pit-chains_at_high_resolution</a><br /><br />What Wal and Mel are suggesting above is that the entire trip to Titan that Tyson graphically visualized for his audience in Episode 2 of <i>Cosmos</i> could turn out to be asking the wrong questions.  And of course any future mission which goes there will be selecting instrumentation on the basis of those questions.  But, Tyson&#39;s audience would have no way of knowing about a potential mistake in the making, because his presentation did not include the underlying process of reasoning that NASA and theorists went through to get to their preferred conclusion.  The audience is never given a chance to think about and analyze the riddle for themselves.  It is the epitome of popularizing science as fact, disconnected from its larger context and without any intention of teaching the person receiving this information about how to properly use it.<br /><br />Nor did Tyson include any mention of opposing views, as I&#39;ve presented here.  Wal has helpfully pointed out that <b>the mystery of Titan&#39;s atmospheric methane could just as easily be solved if Titan is not actually an old planet.</b><br /><br />When Tyson&#39;s approach to the subject is contrasted with Wal&#39;s, we see Burnham&#39;s critiques of science journalism and the science celebrity vindicated.  Without committing to the validity of Wal&#39;s critique -- which is not a necessary aspect of critical thinking -- it should be apparent that his treatment of the subject is a level of discourse more scientific than Tyson&#39;s.  Not only is he including the reasons which led to the conventional inference, but he&#39;s also providing a specific and ongoing (&quot;live&quot;) example for how the scientific community could come to be spectacularly wrong on Titan.  It&#39;s the perfect example of what we stand to lose when we learn science through celebrity.<br /><br />From <a href=\"https://www.thunderbolts.info/tpod/2006/arch06/060731titansrilles.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2006/arch06/060731titansrilles.htm</a> ...<br /><br /><i>&quot;Though no one can predict the actual state of things on Titan with certainty, </i><b><i>we are confident that neither rains of methane nor rivers of methane nor lakes of methane will be found on Titan’s surface. The entire line of reasoning follows from a precarious assumption. We further predict that if the NASA theorists will look at the topography with sufficient care, they will see that the so-called rivers do not follow terrain in the fashion of flowing liquids – which of course is always downhill.&quot;</i></b><br /><br />Episode 2 of <i>Cosmos</i> aired publicly on March 16, 2014.  A mission to explore Titan&#39;s seas received a second round of funding on July 15, 2015 ...<br /><br />... from <a href=\"http://www.space.com/29953-titan-submarine-nasa-niac-proposals.html\" class=\"ot-anchor\">http://www.space.com/29953-titan-submarine-nasa-niac-proposals.html</a> ...<br /><br /><i>&quot;Titan Submarine (Steven Oleson, NASA Glenn Research Center):A submarine, incorporating the latest research into cryogenic engineering and data on Saturn&#39;s moon Titan, would probe deep beneath Titan&#39;s largest sea, Kraken Mare.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The Science Journalist","summary":"Replaced the \"Men of Science\" as the Primary Popularizers of Science / Introduced News Values, Sensationalism and Emphasis Upon Fact Over Context and Science as a System of Thought / This Ultimately Encouraged Science-as-Magic Thinking and Undermined the Public's Ability to Differentiate Science from Pseudoscience / How Many Consumers of Scientific Press Releases Ever Take the Time to Learn the Origin of Science Journalism?","image":"https://lh3.googleusercontent.com/-mP1eBJFWurg/VrZtfZYKzWI/AAAAAAAAGnA/ixcav0LzQfI/w1800-h2520/the-science-journalist-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/4JJvZu18D9L","publishDate":"2016-02-06T22:07:58.683Z","updateDate":"2017-01-03T02:58:45.369Z","text":"<b>The Science Journalist: Replaced the &quot;Men of Science&quot; as the Primary Popularizers of Science / Introduced News Values, Sensationalism and Emphasis Upon Fact Over Context and Science as a System of Thought / This Ultimately Encouraged Science-as-Magic Thinking and Undermined the Public&#39;s Ability to Differentiate Science from Pseudoscience / How Many Consumers of Scientific Press Releases Ever Take the Time to Learn the Origin of Science Journalism?</b><br /><br />As you read this, consider the claim by education theorist and inventor of the concept map, Joseph Novak ...<br /><br /><i>&quot;The meaning of an event or object we observe depends upon what we already know about that kind of event or object ... </i><b><i>It is evident that the context of experience has an important impact on the meaning of experience.&quot;</i></b> (p41, <i>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations</i>, 2nd Ed, Joseph D. Novak)<br /><br />A review by John C Burnham from <i>How Superstition Won and Science Lost: Popularizing Science and Health in the United States</i> of the origin of science journalism, not in any particular order.  What is rather amazing is that this book doesn&#39;t have a single Amazon.com book review; it&#39;s as if nobody has ever even thought to look into the history of science journalism.<br /><br /><b>Gee Whiz Science (pre-WWI)</b><br /><br /><i>&quot;In general, the very term &#39;popular science&#39; carried negative connotations around the turn of the century, and for good reason ...&quot;</i><br /><br /><b>jellow journalism:</b> dishonest in editorial comment and the presentation of news, especially in sacrificing truth for sensationalism<br /><br /><i>&quot;... Newspaper science reached its nadir at that time, as yellow journalism continued to flourish before World War I, particularly in the notorious Sunday supplements. </i><b><i>The idea of the journalists then, Will Irwin recalled, was to make the reader look at the paper and say, &#39;Gee whiz!&#39; It was in this context, he continued, that editors made a &#39;surprising discovery: the public liked science or pseudo science!  So in tabloid doses, the yellows gave them archaeology, gave them medical discovery -- always jazzed up to the emotional point.&#39;</i></b><i> (Irwin went on to recall fondly how on one occasion the Sunday supplements reported the finding of &#39;the germ of baldness.&#39;)  Hillier Krieghbaum, looking back in 1941, concluded that </i><b><i>&#39;science reporting before the World War was a combination of editorial whim, hoax, newspaper-financed stunts, garbling and faking of details when truthful information was scarce,</i></b><i> plus an occasional job well done.&#39;&quot;</i> (p172)<br /><br /><i>&quot;The generally low level of popularizing at the opening of the twentieth century is baffling because the prestige of science was so high at the time.  Progressive reformers held the dispassionate professional scientist up as a model for emulation in their areas of life.  The boys&#39; adventure book hero, as Russel Nye points out, was Tom Swift in his laboratory.  And yet not only did measures of public interest decline, but the percentage of high-school students electing science courses declined precipitously and did not increase much even in absolute numbers.</i><br /><br /><i>The keys to this seeming paradox of low quantity combined with high esteem can be found in the changing nature of popularizing science and in institutional circumstances. </i><b><i>As scientists increasingly devoted themselves to the ideal of pure research, writers in the popular media directed attention less to science than to technology.</i></b><i> In what Lawrence Badash calls &#39;the radium craze,&#39; which hit just after 1900, for example, the wonders of radioactivity were overshadowed by the practical results that journalists thought up or publicized.  An ounce of radium would drive a fifty-horsepower car clear around the globe at thirty miles an hour, or radium mixed with chicken feed might result in eggs that would hardboil themselves or hatch chicks without an incubator.</i><br /><br /><i>Whereas in the nineteenth century popularizers drew few distinctions between science and technology, by the 1900s members of the public increasingly respected research but gave their attention to application and development.&quot;</i> (p173)<br /><br /><i>&quot;Although World War I effectively reduced the amount of science popularization (with the notable exception of that concerning the area of chemistry), the role that applied science played in that war for both good and evil made a deep impression on scientists and members of the public alike.  The result was a series of institutional developments that led to a very large increase in popularization.&quot;</i> (p174)<br /><br /><b>The Origin of Science Journalism</b><br /><br /><i>&quot;Typically the next medium below books in the [media] hierarchy was the magazine.  In terms of the grand quarterlies and monthlies, the North American Review, Harper&#39;s, the Atlantic, Century, and so on, the magazines did operate on a very high level, with long essays and thoughtful reviews. </i><b><i>But by the turn of the century, the rise of inexpensive magazines brought a number of them into direct competition with the new yellow press, and editors responded with newsy and sensationalistic approaches. The change was particularly noticeable in the area of science.</i></b><i> Moreover, the newspapers in turn competed with magazines by developing the notorious Sunday supplements, notable not only because of sensationalism but because writers in this part of the newspaper followed a magazine and not a news format.  It was these magazine supplements ... that were the source of the most anguished complaints about &#39;newspaper science.&#39;</i> (p190)<br /><br /><i>&quot;[U]ntil the 1970s, few skeptics complained about the science writers, </i><b><i>for the two relevant specialized groups in American society had co-opted each other: the scientists and the journalists.</i></b><i> For years, the two took turns singing each other&#39;s praises.  Before, newspaper reporters wrote ignorant and sensational copy; now, as the younger of the Drs. Mayo put it in 1931, &#39;The medical meetings ... last week covered by the best men on the staffs of metropolitan newspaper and press services, and while their reports had to carry headlines, there was in the headlines none of the false rainbows that used to make conservative doctors grind their teeth.&#39;  On the other side, the eminent scientists who used to be so difficult because of almost universally bad experiences with the press drew praise when they worked with journalists, first notably in the American Chemical Society publicity program and then Science Service.  The praise of course included, incidentally, favorable newspaper attention.  The mutual admiration grew saccharine; yet it did serve to draw together scientits who were conscious of the requirements of public relations and newswriters who served them, altogether a tour de force.  As a later middleman of science news commented, </i><b><i>&#39;Few newspapermen are interested in a scientific finding unless it can cure cancer while in orbit.  This could be called the &#39;rocket-scalpel complex.&#39;&#39; Likewise, he noted, &#39;few academicians are interested in press coverage unless the facts are expressed in mathematical Latin and are heavily qualified to prove that nothing really important happened. This is, of course, the &#39;scientific dignity-protective obscurity syndrome.&#39;&#39;</i></b><i> But for decades the two in fact often worked together.  Together they communed cosily, bemoaning to each other the antijournalistic holdouts in science and the editors who still did not understand that science was important and interesting.  Both sides agreed with Dr. Mayo: a lively headline need not be inaccurate.&quot;</i> (p196)<br /><br /><i>&quot;The actual process by which the scientists consciously obtained the cooperation of the newspapers through Science Service has been told often in positive terms, as if the relatively sudden enlightenment of both parties were a natural development.  The fact was that leading American scientists, as has been suggested, repeatedly pressured the writers and editors to upgrade their treatment of science. </i><b><i>As the public relations of science became remarkably deeply enmeshed in research-funding efforts, a special urgency crept into the scientists&#39; hectoring of journalists to give science a good press.</i></b><i> The campaign utilized particularly the professional pride of journalists in motivating them to improve their performance ...</i><br /><br /><i>But it would be unrealistic to ignore the effects of the relentless negative campaign of the scientists who over the years ridiculed &#39;newspaper science&#39; and, at least for some decades, held the feet of the writers and editors to the fire when a popularizer or reporter deviated from the scientists&#39; standards ...</i><br /><br /><i>As late as 1954 a leading journalism educator was reminding writers and editors that scientists would be offended by sensational and inaccurate writing. </i><b><i>(Of course in thus continuing to pursue the traditional campaign against superstition and error, scientists were working unwittingly to increase the credibility of the media, in which, however excellent the content, the form was subversive of the many traditional goals of popularizing ...)</i></b><br /><br /><i>What the scientists came to want, and frequently got from the editors, was not just proper coverage but </i><b><i>coverage by the sympathetic special science writers</i></b><i> -- typically reporters who received a special assignment. </i><b><i>As these writers developed their symbiotic relationship with scientists, the journalists on their parts gloried in sharing the high prestige of science and in advancing the cause of science ... The science writers not only praised and imitated scientists but took on certain of their values</i></b><i>, to an extent that became obvious in the 1970s when a number of journalists broke ranks and were among the leaders who raised questions about the benefits of scientific activity; it was only then that some reporters explicitly expressed doubt that a science writer necessarily had to be the scientists&#39; advocate.</i><br /><br /><b><i>The fact was that science writers enjoyed a special status, not only before the public but in the newspaper offices where they worked.</i></b><i> Other staff members deferred to them and, because they had access to the latest medical findings, called on the science writers for advice on all personal medical matters, so that typically these writers came to be known in the office as &#39;Doc.&#39;  Editors, too, deferred, giving science writers far more latitude with regard to length and deadlines than other reporters enjoyed. &#39;Are we getting some reputation as snobs?&#39; asked the president of the science writers&#39; organization in 1955, urging his colleagues not to treat their colleagues so patronizingly ... </i><b><i>The scientists helped confirm the writers&#39; special status, for the scientists had every reason to give friendly aid and support to writers who of course tended to report favorably as well as relatively reliably about science.</i></b><br /><br /><i>Without really intending to, then, leading American natural scientists, like those in psychology and the health fields, increasingly left popularizing to nonscientist specialists, the science writers.  One survey showed that the science writers were indeed nonscientists; only about nine percent of them had science training even at the B.A. level or had come into the field from science.  From the point of view of the public relations of science, the journalists served well ...&quot;</i><br /><br /><b><i>&quot;One instructive way to view what happened is to see that the gatekeepers and journalists throughout the twentieth century published science news beyond the sensational and political only when they answered pressure from various media constituencies,</i></b><i> just as newspaper editors in the nineteenth century had responded to groups of science amateurs.  The midcentury research on science journalism symbolizes the concern: the research was almost always on the question of quantity -- what percentage of space was devoted to science subjects -- rather than content or quality.  As early as 1914, for example, J.A. Udden of the University of Texas identified this confusion of quantity with content.  He found that extensive local press coverage of a geological congress, coverage that passed for science news, consisted almost entirely of descriptions of formalities, including personalities and social events, rather than knowledge or even discovery, much less intellectual excitement.&quot;</i> (p197-200)<br /><br /><b>The Change in Popularizing Personnel</b><br /><br /><b><i>&quot;Even in the 1970s, scientists disagreed with the popularizer specialists about what the public ought to hear and read.  As two researchers reported in their study, &#39;A basic divergence was clear throughout: [science writers] wrote what they believed would interest their audience, while scientists held to the academic tradition of trying to teach people what they believed was new and important ... whether the audience was willing to listen or not.&#39; The scientists&#39; goals were simply not honored by either writers or gatekeepers. In 1957, a science writer declared to a group of scientists, &#39;You think science writing should be educational for the public, primarily. But no newspaper editor expects to publish science stories because they are primarily educational. They are published because they are news. If we can educate the public along the way, we&#39;re doing well. We can&#39;t always do it.&#39;</i></b><br /><br /><i>The problem therefore was not just that, as another pair of researchers concluded, journalists took liberties &#39;with the substance and perspectives of science&#39;  That was bad enough. </i><b><i>The problem was also that the specialists&#39; priorities of entertainment and news clashed with those of enlightenment. &#39;Good science reporting is impossible as long as its purpose is assumed to be entertainment and not education,&#39;</i></b><i> noted the editor of the Bulletin of the Atomic Scientists.  Even though the best science writers might have striven to educate, </i><b><i>their editors as a group judged stories on the basis of color and excitement.</i></b><i> Benjamin T Brooks, a New York chemist, in 1928 denounced &#39;tabloid&#39; science and asked directly, &#39;Must morons be supplied with moron science?&#39;  For generations in the twentieth century, many editors (and, later, producers) seemed to think so.&quot;</i> (p211)<br /><br /><b>Science Gets Complicated</b><br /><br /><b><i>&quot;One reason that nineteenth-century evangelical popularization of science receded in the twentieth century was that what scientists had to communicate -- beyond image and attitude -- grew much more difficult to put into popular form after 1900. At one time, the task of popular science had been to clear up mysteries.</i></b><i> Indeed, as late as 1924, Slosson, the voice of Science Service, was still denouncing false beliefs and commercial distortions by saying that &#39;the test of real science&#39; was &#39;its honesty&#39; in attacking &#39;the real mysteries of nature.&#39;  Explanation was for him, as it had been earlier for &#39;men of science,&#39; the essence of popularizing. </i><b><i>But by the midcentury period, popularizers of all varieties were bemoaning the fact that pure science had become so complicated that they could hardly explain it. The science of the new century did not necessarily confirm the gospel of certain progress, unity, and reductionism of an earlier day.</i></b><i> Biologist Francis Sumner of the University of California in 1937 summarized the dilemma for example of those who read about the new physics: </i><b><i>&#39;There would seem to be a vast inconsistency between the traditional notion of the man of science, with his uncompromising insistence on evidence and his lofty scorn of guesses and unproved assumptions, and the quasi-mystic who tells us all these strange things about space and infinity and who describes with such assurance the detailed intricacies of an infinitesimal world forever beyond the range of human observation.&#39;</i></b><br /><br /><b><i>While innumerable newspapermen of the 1920s tried to explain relativity, and in later years other baffling ideas, or to explain why something could not be explained, only a few scientists attempted to draw conclusions for the public from the unsettling findings of the new physics and other mathematical or abstract studies. Popularizers of every stripe found themselves instead portraying confusion. They therefore often had to readjust what they were doing, because they could no longer appeal to the paradigmatic act of explaining the mystery. They were, rather, being called upon to explain the confusion, which was particularly difficult without the vision of ultimate unity that earlier popularizers had enjoyed and exploited.</i></b><i> The newspapers were not to blame, instead the editor of the New York Times in 1935: &#39;The function of the newspaper is primarily to report what the leading scientists do and say.  If they contradict each other, and there is confusion, the newspapers merely picture the confusion and do not created it.&#39;</i><br /><br /><b><i>The result of this challenge to the unity of science was to encourage popularizers to redouble the emphasis that had been developing on portraying the results, rather than the ideas, of science.</i></b><i> The Science Year Book published in the 1940s, for example, usually had &#39;aviation&#39; as a major category and consisted almost entirely of articles about applied science developments.  As early as 1903, W.S. Franklin, a physicist at Lehigh, observed: </i><b><i>&#39;Everything that appears in the name of science in our newspapers and magazines relates only to results. Have any of you seen in our newspapers or popular magazines any detailed description of the principles and methods used by Marconi in his wireless telegraphy?&#39;</i></b><i> More than half a century later, Palmer Wright, a Dow Chemical chemist, wondered if popularizers should not build on applied science for understanding. </i><b><i>&#39;The popular thrust,&#39; he observed, &#39;is toward the what, not the why of science.&#39;</i></b><br /><br /><i>So it was that popularized science in the twentieth century continued as in the nineteenth to emphasize progress -- </i><b><i>but progress now in terms more exclusively of the &#39;applications mankind can make from [the] marvelous findings&#39; of &#39;pure science,&#39;</i></b><i> as a 1926 writer put it.  But the new context of progress had further implications.  Any facts at all could be digested into the popular science of results; </i><b><i>they needed no further context than that they were part of the advance of science. Thus a discovery at any level of science reported in the 1920s was a fact and a part of the progress of science;</i></b><i> so, too, was a machine of the 1930s or a cure of the 1940s a fact and a part of the progress of science.  This approach to popularizing was compatible with the rise of general science in the schools, in which teachers emphasized application and turned away from the work of researchers that was abstract and unpopular. </i><b><i>Even in the 1960s, reported Howard E. Gruber, &#39;high school teachers generally approach[ed] science teaching as a matter of conveying science as established facts and doctrines&#39; rather than &#39;science teaching in which science [was] treated as a way of thought.&#39;</i></b><i> In the more general popularizing of the twentieth century, audiences would have been lucky to get doctrines in addition to lists of products.&quot;</i> (p214-216)<br /><br /><b>The Destructive Effect of Facts</b><br /><br /><i>&quot;The first step in destroying the classic uplift popularization of science was the phenomenon that I have described in health, psychology and the natural sciences alike, </i><b><i>namely, reducing the context of science in popularizing and at the same time emphasizing &#39;facts&#39; so that &#39;science&#39; in the new mode of popularization consisted of isolated bits and pieces.</i></b><i> As early as 1920, the editor of Modern Medicine complained that such knowledge of scientific investigations as was &#39;current among the general public [was] fragmentary, unrelated and, for the most part, acquired through spectacular accounts of the Sunday newspaper,&#39; leading, he thought, to &#39;the tendency to mysticism in the absence of definite knowledge.&#39;</i><br /><br /><i>From earliest times, science was distinctive in part because, in contrast to superstition, science was part of a system.  In 1830, the editor of the Mechanics&#39; &amp; Farmers&#39; Magazine of Useful Knowledge assured readers that </i><b><i>scientific subjects would &#39;be treated systematically, no part being omitted that [might] be deemed essential to a right understanding of them,&#39; hoping, he said, to &#39;succeed in arresting the attention of those who ... read, but read to no purpose.&#39;&quot;</i></b> (p229)<br /><br /><b><i>&quot;As late as the 1930s -- as Jennie Mohr shows in her careful analysis of popular books on science -- authors, whatever their credentials, tried not only to convey to readers an appreciation for the scientific way of thinking but &#39;to enable the layman to see the world as the scientist sees it; to show him that the world is not chaotic, but ordered, and that apparently discrete and unrelated events follow a regular and understandable pattern.&#39;  A few years later an Illinois librarian noted that popular books that were successful succeeded because the authors took the trouble to explain fully each step in thinking and how it connected to others.</i></b><br /><br /><b><i>The danger of unconnected facts outside of a systematic context was recognized early in all fields.</i></b><i> In 1901, Stratton D. Brooks, who taught pedagogy at the University of Illinois, criticized untrained science teachers who, lacking &#39;the spirit of scientific investigation,&#39; taught useless facts unrelated to the body of science.  More than thirty years later, another educator was still denouncing </i><b><i>&#39;giving the world facts without teaching it how to think about those facts and how to use them properly.&#39;</i></b><i> Nor was this phenomenon merely an aspect of bad teaching.  F.C. Robinson of Bowdoin in 1905 denounced &#39;scraps of health knowledge ... - predigested knowledge brought on with one&#39;s predigested breakfast food,&#39; facts that could entertain but not influence.  By 1945, Carl J Potthoff of the University of Minnesota was still saying, &#39;Health education today emphasizes facts; </i><b><i>its contribution otherwise to the thinking discipline appears to be fragmentary,&#39;</i></b><i> and in 1972 the authors of a Food and Drug Administration report found that most Americans did not have a systematic approach to health but rather used a &#39;rampant empiricism&#39; based upon isolated facts and personal &#39;experience.&#39;  In 1947, historian of science I. Bernard Cohen was observing that specialization had broken popular science presentations into fragments: </i><b><i>&#39;The average reader is not at fault for having little feeling for the life of science as a whole,&#39; because modern popularization &#39;has introduced him to unconnected scientific achievements,&#39; mostly, of course, practical. And in 1981, Leon Tractman of Purdue was complaining that he was so overcome by the facts of popularized science that he doubted their utility for anything.</i></b><br /><br /><i>All of these witnesses were commenting on a tendency that became more and more pronounced in the twentieth century as the mass media grew in importance in popularization. </i><b><i>Producers of facts flooded the public with &#39;a glut of occurrences,&#39;</i></b><i> as Herbert Brucker pointed out in 1937, occurrences out of many special fields and not just science.  The illogical associations that the mass media induce from the overwhelming volume of information, Daniel Lawrence O&#39;Keefe points out, &#39;naturalize a crazy world.&#39; </i><b><i>Paul F. Lazarsfeld and Robert K. Merton in 1948 named the effects of being swamped by media facts &#39;narcotizing dysfunction&#39;: &#39;Exposure to this flood of information may serve to narcotize rather than to energize the average reader.&#39;</i></b> (p230-231)<br /><br /><i>&quot;Sometimes, as in the Popular Science Monthly in the nineteenth century or in the Scientific Monthly in the twentieth, a popularizer could give facts and assume some of the context, such as the promise that every isolated &#39;discovery&#39; was a contribution to &#39;progress&#39; -- especially if the reader might know what had gone before.  Any educated person should have known, therefore, at least some of the significance of a new chemical element or a fossil anthropoid.  But the emphasis on snippets without context, often presented in the name of objectivity, ultimately was subversive not only of the religion of science but of popularization itself.  The moral superiority of objectivity derived from the method and attitude, not particular findings.&quot;</i> (p232)<br /><br /><b>The Problem of Conclusions First</b><br /><br /><i>&quot;A structural element in news reporting also intensified the sensationalism issue in science popularization throughout the twentieth century.  Journalistic reports almost always started with conclusions and implications and then were filled out with supporting detail -- </i><b><i>exactly the opposite format from the restrained scientific report in which conclusions and discussions came only after the methodology and findings were fully set forth. The news format therefore put great emphasis on the journalists&#39; interpretations. The whole problem was exacerbated by the fact that news items tended to be short -- in contrast to the longer magazine format -- and so journalists cut the scientific context at the same time that they played up attention-grabbing elements and products.</i></b><i> Again, a combination of sensationalism with the briefly stated facts removed science news substantially from older standards of responsible popularizing.</i><br /><br /><i>What was new in twentieth-century popularizing of science and health, then, was not sensationalism but the bits-and-pieces format, and the early to mid-century timing of the coming of journalists and their format explains when and why they had such an impact.  Journalism itself, as Michael Schudson has pointed out, developed, around the 1930s especially, a strategy to strive for objectivity, whereby fact and opinion were separated -- </i><b><i>opinion to go into signed columns and objective fact into the news columns.</i></b><i> This development, Schudson shows, was the end product of the nineteenth-century belief in objective reality, when the daily flow of events came to constitute a reality in itself. </i><b><i>For most news, then, the interpretive signed column served to provide a context, so that the endless flow of bits and pieces was at least given a place within the socio-political world of the journalists.</i></b><br /><br /><b><i>For science and health, however, only half of this journalistic strategy was effective. By and large, columnists in those fields, when such columns existed, merely multiplied facts</i></b><i> -- as typically, and best known, in the health columns of newspapers or in Science Service syndications. </i><b><i>Such columnists did not provide context. And ordinary journalistic columnists were not competent to act the part of men of science. The only interpreters of science who had any impact, then, were the advertisers. The result was that sports journalism became the most recognizable model for all reporting, including science and health -- a kaleidoscope, as Gilman Ostrander puts it, &#39;a jarring and distracting montage of unrelated pictures and news items and advertisements, all competing for attention and most of them immediately forgotten.&#39;</i></b><i> Science, health, and psychology were incidental elements in this twentieth-century torrent of items, and when they occurred in a sensationalistic guide, they became just one of the &#39;tremendous trifles,&#39; media fads that Frederick Lews Allen described as characteristic of the 1920s -- </i><b><i>insignificant items given emotional appeal, rapidly becoming known to everyone who shared the mass media world and then passing into oblivion.&quot;</i></b> (236-237)<br /><br /><b>Consequences of Bits-and-Pieces Popularization</b><br /><br /><i>&quot;Other commentators have observed how mass, urban society spawned a &#39;bewildering sense of purposeless energy, abundance and diversity, operating without plan or pattern, divorced from human or divine reason.&#39;  Within that context, an educator at New York University, George E. Axtelle, deplored the specialization that had compartmentalized science to the disadvantage of teaching: </i><b><i>&#39;Unless the specialization can be grasped in the context of the whole, its meaning is seen as little more than the part ... The whole is nothing less than the whole of nature and culture.&#39;  What Axtelle and others concerned with popularization objected to was not only the segmentation and lack of context but the fact that the bits-and-pieces format furnished its own closure, as if the fact were an end in itself.  Any uplift, much less the missionary zeal of the primitive popularizers, was completely defeated.</i></b><br /><br /><b><i>As the contextless fact variety of popularizing science evolved, innumerable commentators noted the way in which such a variety of science encouraged the tendency of &#39;folk&#39; to believe in magic.  &#39;We live in the &#39;scientific age,&#39; to be sure,&#39; wrote Everett Dean Martin in 1926, &#39;but it is the fruits of science rather than its methods and discipline which the majority possess. The &#39;wonders&#39; of science are to many minds, the miracles of science.  Mankind at large still believes in miracles and magic. Science has not dispelled this superstition. Science itself becomes accepted as a new kind of magic.&#39; Journalists in particular utilized the equating of magic and science as metaphor, but the metaphor came to represent substantial reality in many Americans&#39; understanding of science. As a writer in the Nation noted in 1902, &#39;Something of the mediaeval notion of science as a variant of the black arts seems to survive in certain popular modes of speaking of modern scientists. Marconi is a &#39;magician&#39;; Edison, a &#39;wizard.&#39;  Nor are these phrases purely chance-sown. They well express the real mental attitude of millions of honest folk towards science.  To them it remains a region of wonder and mystery. Any miracle may come out of it any day.</i></b><br /><br /><i>Other popularizers joined the journalists in unwittingly encouraging this way of thinking as they described mysteries that science had explained or would in the future. </i><b><i>Particularly when only facts or findings were described as science, there was no reason for members of the popular audience to think of the solutions as other than mysterious.</i></b><i> In this way magical science could function as a substitute for other magics.  At the same time, so were created the circumstances that permitted pseudoscience to flourish, when consumers could not identify imposters, whether in science, technology, health, or psychology -- if Marconi or Edison, why not Keely?  Or Velikovsky?  One wizard (or authority) was as good as another.  By the mid-twentieth century, popularizer Isaac Asimov undertook his work, he said, specifically because science had &#39;increasingly lost touch with nonscientists.  Under such circumstances scientists [had] come to be regarded almost as magicians.&#39;  He therefore wrote, Asimov continued, in the traditional popularizing way to remove some of the mystery from science and at the same time to help create a taste for it. </i><b><i>To remove mystery of course meant to provide a meaningful context for facts and findings.</i></b><i> But Asimov was already one of a dying breed.&quot;</i> (p233-234)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"The Men of Science","summary":"The Original Science Evangelists / Took on the Responsibility for the Fate of Science in the Public Arena / Each Possessed a Broad Interest in All Sciences / They All Shared a Common Loyalty to Science, Inspired by the Universality of the Scientific Method / The Story of What Replaced Them as Populizers of Science is the Story You Should Know","image":"https://lh3.googleusercontent.com/-yCEeqodb0WY/VrZMkpE86GI/AAAAAAAAGlw/1oSQfayn0yI/w1800-h2520/the-men-of-science-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/Zzr2XYML8vU","publishDate":"2016-02-06T20:00:16.086Z","updateDate":"2017-01-03T02:59:02.528Z","text":"<b>The Men of Science: The Original Science Evangelists / Took on the Responsibility for the Fate of Science in the Public Arena / Each Possessed a Broad Interest in All Sciences / They All Shared a Common Loyalty to Science, Inspired by the Universality of the Scientific Method / The Story of What Replaced Them as Populizers of Science is the Story You Should Know</b><br /><br />The image is of the Solvay Conference in 1927, from the approximate era of the &quot;men of science.&quot;  Although the phrase sounds vague and broad, it bears a specific historical meaning, and only a handful of the actual &quot;men of science&quot; would today be recognized.  The Solvay Conference photograph will have to do.<br /><br />Science historian John C. Burnham explains in <i>How Superstition Won and Science Lost: Popularizing Science and Health in the United States</i> who these men (and women) were and what happened when they decided to stop participating in the public&#39;s education about science.<br /><br /><b>The &quot;Religion of Science&quot;</b><br /><br /><i>&quot;For several decades in the nineteenth century ... most Americans assumed that science reinforced their religion and the commonsense philosophy that went with it.  Virtually all scientists and other well-educated people were at least nominally pious, and one of the major reasons that they gave for studying science was that it confirmed religious truths.  The author of a children&#39;s geography and astronomy book published in 1818, for example, described the immensity of the universe and then noted &#39;the greatness, the wisdom, and power of that Almighty Being who has created, preserves and governs this universe of worlds.&#39; </i><b><i>But about the time of the Civil War, these pious assumptions began to disappear from popularized science. In their place the religion of science flourished.</i></b><br /><br /><i>The religion of science consisted of an informal set of beliefs that filled followers with evangelical fervor.  They thereupon attempted to convert both individuals and the public at large.  Their enthusiasm and their approach were taken, as already noted, from Protestant evangelism.  These enthusiasts had their martyrs to superstition -- Copernicus, Galileo, Servetus, and, later on, the victims of antievolutionary churchmen.  Moreover, the apostles had visible forces of evil with which to contend: superstition, ignorance, and, now, added in accordance with Anglo-American liberal political traditions, intolerance -- since scientific findings presumably could stand up to any opinion. </i><b><i>The goal of the evangelicals of the religion of science was to bring enlightenment by exposing everyone to the truth, that is, by popularizing science.</i></b><i> They talked openly of &#39;converting&#39; people, especially after Darwinism came along.  Given the religious upbringing of almost all of the American adherents, the religious parallels were not merely a metaphor but conveyed the style and manner of the evangelicals.  Most of what did differ was merely the content.&quot;</i> (p23)<br /><br /><b>The Classic &quot;Men of Science&quot; of the Late 19th Century</b><br /><br /><i>&quot;What distinguishes those who did take on responsibility for the fate of science in the public arena from colleagues who did not? </i><b><i>The answer lies largely in the broad view that the men of science took of their enterprise, identifying science with both high culture and social improvement of all kinds. A writer, presumably Youmans, noted in the Popular Science Monthly in 1877 that many scientists, in contrast to men of science, were narrow and specialized, caught up in details of their study. &#39;It is a great mistake,&#39; he noted, &#39;to suppose that all the influences exerted on the mind by scientific study are necessarily of a widening or liberalizing character.&#39; And most regrettable, he continued, was the fact that excellent workers because of their narrowness were not contributing to popularizing.</i></b><br /><br /><i>American men of science were, then, a special breed, more than just field or laboratory workers or thinkers. </i><b><i>One mark of the man of science was that he took &#39;more or less interest in all sciences,&#39; as one of them put it in 1916.  However specialized, the man of science had, besides general breadth, the overarching identity that made him rise above parochial interests to embrace loyalty to science in general (an attribute, as will be noted in the last two chapters, that had largely died out among scientists by the mid-twentieth century. And clearly the intellectual justification for all scientists to stand together was the universality of the scientific method.&quot;</i></b> (p29-30)<br /><br /><i>&quot;Men of science did have some successes against superstition and its allies, especially as secularization in general affected American life. </i><b><i>But eventually, in the twentieth century, two elements changed: the personnel who carried out the popularization, on the one hand, and the institutions through which popularization proceeded, on the other.  Whereas in the nineteenth century scientists tended to do their own popularizing, in the twentieth others gradually took over.  And, simultaneously, as journalism changed and the public moved to electronic media, popularization, too, moved into a very different phase, in which popularizers faced a debasement of public taste and aspiration.&quot;</i></b> (p31)<br /><br /><i>&quot;That superstition won tells, however, but part of the story of popularizing; the other part is how science lost -- and particularly when popularization of science and health traditionally had consisted of a battle against superstition and error.  What happened to the popularizers who for so long had fought against the forces of occultism and self-interested authority, indeed, who had even opposed their altruistic scientism to commercialism?</i><br /><br /><i>Science lost, as my narrative has shown, because the scientists left the field of popularization ...</i><br /><br /><i>The technically educated leaders who could have continued popularization in the traditional way in fact did not do so.  Beyond their focus on &#39;image,&#39; </i><b><i>large numbers of researchers found their specialized worlds so far from even a high-class popular audience that it was easy to let other specialists, specialists in dealing with either adults or children, do the work and provide at least the guidance when a scientist did turn to a popular audience.</i></b><i> As specialization, sophisticated quantification, uncertainty, and paradox alienated the popular level of understanding from that of the laboratory, the difficulties of filling the role of popularizer increased greatly for any investigator ...</i><br /><br /><b><i>When such scientists stopped trying, and withdrew, popularization, as I have shown, fell into the hands of other kinds of translators, simplifiers, and nonscientific specialists.</i></b><i> But beyond that, the cultural symbolism of popular science disintegrated -- the idea that science stood for something: opposition to superstition, if not for a naturalistic &#39;philosophy of life.&#39; ...</i><br /><br /><i>The influence of journalists, for their part, was altogether corrupting, turning high culture into trivial news items and reducing men of science to purveyors of bits and pieces of information ...&quot;</i> (p247-249)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23critique\">#critique</a>  ﻿","category":"critique"},{"name":"Donald W Braben","summary":"16 Years as a Nuclear Structure & High-Energy Physics Researcher / Created and Ran Venture Research Initiative Sponsored by BP for 10 Years / Prolific Author, Honorary Professor, Academic Reformer and Advocate for Maverick Science","image":"https://lh4.googleusercontent.com/-SBH0yovE9FM/Vq1cFLr1DWI/AAAAAAAAGkM/0f0V2z_AK10/donald-braben-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/j1HEC42RNZs","publishDate":"2016-01-31T01:23:59.752Z","updateDate":"2017-01-03T02:59:16.155Z","text":"<b>Donald W Braben: 16 Years as a Nuclear Structure &amp; High-Energy Physics Researcher / Created and Ran Venture Research Initiative Sponsored by BP for 10 Years / Prolific Author, Honorary Professor, Academic Reformer and Advocate for Maverick Science</b><br /><br />From <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=8F1E4088CAF25454A7454CADD92C6E32?doi=10.1.1.686.8216&amp;rep=rep1&amp;type=pdf\" class=\"ot-anchor\">http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=8F1E4088CAF25454A7454CADD92C6E32?doi=10.1.1.686.8216&amp;rep=rep1&amp;type=pdf</a> ...<br /><br /><i>&quot;Following 16 years as a researcher in nuclear structure and high-energy physics, Braben was at the Cabinet Office in Whitehall, the Science Research Council in London, and the Bank of England. In 1980, he created and ran Venture Research, an initiative sponsored by British Petroleum that was fully operational for ten years (1980-90), and ran until 1993. He is now Honorary Professor in University College London’s Department of Earth Sciences and also in the Office of the Vice Provost for Research.</i><br /><br /><i>In addition to papers on nuclear and elementary-particle physics, and articles on science policy, he is the author of the new book, Promoting the Planck Club: How defiant youth, irreverent researchers and liberated universities can foster prosperity indefinitely (2014), as well as To Be a Scientist (1994), Pioneering Research: A Risk Worth Taking (2004), and Scientific Freedom: The Elixir of Civilization (2008).&quot;</i><br /><br />What you need to know about Dr. Braben, from the above article ...<br /><br /><i>&quot;Venture Research</i><br /><br /><b><i>In the Venture Research initiative sponsored by British Petroleum during the 1980s and 1990s, we found, before its untimely closure, some 26 groups of researchers whose proposals (with the possible exception of one) had been rejected by the conventional funding agencies. We were the agency of last resort. The research, in Europe and North America, was initially well outside the current mainstreams, and scientists worked on the most basic and fundamental problems chosen by themselves. Despite rejection by peer review, the initiative led eventually to at least 13 breakthroughs; that is, the researchers succeeded in radically changing the ways we think about important subjects.</i></b><i> For example, Ken Seddon, now at Queen’s University Belfast, had in the mid-1980s submitted a proposal to a precursor of the EPSRC to study the chemistry of ions in an ionic environment. Hitherto, chemistry had been almost exclusively concerned with the study of covalently bonded molecules in a molecular environment — a description that virtually covers the entire field of organic chemistry. But </i><b><i>Seddon had identified an unexplored domain. Astonishingly, after the usual trial by peer review, it was rejected with a gamma rating; the Council’s lowest possible.</i></b><i> But Seddon knew about Venture Research, having been a junior participant in the scheme a few years before, and brought the same proposal to us, which we gladly funded. </i><b><i>His Venture Research not only turned out to be scientifically very successful (as usually happens when new questions are asked in important fields), but it also transformed the field of green chemistry, [2]  and Seddon became the UK’s most cited chemist. [3]&quot;</i></b><br /><br />Dr. Gerald Pollack cites Braben&#39;s work in his academic reform paper, &quot;Revitalizing Science in a Risk-Averse Culture: Reflections on the Syndrome and Prescriptions for its Cure&quot; at <a href=\"http://faculty.washington.edu/ghp/images/stories/pubPDF/2005-Pollack-Revitalizing_Science.pdf\" class=\"ot-anchor\">http://faculty.washington.edu/ghp/images/stories/pubPDF/2005-Pollack-Revitalizing_Science.pdf</a> ...<br /><br /><i>&quot;Has the Scientific Enterprise Gone Away?</i><br /><br /><i>A half-century ago, breakthroughs were fairly common events that could be counted on to occur from time to time on an unpredictable but not infrequent basis. Pioneering such breakthroughs were scientific heroes – legendary figures such as Linus Pauling, Jonas Salk, Richard Feynman, James Watson, Francis Crick, and others, names familiar even to lay people.</i><br /><br /><i>But things have changed. </i><b><i>While the past 30 years have brought a great outpouring of scientific results, breakthroughs are less common. Modern equivalents of Pauling, Salk, and Watson-Crick are not easy to identify.</i></b><br /><br /><i>Considering the massive investment in science today, why is it that scientific heroes have become so scarce? Why so few conceptual breakthroughs? I refer to realized breakthroughs such as the biochemical nature of heredity or the polio vaccine, not incipient breakthroughs whose realization seems always just around the corner. </i><b><i>Can you name more than a handful of realized breakthroughs that have come during the past three decades?</i></b><br /><br /><i>Some argue that this settling down is all but inevitable. After all, science today is far more complicated than it has been, often requiring teams of investigators and large groups to pursue effectively. Others argue that there is simply not much more to be discovered – that the breakthroughs have had their heyday and we need content ourselves with merely filling in the gaps. Thus, breakthroughs might not be expected to occur on an everyday basis.</i><br /><br /><i>Perhaps some of this is true – but a significant role may also be played by another factor: </i><b><i>the growing aversion to risk taking. Although funding agencies have much to be proud of for past achievements, it is broadly perceived that they have become less agile in dealing with proposals that dissent from orthodoxy. Challengers of the status quo rarely succeed in today’s scientific climate. Hence, those approaches most apt to generate conceptual breakthroughs are throttled before they can emerge from the scientific womb.</i></b><br /><br /><b><i>The funding agencies worldwide are aware of this problem.</i></b><i> Several agencies have held recent workshops to deal with the issue, and some measures have been taken over and above existing remedial programs. In the US, for example, the term &#39;high risk&#39; now permeates review guidelines. And, both the National Science Foundation and the National Institutes of Health have established special programs to encourage novel approaches.</i><br /><br /><i>These institutional responses acknowledge the problem. Yet, it is broadly felt that the responses are nominal. Few dissenters from orthodoxy report any more success than before. The reviewers are largely the same, and have not abruptly changed their well-honed views.</i><br /><br /><i>Admonishing them to be &#39;less conservative&#39; comes with no guarantee that they will be. Thus, effective action has yet to be taken. I am not alone in this view. </i><b><i>A recent, highly praised book by Donald Braben, entitled Pioneering Research: A Risk Worth Taking (Wiley, 2004), concurs. Braben argues that limiting the ability of scientists to dissent from orthodoxy heralds a line of consequences leading ultimately to societal doom. Only a radical departure from the highly bureaucratic, top-down-managed approach to science, he argues, will solve the problem. In the absence of such a departure, progress will remain incremental, despite a vast pool of talent and an abundant pool of monetary resources. Freedom to dissent is essential.</i></b><br /><br /><i>Thus, the scientific enterprise appears to have gone at least somewhat astray. It has for sure generated a massive number of hugely productive enterprises whose outpouring of results seems to come ever closer to generating the hoped-for conceptual breakthroughs. But those breakthroughs don’t always materialize; often, it seems, their realization remains just around the corner. </i><b><i>A reason for this mixed track record is that the very approaches that could lead to breakthroughs – those that challenge the current line of thinking with fresh alternatives – have virtually no chance of advancing in today’s scientific enterprise. This is a serious impediment.&quot;</i></b><br /><br />Dr. Gerald Pollack has since proposed a solution which he calls the Institute for Venture Science.<br /><br />It works like this:<br /><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/HkUnPLpvq6U\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/HkUnPLpvq6U</a><br /><br />For a more thorough explanation, see their website ...<br /><br /><a href=\"http://www.theinstituteforventurescience.net/\" class=\"ot-anchor\">http://www.theinstituteforventurescience.net/</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23person\">#person</a>  ﻿","category":"person"},{"name":"John Taylor Gatto","summary":"30 Years Experience Teaching in the Best and Worst Schools in New York City / Received Both the NY State and NY City Teacher of the Year Awards / Outspoken Historian and Critic of Public Schooling","image":"https://lh6.googleusercontent.com/-wdVlISam6hw/Vq04yf3UKzI/AAAAAAAAGik/i34moFTPPuM/john-taylor-gatto-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/EfntTNmBpyB","publishDate":"2016-01-30T23:31:47.960Z","updateDate":"2017-01-03T02:59:27.947Z","text":"<b>John Taylor Gatto: 30 Years Experience Teaching in the Best and Worst Schools in New York City / Received Both the NY State and NY City Teacher of the Year Awards / Outspoken Historian and Critic of Public Schooling</b><br /><br /><b>Question: Does the widespread proliferation of incredible technologies prove the accuracy of our most popular scientific theories?</b><br /><br />John Taylor Gatto would probably disagree.  He asks a slightly different question which I think can indirectly answer the one I pose here.  His surprising claim is that the apparent material success of our society indicates a second-rate educational system.  The technology is not the result of successful theories so much as it is the product of an efficient system that combines business, schooling and engineering.  It is clear that something we are doing is effectively generating new technologies and material wealth -- but the answer as to what this is might not be the superficial narrative we tend to culturally assume.<br /><br /><b>These questions and claims appear to affect a wide variety of issues, from academic reform to our evaluation of scientific claims.  So, whether or not you agree with him, you need to know his argument.</b><br /><br />From pages 151-155 of <i>The Underground History of American Education: A School Teacher&#39;s Intimate Investigation Into the Problem of Modern Schooling</i><br /><br /><i>&quot;Managerial Utopia</i><br /><br /><i>In an angry letter to the Atlantic Monthly (January 1998), Walter Greene, of Hatboro, Pennsylvania, protested the &#39;myth of our failing schools,&#39; as he called it, on these grounds:</i><br /><br /><i>&#39;We just happen to have the world’s most productive work force, the largest economy, the highest material standard of living, more Nobel prizes than the rest of the world combined, the best system of higher education, the best high-tech medicine, and the strongest military. These things could not have been accomplished with second-rate systems of education.&#39;</i><br /><br /><b><i>On the contrary, the surprising truth is they could not have been accomplished to the degree they have been without second-rate systems of education.</i></b><i> But here it is, writ plain, the crux of an unbearable paradox posed by scientifically efficient schooling. It works. School, as we have it, does build national wealth, it does lead to endless scientific advances. Where is Greene’s misstep? </i><b><i>It lies in the equation of material prosperity and power with education when our affluence is built on schooling (and on entrepreneurial freedom, too, of course, for those libertarian enough to seize it). A century of relentless agit-prop has thrown us off the scent. The truth is that America’s unprecedented global power and spectacular material wealth are a direct product of a third-rate educational system, upon whose inefficiency in developing intellect and character they depend. If we educated better we could not sustain the corporate utopia we have made. Schools build national wealth by tearing down personal sovereignty, morality, and family life. It was a trade-off ...</i></b><br /><br /><i>It takes no great insight or intelligence to see that the health of a centralized economy built around dense concentrations of economic power and a close business alliance with government can’t tolerate any considerable degree of intellectual schooling. This is no vain hypothesis. The recent French Revolution was widely regarded as the work of a horde of underemployed intellectuals, the American uprising more of the same. As the nineteenth century wore on, the Hungarian and Italian revolutions were both financed and partially planned from the United States using cells of marginal intellectuals,third sons, and other malcontents as a volunteer fifth column in advance of the revolutionary moment back home. Ample precedent to fear the educated was there; it was recognized that historical precedent identified thoughtful schooling as a dangerous blessing ...</i><br /><br /><i>The Positive Method</i><br /><br /><i>Most of the anti-intellectual shift in schooling the young was determined by the attitudes and needs of prominent businessmen. The first exhibit for your perusal is the U.S. Bureau of Education’s Circular of Information for April 1872, which centers around what it calls the &#39;problem of educational schooling.&#39; With whose interests in mind did the bureau view education as a problem? The amazing answer is: from a big business perspective. By 1872, this still feeble arm of the federal government is seen filled with concern for large industrial employers at a time when those were still a modest fraction of the total economy.</i><br /><br /><b><i>According to this Circular of Information, &#39;inculcating knowledge&#39; teaches workers to be able to &#39;perceive and calculate their grievances,&#39; thus making them &#39;more redoubtable foes&#39; in labor struggles. Indeed, this was one important reason for Thomas Jefferson’s own tentative support of a system of universal schooling</i></b><i>, but something had been lost between Monticello and the Capital. </i><b><i>&#39;Such an enabling is bound to retard the growth of industry,&#39; continues the Circular. There is nothing ambiguous about that statement at all, and the writer is correct, of course.</i></b><br /><br /><i>Sixteen years later (1888), we can trace the growth in this attitude from the much more candid language in the Report of the Senate Committee on Education. Its gigantic bulk might be summarized in this single sentence taken from page 1,382:</i><br /><br /><b><i>&#39;We believe that education is one of the principal causes of discontent of late years manifesting itself among the laboring classes.&#39;</i></b><br /><br /><i>Once we acknowledge that planned economies of nation or corporation are systems with their own operating integrity, quite sensibly antagonistic to the risks educated minds pose, much of formal schooling’s role in the transformation that came is predictable. If education is indeed &#39;one of the principal causes of discontent,&#39; it performs that subversive function innocently by developing intellect and character in such a way as to resist absorption into impersonal systems: </i><b><i>Here is the crux of the difference between education and schooling —  the former turns on independence, knowledge, ability, comprehension, and integrity; the latter upon obedience.</i></b><br /><br /><i>In The Empire of Business (1902), Andrew Carnegie, author of the Homestead siege which destroyed the steelworkers union, inveighs against &#39;teachings which serve to imbue [children] with false ideas.&#39; </i><b><i>From a transatlantic business perspective, education taught what was socially and economically useless, transmitting bad attitudes which turned students against the ripening scheme of centralized national management.</i></b><i> Carnegie’s new empire demanded that old-fashioned character be schooled out of children in a hurry. It would be a large mistake to assume this new empire of business of which Carnegie boasts was only a new face on old style greed. While it did take away liberty and sovereignty, it put forth serious intellectual arguments for doing so. </i><b><i>Ordinary people were promised what Walter Greene’s outraged letter quoted earlier at the beginning of this chapter tells you they got: the best space program, the best high-tech medicine, the strongest military, the highest material standard of living. These things could not have been accomplished without a kind of forced schooling that terminated most independent livelihoods. That was the price paid for a gusher of easy prosperity.</i></b><br /><br /><i>To understand this paradox better requires some insight into what inspired such certainty among the architects of modern schooling that this disruption would work to produce material prosperity. Their faith that wealth would inevitably follow the social mechanization of the population is founded on a magnificent insight of Francis Bacon’s, set down in startlingly clear prose back in the early seventeenth century. Thanks to the patronage of John Stuart Mill, by the mid-nineteenth century, the seeds that Bacon planted grew into the cult of </i><b><i>scientific positivism</i></b><i>, a movement we associate today with the name of a Frenchman, Auguste Comte. It’s hard to overestimate the influence positivism had on the formation of mass schooling and on the shaping of an international corporate economy made possible by coal.</i><br /><br /><b><i>Positivism holds that if proper procedures are honored, then scientific marvels and inventions follow automatically. If you weigh and measure and count and categorize slowly and patiently, retaining the microscopic bits of data which can be confirmed, rejecting those that cannot, on and on and on and on, then genius and talent are almost irrelevant — improvements will present themselves regularly in an endless progression despite any fall-off in creative power.</i></b><i> Advances in power and control are mainly a function of the amount of money spent, the quantity of manpower employed, and correct methodology.</i><br /><br /><i>Mankind can be freed from the tyranny of intelligence by faithful obedience to system! This is a shattering pronouncement, one made all the more difficult to resist because it seems to work. Even today, its full significance isn’t widely understood, nor is the implacable enmity it demands toward any spiritual view of humanity.</i><br /><br /><i>In the positivist method, the managerial classes of the late nineteenth century, including their Progressive progeny in the social management game, knew they had a mill to grind perpetual profits — financial, intellectual, and social. </i><b><i>Since innovations in production and organization are a principal engine of social change, and since positive science has the power to produce such innovations without end, then even during the launch of our era of scientific management it had to be clear to its architects that nonstop social turbulence would be a daily companion of exercising this power.</i></b><i> This is what the closet philosophy of bionomics was there to explain. It preached that </i><b><i>the evolutionarily advanced would alone be able to tolerate the psychic chaos</i></b><i> — as for the rest, the fate of Cro-Magnon man and the Neanderthal were history’s answer. And the circularity of this convenient proposition was lost on its authors.</i><br /><br /><i>Faced with the problem of dangerous educated adults, what could be more natural than a factory to produce safely stupefied children? You’ve already seen that the positive system has only limited regard for brainy people, so nothing is lost productively in dumbing down and leveling the mass population, even providing a dose of the same for &#39;gifted and talented&#39; children. And much can be gained in social efficiency. What motive could be more &#39;humane&#39; than the wish to defuse the social dynamite positive science was endlessly casting off as a byproduct of its success?</i><br /><br /><i>To understand all this you have to be willing to see there is no known way to stop the social mutilation positive science leaves in its wake. </i><b><i>Society must forcibly be adapted to accept its own continuing disintegration as a natural and inevitable thing, and taught to recognize its own resistance as a form of pathology to be expunged. Once an economic system becomes dependent on positive science, it can’t allow any form of education to take root which might interrupt the constant accumulation of observations which produce the next scientific advance.</i></b><br /><br /><i>In simple terms, what ordinary people call religious truth, liberty, free will, family values, the idea that life is not centrally about consumption or good physical health or getting rich — all these have to be strangled in the cause of progress. </i><b><i>What inures the positivistic soul to the agony it inflicts on others is its righteous certainty that these bad times will pass. Evolution will breed out of existence unfortunates who can’t tolerate this discipline.</i></b><br /><br /><i>This is the sacred narrative of modernity, its substitute for the message of the Nazarene. History will end in Chautauqua. School is a means to this end.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23person\">#person</a>  ﻿","category":"person"},{"name":"The Unknown Graduate Student","summary":"Anonymous Online Critic of Graduate Programs / Blasted Tendency Towards Bandwagon Research / Quit Program Just Weeks Shy of PhD So That You Would Pay Attention","image":"https://lh4.googleusercontent.com/-ETHn-zLlrfg/Vq0PrnhHs3I/AAAAAAAAGhc/zKYbALqZbvo/the-unknown-graduate-student-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/2rCpQP8mR4j","publishDate":"2016-01-30T19:43:03.210Z","updateDate":"2017-01-03T03:00:14.061Z","text":"<b>The Unknown Graduate Student: Anonymous Online Critic of Graduate Programs / Blasted Tendency Towards Bandwagon Research / Quit Program Just Weeks Shy of PhD So That You Would Pay Attention</b><br /><br />From <a href=\"http://crypto.junod.info/2013/09/09/an-aspiring-scientists-frustration-with-modern-day-academia-a-resignation/\" class=\"ot-anchor\">http://crypto.junod.info/2013/09/09/an-aspiring-scientists-frustration-with-modern-day-academia-a-resignation/</a><br /><br /><i>&quot;Dear EPFL,</i><br /><i>I am writing to state that, after four years of hard but enjoyable PhD work at this school, I am planning to quit my thesis in January, just a few months shy of completion ...</i><br /><br /><i>While I could give a multitude of reasons for leaving my studies – some more concrete, others more abstract – the essential motivation stems from my personal conclusion that I’ve lost faith in today’s academia as being something that brings a positive benefit to the world/societies we live in. </i><b><i>Rather, I’m starting to think of it as a big money vacuum that takes in grants and spits out nebulous results, fueled by people whose main concerns are not to advance knowledge and to effect positive change, though they may talk of such things, but to build their CVs and to propel/maintain their careers ...</i></b><br /><br /><i>(4) Academia: Where Originality Will Hurt You</i><br /><i>The good, healthy mentality would naturally be to work on research that we believe is important. Unfortunately, most such research is challenging and difficult to publish, and the current publish-or-perish system makes it difficult to put bread on the table while working on problems that require at least ten years of labor before you can report even the most preliminary results. Worse yet, the results may not be understood, which, in some cases, is tantamount to them being rejected by the academic community. I acknowledge that this is difficult, and ultimately cannot criticize the people who choose not to pursue such “risky” problems.</i><br /><br /><i>Ideally, the academic system would encourage those people who are already well established and trusted to pursue these challenges, and I’m sure that some already do. However, </i><b><i>I cannot help but get the impression that the majority of us are avoiding the real issues and pursuing minor, easy problems that we know can be solved and published. The result is a gigantic literature full of marginal/repetitive contributions.</i></b><i> This, however, is not necessarily a bad thing if it’s a good CV that you’re after.</i><br /><br /><i>(5) Academia: The Black Hole of Bandwagon Research</i><br /><b><i>Indeed, writing lots of papers of questionable value about a given popular topic seems to be a very good way to advance your academic career these days. The advantages are clear: there is no need to convince anyone that the topic is pertinent and you are very likely to be cited more since more people are likely to work on similar things. This will, in turn, raise your impact factor and will help to establish you as a credible researcher, regardless of whether your work is actually good/important or not. It also establishes a sort of stable network, where you pat other (equally opportunistic) researchers on the back while they pat away at yours.</i></b><br /><br /><b><i>Unfortunately, not only does this lead to quantity over quality, but many researchers, having grown dependent on the bandwagon, then need to find ways to keep it alive even when the field begins to stagnate. The results are usually disastrous. Either the researchers begin to think up of creative but completely absurd extensions of their methods to applications for which they are not appropriate, or they attempt to suppress other researchers who propose more original alternatives (usually, they do both). This, in turn, discourages new researchers from pursuing original alternatives and encourages them to join the bandwagon, which, though founded on a good idea, has now stagnated and is maintained by nothing but the pure will of the community that has become dependent on it. It becomes a giant, money-wasting mess.&quot;</i></b><br /><br />From <a href=\"https://en.wikipedia.org/wiki/Bandwagon_effect\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Bandwagon_effect</a> ...<br /><br /><i>&quot;The bandwagon effect is a phenomenon whereby the rate of uptake of beliefs, ideas, fads and trends increases the more that they have already been adopted by others. In other words, the bandwagon effect is characterized by the probability of individual adoption increasing with respect to the proportion who have already done so.[1] As more people come to believe in something, others also &#39;hop on the bandwagon&#39; regardless of the underlying evidence.&quot;</i><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23reform\">#reform</a>  ﻿","category":"reform"},{"name":"Jeff Schmidt","summary":"Associate Editor of Physics Today for 19 Years / Physics PhD / Author of Disciplined Minds","image":"https://lh5.googleusercontent.com/-0rPIhX6bo5o/VqmpZTa6NRI/AAAAAAAAGfo/uhxMPQNpQk8/jeff-schmidt-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/JUHGSuFSXPQ","publishDate":"2016-01-28T05:39:52.529Z","updateDate":"2017-01-03T04:22:06.775Z","text":"<b>Jeff Schmidt: Associate Editor of Physics Today for 19 Years / Physics PhD / Author of Disciplined Minds: A Critical Look at Salaried Professionals and the Soul-battering System That Shapes Their Lives / His firing led to North American Physics&#39; Largest Freedom-of-Expression Case in its History</b><br /><br />In May of 2000, UC Irvine graduate physicist, Jeff Schmidt, was fired from his job of 19 years as an editor at <i>Physics Today</i> for publishing a detailed critique of how physicists -- and more generally, all professionals -- are trained in university graduate programs.  The issues raised for all sciences were profound, but have never actually been engaged by the news media.<br /><br /><b><i>&quot;My thesis is that the criteria by which individuals are deemed qualified or unqualified to become professionals involve not just technical knowledge as is generally assumed, but also attitude -- in particular, attitude toward working within an assigned political and ideological framework.&quot;</i></b> (p16)<br /><br /><i>&quot;The qualifying attitude, I find, is an uncritical subordinate one, which allows professionals to take their ideological lead from their employers, and appropriately fine-tune the outlook that they bring to their work. </i><b><i>The resulting professional is an obedient thinker, an intellectual property whom employers can trust to experiment, theorize, innovate and create safely within the confines of an assigned ideology. The political and intellectual timidity of today&#39;s most highly educated employees is no accident.&quot;</i></b> (p16)<br /><br />Before it was over, Jeff would have a letter signed by more than 500 fellow physicists demanding that Jeff have his job back, and an IEEE committee agreed <i>&quot;it is possible that Jeff was fired for the politics expressed in the book. Many physicists believe this and have make [sic] their concerns known to AIP.&quot;</i><br /><br />(<a href=\"https://www.uow.edu.au/~bmartin/dissent/documents/Schmidt/investigation.htm\" class=\"ot-anchor\">https://www.uow.edu.au/~bmartin/dissent/documents/Schmidt/investigation.htm</a>)<br /><br />The contents of the book leave little doubt, really.<br /><br />When the public today discusses the meaning of scientific consensus, they never bring up Jeff Schmidt&#39;s name.  And that&#39;s a huge problem.  Jeff&#39;s case reveals the danger in the now-common practice of simply believing whatever the scientific consensus is -- and for what really should be a fairly straightforward reason:<br /><br /><i>&quot;The scientific ideologies, or &#39;paradigms,&#39; that scientists internalize during their training guide their thinking in every important area of their work, determining, for example, the particular abstractions or models they use, the procedures they consider valid and even their notion of what constitutes progress and understanding.  But how are the paradigms chosen?  As philosopher of science Thomas Kuhn observed, paradigms are incommensurable -- that is, there is no transcendent scientific framework in which one can compare paradigms and choose the best, and so such choices are made on the basis of values, or social factors.  Since no two paradigms solve the same problems, the choice between them involves deciding which problems it is most important to solve -- clearly a question of values.  In any historical era the values of those at the top of the social hierarchy dominate: as a result the paradigms that emerge from the scientific competition have a built-in tilt toward establishment priorities.  Through the paradigms, then, social forces direct scientific work even in the rare cases when employers or funding agencies do not.</i><br /><br /><b><i>Because they internalize both the paradigms and their employers&#39; priorities and values, scientists, at least in their own eyes, are completely nonpartisan in their work: They don&#39;t &#39;get political.&#39; They don&#39;t think about, let alone challenge, the ideology built into their techniques. Contrary to popular images of scientists as challengers of established beliefs (like Galileo or Einstein), the vast majority of scientists never seek to test their paradigms and do not participate in paradigm disputes. They don&#39;t waste their employers&#39; coin by getting caught up in efforts to overthrow existing worldviews or to establish new ones. Instead, they tend to treat the accepted models of reality as reality itself.</i></b><i>&quot;</i> (p82)<br /><br />What is particularly troubling is that even after a full year by now of showing Jeff&#39;s quotes to climate change activists online ...<br /><br /><b>this graphic:</b> <a href=\"https://plus.google.com/108466508041843226480/posts/UBHrj2f3zKs\" class=\"ot-anchor\">https://plus.google.com/108466508041843226480/posts/UBHrj2f3zKs</a><br /><br />... <b>I&#39;ve yet to meet a single climate change advocate who was already familiar with his claims.</b><br /><br />Not surprisingly, since Jeff&#39;s settlement with the AIP, both science journalists and the universities have basically pretended as though neither Jeff nor this book even exist.<br /><br /><b>MR:</b> <i>&quot;When you first thought of writing this book, you were in graduate school, right?&quot;</i><br /><br /><b>JS:</b> <i>&quot;Yes, that’s right. I got interested in the topic when I was going to professional training myself, getting a PhD in physics at the University of California, Irvine. </i><b><i>It seemed like the best of my fellow graduate students were either dropping out or being kicked out. And by ‘best,’ those were the most concerned about other people and seemed less self-centered, less narrowly-focused, most friendly people ... they seemed to be handicapped in the competition. They seemed to be at a disadvantage not only because their attention was divided, but because their concerns about big picture issues like justice and the social role of the profession and so on, caused them to stop and think and question, whereas their unquestioning gung-ho classmates just plowed right through with nothing to hold them back.</i></b><i> As I mentioned, there’s about a 50% drop-out rate for students entering University programs in all fields; and what I found was that this weeding out is not politically neutral. </i><b><i>To put it bluntly, the programs favor ass-kissers.</i></b><i> I don’t know if that’s an acceptable term on KFAI, but that’s the fact of the matter....&quot;</i><br /><br />(from <a href=\"http://www.julesnyquist.com/articles/article/1430100/16489.htm\" class=\"ot-anchor\">http://www.julesnyquist.com/articles/article/1430100/16489.htm</a>)<br /><br />The AIP&#39;s desire to limit the damage to AIP and the academic enterprise more broadly is indicated by the terms of the settlement they struck with him:<br /><br /><i>&quot;SCHMIDT’S CONCESSIONS TO AIP’S DEMANDS ...</i><br /><br /><i>Deletion of text from articles published by the American Physical Society and the Canadian Undergraduate Physics Journal</i><br /><br /><i>Schmidt has complied with AIP’s demand that he white out certain text specified by AIP in articles published by the American Physical Society and the Canadian Undergraduate Physics Journal and posted at </i><i><a href=\"http://disciplinedminds.com\" class=\"ot-anchor\">http://disciplinedminds.com</a></i><i>.</i><br /><br /><i>Other website deletions</i><br /><i>Schmidt has complied with AIP’s demand that he remove from the </i><i><a href=\"http://disciplinedminds.com\" class=\"ot-anchor\">http://disciplinedminds.com</a></i><i> website various documents specified by AIP. The deleted material includes news reports published by various newspapers and magazines.  However, the deleted documents are fully referenced in the settlement agreement, which is public. Schmidt would not agree to conceal the deletion list.</i><br /><br /><i>Non-disparagement</i><br /><i>Schmidt has agreed to AIP’s demand that he not say anything disparaging of AIP. (A disparaging statement may be true or false.)</i><br /><br /><i>Payment amount confidential</i><br /><i>Schmidt has agreed to AIP’s demand that he keep secret how much money AIP gave him.&quot;</i><br /><br />A couple of examples of the redacted documents ...<br /><br /><b>REDACTED:</b><br /><a href=\"http://disciplinedminds.tripod.com/biological-censored.pdf\" class=\"ot-anchor\">http://disciplinedminds.tripod.com/biological-censored.pdf</a><br /><b>ORIGINAL:</b><br /><a href=\"http://www.inference.phy.cam.ac.uk/sanjoy/schmidt/archive/biological-physicist.pdf\" class=\"ot-anchor\">http://www.inference.phy.cam.ac.uk/sanjoy/schmidt/archive/biological-physicist.pdf</a><br /><br /><b>REDACTED:</b><br /><a href=\"http://disciplinedminds.tripod.com/cupj-censored.pdf\" class=\"ot-anchor\">http://disciplinedminds.tripod.com/cupj-censored.pdf</a><br /><b>ORIGINAL:</b><br /><a href=\"http://www.inference.phy.cam.ac.uk/sanjoy/schmidt/archive/cupj.pdf\" class=\"ot-anchor\">http://www.inference.phy.cam.ac.uk/sanjoy/schmidt/archive/cupj.pdf</a><br /><br />For example, this was redacted in the second listed document:<br /><br /><i>&quot;To date, more than one thousand people in a wide variety of fields have protested his dismissal. Among the protestors are over 500 physicists, the largest number of physicists ever to speak out on a freedom-of-expression issue in North America.&quot;</i><br /><br />In sharp contrast to the vague, simplistic views that the public at large tends to adhere to on how academic research happens in practice, Jeff&#39;s analysis is both specific and surprising ...<br /><br /><b><i>&quot;Professionals generally avoid the risk inherent in real critical thinking and cannot properly be called critical thinkers.</i></b><i> They are simply ideologically disciplined thinkers. Real critical thinking means uncovering and questioning social, political and moral assumptions; applying and refining a personally developed worldview; and calling for action that advances a personally created agenda. An approach that backs away from any of these three components lacks the critical spirit ... </i><b><i>Ideologically disciplined thinkers, especially the more gung-ho ones, often give the appearance of being critical thinkers as they go around deftly applying the official ideology and confidently reporting their judgments. The fact that professionals are usually more well-informed than nonprofessionals contributes to the illusion that they are critical thinkers.&quot;</i></b> (p41)<br /><br /><i>&quot;At the end of the week the entire physics faculty gathers in a closed meeting to decide the fate of the students. Strange as it may seem, in most physics departments a student&#39;s score on the test is only one factor in the faculty&#39;s decision as to whether or not that student has passed the test.  Students are not usually told their scores: this gives faculty members the option of deciding that a student has failed the test even if that student has outscored someone they are going to pass. In arriving at their personal opinions on whether to pass or fail a student, individual faculty members consider anything and everything carried away from informal discussions with the student and with others around the department.</i><br /><br /><i>A faculty member who talks informally with a student in the hallway or at the weekly after-colloquium reception inevitably comes away with a feeling about whether or not that student &#39;thinks like a physicist.&#39; </i><b><i>The student&#39;s political outlook can easily make a difference in the faculty member&#39;s assessment. For example, in the usual informal discussion of an issue in the news, the student who rails against technical incompetence and confines his thoughts to the search for technical solutions within the given political framework builds a much more credible image as a professional physicist than does the student who emphasizes the need to alter the political framework as part of the solution. Indeed, the latter approach falls outside the work assignments given to professional physicists in industry and academe and so represents thinking unlike a physicist&#39;s.&quot;</i></b> (p134)<br /><br />And importantly, Jeff links academic practices to the source which ultimately motivates their design: the needs of industry and government.  It&#39;s an extraordinary big picture view of our society ...<br /><br /><i>&quot;No matter what the product is, employers divide the work into many parts and assign each employee to one type of activity. Narrowly focused individuals can work in a more machine-like way and get more work done per hour. Moreover, people who exercise fewer skills or simpler skills can be paid less. Hence, employers label the division of labor &#39;efficient.&#39; But it is efficient only if one ignores the social cost of organizing production in a way in which jobs tend to be monotonous and unsatisfying. </i><b><i>Such jobs, instead of allowing individuals to develop their mental and physical faculties by exercising them freely and fully (that is, instead of being fun), numb the mind and the body and retard the personal development of those employed to do them.</i></b><i> A system of production that works efficiently toward the goals of employers does not necessarily work efficiently toward the goals of employees or toward the goals of society in general.</i><br /><br /><i>More important to employers than the economic benefits, however, are the political benefits of the division of labor -- benefits that help management maintain its authority in the workplace. </i><b><i>Confined to a range of activity that is limited both horizontally and vertically, employees do not gain firsthand knowledge of the overall organization, strategy or goals of the institution that employs them. Those who work within this division of labor see the consequent ignorance in themselves and in their coworkers and feel a need to be directed by people who comprehend the whole operation. Management has the broadest view of what is going on, and this helps make its supreme authority in the workplace seem natural and justified.</i></b><br /><br /><b><i>By making employees easier to replace and by deflating their feeling of accomplishment in their work, the division of labor strips workers of their sense of power in the workplace, discouraging them from challenging management on the way the work is organized. And the division of work into narrow tasks (most of which are the same even when the product is different) denies workers a feeling for what they are producing, thereby discouraging them from challenging management on the nature or design of the product or service. Hence the division of labor, by making self-management seem impossible and by strengthening management&#39;s control over the workforce and over the content of the work, helps make the hierarchical system of production more secure.&quot;</i></b> (p91-92)<br /><br />But, what does this have to do with academic research?  Jeff&#39;s thesis is that indoctrination within the graduate programs in a sort of hidden curriculum creates stability for any large managed endeavor.  In other words, the graduate programs&#39; primary purpose, in his view, is to act as a filter for weeding out future troublemakers.  Given that Jeff made it through this same system, it&#39;s apparently a &quot;leaky&quot; filter ...<br /><br /><i>&quot;Beginning physics graduate students must devote an entire year or two of their lives to homework. Indeed, the first part of physics graduate school is well described as a boot camp based on homework. </i><b><i>One characteristic of any boot camp is that the subject matter the instructors present in their day-to-day work is not really the main thing they are teaching.</i></b><i> Teaching the subject matter is certainly one goal, but it is not the main one.  In military boot camp, for example, drill instructors make recruits spend large amounts of time learning to dress to regulation, march in precise formation, chant ditties, disassemble and reassemble rifles, carry heavy backpacks, and so on, yet the main goal of all this is something much more profound: to create soldiers who will follow orders, even to their deaths. Similarly, the most apparent goal of graduate physics courses is to indoctrinate the students into the dominant paradigms, or theoretical frameworks, of physics. </i><b><i>But the primary goal is to train physicists who will maintain tremendous discipline on assigned problems.</i></b><i>&quot;</i> (p129)<br /><br />Key to his assertion are the specific claims that Jeff makes about the nature of the problem sets in the physics PhD&#39;s qualifying examinations.  <br /><br /><b><i>&quot;Questions on qualifying examinations often penalize the creative student while rewarding the student who mindlessly applies memorized &#39;tricks.&#39;</i></b><i> To solve a typical math or science problem, you manipulate symbols in a sequence of steps, each of which takes you closer to the solution in a way that is apparent. However, it is possible to contrive problems whose solutions require a step that appears to be arbitrary or even counterproductive, the logic of that step becoming clear only later in the problem. This crucial step is the trick. The student who doesn’t know in advance the trick required to solve a test problem is extremely unlikely to discover it while working on the problem, especially if no time is allotted for that purpose, and so </i><b><i>problems based on tricks favor the memorizer over even the creative individual who has a good overall understanding of the subject.</i></b><br /><br /><i>[... Jeff goes into detail on a specific example ...]</i><br /><br /><i>In either case the tricks are optional — you can do the multiplication the long way and the cube root by trial and error. However, with a slight change — say, a stringent time limit on the problems — the tricks become mandatory. </i><b><i>Under this condition the questions would still look like tests of arithmetic skills, but they would really be disguised tests of memory and of the ability to &#39;psych out&#39; the questions.</i></b><br /><br /><b><i>One often finds that a qualifying examination problem is impossible to solve without a special trick that is good for only that one problem. This kind of problem is usually constructed by starting with the trick and working backwards. The student who attacks such a problem with creativity, with an understanding of the subject, with insight and with the standard tricks of the trade gets absolutely nowhere. The special trick is the only approach to the problem that works. The unwitting student who uses the &#39;wrong&#39; approach, as logical as that approach may be and as effective as that approach may be in general, sinks deeper and deeper into increasingly complicated calculations — a frustrating &#39;mess,&#39; as it is often called — in an ultimately futile effort to get an answer. Because only the special trick works, only the student who has seen the problem before — and memorized it — can solve it. And because the student’s creativity; understanding, insight and experience do not lead to the trick, they contribute nothing to the student’s score.&quot;</i></b><br /><br />This particular aspect of Jeff&#39;s critique is not commonly emphasized in the public discussions about his work, and yet it&#39;s not possible to fully appreciate what he&#39;s arguing without understanding it.<br /><br />This big picture view that Jeff provides of how professional work firts into our modern society makes Jeff&#39;s work a crucial resource for academic reformers because it implies the existence of a paradox at the heart of &quot;Big Science&quot; which reformers must take into account ...<br /><br />We need professional training in order ask the biggest questions in science, because those problems tend to require large teams of scientists all working towards the same specific objective.  But, this notion of &quot;Big Science&quot; is itself a paradox, because to have that ability to ask those types of questions, we have to create teams of scientists which have some <b>common core</b> understanding.  The types of questions asked by this team are necessarily constrained as a result of this consensus.  On the other hand, if we tried to create a team entirely of maverick thinkers, who are free to disagree on <b>any</b> of the fundamentals, then such teams would be at a greater risk of failing to produce any result at all.<br /><br />Thus, although the reaction of those encountering Jeff&#39;s claims for the first time might be a sense of anger or disillusionment with science itself, the problem seems to me more like a philosophical trade-off, and a very deep insight into what is apparently a limitation of the scientific enterprise.<br /><br />And this is why I agree with Dr. Gerald Pollack&#39;s plans for his Institute for Venture Science -- because simplistic solutions like scrapping the existing peer review system would only create a different kind of problem; the correct response to what is happening today with challenges to entrenched theories should be more like a patch for a software bug:<br /><br />We should leave Big Science in place, and seek to create a protected path for challenges to textbook theories to elaborate into fully-formed theories.  And then, with that bandwagon science loophole closed, we will be happy that we have preserved the larger system.  The fact of the matter is that these two systems will complement one another.  Big Science will very rapidly elaborate the new hypotheses generated by the IVS.<br /><br />For a quick review of how the IVS wants to reform the way that academia handles challenges to textbook theory, see this graphic:<br /><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/HkUnPLpvq6U\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/HkUnPLpvq6U</a><br /><br />For a more detailed look, see their website:<br /><br /><a href=\"http://www.theinstituteforventurescience.net/\" class=\"ot-anchor\">http://www.theinstituteforventurescience.net/</a><br /><br />For more information on Jeff Schmidt, see his website:<br /><br /><a href=\"http://disciplinedminds.tripod.com/\" class=\"ot-anchor\">http://disciplinedminds.tripod.com/</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23person\">#person</a>   ﻿","category":"person"},{"name":"Martín López Corredoira","summary":"Cosmologist / Astrophysicist / Philosopher / Published 50 Academic Papers, Often as Lead / Academic Whistleblower","image":"https://lh3.googleusercontent.com/-vy6PH5z0SzE/VqhDwah-qwI/AAAAAAAAGdw/2cPuvCisGgM/w1800-h2520/martin-lopez-corredoira-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/gVUCTWLWHin","publishDate":"2016-01-27T04:14:51.155Z","updateDate":"2017-01-03T03:00:37.872Z","text":"<b>Martín López Corredoira: Cosmologist / Astrophysicist / Philosopher / Published 50 Academic Papers, Often as Lead / Academic Whistleblower</b><br /><br />This snapshot of Corredoira is from a scene in the Big Bang critique, <i>Universe: The Cosmology Quest</i>, available for viewing at ...<br /><br /><a href=\"https://www.youtube.com/watch?v=IFFl9S39CTM\" class=\"ot-anchor\">https://www.youtube.com/watch?v=IFFl9S39CTM</a><br /><br />... where he provides his nuanced position on Big Bang cosmology.  See the 44:41 timestamp on the first part online (<a href=\"https://youtu.be/IFFl9S39CTM?t=2681\" class=\"ot-anchor\">https://youtu.be/IFFl9S39CTM?t=2681</a>) ...<br /><br /><i>&quot;Cosmology is ... is not a science.  It ... has a lot of scientific aspects.  We can know many things with the science.  We can know how the galaxies are distributed; we can measure them with observations; we can know how many matters are in the interstellar medium or in the galaxies (?), and all of these aspects are scientific.</i><br /><br /><i>But, with regards and considerations about the beginning of the universe, this is in some way crossing the boundary of the science, and going to something between the science and metaphysics aspects, in my opinion.&quot;</i><br /><br />Randall Meyers&#39; documentary is a groundbreaking critical analysis of modern cosmology in that it provides a critique of the Big Bang which can be understood without a degree in astrophysics.<br /><br />From <a href=\"https://youtu.be/IFFl9S39CTM?t=2847\" class=\"ot-anchor\">https://youtu.be/IFFl9S39CTM?t=2847</a> ...<br /><br /><i>&quot;Cosmology as a science has begun one century ago with Einstein&#39;s theory. So, in one hundred years, you cannot produce a theory of everything. This is crazy.  Even from a philosophical point of view, a historical point of view, we have begun a hundred years ago ... In 1920, we thought that the Milky Way was all the universe. And now, you want to produce the belief that in 80 years or something, you have produced the theory of all the universe, from the beginning to now.  This is not ... credible.&quot;</i><br /><br />Corredoira has attempted to clarify in his text (<i>The Twilight of the Scientific Age</i>) that questioning the Big Bang does not dictate that a person also must be an advocate of an alternative scientific framework, a crucially important point ...<br /><br /><i>&quot;Although most of my papers are quite conventional for a professional astrophysicist, there are many of them (maybe 10-20% of them) with a substantial degree of challenge to established ideas. This brought me some fame among my colleagues for holding unorthodox views. Indeed, most of my works are quite orthodox. And indeed, although I have expressed several times my sceptisicm about some orthodox ideas, in particular about the Big Bang hypothesis in cosmology, I am not an anti-Big Bang cosmologist, as many people have thought. I do not defend any alternative theory. Scepticism, doubting a dogma, does not mean a new dogma should be defended to substitute for the first one ...&quot;</i><br /><br />Although Meyers&#39; documentary features an all-star cast of scientific critics, Dr. Corredoira has become one of the most important whistleblowers featured in the documentary. <b>What he does is deconstruct the simplistic narratives that currently dominate modern science journalism.</b>  And the fact that he is a remarkably successful academic insider himself, having published a large number of conventional astrophysics and cosmology papers, makes his critique all the more difficult to dismiss.  Dr. Corredoira has published papers on subjects like the structure of the Milky Way and stellar populations, as well as observational astronomy topics which required analytical calculations, computer simulations, statistics, photometrical and spectroscopical observations and analysis.<br /><br />In his critique of academic research, <i>The Twilight of the Scientific Age</i>, he writes ...<br /><br /><i>&quot;A superficial view may lead us to think that we live in the golden age of science but the fact is that the present-day results of science are mostly mean, unimportant, or just technical applications of ideas conceived in the past.&quot;</i><br /><br /><i>&quot;There are several reasons to write about this topic. First of all, because I feel that things are not as they seem, and the apparent success of scientific research in our societies, announced with a lot of ballyhoo by the mass media, does not reflect the real state of things.&quot;</i><br /><br />The role of simplistic narratives about scientific research in guiding an uncritical public reaction to scientific consensus cannot be over-stated.  If Martín López Corredoira is to be believed, then the public&#39;s perception of how research gets done today is wildly out-of-step with what is today happening.<br /><br /><i>&quot;the more controversial the topic, and the more of a challenge it is to established ideas, and the newer the approach, then the more difficult will be the problems in publishing it, and the higher the probability of its being rejected. Gillies (2008, ch. 2) argues that when a researcher makes an advance which is later seen as a key innovation and a major breakthrough, a peer review may very well judge it to be absurd and of no values. As noted by Van Flandern (1993, ch. 21), peer review in journals interferes with the objective examination of extraordinary ideas on their merits. Maddox (1993), who was editor of the journal Nature, has said that if Newton submitted his theory of gravity to a journal today, it would almost certainly be rejected as being too preposterous to believe. </i><b><i>On the one hand, there is a failure to select novel ideas (Brezis, 2007; Horrobin, 1990). On the other hand, the refereeing process trends to conformity.&quot;</i></b><br /><br /><i>&quot;From my own experiences and those of others, I have observed that doors are opened and offers made to those who are servile and uncritical. A lot of work must be produced, but without any great aspiration towards saying something important. </i><b><i>To obtain an academic position, to obtain tenure, to be successful in obtaining research funds, etc. it is necessary to conform.&quot;</i></b><br /><br /><i>&quot;In the last two decades, I have observed, at least in my speciality, how the number of offers of postdoctoral positions with a free choice of research topic has been much reduced, substituted by positions working on a major project under the orders of senior &#39;priests&#39;. This is, in my opinion, a huge obstacle for the creativity of young scientists, an unfortunate trend in the present-day bureacratized system.  Moreover, among people who are going to develop a topic freely, there is also a strong bias against all the applications which propose topics which are not suitable according to the mainstream of normal science. </i><b><i>Certainly, this is the perfect way to uphold the power of tradition and to castrate new ideas. No revolution is possible within this system; only an outsider can do it.&quot;</i></b><br /><br /><i>&quot;Scientists are educated nowadays in a habit of self-censorship. The system promotes self-repression in the spread of ideas, </i><b><i>so most scientists, when writing a paper, think something like &#39;I think this and that, but I cannot say so in my paper because this will not pass the referee&#39;s control, so I will not say it&#39;.</i></b><i> This causes serious harm to creativity among people who dare to think new things.&quot;</i><br /><br /><i>&quot;... scientists have to choose between developing their own ideas freely or being constrained by subjects which allow academic success.&quot;</i><br /><br /><i>&quot;The situation is that society is drowned in ideas and information without assimilating any of it, and only a few ideas, those selected by the establishment, will make some impact. We are in the era of mass media and propaganda. Only science which is given publicity in newspapers, TV, etc. will have any resonance in our society. But in order to control the mass media one needs money and social status. Only administrators and science politicians, who worry more about getting funds and prestige than about solving scientific problems, will be able to pass through this filter. </i><b><i>People who dedicate great effort and time to thinking about new ideas in science are not paid enough attention.</i></b><i> It is not the time of individual scientists. It is the time of big corporations, of megaprojects which know how to make maximum profit from state funds.&quot;</i><br /><br /><b><i>&quot;Creativity is blocked. It seems that the system gives the message that no ideas are needed. It seems the system, through its higher authorities, is saying that science only needs to work out the details. It is accepted that the basis of what is now known is correct, that present-day theories are more or less correct and only manpower is needed to sort out some parameters of minor importance. A Copernican revolution is totally unthinkable within the current system.&quot;</i></b><br /><br /><i>&quot;Science is not a direct means for reaching the truth. Science works with hypotheses rather than with truths. </i><b><i>This fact, although recognized, is usually forgotten. It gives rise to the creation of certain key groups within science which think that their hypotheses are indubitably solid truths, and think that the hypotheses of other minority groups are just extravagant or crackpot ideas ...</i></b><br /><br /><b><i>all through history, and even now, there have been many instances of discussion about how to interpret aspects of nature, with various possible options without a clear answer, in which a group of scientists have opted to claim their position is the good or orthodox one while other positions are heresies.&quot;</i></b><br /><br />If Martín López Corredoira is right, our System 1 narratives on science are wrong, and we cannot defer to them.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23person\">#person</a>  ﻿","category":"person"},{"name":"APODNereid","summary":"Anonymous Online Critic of Electrical Cosmologies","image":"https://lh3.googleusercontent.com/-UkEFNRF9vm0/Vqb_8GR1NGI/AAAAAAAAGcM/Tnm5kEicMFQ/apodnereid-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/gdiuDLxhM3n","publishDate":"2016-01-26T05:12:59.779Z","updateDate":"2017-01-03T04:18:40.803Z","text":"<b>APODNereid: Anonymous Online Critic of Electrical Cosmologies</b><br /><br />One of the most perplexing riddles in the history of modern scientific theories -- and a situation that cannot forever hold -- is the almost complete lack of awareness and interest amongst Silicon Valley&#39;s high-tech workers in electrical cosmology.  Future science historians will look at the situation with considerable confusion.  And let&#39;s be clear that the situation is no less surprising when viewed in real-time and from <b>within</b> these social groups.  <br /><br />There is not even a shadow of awareness today amongst the large majority of computer hardware and software engineers that there even exists a debate over the role of electromagnetism in cosmology and astrophysics.  There is no understanding in the high-tech community, to date, of what a plasma is, nor even that it is today widely recognized to be the universe&#39;s dominant state for matter.  And given those failures to understand, there is not even a remote chance that anybody within the high-tech community is using knowledge of this controversy to act upon any of the potential implications or applications.<br /><br />When I first learned of the existence of electrical cosmology, I was only able to afford the time to do so because I was between jobs.  Years earlier, when I was a computer hardware engineer working at a major semiconductor corporation, there was no time or apparent reason to pursue such lofty topics.  Years before that, as a consequence of attending Carnegie Mellon for a bachelor&#39;s in computer engineering, I was automatically inducted into IEEE.  But, there was no awareness at all at CMU -- and I assume all other universities -- that IEEE had by that point been involved in a longstanding dispute over how to model the universe&#39;s most common state for matter.<br /><br />Once I had a basic grasp of the claims and controversy, the riddle of the tech community&#39;s lack of awareness consumed me.  And my reaction involved a set of four complementary actions to seek out the cause:<br /><br />(1) I established direct contact with the Thunderbolts Group, which permitted me an opportunity to more directly learn the specifics of their worldview, and when necessary ask for clarification.<br /><br />(2) I immediately begin reaching out to laypeople in online forums like Digg.com and Slashdot.org (a popular tech forum), in an attempt to understand how ordinary people engage complex scientific claims.<br /><br />(3) Third, I began running claims directly against the Electric Universe&#39;s most notorious critics.<br /><br />(4) And as a reaction to those interactions, I began to pay attention to any press release or text which seemed relevant to psychology, sociology or science education.<br /><br />With this interaction-focused approach, I now understand that it was probably  inevitable that I would be discovered by APODNereid.<br /><br />Nereid&#39;s identity remains unknown to this day, but I will adopt the convention here of referring to them as a woman.  Asked who Nereid is, those who know of her will give a variety of answers.  I&#39;m going to provide my own vague hypothesis ...<br /><br /><b>The Peer Reviewer Hypothesis</b><br /><br />It seems likely from her knowledge of the challenged academic subjects and her prolific online activity that she&#39;s either a researcher or a peer reviewer who is defending her own established line of investigation.  <br /><br />According to an expert in venture science, Donald W Braben (in an article titled &quot;The Planck Club: Why Scientific Mavericks Are Essential to Prosperity&quot; at <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=8F1E4088CAF25454A7454CADD92C6E32?doi=10.1.1.686.8216&amp;rep=rep1&amp;type=pdf\" class=\"ot-anchor\">http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=8F1E4088CAF25454A7454CADD92C6E32?doi=10.1.1.686.8216&amp;rep=rep1&amp;type=pdf</a>), the practice is today commonplace in the peer review system ...<br /><br /><i>&quot;Today, [peer reviewers] are usually allowed to express their opinions anonymously, which means that they can say what they think about proposals without fear of the consequences, a laudable aim. However, scientists are also people, and when asked to comment on a close rival’s (or would-be rival’s) bid for the funds they might also be bidding for, it should be expected that some might be unable to resist the chance to put the boot in if they can get away with it. </i><b><i>In fact, these well intentioned but misguided policies are having disastrous consequences and are, in effect, unprecedented, global-scale gambles with future prosperity.&quot;</i></b><br /><br />It should not be a stretch to suggest that a practice which has already established itself as an effective defense in the peer review system would have also by now spread to our Internet communication networks.  There should be nothing at all outrageous about the idea that academic researchers who feel threatened by some outsider ideas today consider participation on the forums to be an important aspect of &quot;nipping an idea in the bud&quot; before it can evolve into a more serious threat to their funding.  They surely realize themselves that there is nobody who can stop them.<br /><br />Another critic of these same ideas, Phil Plait, has already said as much ...<br /><br /><i>&quot;Pseudoscience is like a virus. At low levels, it&#39;s no big deal, but when it reaches a certain threshold it becomes sickening.&quot;</i><br /><br />(<a href=\"http://www.azquotes.com/author/47073-Phil_Plait\" class=\"ot-anchor\">http://www.azquotes.com/author/47073-Phil_Plait</a>)<br /><br />Even if my Nereid hypothesis is slightly off in some way, it is the most accurate context that I can suggest for people to consider when observing APODNereid&#39;s online comments: <b>Her point is consistently to provide reasons for why people should not bother investigating the possibility that electricity might play a larger role in the universe&#39;s dynamics</b> than is today acknowledged by astrophysical experts.  She&#39;s extremely effective at what she does.<br /><br />However, it&#39;s not actually the only hypothesis worth mentioning; there is yet another, and it raises an enormously complex set of issues ...<br /><br /><b>The Dual-Purpose Research Hypothesis</b><br /><br />Regardless of a person&#39;s views on electrical cosmology, the fact of the matter is that plasma physics is also involved in the study of nuclear explosions as well as a wide variety of military technologies.  So, there is the unfortunate situation that the need to understand our surroundings and human history might clash with our needs for defense or mitigation of nuclear proliferation.<br /><br /><i>&quot;The way research is described, funded and carried out conceals its social origins.  The titles that scientists give their research projects (and publications) usually make their work look very abstract and esoteric.  These titles, and the technical descriptions that accompany them, make no mention of underlying technological goals ... A government funding agency&#39;s description of its particular interest in a research project -- its reasons for funding it -- is usually not made public.&quot;</i> (Jeff Schmidt, <i>Disciplined Minds: A Critical Look at Salaried Professionals and the Soul-battering System That Shapes Their Lives</i>, p72-73)<br /><br />Plasma physics could simply be more important to a modern military program than we currently realize, and if so, then that would provide yet another motivation for attempts to muddy the public&#39;s understanding of plasmas.<br /><br />To be clear, the idea that there might be people online intentionally creating distractions on the issue of plasmas online would not be the most unusual aspect of the cosmic plasma debate.  It&#39;s sufficiently expected that there&#39;s no need at all to invoke a conspiracy. <b>By far, the most unusual aspect of the cosmic plasma debate is that the public is still not clear what a plasma is, how they tend to behave in the laboratory, or even that plasma is the universe&#39;s dominant state of matter</b>.<br /><br />Nereid so far has three confirmed identities:<br /><br /><b>APODNereid:</b> <i>&quot;Thornhill&#39;s major opus is an academic fraud&quot;</i><br /><br />(<a href=\"http://dealingwithcreationisminastronomy.blogspot.com/2010/07/electric-universe-everything-i-needed.html\" class=\"ot-anchor\">http://dealingwithcreationisminastronomy.blogspot.com/2010/07/electric-universe-everything-i-needed.html</a>)<br /><br /><b>Nereid:</b> <i>&quot;Maybe, Tom, you could write some words on Thornhill&#39;s own scientific clangers?....And Thornhill claims to have a degree in physics!&quot;</i><br /><br />(<a href=\"http://dealingwithcreationisminastronomy.blogspot.com/2009/07/more-comments-on-electric-sky-short.html\" class=\"ot-anchor\">http://dealingwithcreationisminastronomy.blogspot.com/2009/07/more-comments-on-electric-sky-short.html</a>)<br /><br /><b>DeiRenDopa:</b> <i>&quot;Thornhill, and as he is a well-known academic fraud&quot;</i><br /><br />(<a href=\"http://www.internationalskeptics.com/forums/showpost.php?p=6525214&amp;postcount=604\" class=\"ot-anchor\">http://www.internationalskeptics.com/forums/showpost.php?p=6525214&amp;postcount=604</a>)<br /><br />Let&#39;s run through an actual example at <a href=\"http://science.slashdot.org/comments.pl?sid=2967801&amp;cid=40606287\" class=\"ot-anchor\">http://science.slashdot.org/comments.pl?sid=2967801&amp;cid=40606287</a> ...<br /><br /><b>EXPLORER:</b> <i>&quot;[The electrical theorists are] overall arguing that electrodynamics can better explain many astronomical observations than gravitation + dark matter, dark energy and modifications to cosmological constants.</i><br /><br /><i>That seems reasonable; correct or not is a matter to be determined.</i><br /><br /><i>Dark matter, dark energy, etc. are the first examples I give to friends who are skeptical of big bang cosmology or even science in general, showing it as an example of how science is full of bad &#39;working&#39; theories, but we know it and keep chipping away. I hope I&#39;m not wrong.&quot;</i><br /><br />This critical thinker makes a very powerful argument here that in its current state, modern cosmology sets the bar quite low for competing ideas.<br /><br /><b>NEREID:</b> <i>&quot;LOL. We must be reading different websites; I found nothing at all to suggest that anyone has developed any models which show &#39;that electrodynamics can explain any astronomical observations&#39;!</i><br /><br /><i>At least none that aren&#39;t already well-established parts of mainstream astrophysics. Care to share, freality?&quot;</i><br /><br />Do you see the subtle shift?  The initial comment was about the propensity for electrodynamics to offer a better explanation for astronomical observations.  But, Nereid immediately switched the conversation to one centered on the current state of the <b>mathematical models</b>.<br /><br />Let&#39;s be clear: The astrophysical discipline has a very strong tradition of mathematics to it which is difficult to outdo.  But, the argument from the start of the debate over how to model cosmic plasmas has been that plasmas are inherently tricky to model, and that attempts by astrophysicists to this day have failed ...<br /><br /><i>&quot;Alfven came to recognize that real plasma behavior is too &#39;complicated and awkward&#39; for the tastes of mathematicians.  It is a subject &#39;not at all suited for mathematically elegant theories.&#39;  It requires hands-on attention to plasma dynamics in the laboraotry.  Sadly, he said, the plasma universe becames &#39;the playground of theoreticians who have never seen a plasma in the laboratory.  Many of them still believe in formulae which we know from laboratory experiments to be wrong.</i><br /><br /><i>Again and again Alfven reiterated the point: the underlying assumptions of cosmologists today &#39;are developed with the most sophisticated mathematical methods and it is only the plasma itself which does not &#39;understand&#39; how beautiful the theories are and absolutely refuses to obey them.&#39;&quot;</i><br /><br />(See more of that story at <a href=\"https://plus.google.com/108466508041843226480/posts/YoTuHGL16ur\" class=\"ot-anchor\">https://plus.google.com/108466508041843226480/posts/YoTuHGL16ur</a>)<br /><br />Rens van der Sluijs provides a useful review of the mathematization of astrophysics which I&#39;ve excerpted here:<br /><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/96KV1RwZNnX\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/96KV1RwZNnX</a><br /><br />Nereid&#39;s rebuttal might be sufficient to convince this explorer to stop looking further, but it completely fails to address the larger debate that the electrical theorists have been seeking to cultivate.<br /><br /><b>EXPLORER:</b> <i>&quot;The one I looked at most closely was in the intro chapter to one of the books they linked:</i><br /><br /><i><a href=\"http://www.thunderbolts.info/EU%20Intro%20and%20Chap1.pdf\" class=\"ot-anchor\">http://www.thunderbolts.info/EU%20Intro%20and%20Chap1.pdf</a></i><i> [</i><i><a href=\"http://thunderbolts.info\" class=\"ot-anchor\">thunderbolts.info</a></i><i>]</i><br /><br /><i>That the shapes and spins of galaxies can be shown in simulation by collapsing parallel electric filaments (&quot;pinch&quot; effect), p. 26.. In contrast, from what I understand, you have to introduce a majority of dark matter &amp; energy into such a simulation to get a stable galaxy if the stars interact otherwise with only gravity.</i><br /><br /><i>[...]</i><br /><br /><b>NEREID:</b> <i>&quot;No, your source is lying.&quot;</i><br /><br /><i>&quot;In 1986, Peratt published two Plasma Cosmology (NOT Electric Universe) papers, reporting the results of some simulations (similar to, but not quite the same as, what you wrote).&quot;</i><br /><br />Nereid commonly makes the distinction, but there is no reason at all why Peratt&#39;s findings cannot apply to the Electric Universe.<br /><br /><b>NEREID:</b> <i>&quot;However, these were not simulations of real galaxies. Why not? Because real galaxies contain stars (duh!), whose motions (&quot;spins&quot;, to use your term) cannot possibly be represented in Peratt&#39;s simulations (stars have charge-to-mass ratios vastly different from the ions Peratt assumed, in his simulations).&quot;</i><br /><br />Nereid makes no mention to the explorer that plasma behavior has been observed to scale over enormous magnitudes ...<br /><br /><a href=\"http://www.plasma-universe.com/Plasma_scaling\" class=\"ot-anchor\">http://www.plasma-universe.com/Plasma_scaling</a><br /><br />Such observations support the explorer&#39;s contention that electric filaments may indeed be used to explain galactic rotation curves.  The point being made by this thinker and the electrical theorists is not so much that Peratt&#39;s simulation is precisely what is happening in galaxies -- but rather that electromagnetic forces can better explain galactic rotation curves than gravity.  Yet, Nereid has once again successfully distracted from that legitimate point.<br /><br /><b>NEREID:</b> <i>&quot;Worse, real galaxies do not have double nuclei with the same shape as those in Peratt&#39;s simulations. And so on.&quot;</i><br /><br />You can see the double nuclei she&#39;s referring to here ...<br /><br /><a href=\"http://www.plasma-universe.com/Galaxy_formation\" class=\"ot-anchor\">http://www.plasma-universe.com/Galaxy_formation</a><br /><br />Observe how Nereid notes that distinction, but is silent on the larger dark matter problem.  Which is truthfully a more serious difficulty?<br /><br /><b>EXPLORER:</b> <i>&quot;Hm, I think you&#39;re missing my point... I wasn&#39;t supporting their claims as more correct than a gravitationally-based cosmology, just noting that they seemed to be making reasonable conjectures, albeit non-mainstream, and that they didn&#39;t deserve to be called names. I said:</i><br /><br /><i>&#39;That seems reasonable; correct or not is a matter to be determined.&#39;</i><br /><br /><i>The same can be said about dark energy/matter. Reasonable, but correctness TBD. It is problematic for a simulation to not model all know behaviors of a system, </i><b><i>but nonetheless, we do it all the time and often find useful models in them.&quot;</i></b><br /><br /><i>[...]</i><br /><br />Isn&#39;t it clear that this person is sufficiently fluent in modeling to have a strong grasp on the subject?  What is really the purpose of Nereid&#39;s corrections?  This person is not confused.<br /><br /><b>NEREID:</b> <i>&quot;And I think you&#39;re missing mine.</i><br /><br /><i>You seemed pretty clear, when you claimed &#39;They&#39;re overall arguing that electrodynamics can better explain many astronomical observations than gravitation + dark matter, dark energy and modifications to cosmological constants&#39;, where &#39;they&#39; refers to what&#39;s found on a particular website (actually, PDF), that you linked to. My point: there&#39;s no substance to any such claims (other than those which merely repeat what you can easily find from any mainstream source).</i><br /><br /><i>At least, none which has any quantitative, internally consistent legs to stand on.&quot;</i><br /><br />The subtlety is easy to miss, but Nereid is effectively arguing that we should not seek to create better electrical cosmology models <b>because they do not already exist.</b><br /><br /><b>NEREID:</b> <i>&quot;&#39;Agreed, this is better termed Plasma Cosmology, not Electric Universe [...] I was reading more of the folks they cited than the .info website.&#39;</i><br /><br /><i>Then why not say so? Why give airtime to folk who, not to put too fine a point on it, are deliberately lying? Who might, therefore, be reasonably called crackpots?</i><br /><br /><i>&#39;That seems reasonable; correct or not is a matter to be determined.&#39;</i><br /><br /><i>Yes, it (the PC stuff) may indeed seem reasonable. However, it has long since been determined that it cannot possibly be correct.&quot;</i><br /><br />Once again, the same could be said about the dark matter problem, which shows no real sign of resolution.  In fact, the problem is a bit worse than most people realize, as gravity is just far too weak to explain how two typically distant stars can affect one another through gravity at all.  See the straightforward calculation here:<br /><br /><a href=\"https://plus.google.com/108466508041843226480/posts/D821sJn9hEF\" class=\"ot-anchor\">https://plus.google.com/108466508041843226480/posts/D821sJn9hEF</a><br /><br /><b>NEREID:</b> <i>&quot;Or, more precisely, every single one of the principals involved in those efforts is either dead or has stopped doing research into showing that the reasonable-sounding ideas actually match the relevant, cold, hard, objective, quantitative astronomical data.&quot;</i><br /><br />Yes, the problem is that there are not enough people looking into these ideas, and we need to fix that.  But, the problem has nothing to do with the legitimacy of the idea itself; the problem has been discussed by more than one academic whistleblower.<br /><br />Martín López Corredoira writes in <i>The Twilight of the Scientific Age</i>:<br /><br /><i>&quot;The situation is that society is drowned in ideas and information without assimilating any of it, and only a few ideas, those selected by the establishment, will make some impact. We are in the era of mass media and propaganda. Only science which is given publicity in newspapers, TV, etc. will have any resonance in our society. But in order to control the mass media one needs money and social status. Only administrators and science politicians, who worry more about getting funds and prestige than about solving scientific problems, will be able to pass through this filter. </i><b><i>People who dedicate great effort and time to thinking about new ideas in science are not paid enough attention.</i></b><i> It is not the time of individual scientists. It is the time of big corporations, of megaprojects which know how to make maximum profit from state funds.&quot;</i><br /><br />Martín López Corredoira is an astrophysicist, philosopher and academic whistleblower, and has published <b>more than 50 cosmology and astrophysical papers</b> on subjects like the structure of the Milky Way, stellar populations, and observational astronomy topics which required analytical calculations, computer simulations, statistics, photometrical and spectroscopical observations and analysis.<br /><br />Is it not at least a little bit ironic that Nereid is chastizing these theorists for their lack of adherents <b>even as she is convincing this person that it&#39;s not worth learning further about?</b><br /><br /><b>NEREID:</b> <i>&quot;For example, Peratt has published nothing new, on this topic, since the late 1980s (yes, he&#39;s published stuff, but none of it involves new research), and Lerner&#39;s last work on this seems to be sometime in the 1990s.&quot;</i><br /><br />The simple explanation is that Peratt and Lerner have simply moved on to other subjects.  There is no lesson to be learned here about the validity of the simulation from their new focus.<br /><br /><b>NEREID:</b> <i>&quot;In the meantime, the quantity and quality of the directly relevant astronomical data has grown enormously, from the WMAP results to the thousands of hours of Hubble observations to the SDSS to data from Spitzer, Herschel, Chandra, Fermi, WISE, ...&quot;</i><br /><br />What astrophysicists consistently fail to mention is that a <i>&quot;relativistic electron beam that does not produce microwave radiation is unknown.&quot;</i>  Anthony Peratt points this out on page 34 of his 2nd Edition of <i>Physics of the Plasma Universe</i>.<br /><br />(See <a href=\"https://books.google.com/books?id=wt2CBAAAQBAJ&amp;pg=PA34&amp;lpg=PA34&amp;dq=relativistic+electron+beam+that+does+not+produce+microwave+radiation+is+unknown&amp;source=bl&amp;ots=QKc-ThrxmZ&amp;sig=B4wv5jjx-YZmRNLAEdnjxcJ9JkA&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjCjpf9urHKAhVE8WMKHQmnCxwQ6AEIHDAA#v=onepage&amp;q=relativistic%20electron%20beam%20that%20does%20not%20produce%20microwave%20radiation%20is%20unknown&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=wt2CBAAAQBAJ&amp;pg=PA34&amp;lpg=PA34&amp;dq=relativistic+electron+beam+that+does+not+produce+microwave+radiation+is+unknown&amp;source=bl&amp;ots=QKc-ThrxmZ&amp;sig=B4wv5jjx-YZmRNLAEdnjxcJ9JkA&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjCjpf9urHKAhVE8WMKHQmnCxwQ6AEIHDAA#v=onepage&amp;q=relativistic%20electron%20beam%20that%20does%20not%20produce%20microwave%20radiation%20is%20unknown&amp;f=false</a>)<br /><br />A crucial implication of this observational fact is that there is a potential answer to microwaves coming at us from all directions -- the CMB that Nereid refers to -- in the observed emissions of laboratory plasmas.  In other words, it is possible that the cosmic microwave background is not a background at all -- but rather a foreground electromagnetic fog generated as a consequence of interstellar electric currents. Peratt has published a number of papers in pursuit of this line of investigation with Gerrit Verschuur.  Nereid&#39;s vague reference to the WMAP results skips over this enormously important point, as do all other Big Bang experts.<br /><br /><b>NEREID:</b> <i>&quot;Don&#39;t you find it odd that, given the tens of thousands of hours EU and PC proponents have spent promoting their ideas on websites across the internet, and given the hundreds (thousands?) of ardent fans, not a single one has done anything to develop these ideas, on a scientifically-sound basis?&quot;</i><br /><br />No, Nereid, once a person immerses themselves into the claims of modern academic whistleblowers, it is quite clear why proponents of an important challenge to textbook theory are seeking to raise public awareness ...<br /><br />(1) Astrophysics whistleblower, Martín López Corredoira in <i>The Twilight of the Scientific Age</i>:<br /><br /><i>&quot;the more controversial the topic, and the more of a challenge it is to established ideas, and the newer the approach, then the more difficult will be the problems in publishing it, and the higher the probability of its being rejected. Gillies (2008, ch. 2) argues that when a researcher makes an advance which is later seen as a key innovation and a major breakthrough, a peer review may very well judge it to be absurd and of no values. As noted by Van Flandern (1993, ch. 21), </i><b><i>peer review in journals interferes with the objective examination of extraordinary ideas on their merits.</i></b><i> Maddox (1993), who was editor of the journal Nature, has said that if Newton submitted his theory of gravity to a journal today, it would almost certainly be rejected as being too preposterous to believe. On the one hand, there is a failure to select novel ideas (Brezis, 2007; Horrobin, 1990). On the other hand, </i><b><i>the refereeing process trends to conformity.&quot;</i></b><br /><br />(2) Another whistleblower, Jeff Schmidt, from an interview at <a href=\"http://www.julesnyquist.com/articles/article/1430100/16489.htm\" class=\"ot-anchor\">http://www.julesnyquist.com/articles/article/1430100/16489.htm</a> ...<br /><br /><b>MR:</b> <i>&quot;When you first thought of writing this book, you were in graduate school, right?&quot;</i><br /><br /><b>JS:</b> <i>&quot;Yes, that’s right. I got interested in the topic when I was going to professional training myself, getting a PhD in physics at the University of California, Irvine. </i><b><i>It seemed like the best of my fellow graduate students were either dropping out or being kicked out. And by ‘best,’ those were the most concerned about other people and seemed less self-centered, less narrowly-focused, most friendly people ... they seemed to be handicapped in the competition. They seemed to be at a disadvantage not only because their attention was divided, but because their concerns about big picture issues like justice and the social role of the profession and so on, caused them to stop and think and question, whereas their unquestioning gung-ho classmates just plowed right through with nothing to hold them back.</i></b><i> As I mentioned, there’s about a 50% drop-out rate for students entering University programs in all fields; and what I found was that this weeding out is not politically neutral. </i><b><i>To put it bluntly, the programs favor ass-kissers.</i></b><i> I don’t know if that’s an acceptable term on KFAI, but that’s the fact of the matter....&quot;</i><br /><br />(3) And from a graduate student who quit their program just a few weeks shy of getting their PhD, key excerpts captured at <a href=\"https://plus.google.com/108466508041843226480/posts/StWeeWPhhU6\" class=\"ot-anchor\">https://plus.google.com/108466508041843226480/posts/StWeeWPhhU6</a> ...<br /><br /><i>&quot;Academia: The Black Hole of Bandwagon Research</i><br /><br /><i>Indeed, writing lots of papers of questionable value about a given popular topic seems to be a very good way to advance your academic career these days. The advantages are clear: there is no need to convince anyone that the topic is pertinent and you are very likely to be cited more since more people are likely to work on similar things. This will, in turn, raise your impact factor and will help to establish you as a credible researcher, regardless of whether your work is actually good/important or not. It also establishes a sort of stable network, where you pat other (equally opportunistic) researchers on the back while they pat away at yours.</i><br /><br /><i>Unfortunately, not only does this lead to quantity over quality, but many researchers, having grown dependent on the bandwagon, then need to find ways to keep it alive even when the field begins to stagnate. The results are usually disastrous. Either the researchers begin to think up of creative but completely absurd extensions of their methods to applications for which they are not appropriate, </i><b><i>or they attempt to suppress other researchers who propose more original alternatives (usually, they do both). This, in turn, discourages new researchers from pursuing original alternatives and encourages them to join the bandwagon, which, though founded on a good idea, has now stagnated and is maintained by nothing but the pure will of the community that has become dependent on it.</i></b><i> It becomes a giant, money-wasting mess.&quot;</i><br /><br />The fact that there are hoardes of critical thinkers flooding the Internet&#39;s forums today to convince others to ask these same questions is not at all odd in light of the claims of academic whistleblowers.  And within the context of those claims, I think we can be confident that we understand <b>why</b> Nereid does what she does even if we will never know <b>who</b> she is.<br /><br />There&#39;s just not a lot of mystery here. <br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23person\">#person</a>  ﻿","category":"person"},{"name":"Joshua Schroeder AKA ScienceApologist","summary":"Astrophysics Graduate Student / Plasma Cosmology Critic / 12,000 Wikipedia Edits in Just the Period, 2004-2006, Alone / Indefinite Wikipedia Ban","image":"https://lh6.googleusercontent.com/-F7dOyA6Cd8E/VqUsrb3DlUI/AAAAAAAAGbA/rA6NDHGoFfU/scienceapologist-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/CLgp7whBhB1","publishDate":"2016-01-24T19:59:11.295Z","updateDate":"2017-01-03T03:00:48.301Z","text":"<b>Joshua Schroeder AKA ScienceApologist: Astrophysics Graduate Student / Plasma Cosmology Critic / 12,000 Wikipedia Edits in Just the Period, 2004-2006, Alone / Indefinite Wikipedia Ban</b><br /><br />It appears necessary to remind people today that astrophysics and cosmology are not traditional &quot;empirical&quot; scientific domains.  Experiments in these domains can be enormously expensive; can involve very large teams; are rarely reproduced by others; and the results are oftentimes subject to interpretation, and base upon layers of supporting assumptions dictating what the observations mean.  Each observational interpretation -- each &quot;inference&quot; -- requires the validity of every single one of these assumptions.<br /><br />Nobel laureate Daniel Kahneman, who is famous for having studied how the two aspects of the mind interact in decision-making, has noted certain clues that can indicate when a judgement reflects true expertise:<br /><br />[1] it requires &quot;an environment that is sufficiently regular to be predictable&quot; and &quot;an opportunity to learn these regularities through prolonged practice&quot;<br /><br />[2] when there is low risk of professionals learning the wrong lessons from experience (sometimes referred to as &quot;wicked&quot; problems).<br /><br />[3] when feedback is immediate<br /><br />By Kahneman&#39;s objective measure (he never explicitly mentions astrophysics or cosmology), the public should approach the astrophysical and cosmological domains with a healthy dose of skepticism.<br /><br />The original quote here:<br /><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/JVQtWErjFBW\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/JVQtWErjFBW</a><br /><br />But, you&#39;d be hard-pressed to find any skepticism of these domains on Wikipedia.  Joshua Schroeder, who has commonly referred to himself online as &quot;ScienceApologist,&quot; presents a character study which seems to challenge many of the simplistic notions that the public today holds to on what props up scientific consensus and how academics behave online.<br /><br />Schroeder is fairly known in the Wikipedia community for his aggressive &quot;edit-warring&quot; ...<br /><br /><i>&quot;I act to mitigate, redesign, and occasionally destroy the offerings of users who think that a particular &#39;breakthrough&#39; or &#39;notable idea&#39; deserves more consideration than it has gotten in the academic world. Such grandstanding is forbidden by a variety of Wikipedia policies and guidelines (WP:V, WP:SOAP, WP:NOR, WP:FRINGE, WP:WEIGHT, WP:NOT, and WP:REDFLAG to name just a few).&quot;</i><br /><br />(from <a href=\"https://en.wikisource.org/wiki/User:ScienceApologist\" class=\"ot-anchor\">https://en.wikisource.org/wiki/User:ScienceApologist</a>)<br /><br />Academic whistleblower, Martín López Corredoira, has stated in <i>The Twilight of the Scientific Age</i> that ...<br /><br /><i>&quot;as pointed out by Luminet (2008), a consensus in the scientific community is transformed into orthodoxy and ends up becoming counter-revolutionary.</i><br /><br /><i>This counter-revolution is constituted as a mechanism of self-defence created by the autonomous structure of knowledge in the building of theories. According to Iradier (2009), all disciplines or specialisations were constituted and directed by a process of removing questions. Once a theory is established by the system, its fundamentals are not investigated any further ... While a theory of science as an exploration without restrictions or prejudices and as a progressive approach to the truth is given publicity, research on the fundamentals of science is despised, and tricks are played against the people who have alternative ideas to these fundamentals. The first thing to be established becomes the last thing that is allowed to be revised.&quot;</i><br /><br />Joshua Schroeder would seem to fit that description to a tee.  Is the public aware that physics and astrophysics students are apparently active on wikipedia defending the subject they are investing themselves into?  Is it normal to make 12,000 Wikipedia edits over the course of just two years?<br /><br />ScienceApologist (2004-2006): <i>&quot;over 12,000 edits and two years of service&quot;</i><br /><br /><a href=\"https://en.wikipedia.org/wiki/Wikipedia:Requests_for_arbitration/ScienceApologist/Proposed_decision#ScienceApologist\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Wikipedia:Requests_for_arbitration/ScienceApologist/Proposed_decision#ScienceApologist</a><br /><br />And yet, that is apparently just the tip of the iceberg ...<br /><br /><i>Sockpuppetry</i><br /><br /><i>On Wikipedia, Schroeder had been found to be using his username accounts (&quot;sock puppets&quot;) inappropriately. In Nov 2007, it was found that Schroeder had used sockpuppets abusively.&quot;[43] His sockpuppets have included the usernames:</i><br /><br /><i>ScienceApologist</i><br /><i>Joshua P. Schroeder</i><br /><i>IvoryMeerkat</i><br /><i>VanishedUser314159</i><br /><i>Fradulent Ideas</i><br /><i>Mainstream astronomy</i><br /><i>Nondistinguished</i><br /><i>Velikovsky</i><br /><br /><i>Although indefinitely blocked at the time, Schroeder continued to evade the block in 2012, and was found to have continued to have used sockpuppet usernames and IP addresses,[48] including:</i><br /><br /><i>Goodsheard1</i><br /><i>Hudn12</i><br /><i>216.236.252.234</i><br /><i>192.80.65.234 (Boston Public Library)</i><br /><br />(many of these links are disappearing from the Internet record, this information for the time being traces back to <a href=\"http://web.archive.org/web/20120901213441/http://scienceapologist.wikinet.org/wiki/Joshua_P._Schroeder_%28ScienceApologist%29\" class=\"ot-anchor\">http://web.archive.org/web/20120901213441/http://scienceapologist.wikinet.org/wiki/Joshua_P._Schroeder_%28ScienceApologist%29</a>)<br /><br />What sort of picture does it paint when a student of astrophysics disguises his online identity in order to suppress the scientific framework which challenges his own educational investment?<br /><br />Much of Schroeder&#39;s Wikpedia activities have been specifically directed at plasma cosmology and the Electric Universe:<br /><br /><i>&quot;Plasma cosmology advocates are the reason it is sometimes said that a little knowledge is a bad thing. These advocates sound to 90% of the unschooled population like the real deal. They make it seem like there really is a &quot;controversy&quot;. The problem is they basically don&#39;t know what they are talking about. They make up outlandish claims, analyze data improperly, and have almost no understanding of the physics and analytics involved in modern cosmology while at the same time deriding it. This kind of advocacy is obnoxious and rude to those people who actually take the time to carefully consider their models in contrast to those who cannot create a reasonably rational one.&quot;</i><br /><br />(link may not work anymore, page formerly (?) at <a href=\"http://blog.myspace.com/index.cfm?fuseaction=blog.view&amp;friendID=71159560&amp;blogID=208476914\" class=\"ot-anchor\">http://blog.myspace.com/index.cfm?fuseaction=blog.view&amp;friendID=71159560&amp;blogID=208476914</a>)<br /><br />For an alternative view of the nature of this controversy, see my introduction to Ian Tresman at <a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/iECMeMJBrHV\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/iECMeMJBrHV</a>.<br /><br />Joshua Schroeder&#39;s activities are not limited to Wikipedia, for he has also engaged in letter-writing campaigns to the journal which has engaged these research topics:<br /><br /><i>&quot;Finally, thanks for the information on Tony Perratt. I wrote IEEE Plasma Transactions a few months back demanding that they remove him from their editorial board. Now I have even more information about this. I remember coming across Tony&#39;s bizarre papers on petroglyphs and thought to myself that the guy has finally gone over the deep end. No, he&#39;s just connected plasma cosmology completely to Velikovsky. Interesting because the phrase &quot;this is not a Velikovskian fantasy&quot; was bandied about by Plasma cosmology adherents. Now the chickens really have come home to roost. - ScienceApologist (talk) 00:45, 30 December 2008 (UTC)&quot;</i><br /><br /><a href=\"https://en.wikipedia.org/w/index.php?title=User_talk:Phaedrus7&amp;oldid=260782229#Response_to_your_excellent_information\" class=\"ot-anchor\">https://en.wikipedia.org/w/index.php?title=User_talk:Phaedrus7&amp;oldid=260782229#Response_to_your_excellent_information</a><br /><br />For more information on Dr. Peratt&#39;s groundbreaking work linking petroglyphs to plasma morphologies, see <a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/fBqAYRqoXHz\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/fBqAYRqoXHz</a>.<br /><br />One of Schroeder&#39;s victims is Big Bang critic, Eric Lerner.  Lerner&#39;s Wikipedia entry emphasizes the critiques against his book, <i>The Big Bang Never Happened: A Startling Refutation of the Dominant Theory of the Origin of the Universe</i> (1991).<br /><br /><i>&quot;Lerner&#39;s criticisms of the Big Bang theory have been rejected by some physicists and cosmologists who have reviewed his book. The size of superclusters is a feature that has been limited by subsequent observations to the end of greatness and explained in the astronomical journals as arising from a power spectrum of density fluctuations growing from the quantum fluctuations predicted in inflationary models.[22][23][24] Anisotropies were discovered in subsequent analysis of the both COBE and BOOMERanG experiments and were more fully characterized by the Wilkinson Microwave Anisotropy Probe[22][23] and Planck.</i><br /><br /><i>Physical cosmologists who have commented on the book have generally dismissed it.[22][24][25][26][27][28] In particular, Edward L. Wright, the American astrophysicist and cosmologist, was critical of Lerner for making errors of fact and interpretation and criticized specifics of Lerner&#39;s alternative cosmology,[23] claiming that ...&quot;</i><br /><br />ScienceApologist&#39;s role in these edits can be observed at <a href=\"https://en.wikipedia.org/w/index.php?title=Eric_Lerner&amp;limit=500&amp;action=history\" class=\"ot-anchor\">https://en.wikipedia.org/w/index.php?title=Eric_Lerner&amp;limit=500&amp;action=history</a>.  Apparently, some of SA&#39;s earlier edits have since been reverted (?).<br /><br />Eric Lerner has commented on ScienceApologist at <a href=\"https://en.wikipedia.org/w/index.php?title=User_talk:Elerner&amp;diff=202094443&amp;oldid=202087061\" class=\"ot-anchor\">https://en.wikipedia.org/w/index.php?title=User_talk:Elerner&amp;diff=202094443&amp;oldid=202087061</a> ...<br /><br /><i>&quot;Schoerder [sic] (scienceapologist) has been consisently attempting to make this article [Lerner&#39;s biography] as unfavorable to me as possible and eliminate anything favorable. </i><b><i>He has a major conflict of interest because he is a graduate student in astronomy, working directly under astrophysicists who disagree with my work. He should be banned from editing this article. I would remind you that I was banned from editing the article on plasma cosmology because I work in that field. How can Scienceapologist be allowed to edit the article on me when he too now is in the same field, cosmology, and has made it his special task to attack anything that disagrees with what his professors think?&quot;</i></b> - Elerner (talk) 17:46, 30 March 2008 (UTC)<br /><br />From <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Requests_for_arbitration/Pseudoscience\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Wikipedia:Requests_for_arbitration/Pseudoscience</a> ...<br /><br /><i>&quot;SA&#39;s edits of my page have systematically aimed to create the false impression that no one in the community takes my work seriously, that I should be grouped with someone like, say, Velikovsky. For that reason he has eliminated references to my peer-reviewed publications, to leading academic institutions where I have been invited to present my work and to my stay at ESO as a Visiting Astronomer.</i><br /><br />[..]<br /><br /><i>&quot;SA and his colleague BKramer have pursued this campaign against me and everyone else they see as minority thinkers. This has led to ludicrous stunts such as questioning my (BA!) degree from Columbia as “unsourced”. I would like to see anyone’s degree which is publicly sourced to anyone except themselves. University records are not generally on-line!&quot;</i><br /><br />In making the case for his Institute for Venture Science (IVS), Dr. Gerald Pollack has poignantly remarked:<br /><br /><i>&quot;Dismissive treatment of challenge is not new. It is an expected feature of human nature that has been in evidence over the ages. In the past, however, challengers could sometimes succeed; they could prevail because success was merely a matter of convincing an intimate group of influential scientific leaders, who were few in number. Achieving success today has become, ironically, more formidable because the scientific enterprise has grown to be so massive. It is no longer a matter of convincing a few, but of convincing huge masses whose collective influence and control over funds is daunting. As a consequence, few potential breakthrough ideas ever come to fruition.</i><br /><br /><i>The number of unconventional schools of thought around today is surprisingly large. Those who take the trouble to look will find meaningful alternatives distributed throughout practically all domains of science, many of them rich with promise. All but a few are ignored or repressed by the prevailing orthodoxy — </i><b><i>either because of perceived threats to self-interest or because of suspicion stemming from a cultural bias that fundamental discoveries are no longer possible.</i></b><i> These unconventional schools of thought represent potentially ripe fruit, waiting to be plucked.&quot;</i><br /><br />(from <a href=\"http://www.theinstituteforventurescience.net/#!2-rationale-and-strategy/c16ka\" class=\"ot-anchor\">http://www.theinstituteforventurescience.net/#!2-rationale-and-strategy/c16ka</a>)<br /><br />Is it really the case that fundamental discoveries are no longer possible?  How can such a simple narrative survive in light of the aggressive attempts by a single astrophysics graduate student to eliminate the public&#39;s access to these very challenges?<br /><br />&quot;Scientism&quot; is the belief that contemporary science is the place to get true answers to everything.  The fact is that astrophysics and cosmology are at the very edge of the scientific enterprise.  So, what does it mean when an astrophysics graduate student asks ...<br /><br /><i>&quot;Does mainstream academia own the subjects it studies?&quot;</i><br /><br />(see last slide of presentation by Joshua Schroeder at <a href=\"http://web.archive.org/web/20140328070942/http://upload.wikimedia.org/wikipedia/meta/5/56/Wikipedia_pseudoscience.pdf\" class=\"ot-anchor\">http://web.archive.org/web/20140328070942/http://upload.wikimedia.org/wikipedia/meta/5/56/Wikipedia_pseudoscience.pdf</a>)<br /><br />What does such a question say about the person asking it?<br /><br />Next time you see a conversation about some topic in science that involves an against-the-mainstream idea, do not forget about the science apologists of the world. <b>How much of our own worldviews are shaped by their attempts to limit our access to information?</b><br /><br /><i>&quot;Wikipedia is inherently a non-innovative reference work: it stifles creativity and free-thought. If Wikipedia had been around at the time of Galileo, his ideas would have been subject to my incisive commentary and editorial braggadocio — even if I agreed with him. I am a status quo promoter.&quot;</i><br /><br />(from <a href=\"https://en.wikisource.org/wiki/User:ScienceApologist\" class=\"ot-anchor\">https://en.wikisource.org/wiki/User:ScienceApologist</a>)<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23person\">#person</a>  ﻿","category":"person"},{"name":"Ian Tresman","summary":"Amateur-Professional Online Wiki Warrior and Public Educator","image":"https://lh3.googleusercontent.com/-oCC4d8v8Y3E/VqPKHUP0ZQI/AAAAAAAAGaI/lozgFrE1T_0/w1800-h2520/ian-tresman-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/iECMeMJBrHV","publishDate":"2016-01-24T00:06:09.306Z","updateDate":"2017-01-03T03:01:00.746Z","text":"<b>Ian Tresman: Amateur-Professional Online Wiki Warrior and Public Educator</b><br /><br />Ian Tresman is not a scientist, and yet he&#39;s the creator of the world&#39;s most authoritative source on the universe&#39;s most common state for matter -- plasma.  That oftentimes surprising and superbly referenced wiki is here:<br /><br /><a href=\"http://www.plasma-universe.com/Plasma-Universe.com\" class=\"ot-anchor\">http://www.plasma-universe.com/Plasma-Universe.com</a><br /><br />But, why Ian?  Why not an actual scientist?  It&#39;s an important story which can inform us about an ongoing problem in the communication of scientific ideas today.<br /><br />It&#39;s important to first provide the larger context before bringing up the wiki wars.  What Ian has done is the meticulous documentation of the lesser known side of a longstanding scientific controversy between two prominent scientific journals: <i>IEEE&#39;s Transactions on Plasma Science</i> and <i>The Astrophysical Journal</i>.  Although the AJ is the more widely known of the two, IEEE is the world&#39;s largest technical organization, and it has played a crucial role in the computer software and hardware revolutions we see today.<br /><br />Due to the empirical issues associated with the domains of astrophysics and cosmology, the AJ&#39;s success or failure rests upon the widespread perception that their ideas are correct, whereas IEEE can point to the physical technology which it has helped to create as proof of its accomplishment.  This creates a strange dynamic where advocates of the AJ, which basically includes most science journalists today, <b>have positioned themselves against cosmic plasma models which derive from laboratory observation.</b><br /><br />That bears repeating: <b>It is extraordinary -- but also a fact -- that the dominant astrophysical belief today is that cosmic plasma models should not base off of laboratory plasma observations,</b> and those who deviate from this worldview are today branded as pseudo-scientists and fringe.<br /><br />A little-known fact amongst the public is that the two mentioned journals publish opposing papers on the subject of cosmic plasmas, based upon this epistemological disagreement over how to approach the subject.  It&#39;s actually a common clash that can be observed throughout the scientific disciplines over how to create theories: On the one hand, a theory can be thought of as an <b>invention</b> (and this is largely what we see with the mathematically-driven and observationally-tweaked ideas advanced by the AJ).  But on the other, some theories might to some extent be thought more of as a <b>discovery</b> (like when it is more driven by laboratory observations, the approach promoted for cosmic plasma by IEEE).  There are deep epistemological questions here about how we know what we know.<br /><br />Plasmas are the perfect example of this epistemological clash, because laboratory plasma science is a very advanced science by now.  For example, the z-machine ...<br /><br /><a href=\"http://www.sandia.gov/z-machine/\" class=\"ot-anchor\">http://www.sandia.gov/z-machine/</a><br /><br />But, at the same time, plasma is also an astrophysical phenomenon, insofar as it was (for the mainstream, inconveniently) realized some decades ago that it is the universe&#39;s most common state for matter.  For a refresher on how that came to be and snapshots from astrophysics and plasma textbooks noting its astrophysical significance, see this graphic ...<br /><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/1noN6Miph3x\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/1noN6Miph3x</a><br /><br />The question of how to model the cosmic plasma arrives at two opposing answers based upon the chosen approach.  This wouldn&#39;t actually be an issue were it not for the fact that the public commonly looks to wikipedia to learn about scientific subjects, and wikipedia strictly defines itself as an encyclopedia.  And since an encyclopedia only has <b>one</b> entry, that larger context dictates a winner and a loser -- regardless of the fact that the debate rages on and new observations of consequence to it are routinely revealed.<br /><br />Dave Smith has eloquently stated the essence of the problem with scientific controversies on wikipedia:<br /><br /><i>&quot;Challenges to the consensus are the lifeblood of progress in the sciences.&quot;</i><br /><br />(from <a href=\"http://www.thunderbolts.info/thunderblogs/archives/davesmith_au08/091226_wikiwoes.htm\" class=\"ot-anchor\">http://www.thunderbolts.info/thunderblogs/archives/davesmith_au08/091226_wikiwoes.htm</a>)<br /><br />For those who might be considering becoming active on wikipedia, arguably the best place to start would be with the 5 pillars here:<br /><br /><a href=\"https://en.wikipedia.org/wiki/Wikipedia:Five_pillars\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Wikipedia:Five_pillars</a><br /><br /><b>The basic idea behind Wikipedia is that articles should be notable and subject to verifiability with reliable sources, and that Wikipedia should not be used as a vehicle for original research.</b> Yet, in practice, some wiki editors report that reliable sources act merely as a guideline, and conduct standards apply to some editors but not others.<br /><br />Either way, it&#39;s not clear that the public appreciates the larger philosophical implications of this limiting approach to knowledge in the sciences.  In a sense, it is an anti-innovative approach which favors ideas simplistically based upon the populations of their adherents.<br /><br /><b>Note that if Galileo&#39;s claims had only recently been made, we would not be allowed to publish in Wikipedia that he considered that the Earth rotated around the Sun, due to the lack of recognition of his theory.”</b><br /><br />Prior to actually participating in a controversial wiki edit -- if a person is so inclined -- it would be wise to learn about the experiences of others who have already done the same.  A good example, for those who have the time to develop a deep fluency in Wikipedia&#39;s known arbitration issues, is Rome Viharo&#39;s analysis here:<br /><br /><a href=\"http://wikipediawehaveaproblem.com/we-have-a-problem/\" class=\"ot-anchor\">http://wikipediawehaveaproblem.com/we-have-a-problem/</a><br /><br />But, we&#39;ll limit our focus here to what happened to Ian, and what this means more generally for the public&#39;s access to information about ideas which challenge the dominant paradigms in science.  Let&#39;s say that a person has heard something from another individual about this idea called the &quot;Electric Universe.&quot;  A very typical response is to first do a search on wikipedia.  This is what they will find ...<br /><br /><i>Electric universe may refer to:</i><br /><br />(1) <i>Electric Universe (album), the 1983 studio album by Earth, Wind &amp; Fire</i><br /><br />(2) <i>Electric Universe, a psychedelic trance music group from Germany</i><br /><br />(3) <i>Any of a class of cosmological theories such as that by George Woodward Warder; see Plasma cosmology</i><br /><br />(4) <i>Electric Universe: How Electricity Switched On the Modern World, a book by science writer David Bodanis</i><br /><br />Realizing the cosmological context, a person will then click on the 3rd link above, where they will find this entry at <a href=\"https://en.wikipedia.org/wiki/Plasma_cosmology\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Plasma_cosmology</a>:<br /><br /><i>Plasma cosmology is a non-standard cosmology whose central postulate is that the dynamics of ionized gases and plasmas play important, if not dominant, roles in the physics of the universe beyond the Solar System.[2][3][4] This is contrary to the general consensus by cosmologists and astrophysicists which strongly supports that astronomical bodies and structures in the universe are mostly influenced by gravity, Einstein&#39;s theory of general relativity, and quantum mechanics. These can be used to explain the origin, structure and evolution of the universe on cosmic scales. As of 2015, the vast majority of researchers openly reject plasma cosmology because it does not match modern observations of astrophysical phenomena or accepted cosmological theory.</i><br /><br />So, before the reader is actually informed of any of the claims or evidence behind those claims, Wikipedia emphasizes the larger context that the idea is rejected by the vast majority of researchers.  Notice no mention at all upfront of reasons to believe in the idea, such as ...<br /><br />- this is a longstanding debate between the adherents of two competing scientific frameworks who publish in two prominent scientific journals;<br /><br />- rather than pointing out the crucial laboratory-observed fact that all plasma beams emit microwaves (thus, plasma cosmologies are extremely well-suited to explaining microwaves coming at us from all directions), the wikipedia entry for plasma cosmology claims that &quot;the CMB [cosmic microwave background] has been heralded as a major confirmation of the Big Bang model to the detriment of alternatives&quot;!<br /><br />- no mention at all of just how little ionization is required (less than 1%!) in order to elicit electrodynamic plasma behavior in a gas -- another absolutely crucial observation necessary for an independent thinker to formulate their own meaningful opinion on the debate over plasma&#39;s role in the cosmic dynamics.<br /><br />(etc)<br /><br />In essence, the article is really more about why the reader should stop learning about the idea (its debunking) than what it is (the idea&#39;s description).<br /><br />So, how can something like this happen?  What is the nature of the wikipedia editing process which permits one wikipedia editor to obstruct the public&#39;s search for information on a controversial scientific topic?<br /><br />Let&#39;s try to review the problem without getting overly bogged down by wikipedia&#39;s cryptic arbitration process ...<br /><br />(1) <b>Wikipedia can be edited any time, anywhere, by anyone anonymously,</b> and this can be used to game the system.  Open authorship is both Wikipedia&#39;s greatest feature and its biggest weakness.<br /><br />(2) <b>There are minimal checks and balances in the system.</b>  Edits can be reverted in the case of obvious tampering or vandalism.  However, more subtle issues my go uncorrected.  Sometimes perfectly good edits are arbitrarily reverted.<br /><br />(3) <b>Users or IP addresses suspected of gaming the system, tampering or spamming can be reported and possibly warned, suspended, or banned (temporarily or permanently). However, that process can be abused as well.</b><br /><br />(4) <b>Wikipedia has an arbitration process for disputes.</b> However, one of Wikipedia&#39;s unfortunate guidelines is &quot;Wikipedia is not a court.&quot;<br /><br />What this generally means is that there are no &quot;rules of evidence&quot; or &quot;innocent until proven guilty.&quot; There is an &quot;assume good faith&quot; clause, but it only extends so far and is often ignored.  Thus, parties on opposite sides of an issue can manipulate the system by numbers.<br /><br />For example, if you can get more people to come to an arbitration hearing and offer positive or negative reviews of the user, you can sometimes swing the vote or lobby for a different outcome.  If a group decides to collaborate against you, then they can show up at an arbitration with superior numbers and a few relatively weak assertions and overrun the one or two friends you may have on Wikipedia.  The assertions generally don&#39;t have to be proven -- just believed by enough administrators.<br /><br />If you bring several people to defend you, they can claim that there&#39;s a &quot;conspiracy to defend.&quot;  For example, they might insinuate that you gathered a bunch of either &quot;meat-puppets&quot; or &quot;sock-puppets&quot; to help you to make your case.  Meat puppets are people/friends signed up strictly to come to the arbitration or to come and offer a favorable opinion at a particular article&#39;s talk page.  Sock-puppets are accounts that all attribute to the same person, and which are used to &quot;bail you out.&quot;  The process presupposes that the offense is purely innocent in their motives and that the defense is purely insidious in their intentions, rather than adopting good faith toward all parties (unless proven otherwise).<br /><br />In any event, the lack of a court mentality leads to what often amounts to &quot;kangaroo courts&quot; or lynchings, where one party manages to get an opponent banned over either minor infractions or over what amounts to nothing. That&#39;s not to say that it always goes that way, but that it&#39;s quite possible due to the lax way that such things are often handled.<br /><br />Compounding all of these apparent arbitration difficulties is the fact that records and information on Wikipedia which is not convenient to the textbook narrative in science has a tendency to just evaporate.  Here are some records of Ian&#39;s work at wikipedia which have managed to survive:<br /><br /><a href=\"https://en.wikipedia.org/wiki/Wikipedia:Mediation_Cabal/Cases/2006-02-11_Electric_Universe_Concept,_NPOV_clarification\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Wikipedia:Mediation_Cabal/Cases/2006-02-11_Electric_Universe_Concept,_NPOV_clarification</a><br /><br /><a href=\"https://en.wikipedia.org/wiki/Wikipedia:Requests_for_arbitration/Pseudoscience/Evidence#Evidence_presented_by_iantresman\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Wikipedia:Requests_for_arbitration/Pseudoscience/Evidence#Evidence_presented_by_iantresman</a><br /><br />Here&#39;s an example of the deletion of a wikipedia entry on an entire <b>journal</b>.  Note that this search no longer produces a direct hit ...<br /><br /><a href=\"https://en.wikipedia.org/wiki/Wikipedia:Articles_for_deletion/Society_for_Interdisciplinary_Studies\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Wikipedia:Articles_for_deletion/Society_for_Interdisciplinary_Studies</a><br /><br />... and despite the request to ...<br /><br /><i>&quot;KEEP! I find the SIS web site to be an excellent resource. I work as a geologist, so I know only too well that the gradualist approach cannot answer all questions. Many catastrophist related ideas have some merit.&quot;</i><br /><br />Ian&#39;s larger online interactions with scientific experts exemplify how ordinary, open-minded laypeople can make a significant difference in the world of scientific controversies.  His thought-provoking interactions with scientific experts can be found scattered across Internet forums.<br /><br />One such conversation can be seen at <a href=\"http://cosmoquest.org/forum/showthread.php?20051-Solar-Wind-as-an-electric-current&amp;s=8bd4b18dec956bed33cfda9b95c87829&amp;p=451127#post451127\" class=\"ot-anchor\">http://cosmoquest.org/forum/showthread.php?20051-Solar-Wind-as-an-electric-current&amp;s=8bd4b18dec956bed33cfda9b95c87829&amp;p=451127#post451127</a>.<br /><br />He starts out with a very simple question ...<br /><br /><i>&quot;Is it fair to describe the Solar Wind as an electric current, after all, it is a movement of charge particles?&quot;</i><br /><br />... and the moderator demonstrates in his initial reaction a response which by the end of his interactions with Ian, he will have backed away from ...<br /><br /><i>&quot;I don&#39;t think so, because as far as I know the solar wind is neutral, which would mean equal amounts of positive and negative moving at the same speed, ergo no current&quot;</i><br /><br />... After some discussion, Ian clarifies his position ...<br /><br /><i>&quot;Whether the Solar Wind is an electric current&quot; has become much more complicated than I anticipated. What I have learnt is as follows:</i><br /><br /><i>1. Although moving charges (electrons or protons) define an electric current, it is also a vector quantity. So the simply answer is that the NET electric current in a neutral ideal plasma is ZERO because the electrons and protons cancel.</i><br /><br /><i>2. However, plasma is not simple, and [except] in text books, it is not ideal. We don&#39;t get each proton pairing with a specific electron (as we do in a non-ionized gas). As a result, currents may arise.</i><br /><br /><i>3. Consequently it is important to specify a frame of reference. So astronomers can talk about the Solar Wind&#39;s electron current (in its own frame of reference), or ion current, but which taken together, cancel.</i><br /><br /><i>4. There are lots of forces on Solar System ions and protons; they don&#39;t just move outward from the Sun in pairs. Anywhere these forces cause the ions and protons to move differently to one another, so that there is net movement of electrons or protons, will cause a current. And this does happen, and are called drifts. For example, the effect of gravity on ions depends on their mass, and as Alfvén suggests, may produce chemical separation, and produce a current.</i><br /><br /><i>[...]</i><br /><br /><i>So I guess that to ask &quot;whether the Solar Wind is a current&quot; is poor question, like asking whether the sea is a current; it has currents, but it is not a current.&quot;</i><br /><br />... then further down the thread, the moderator essentially agrees with Ian&#39;s clarified points ...<br /><br /><i>&quot;The OP was about, is the solar wind as a whole an outflowing current, and the answer was no. Yes, there are currents in the solar wind, because there are magnetic structures in the solar wind, like CMEs.&quot;</i><br /><br />The world today badly needs more open-minded layperson-science participants to bring fresh questions and thinking to the oftentimes rigid thinking of scientific experts.  My hope is that telling Ian&#39;s story can convince others that they can also have an impact.<br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23person\">#person</a>  ﻿","category":"person"},{"name":"Gerrit Verschuur","summary":"Radio Astronomer / Author / Against-the-Mainstream Academic Researcher / Pioneered the Measurement of the Interstellar Magnetic Field / Challenged the CMB as a Background Radiation","image":"https://lh3.googleusercontent.com/-PkdIRDx2xGw/Vp8FNtouyOI/AAAAAAAAGXc/ydF0IcPcdV0/w1800-h2520/gerrit-verschuur-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/KjBiRhbY9zF","publishDate":"2016-01-20T04:25:03.733Z","updateDate":"2017-01-03T03:01:13.415Z","text":"<b>Gerrit Verschuur: Radio Astronomer / Author / Against-the-Mainstream Academic Researcher / Pioneered the Measurement of the Interstellar Magnetic Field / Challenged the CMB as a Background Radiation</b><br /><br />Gerrit Verschuur is perhaps known best amongst conventional physicists as the author of the Encyclopædia Britannica articles on the &quot;Interstellar Medium&quot; and &quot;Interstellar Matter.&quot;<br /><br />... Or, alternatively for pioneering the measurement of the interstellar magnetic field using the 21-cm Zeeman effect technique.<br /><br />But, a better way for laypeople to get to know him is through his excellent popular science books.  His subject matter -- magnetic fields, radio waves and interstellar matter -- are complex, challenging and controversial topics.  His mastery of these subjects is evidenced by his ability to teach a layperson -- in just two chapters (!) -- how to read a radio astronomy paper (see chapters 5 &amp; 6 of his text, <i>Interstellar Matters</i>).<br /><br />Verschuur is unique amongst scientists in that he is cultured in his discipline&#39;s rich history.  The introduction to his text, <i>The Invisible Universe</i> cannot be missed, insofar as it is possibly (?) the only modern explanation of the initial refusal by the astronomical community to accept the existence of radio waves from space.<br /><br /><i>&quot;Radio engineers didn&#39;t care where the radio waves came from, and the astronomers &#39;... could not dream up any rational way by which the radio waves could be generated, and since they didn&#39;t know of a process, the whole affair was (considered by them) at best a mistake and at worst a hoax.&#39;&quot;</i><br /><br />More of that history is at <a href=\"https://plus.google.com/108466508041843226480/posts/AFh4Q5Ceggb\" class=\"ot-anchor\">https://plus.google.com/108466508041843226480/posts/AFh4Q5Ceggb</a><br /><br />The story is no longer apparently told amongst academics.  But once it is understood, it sets a very different context for the modern investigation of the interstellar medium than the simple progression narrative implied by popular science journalism.<br /><br />Of all interstellar matter, hydrogen is the most common and the 21-cm HI hydrogen spectral line is perhaps the most observationally important since those microwave radio waves can apparently penetrate the interstellar cosmic dust even when it is opaque to visible light.  But, with that important characteristic comes an added complexity -- for the signals emitted by distinct three-dimensional clouds (which of course might be separated by vast distances) are seen by the radio astronomer as overlapping, and on a 2D surface. <b>There is accordingly much room for confusion and the application of unfounded assumptions about what the theorist expects to see.</b><br /><br />With that as our epistemological and historical context, it is really quite remarkable that we do not see more written about the interstellar medium&#39;s <b>filamentary</b> nature -- by now an observational fact emphasized by Herschel (see <a href=\"http://sci.esa.int/herschel/55942-herschels-hunt-for-filaments-in-the-milky-way/\" class=\"ot-anchor\">http://sci.esa.int/herschel/55942-herschels-hunt-for-filaments-in-the-milky-way/</a> and the remarkable stellar birth image at <a href=\"http://www.holoscience.com/wp/wp-content/uploads/2011/05/Star-forming-filaments.jpg\" class=\"ot-anchor\">http://www.holoscience.com/wp/wp-content/uploads/2011/05/Star-forming-filaments.jpg</a>).<br /><br />A serious historical oversight in the space sciences which Verschuur is now tuned into is the fact that Nobel laureate Hannes Alfven predicted that a plasma-dominated universe would be permeated by filaments.  Yet, a review of the ESA&#39;s press release for this Herschel discovery fails to make any mention at all of that earlier prediction.  We see this problematic approach throughout the sciences: An idea (like plasma cosmology) will be rejected for some reason -- in this case, due to the observation of the CMB -- and once it is, there is no longer any effort to keep track of any of its future successes.  It&#39;s a problem which largely sources to science journalism today, and suggests that the endeavor is ripe for a significant disruption in the coming decades.<br /><br />Filaments are not an inherent property of gases, of course -- except sporadically and by circumstance.  Further, <b>twisted</b> filaments -- which appear quite common throughout interstellar imagery, particularly in visible light nebulae, but also seemingly with HI hydrogen -- would seem to specifically point to the action of magnetic fields and electrodynamic plasmas (not inert gases).  Any attempt to force-fit fluids-based models (as is the case with magnetohydrodynamics, or MHD) would fail to accurately predict the behaviors of these phenomena, and leave us with a raft of persistent mysteries in the space sciences.<br /><br />It seems that science journalists have seized upon the upside of the mystery, but not adequately engaged the critics of textbook theory sufficient to act as a meaningful check upon scientific expertise.  Verschuur is a perfect example of the problem, as there is not a single science journalist, to date, who has reported on his claims beyond his direct statements to them.<br /><br />Verschuur plainly understands the implications of filamentary plasmas ...<br /><br /><i>&quot;The lower-right frame is a schematic representation of several filaments that cross this field of view ... The numbers indicate the velocity of the gas at various locations along Filament A.  H0827+10 would traditionally be referred to as a &#39;cloud.&#39;&quot;</i><br /><br />(from Interstellar Neutral Hydrogen Filaments at High Galactic Latitudes and the Bennett Pinch at <a href=\"http://plasmauniverse.info/downloads/VerschuurAPSS.pdf\" class=\"ot-anchor\">http://plasmauniverse.info/downloads/VerschuurAPSS.pdf</a>)<br /><br />No different than most scientists today, radio astronomers have also generally failed to appreciate the significance of cosmic plasmas (instead assuming the behavior of gases).  The current craze in the radio astronomy domain, in a general sense, are the popular cosmological questions related to dark matter, dark energy and all of the other wild speculations which are today taken for granted as &quot;normal&quot; science.  Fundamental forces like electromagnetism have difficulty competing for the public&#39;s attention against science fiction themes, but don&#39;t for a second be fooled into thinking that the implications of an electromagnetic universe are any less interesting (and the public has a whole new exciting world of discovery ahead of them).<br /><br />There are implications in HI hydrogen for the claim that the smooth microwave signal coming at us from all directions is some sort of remnant of the Big Bang event.  As Wal Thornhill notes in a section of his paper, &quot;Toward a Real Cosmology in the 21st Century&quot; titled &quot;What is the Microwave Background Radiation?&quot;, he states ...<br /><br /><i>&quot;CMBR is too smooth to account for the lumpiness of galaxies and galactic clusters in the universe. We cannot ‘see’ them through the local microwave fog. Fred Hoyle wrote of the CMBR, &#39;A man who falls asleep on the top of a mountain and who awakes in a fog does not think he is looking at the origin of the Universe. He thinks he is in a fog.&#39;&quot;</i><br /><br />That paper provides some crucial background on Verschuur&#39;s work -- often with help from Anthony Peratt (<a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/fBqAYRqoXHz\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/fBqAYRqoXHz</a>) -- on critical ionization velocities (CIV&#39;s) associated with HI hydrogen filaments.<br /><br />Read more at <a href=\"http://benthamopen.com/contents/pdf/TOAAJ/TOAAJ-4-191.pdf\" class=\"ot-anchor\">http://benthamopen.com/contents/pdf/TOAAJ/TOAAJ-4-191.pdf</a>.<br /><br />Wikipedia explains ...<br /><br /><i>&quot;Critical ionization velocity (CIV), or critical velocity (CV), is the relative velocity between a neutral gas and plasma (an ionized gas), at which the neutral gas will start to ionize. If more energy is supplied, the velocity of the atoms or molecules will not exceed the critical ionization velocity until the gas becomes almost fully ionized.&quot;</i><br /><br />An important table at that page (<a href=\"https://en.wikipedia.org/wiki/Critical_ionization_velocity\" class=\"ot-anchor\">https://en.wikipedia.org/wiki/Critical_ionization_velocity</a>) relates these velocities, which appear to radio astronomers as redshifts at very specific frequencies.  It&#39;s really two coincidences in one: First, that there would be particular values for redshift that consistently show up at all, but second, that these values would tend to correspond to the general vicinity of the CIV&#39;s for the universe&#39;s most common elements!<br /><br />Verschuur&#39;s laborious non-algorithmic approach to studying all-sky HI hydrogen surveys produced many dozens of examples where such foreground contamination was embedded within the WMAP &quot;background&quot; radiation.  His work raises crucial questions about the source of the CMB (e.g., is it actually all local, thus eliminating a crucial piece of evidence for the Big Bang?).  Such questions have yet to be fully engaged by science journalists in any sort of independent manner.<br /><br /><i>&quot;Because the WMAP team didn&#39;t consider or know about the contribution of such a phenomenon they didn&#39;t try and subtract it as they did numerous other electromagnetic &#39;contaminants&#39; in their data reduction, says Verschuur.&quot;</i><br /><br />(from <a href=\"http://news.discovery.com/space/cosmic-background-radiation-big-bang-120615.htm\" class=\"ot-anchor\">http://news.discovery.com/space/cosmic-background-radiation-big-bang-120615.htm</a>)<br /><br />Verschuur&#39;s work has exposed a rich scientific controversy, and despite the inappropriate web URL, Tom Bridgman has provided us with a worthwhile rebuttal ...<br /><br /><a href=\"http://dealingwithcreationisminastronomy.blogspot.com/2014/09/electric-universe-hi-regions-cmb-and.html\" class=\"ot-anchor\">http://dealingwithcreationisminastronomy.blogspot.com/2014/09/electric-universe-hi-regions-cmb-and.html</a><br /><br />Many critics of the Electric Universe fashion Tom Bridgman&#39;s website as a sort of tombstone for electrical cosmology claims.  They arrive at his site, forward the link, and imagine that the argument is settled.  But, Bridgman&#39;s claims at best simply seed doubt.  For example ...<br /><br /><i>&quot;The existence of this [21-cm HI hydrogen] transition was predicted theoretically from quantum mechanics, some years before it was actually observed. Again, this success was thanks to the power of mathematics with a reliable experimental and theoretical framework.&quot;</i><br /><br />It sounds impressive, but is it really the case that this would necessarily be the final word on the universe&#39;s most common element?  Even if quantum mechanics can explain something about the energy transition itself, we should nevertheless look to the discipline of plasma physics to understand the <b>macroscopic</b> behavior of hydrogen in light of plasma&#39;s prevalence and tendency to form filaments (what is observed).<br /><br />He goes on ...<br /><br /><i>&quot;With a transition rate on the order of 10^-15 per second, the state has a lifetime of about 10 million years. Any collisions with other atoms in that time frame can change the atomic state, so the 21 cm HI emission is limited to regions of extremely low temperature and/or density, so that there is a LONG time between collisions which can reset the atomic state so the photon will not be emitted. This long lifetime means this emission of 21 cm radiation has never been observed in the laboratory. Since EU advocates often like to claim only laboratory-verified phenomena should be valid science (which appears to include almost everything except neutrino oscillations), it&#39;s rather hypocritical when they resort to the 21cm radiation.&quot;</i><br /><br />Notice that what Bridgman is specifically <b>not</b> doing here is raising the issue in the context of the phenomena thought by electrical theorists to be at play.  In fact, he leaves his audience without any understanding at all about the unique nature of collisions in a plasma dominated by electromagnetic fields, nor where these ions might have come from, to begin with.<br /><br />But, not all of Bridgman&#39;s claims are so easily discounted, and there exists a pressing need to methodically map out the claims on both sides of debates like this.  The theorists involved in these disputes are oftentimes too busy pursuing their line of investigation to bring a sense of order for the public to the debates, and this is where a fresh eye can be very valuable in identifying what aspects of a debate demand more explanation.  This is part of what is so fascinating about the subject of scientific controversies: With a little bit of effort, <b>any determined layperson can teach themselves these subjects</b>, and with a helpful nudge in the proper direction, they can become an important part of the process of discovery which exists at the very cutting edge of modern science.<br /><br />The scientific community frequently overlooks the public&#39;s contributions to the history of science in the narratives they tell about science.  Part of my objective with this series is to show how members of the public are helping to push our understanding of controversial science forward <b>today.</b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23person\">#person</a>   ﻿","category":"person"},{"name":"Anthony Peratt","summary":"High-Energy Plasma Physicist / Scientific Advisor to the U.S. Department of Energy / IEEE Peer Reviewer / Petroglyph Investigator / Plasma Physics Textbook Author / Simulated Proper Galactic Rotation Curves w/o Dark Matter","image":"https://lh3.googleusercontent.com/-BiY8S-LHKvo/VpvXQGFyCrI/AAAAAAAAGWI/HAE2srGZqkk/w1800-h2520/anthony-peratt-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/fBqAYRqoXHz","publishDate":"2016-01-17T18:45:26.527Z","updateDate":"2017-01-03T03:01:27.419Z","text":"<b>Anthony Peratt: High-Energy Plasma Physicist / Scientific Advisor to the U.S. Department of Energy / IEEE Peer Reviewer / Petroglyph Investigator / Plasma Physics Textbook Author / Simulated Proper Galactic Rotation Curves w/o Dark Matter</b><br /><br />In this historical image, Dr. Peratt is explaining to the Thunderbolts Group how the mythological archetype known as the &quot;chain of arrows&quot; can have a high-energy plasma physics explanation:<br /><br /><i>&quot;Peratt has put a firmer backbone into such early researches using cutting-edge technology and simulation software run on the world’s fastest computer. In 2003, he outlined his finding that an initial set of 7 to 9 plasmoids, loosely aligned, will form a rigid &#39;backbone&#39; - or &#39;ladder&#39; - structure under increasing electrical charge. As the current intensifies, the &#39;branches&#39; will begin to warp and merge, until the pinch collapses.&quot;</i><br /><br />(from <a href=\"https://www.thunderbolts.info/wp/2011/09/19/shots-in-the-dark-part-two/\" class=\"ot-anchor\">https://www.thunderbolts.info/wp/2011/09/19/shots-in-the-dark-part-two/</a>)<br /><br />Peratt is perhaps better known for being <b>the first researcher to simulate on government supercomputers proper galactic rotation curves without the need for any dark matter:</b><br /><br /><i>&quot;... modeled as two adjacent interacting Birkeland filaments. The simulation produces a flat rotation curve, but no hypothetical dark matter is needed, as required by the conventional model of galaxy formation.</i><br /><br /><i>The simulations derive from the work of Winston H. Bostick who obtained similar results from interacting plasmoids.[1] [2]</i><br /><br /><i>In the early 1980s Anthony L. Peratt, a student of Alfvén&#39;s, used supercomputer facilities at Maxwell Laboratories and later at Los Alamos National Laboratory to simulate Alfvén and Fälthammar&#39;s concept of galaxies being formed by primordial clouds of plasma spinning in a magnetic filament.&quot;</i><br /><br />(from <a href=\"http://www.plasma-universe.com/Galaxy_formation\" class=\"ot-anchor\">http://www.plasma-universe.com/Galaxy_formation</a>)<br /><br />The implication which demands explicit statement is that <b>the dark matter problem is an artifact of the initial hypothesis that gravity is the dominant actor at the galactic scale</b>.  It is possible that gravity&#39;s inherent weakness confines its dominance to the scale of existence we see and are therefore most familiar with; as the scale is increased, Peratt&#39;s research suggests that classical electrodynamics forces might instead dominate at the larger scales, due to the collective and self-similar behavior of plasmas, observed to occur over orders of magnitude (features which, it should be apparent, are not shared by the phenomenon of gravity).<br /><br />Galactic expert Tim Thompson has claimed that Peratt&#39;s decision to publish in IEEE was an attempt to avoid scrutiny; that no galactic researcher has ever read IEEE and wouldn&#39;t know that it even exists; and even advised that galactic researchers <b>intentionally avoid</b> reading IEEE.  See snapshot of online forum post here ...<br /><br /><a href=\"https://plus.google.com/108466508041843226480/posts/N6pX2z9v3HT\" class=\"ot-anchor\">https://plus.google.com/108466508041843226480/posts/N6pX2z9v3HT</a><br /><br />Yet, it should be apparent that such claims fail to address any apparent need to question the initial hypothesis and larger scientific framework -- a problem which is today widely known to funding agencies, <b>which were largely created after Thomas Kuhn&#39;s claims became widely known.</b> We can safely assume that ideas related to Hannes Alfven&#39;s work -- of whom Peratt was a student -- will not receive a fair hearing in <i>The Astrophysical Journal</i> since they base upon a competing scientific framework with a very different set of assumptions.  The fact is that both Thompson and Peratt fully understand this, and yet Thompson nevertheless leaves this detail out, as if the public need not take it into account.<br /><br />Dr. Peratt has also published a number of important papers with radio astronomer, Gerrit Verschuur, on the apparent ongoing ionization within the interstellar medium (search on critical ionization velocity) -- a claim which contradicts assumptions still taught to astrophysics students to this day that ionization is not common ...<br /><br /><i>&quot;Among the common arguments against charge separation in space is the calculation that it would take more energy than there is in the universe just to separate all the electrons from the atoms in a single teaspoon of salt.&quot;</i><br /><br />(from <a href=\"https://www.thunderbolts.info/tpod/2004/arch/040803charge-space.htm\" class=\"ot-anchor\">https://www.thunderbolts.info/tpod/2004/arch/040803charge-space.htm</a>)<br /><br />Dr. Peratt&#39;s important textbook, <i>Physics of the Plasma Universe</i> (2014), has recently been revised with a second edition, and includes a number of insights on cosmic plasmas which are not available anywhere else -- e.g., the claim on page 34 that a <i>&quot;relativistic electron beam that does not produce microwave radiation is unknown.&quot;</i><br /><br />(See <a href=\"https://books.google.com/books?id=wt2CBAAAQBAJ&amp;pg=PA34&amp;lpg=PA34&amp;dq=relativistic+electron+beam+that+does+not+produce+microwave+radiation+is+unknown&amp;source=bl&amp;ots=QKc-ThrxmZ&amp;sig=B4wv5jjx-YZmRNLAEdnjxcJ9JkA&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjCjpf9urHKAhVE8WMKHQmnCxwQ6AEIHDAA#v=onepage&amp;q=relativistic%20electron%20beam%20that%20does%20not%20produce%20microwave%20radiation%20is%20unknown&amp;f=false\" class=\"ot-anchor\">https://books.google.com/books?id=wt2CBAAAQBAJ&amp;pg=PA34&amp;lpg=PA34&amp;dq=relativistic+electron+beam+that+does+not+produce+microwave+radiation+is+unknown&amp;source=bl&amp;ots=QKc-ThrxmZ&amp;sig=B4wv5jjx-YZmRNLAEdnjxcJ9JkA&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjCjpf9urHKAhVE8WMKHQmnCxwQ6AEIHDAA#v=onepage&amp;q=relativistic%20electron%20beam%20that%20does%20not%20produce%20microwave%20radiation%20is%20unknown&amp;f=false</a>)<br /><br />(Note as well the fascinating frequency-year diagram on the same page)<br /><br />The significance of the microwave electron beam claim will not be immediately apparent for most.  But, one crucial implication of this observational fact is that there is a potential answer to microwaves coming at us from all directions in the observed emissions of laboratory plasmas.  In other words, <b>it is possible that the cosmic microwave background is not a background at all -- but rather a foreground fog generated as a consequence of interstellar electric currents.</b> Peratt has published a number of papers in pursuit of this line of investigation with Gerrit Verschuur, like this one ...<br /><br />On the Apparent Associations Between Interstellar Neutral Hydrogen Structure and (WMAP) High Energy Frequency Continuum Emission<br /><a href=\"http://arxiv.org/abs/1002.1661\" class=\"ot-anchor\">http://arxiv.org/abs/1002.1661</a><br /><br />... where more than a hundred local hydrogen filament structures are observed to correlate with structures in the WMAP cosmic microwave background (CMB).<br /><br />One of the outstanding questions remaining for plasma cosmologists to explain is how the (spikey) synchrotron microwave emissions we observe in the laboratory associated with electron beams might become &quot;thermalized&quot; (smoothed out) into the bell curve we see for the CMB.<br /><br />More recently, Peratt has published a couple of groundbreaking papers which appear to link high-energy electrical discharges to mythological archetypes and some of the most common abstract petroglyph forms.  This work has yet to be recognized by petroglyph experts, who we can assume still do not understand what plasmas are.  Towards this end, Dr. Peratt has traveled the world cataloging the rock art, and based upon the assumption that the rock artists were drawing what they were seeing in the sky, used government supercomputers to integrate their various perspectives.<br /><br />The paper can be downloaded at <a href=\"http://www.everythingselectric.com/product/anthony-peratt-z-pinch-aurora/\" class=\"ot-anchor\">http://www.everythingselectric.com/product/anthony-peratt-z-pinch-aurora/</a><br /><br />... and he notes in that paper:<br /><br /><b>&quot;Eighty-four distinct high-energy-density Z-pinch categories have been identified in petroglyphs, nearly all of which belong to the archaic [50] class.  Only a small percentage of these petroglyphs, or petroglyph patterns, do not fall into any of these categories.&quot;</b><br /><br /><b>The controversial implication is that the Earth&#39;s sky has significantly changed, and in human-historical times.</b><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23person\">#person</a>    ﻿","category":"person"},{"name":"Gerald Pollack","summary":"Biologist / Author / Academic Reformer / Advisor to the National Science Foundation / Founder of the Institute for Venture Science","image":"https://lh4.googleusercontent.com/-cfcTU0KVf6g/VprxXLX5VdI/AAAAAAAAGUs/-VcSF_Y78-A/gerald-pollack-bbal-card.jpg","url":"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/N8VEXZxERER","publishDate":"2016-01-17T01:46:17.878Z","updateDate":"2017-01-03T03:01:43.048Z","text":"<b>Gerald Pollack: Biologist / Author / Academic Reformer / Advisor to the National Science Foundation / Founder of the Institute for Venture Science</b><br /><br />His Curriculum Vitae (CV), from <a href=\"http://www.theinstituteforventurescience.net/#!leadership/c1ix8\" class=\"ot-anchor\">http://www.theinstituteforventurescience.net/#!leadership/c1ix8</a> ...<br /><br />Gerald Pollack received his PhD in biomedical engineering from the University of Pennsylvania in 1968. He then joined the University of Washington faculty and is now professor of Bioengineering, and also editor-in-chief of the journal WATER.<br /> <br />Pollack’s academic interests have ranged broadly, from biological motion and cell biology to the interaction of biological surfaces with aqueous solutions. His 1990 book, Muscles and Molecules: Uncovering the Principles of Biological Motion, won an “Excellence Award” from the Society for Technical Communication; his two more recent books, Cells, Gels and the Engines of Life (2001) and The Fourth Phase of Water: Beyond Solid, Liquid, and Vapor (2013), both won that Society’s “Distinguished Award.”<br /> <br />Pollack received an honorary doctorate in 2002 from Ural State University in Ekaterinburg, Russia, and was more recently named an Honorary Professor of the Russian Academy of Sciences as well as Academician and foreign member of the Srpska Academy.<br /> <br />He received the Biomedical Engineering Society’s Distinguished Lecturer Award in 2002. In 2008, he was the faculty member chosen by the University of Washington faculty to receive the annual Faculty Lecturer Award the faculty’s highest distinction. Pollack is a Founding Fellow of the American Institute of Medical and Biological Engineering and a Fellow of both the American Heart Association and the Biomedical Engineering Society.<br /><br />He has recently received an NIH Director’s Transformative R01 Award.<br /><br />He was the 2012 recipient of the Prigogine Medal for thermodynamics. Most recently he received the World Academy of Neural Therapy’s “Scientific Excellence Award” and the Dinsdale Prize from the Society for Scientific Exploration.<br /> <br /><b>Where Things Get Interesting ...</b><br /><br /><b>Beginning more than a decade ago, Pollack has involved himself in the process of doing science. He began by organizing letter-writing campaigns to the National Science Foundation (NSF) and the National Institutes of Health (NIH) to open the doors of those institutions to unconventional approaches. Out of these campaigns came the NSF “Frontiers in Biomedical Research” program, and an NIH workshop that eventually led to the NIH Director’s Pioneer Award. A paper describing some of the proposals is available in a paper entitled: Revitalizing Science in a Risk-Averse Culture: Reflections on the Syndrome and Prescriptions for its Cure (Pollack, 2005).</b><br /><br />See <a href=\"http://faculty.washington.edu/ghp/images/stories/pubPDF/2005-Pollack-Revitalizing_Science.pdf\" class=\"ot-anchor\">http://faculty.washington.edu/ghp/images/stories/pubPDF/2005-Pollack-Revitalizing_Science.pdf</a><br /> <br /><b>More recently, Pollack served as an external advisor to the National Science Board (which governs NSF and reports to the US President) in their task force on transformative science. Recommendations of that body led to a heightened awareness of the need for transformative programs at NSF: the term “transformative” now runs deeply through the Foundation. Similarly with the NIH, Pollack was the main academic speaker at 2007 workshop on “Fostering Innovation” which was attended by top NIH officials and a panel of distinguished scientists. Pollack’s 40-minute talk offering various remedial solutions, some radical, can be found at </b><b><a href=\"http://videocast.nih.gov\" class=\"ot-anchor\">videocast.nih.gov</a></b><b> (starting at 1:17). Again, recommendations from this workshop have begun opening the NIH to dealing more seriously with transformative ideas. Pollack also served as member of the US-Israel Binational Science Foundation’s Transformative Science Program, and then as chair.</b><br /> <br />With his unusually diverse scientific background and deep interest in restoring the scientific enterprise to the highly creative and productive endeavor it once was, Pollack’s energies are now focused on bringing the Institute for Venture Science into full realization.<br /><br />A quick explanation of how he wishes to reform peer review:<br /><br /><a href=\"https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/HkUnPLpvq6U\" class=\"ot-anchor\">https://plus.google.com/+ChrisReeveOnlineScientificDiscourseIsBroken/posts/HkUnPLpvq6U</a><br /><br />A more detailed explanation:<br /><br /><a href=\"http://the-institute-for-venture-science.wikia.com/\" class=\"ot-anchor\">http://the-institute-for-venture-science.wikia.com/</a><br /><br />The full story:<br /><br /><a href=\"http://www.theinstituteforventurescience.net/\" class=\"ot-anchor\">http://www.theinstituteforventurescience.net/</a><br /><br />Original, unprocessed photograph from:<br /><br /><a href=\"http://motionographer.com/2015/08/10/on-the-back-of-a-tiger/gerald-pollack-cu-2/\" class=\"ot-anchor\">http://motionographer.com/2015/08/10/on-the-back-of-a-tiger/gerald-pollack-cu-2/</a><br /><br /> <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23person\">#person</a>  ﻿","category":"person"}]